[{"url": "reasecs_shvaiko_som", "desc": "We view Matching as one of the key operations for enabling the Semantic Web since it takes two schemas/ontologies, each consisting of a set of discrete entities (e.g., tables, XML elements, classes, properties, rules, predicates), as input and determines as output the relationships (e.g., equivalence, subsumption) holding between those entities. In this tutorial we introduce, via examples, the schema/ontology matching problem and its application domains. We provide a detailed discussion of the techniques used for schema/ontology matching with the help of a classification of matching approaches. We overview state of the art systems in light of the classification presented, indicating which part of the solution space they cover. Finally, we outline future research directions and new scientific challenges arising in schema/ontology matching.", "recorded": "2005-07-18T00:00:00", "title": "Schema and Ontology Matching"}, {"url": "rease_euzenat_oma1", "desc": "This is a one-hour video recording of the presentation of J\u00e9r\u00f4me Euzenat at the KnowledgeWeb summer school 2007. It comprises only the video not synchronized with the slides \r\n\r\nTable of Contents: \r\n1) The ontology matching problem\r\n2) Classification\r\n3) Basic techniques\r\n4) Matching process\r\n5) Systems\r\n6) Other topics\r\n7) Discussion", "recorded": "2007-11-23T00:00:00", "title": "Ontology Matching and Alignment"}, {"url": "reasecs_smedt_uopas", "desc": "This presentation was given as part of the industrial day at ESWC 2006.\r\n\r\nWolters Kluwer Belgium publishes about specialized areas related to legislation, jurisprudence and doctrine. The paper reports on an effort to transfer knowledge, scattered over a divers set of classification, coding and index generation systems, into a central thesaurus system, modeled and controlled by an ongtoloy.\r\n\r\nDocuments: \r\n;[[Ontology _Legislation_Jurisprudence_Comments.pdf.pdf]]", "recorded": "2006-12-18T00:00:00", "title": "Use of Ontology for production of access systems on Legislation, Jurisprudence and Comments"}, {"url": "rease_perez_oem", "desc": "This is a one-hour video recording of the presentation of Asun Gomez Perez at the KnowledgeWeb summer school 2005. It comprises either the video synchronized with the slides (but requires Quicktime, hence Windows or MacOS, otherwise the slides have to be switched manually).\r\n\r\nThis is a subset of the other Ontology Engineering tutorials available on REASE.\r\n\r\nTable of Contents:\r\nOntological Engineering: Methodologies\r\nDefinitions of Ontology\r\nTypes of Ontologies Lassila and McGuiness Classification\r\nOntology Libraries\r\nKnowledge Representation Ontologies\r\nOne Unique Top-Level Ontology?\r\nDomain Ontologies: e-Commerce Ontologies\r\nApproaches for Modeling Ontologies\r\nUsing UML for Modeling Ontologies\r\nUsing the Entity Relationship Model for Modeling Ontologies\r\nFrames versus Description Logic\r\nUsing Frames and First Order Logic for Modeling Ontologies\r\nUsing Description Logics for Modeling Ontologies\r\nExample of axioms\r\nConclusions on the Different Approaches to Build Ontologies\r\nOntologies are available anywhere in Internet\r\nOntology Development Process\r\nInter-dependencies\r\nOntology Life cycle\r\nMETHONTOLOGY: Specification\r\nGetting terminology using Competency Questions", "recorded": "2006-12-06T00:00:00", "title": "Ontology Engineering Methodologies"}, {"url": "iswc08_liu_sobdpaciwms", "desc": "Metadata management is an important aspect of today\u2019s enterprise information systems. Metadata management systems are growing from toolspecific repositories to enterprise-wide metadata repositories. In this context, one challenge is the management of the evolving metadata whose schema or meta-model itself may evolve, e.g., dynamically-added properties, which are often hard to predict upfront at the initial meta-model design time; another challenge is to organize the metadata by semantically-rich classification schemes. In this paper, we present a practical system which provides support for users to dynamically manage semantically-rich properties and classifications in the IBM WebSphere Metadata Server (MDS) by integrating an OWL ontology repository. To enable the smooth acceptance of Semantic Web technologies for developers of commercial software which must run 24 hours/day, 7 days/week, the system is designed to consist of integrated modeling paradigms, with an integrated query language and runtime repository. Specifically, we propose the modeling of dynamic properties on structured metadata as OWL properties and the modeling of classification schemes as OWL ontologies for metadata classification. We present a natural extension to OQL (Object Query Language)-like query language to embrace dynamic properties and metadata classification. We also observe that hybrid storage, i.e., horizontal tables for structured metadata and vertical triple tables for dynamic properties and classification, is suitable for the storage and query processing of co-existing structured metadata and semantic metadata. We believe that our study and experience are not specific to MDS, but are valuable for the community trying to apply Semantic Web technologies to the structured data management area.\r\n\r\n", "recorded": "2008-10-28T10:30:00", "title": "Supporting Ontology-based Dynamic Property and Classification in WebSphere Metadata Server"}, {"url": "single_fortuna_ontology", "desc": "This thesis addresses the task of formalizing and implementing the process of semi-automatic ontology construction. We propose a theoretical framework for formalizing the ontology construction process. The process is described as a sequence of operators applied to the ontology. Several types of common operators are identified and each type is abstracted so it can be discovered by a combination of machine learning algorithms and user interactions. The proposed ontology learning framework is generic and can handle various domains. The requirement is, that domain data can be provided in a format supported by the learning algorithms.\\\\\r\nOperators defined as part of the ontology construction process are implemented using several machine learning algorithms. Clustering, active learning and large-scale classifications are used to learn operators for adding concepts and relations. A novel visualization approach for visualizing instances, concepts and ontologies is developed, using a combination of dimensionality reduction techniques. The ability to incorporate additional background data is implemented using a novel feature weighting schema, and the addition of new instances to the ontology is translated to a standard classification task.\\\\\r\nWe also developed a system, which implements the framework, together with the proposed machine learning algorithms. The system takes domain data on the input, and guides the user through the process of constructing the ontology for the given domain. The developed system was applied in several use-cases, where domain data was provided as a text corpus or a social network, to showcase the capabilities.\\\\\r\nThe system was also evaluated in two user studies, to evaluate the user interface and to compare developed ontologies against manually constructed ones. The results of the users studies show, that the system is user friendly enough to be used by domain experts. The users can construct ontologies that are comparable to manually constructed ontology and can do so in a shorter amount of time.", "recorded": "2011-10-12T13:00:00", "title": "Semi-Automatic Ontology Construction"}, {"url": "kdd07_dou_donem", "desc": "Event-related potentials (ERP) are brain electrophysiological patterns created by averaging electroencephalographic (EEG) data, time-locking to events of interest (e.g., stimulus or response onset). In this paper, we propose a generic framework for mining and developing domain ontologies and apply it to mine brainwave (ERP) ontologies. The concepts and relationships in ERP ontologies can be mined according to the following steps: pattern decomposition, extraction of summary metrics for concept candidates, hierarchical clustering of patterns for classes and class taxonomies, and clustering-based classification and association rules mining for relationships (axioms) of concepts. We have applied this process to several dense-array (128-channel) ERP datasets. Results suggest good correspondence between mined concepts and rules, on the one hand, and patterns and rules that were independently formulated by domain experts, on the other. Data mining results also suggest ways in which expert-defined rules might be refined to improve ontology representation and classification results. The next goal of our ERP ontology mining framework is to address some long-standing challenges in conducting large-scale comparison and integration of results across ERP paradigms and laboratories. In a more general context, this work illustrates the promise of an interdisciplinary research program, which combines data mining, neuroinformatics and ontology engineering to address real-world problems.", "recorded": "2007-09-14T15:44:21", "title": "Development of NeuroElectroMagnetic Ontologies (NEMO): A Framework for Mining Brain Wave Ontologies "}, {"url": "iswc08_bonino_domide", "desc": "Home automation has recently gained a new momentum thanks to the ever-increasing commercial availability of domotic components. In this context, researchers are working to provide interoperation mechanisms and to add intelligence on top of them. For supporting intelligent behaviors, house modeling is an essential requirement to understand current and future house states and to possibly drive more complex actions. In this paper we propose a new house modeling ontology designed to fit real world domotic system capabilities and to support interoperation between currently available and future solutions. Taking advantage of technologies developed in the context of the Semantic Web, the DogOnt ontology supports device/network independent description of houses, including both \u201ccontrollable\u201d and architectural elements. States and functionalities are automatically associated to the modeled elements through proper inheritance mechanisms and by means of properly defined SWRL auto-completion rules which ease the modeling process, while automatic device recognition is achieved through classification reasoning.", "recorded": "2008-10-28T16:00:00", "title": "DogOnt \u2013 Ontology Modeling for Intelligent Domotic Environments"}, {"url": "iswc2011_kazakov_classification", "desc": "We describe an optimised consequence-based procedure for\r\nclassification of ontologies expressed in a polynomial fragment ELHR+ of\r\nthe OWL 2 EL profile. A distinguishing property of our procedure is that\r\nit can take advantage of multiple processors/cores, which increasingly\r\nprevail in computer systems. Our solution is based on a variant of the\r\n\u2018given clause\u2019 saturation algorithm for first-order theorem proving, where\r\nwe assign derived axioms to \u2018contexts\u2019 within which they can be used and\r\nwhich can be processed independently.We describe an implementation of\r\nour procedure within the Java-based reasoner ELK. Our implementation\r\nis light-weight in the sense that an overhead of managing concurrent\r\ncomputations is minimal. This is achieved by employing lock-free data\r\nstructures and operations such as \u2018compare-and-swap\u2019. We report on\r\npreliminary experimental results demonstrating a substantial speedup\r\nof ontology classification on multi-core systems. In particular, one of\r\nthe largest and widely-used medical ontologies SNOMED CT can be\r\nclassified in as little as 5 seconds.", "recorded": "2011-10-25T11:00:00", "title": "Concurrent classification of EL ontologies"}, {"url": "rease_zaihrayeu_oma", "desc": "This is a one-hour video recording of the presentation of Ilya Zaihrayeu at the First Asian Autumn School on the Semantic Web. It comprises the video synchronized with the slides (requires Flash) or the video alone.\r\n\r\nTable of Contents: \r\nOntology Matching and Alignment\r\nMatching operation\r\nExample: two XML schemas\r\nExample: two ontologies\r\nStatement of the problem\r\nApplications\r\nApplications: Information integration\r\nApplications: summary\r\nClassification of basic techniques\r\nClassification of techniques (simplified)\r\nBasic techniques\r\nSystems: analytical comparison\r\nSemantic matching in a nutshell\r\nConcept of a label & concept at a node\r\nFour macro steps\r\nStep 1: compute concepts at labels\r\nStep 2: compute concepts at nodes\r\nStep 3: compute relations between (atomic) concepts at labels\r\nStep 3: Element level semantic matchers\r\nStep 4: compute relations between concepts at nodes\r\nStep 4: Example of a node matching task\r\nMotivation: Problem of low recall (incompletness)\r\nOn increasing the recall: an overview\r\nIterative semantic matching (ISM)\r\nISM: Discovering critical points - example\r\nISM: Generating candidate axioms\r\nISM: generating candidate axioms Hierarchy Distance\r\nTest cases\r\nMatching systems\r\nExperimental results, test case #4\r\nExperimental results, test case #5\r\nExperimental results, #6,7,8: incompleteness\r\nExperimental results, #6,7,8: incompleteness (OAEI-2006 comparison)\r\nOutline\r\nSummary\r\nFuture challenges\r\n(Some) references\r\nYou are welcome to attend (11 Nov):\r\n'Thank you'", "recorded": "2007-11-26T00:00:00", "title": "Ontology Matching and Alignment"}, {"url": "iswc2011_fernandez_videolectures", "desc": "This paper presents our work and experience interlinking educational information across universities through the use of Linked Data principles and technologies. More specifically this paper is focused on selecting, extracting, structuring and interlinking information of video lectures produced by 27 different educational institutions. For this purpose, selected information from several websites and YouTube channels have been scraped and structured according to well-known vocabularies, like FOAF, or the W3C Ontology for Media Resources. To integrate this information, the extracted videos have been categorized under a common classification space, the taxonomy defined by the Open Directory Project. An evaluation of this categorization process has been conducted obtaining a 98% degree of coverage and 89% degree of correctness. As a result of this process a new Linked Data dataset has been released containing more than 14,000 video lectures from 27 different institutions and categorized under a common classification scheme.", "recorded": "2011-10-25T15:30:00", "title": "Linking Data Across Universities: an integrated video lectures dataset"}, {"url": "iswc07_wang_por", "desc": "Extracting semantic relations is of great importance for the creation of the Semantic Web content. It is of great benefit to semi-automatically extract relations from the free text of Wikipedia using the structured content readily available in it. Pattern matching methods that employ information redundancy cannot work well since there is not much redundancy information in Wikipedia, compared to the Web. Multi-class classification methods are not reasonable since no classification of relation types is available in Wikipedia. In this paper, we propose PORE (Positive-Only Relation Extraction), for relation extraction from Wikipedia text. The core algorithm B-POL extends a state-of-the-art positive-only learning algorithm using bootstrapping, strong negative identification, and transductive inference to work with fewer positive training examples. We conducted experiments on several relations with different amount of training data. The experimental results show that B-POL can work effectively given only a small amount of positive training examples and it significantly outperforms the original positive learning approaches and a multi-class SVM. Furthermore, although PORE is applied in the context of Wikipedia, the core algorithm B-POL is a general approach for Ontology Population and can be adapted to other domains.", "recorded": "2007-11-13T14:00:00", "title": "PORE: Positive-Only Relation Extraction from Wikipedia Text"}, {"url": "iswc08_wang_lcm", "desc": "Finding mappings between compatible ontologies is an important but difficult open problem. Instance-based methods for solving this problem have the advantage of focusing on the most active parts of the ontologies and reflect concept semantics as they are actually being used. However such methods have not at present been widely investigated in ontology mapping, compared to linguistic and structural techniques. Furthermore, previous instance-based mapping techniques were only applicable to cases where a substantial set of instances was available that was doubly annotated with both vocabularies. In this paper we approach the mapping problem as a classification problem based on the similarity between instances of concepts. This has the advantage that no doubly annotated instances are required, so that the method can be applied to any two corpora annotated with their own vocabularies. We evaluate the resulting classifiers on two real-world use cases, one with homogeneous and one with heterogeneous instances. The results illustrate the efficiency and generality of this method. ", "recorded": "2008-10-29T10:30:00", "title": "Learning Concept Mappings from Instance Similarity"}, {"url": "solomon_kocev_epso", "desc": "In many real-world domains, such as bioinformatics (functional\r\ngenomics), text classification and image annotation, the goal is to\r\npredict a complex output. For example, in functional genomics, the\r\ngoal is to predict the function of a gene, while the set of functions\r\ncan be organized as tree (FunCat) or graph (GO ontology).\r\nIn this talk, we present an approach for predicting structured outputs\r\nusing ensembles of trees. The proposed approach is scalable to large\r\ndatasets, different types of outputs and it is applicable to wide\r\nrange of domains. First, we describe the types of structured outputs\r\nthat we typically encounter, and then we explain the base classifiers\r\n- predictive clustering trees (PCTs). Next, we discuss the ensemble\r\nmethods that we extended (bagging and random forests) to deal with\r\nstructured outputs and accordingly adapted the voting schemes.\r\nAfterwards, we present experimental evaluation of the proposed\r\napproach on wide range of real-world domains. At the end, we present\r\nan application of the proposed approach in functional genomics and\r\nshow that our approach is competitive with state-of-the-art approaches.", "recorded": "2010-02-02T13:00:00", "title": "Ensembles for predicting structured outputs"}, {"url": "kdd2014_yang_twitter", "desc": "We are interested in organizing a continuous stream of sparse and noisy texts, known as \"tweets\", in real time into an ontology of hundreds of topics with measurable and stringently high precision. This inference is performed over a full-scale stream of Twitter data, whose statistical distribution evolves rapidly over time. The implementation in an industrial setting with the potential of affecting and being visible to real users made it necessary to overcome a host of practical challenges. We present a spectrum of topic modeling techniques that contribute to a deployed system. These include non-topical tweet detection, automatic labeled data acquisition, evaluation with human computation, diagnostic and corrective learning and, most importantly, high-precision topic inference. The latter represents a novel two-stage training algorithm for tweet text classification and a close-loop inference mechanism for combining texts with additional sources of information. The resulting system achieves 93% precision at substantial overall coverage.", "recorded": "2014-08-25T13:45:00", "title": "Large Scale High-Precision Topic Modeling on Twitter"}, {"url": "interoperability_course_syllabus", "desc": "Links pointing to [[http://elearning.interop-vlab.eu/|INTEROP-VLab web courses and tutorials]]\n\n==[[http://elearning.interop-vlab.eu/course/view.php?id=9|ENTERPRISE MODELLING ]]\nIntroduction to Enterprise Modelling\n*Introduction\n*Modelling of enterprises\n*Development of Information Systems\n*ARIS \u2013 Phases and Views\n\nIn this tutorial, we give an introduction to Enterprise Modelling. Enterprise modeling expresses the symbolic illustration of information, which is supported by editing and documenting real-world structures of an enterprise and their meaning, so that they are comprehensible to the user. After describing the overall goals and essential terms for the modeling of enterprises, principles, methods and benefits of enterprise modeling are explained. In the next section the Development of (Business) Information Systems, one of the main goals of enterprise modeling, is explained and related to enterprise modeling. The last part introduces an example of an enterprise modeling methodology: The Architecture of Integrated Information Systems (ARIS), its phases and its views are described.\n ||[[left:iesa08_wings_csit/]]||\n\n----\n ==Unified Enterprise Modelling Language (UEML)\n*Preliminary information\n*What is UEML?\n*Why develop UEML?\n*UEML History\n*The UEML Project\n*The UEML Language\no State of the art. The need to relate Enterprise Modelling Languages\no Requirements of the Language\no Definition of the language\no How Enterprise Modelling with UEML can provide added value\no Example on Models interoperability\n*Conclusions. The future of UEML\n*Other information sources\n*References\n\nThis tutorial is an introduction of the Unified Enterprise Modelling Language, also called, UEML. First, we will define the language and will explain why UEML is crucial. Then, we will show how this new language was conceived and the basic principles of the UEML Project. Thirdly, we will explain the process of defining the language and will give some examples of its application. Finally, the conclusions will explain the future of this new language.\n----\n ==Business Process Modelling Introduction\n*Why is business process (BP) modelling needed?\n*What is a BP?\n*BP classification\n*Techniques and tools to model a BP\n*When to use what?\n*Main BP techniques\n*Examples of BP models\n\nA Business Process is the combination of a set of activities within an enterprise with a structure describing their logical order and dependence whose objective is to produce a desired result. Business Process modelling enables a common understanding and analysis of a business process. A process model can provide a comprehensive understanding of a process. An enterprise can be analysed and integrated through its business processes. Hence, the importance of correctly modelling its business processes.\nUsing the right model involves taking into account the purpose of the analysis and, knowledge of the available process modelling techniques and tools. The number of references on business modelling is huge, thus making it very time consuming to get an overview and understand many of the concepts and vocabulary involved. The primary concern of this tutorial is to make that job easier, i.e. review business process modelling literature and describe the main process modelling techniques. In addition, a framework for classifying business process-modelling techniques according to their purpose is presented.\n ||[[left:eswc08_feldcamp_sb/]]||\n\n----\n ==CIMOSA Computer Integrated Manufacturing Open System Architecture\n*CIMOSA Association\n*Enterprise Modelling - Reasons and Benefits\n*CIMOSA Modelling Framework, Modelling Language\n*CIMOSA Enterprise Modelling - Business Process Modelling\n*Standardisation in Enterprise Modelling\n*Conclusion\n\nThe tutorial presents a structured report about Computer Integrated Manufacturing Open System Architecture. First, it is explained what enterprise modelling is, the goals, reasons and the benefits achieved with enterprise modelling. The tutorial tries to answer the reasons of enterprise engineering and integration. CIMOSA Modelling Framework, Modelling Language is exposed, by means these sections: GERAM Framework, CIMOSA Modelling Framework and CIMOSA Modelling Language.\nThe enterprise modelling based on CIMOSA, focuses on function view, information view, resource view and organisation view.\nThe tutorial explains two Case studies: Business Re-Engineering at FIAT and Paper manufacturer KOEHLER. It makes a relevant standards overview, including ISO/CEN (15704, 19439, 19440, \u2026); ISO/IEC (15414, 62264) and OMG (MDA, BPML, UML, \u2026).\n----\n ==The GRAI Method - Global Modelling\n\n*Introduction\n*The GRAI Model\no Introduction\no Decision in the GRAI Model\no Functional Decomposition\no Systemic Decomposition\no Hierarchy\no The three Modelling Domains\n* The GRAI Grid\no Concepts of the GRAI Grid\no Grid Functions\no Links between Functional and Control Grids\no Multi-Grids Modelling (Co-ordination Grid)\no Possible Extensions of the Grid\n\nThis tutorial generally presents the GRAI method. From an operational point of view, are presented here the GRAI model as a consistent set of concepts in order to model production systems and the GRAI grid that uses the concepts of the GRAI model to propose a global model of the decisional system.\n----\n ==The GRAI Method - Detailed Modelling and Methodological Issues\n*Introduction\n*The GRAI nets\n*The structured approach\n*The rules of inconsistencies\n*The GRAI methodology\n\nThis tutorial follows the previous one. This tutorial presents the GRAI nets that aims at a detail model of the decisional system and the structured approach that organises (steps, actors, etc.) the study.\n----\n==General Standards Life Cycle\n*Introduction\n*Interoperability and Standardisation\n*Enterprise Integration and Engineering Standards\n*Enterprise Interoperability Standards\n*B2B Standards\n*INTEROP \u2013 Interoperability Standards\n*Conclusions\n*Credits\n\nThe tutorial presents interoperability in terms of it driving the need for standardization. It covers aspects such as the concepts of interoperability and standardization, enterprise integration and engineering standards, enterprise interoperability standards, B2B standards, and interoperability standards in INTEROP. It provides a useful insight into the complexity and importance of the standardization area in interoperability, important areas where harmonization is needed, as well as how standards affect organizations and their way of working.\n----\n ==Business Process Modelling Language (BPML)\n* BPML History\n* BPML Components\no Activities\no Process\no Contexts\no Properties\no Signals\n* Activities\n* Communication Patterns\no Synchronous Communication - Examples\no Asynchronous Communication\n* The Web Service Choreography Interface (WSCI)\n* Comparison\n* Business Process Management Notation\no What is BPMN?\no Why develop BPMN?\no BPMN Elements\n* Flow Objects\n* Connecting Objects\n* Swimlanes\n* Artifacts\n* Example - BPMN\n* Conclusions\n* References\n* Credit\n\nThis tutorial is a summary about Business Process Modelling Language, called BPML. There is a detailed explanation of what is BPML, its origins and its main components, focussing on activities. A comparison between others modelling languages is shown. Finally, it is depicted Business Process Modelling Notation as a graphical language that can be mapped onto languages such as BPML.\n----\n ==Use of the Event-driven Process Chain (EPC) to Model Business Processes\n* Introduction\no Introducing business processes\no Business process modeling\n* The event-driven process chain as a modeling method\no Development and intention of the EPC\no Advantages of the EPC\no Basic elements of the EPC\no Modeling principles of the EPC\no Enhanced event-driven process chain (eEPC)\no General modeling rules for EPCs\n* Practical examples\n* Guidelines for modeling EPCs\n* Conslusions and outlook\n\nThis tutorial describes the event-driven process chain (EPC) as a method to model business processes, which has found a high degree of acceptance and dissemination in practice because of its practical focus and intuitive comprehensibility. After introducing business processes, the tutorial continues with sections about business process modelling in general and various methods for business process modelling. The second section focuses on the method of the EPC, giving a short description of its development and intention, naming advantages of the EPC, introducing and explaining its basic elements, modelling principles, the enhancement of the EPC by elements of other views of the ARIS concept as well as general modelling rules for EPCs. As next, some examples are given to practically illustrate the method of the EPC and eEPC, followed by general guidelines for modelling EPCs.\n----\n----\n ==[[http://elearning.interop-vlab.eu/course/view.php?id=23|ONTOLOGIES]]\nIntroduction to Ontologies\n* What is the Semantic Web\n* Semantic Web and Ontologies\n* Knowledge and Ontologies dimensions\n* Ontology modelling\n* Conclusions\n\nThis tutorial explains that the Semantic Web is an extension of the current web in which information is given well-defined meaning, better enabling computers and people to work in cooperation. This concept is related to ontologies and ontology building is also characterized by some dimensions. For a deeper focus on ontology modelling, the tutorial explains onto (meta) model, onto modelling formalism, ontology content and onto system. Finally, conclusions summarize the main ideas.\n----\n ==Methodologies to build Ontologies\n* Ontology Building: Basic Concepts\n* Ontological Building Methodologies\n* Ontological Building Methodologies: UPON\n* Ontological Building: Tools\n* Conclusions\n* References\n\nThis tutorial starts with some basic elements of the Ontology Building (concepts, relationships and facets). In Ontological Building Methodologies, the tutorial shows the most known approaches to build an ontology. Then, UPON is described as a methodology for ontology building based on the Unified Software Development Process and supported by UML. Finally, there is a classification of different tools according to some criteria.\n----\n ==Ontology Languages\n* What is an Ontology?\n* What can we do with an ontology?\n* Ontology language\n\nThe tutorial presents an introduction to ontology representation formalisms, treating general concepts and then analyzing the main features of many existing ontology languages.\nAfter a general introduction to ontologies, that includes an analysis of what an ontology is, what are its intended uses and a how the degree of formality used in representation affects the way an ontology can be used, the tutorial gives a wide-range survey of the languages. The analysis is not limited to the most widely known languages proposed for semantic web applications, but covers various knowledge representation languages, programming languages, essentially graphical formalisms like Semantic Networks and UML, and languages initially designed as knowledge interchange formats. Languages are classified in families according to various dimensions, including their degree of formality, and their main features are reviewed.\nThe tutorial is intended for an advanced audience, that already has familiarity with the concept of ontology and basic notions of logics, and it is suitable for use in PhD courses.\n----\n ==Uses of Ontologies\n* What is the Semantic Web\n* Semantic Web and Ontologies\n* Knowledge and Ontologies dimensions\n* Focus on Semantic Annotation\n* Focus on Semantic Mismatches\n\nThe Semantic Web requires that web information is \u201cmachine understandable\u201d, so this tutorial explains the semantic annotation to achieve this goal. Semantic annotation expresses in a formal way the meaning associated to a web resource or to a part of it. Then there is an identification of semantic mismatches and an analysis of the different kinds of mismatches.\n ||[[left:eswc08_pedrinaci_co/]]||\n----\n ==What is the Semantic Web?\nThe general vision\n* What is the Semantic Web?\n* What the Semantic Web is not\n* What can be achieved by the Semantic Web?\n* Interoperability and the Semantic Web\n* Why do we need the Semantic Web?\n\nThis tutorial explains the Semantic Web, a Web where computers will \u201cunderstand\u201d the meaning of semantic data on a web page by following hyperlinks to definitions of key terms and rules for reasoning about them logically. Currently, the Web uses the computer as a device for rendering information for the human reader but neither for information processing nor computing. The tutorial shows how the Semantic Web is aiming on bringing back the computer as an information processing device\n ||[[left:iswc06_gruber_wswms/]]||\n----\n ==Semantic Web Technologies\n* What is the Semantic Web?\n* What is the Semantic Web good for?\n* What lies beneath the Semantic Web?\n* Unicode\n* URI: Uniform Resource Identifier\n* XML: eXtensible Markup Language\n* XML Schemas\n* Is XML enough for the Semantic Web\n* RDF: Resource Description Framework\n* RDF Schema\n* Ontologies\n* Logic and proofs\n* Logical languages for the Semantic Web\n* Trust and credibility\n* Conclusions\n\nThis tutorial is a brief introduction to the technologies beneath the Semantic Web: Unicode,URI (Uniform Resource Identifiers), XML (eXtensible Markup Language), XML Schemas, RDF (Resource Description Framework), RDF Schema, Ontologies and Logic. All these technologies will work together to achieve the Semantic Web goals.\n ||[[left:eswc08_lochman_osm//]]||\n----\n ==Ontology Tools\n* What is an ontology?\n* What is an ontology in Computer Science?\n* Key use of ontologies\n* Ontology based reasoning\n* Building ontologies\n* About ontology development\n* Ontology engineering vs. Object-Oriented modelling\n* Types of ontologies tools\n* An ontology tools survey on the Web\n* Tools for ontology engineering\n* Tools analysed here\n* Prot\u00e9g\u00e9\n* OilEd\n* Conclusions\n\nThis tutorial is an overview of the current ontology tools. After given a general explanation about what an ontology is and about its applications, the tutorial describes different kinds of ontology tools (editors & browswers, translators, merge and integration tools, etc.). Finally, it focuses on two tools: Prot\u00e9g\u00e9 and OilEd, giving many examples of how to use these tools.\n----\n----\n ==[[http://elearning.interop-vlab.eu/course/view.php?id=18|ARCHITECTURES & PLATFORMS]]\n ==Enterprise Architectures and Enterprise Modelling\nIntroduction\n* Basic terms\n* Examples of enterprise architectures\n* Introduction to ARIS\n* ARIS views on enterprises\n* Function view\n* Organization view\n* Data view\n* Output view\n* Control view\n* ARIS Phase Model\n* ARIS House of Business Engineering\n\nThe tutorial presents different enterprise architectures and describes methods and concepts to model enterprise. Examples of various ways to structure and model enterprises are given. It is shown how an enterprise can be divided into various layers to reduce model complexity. Additionally, a framework to transform the abstract models to IT-models is introduced. The Architecture of Integrated Information Systems (ARIS) including the ARIS views as a concept to model different aspects of an enterprise, the ARIS phase model as a concept to create the relation between industrial situation and IT as well as the ARIS house of business engineering (HOBE) as a framework for managing business processes will be explained in detail in this tutorial.\n----\n ==Introduction to Data Quality in Cooperative Information Systems\n* Introduction\n* Dimensions\n* References\n\nThis tutorial introduces to the problem of data quality in multi-organizational environments. The overlapping of data sources in such environments can be an opportunity to improve data quality, but it is also an issue if conflicting copies of the same data are stored.\n----\n ==Data Quality Models in Cooperative Information Systems\n* Models\no Use of models\no Extension of DB models\no Models for management information systems\n- Process models\n- Data models\no Cost models\n* References\n\nThe tutorial describes exitisting data-oriented models for data quality, like extensions of conceptual and logical models for data representation to include also quality. It also describes process-oriented models, useful for data quality improvement.\n----\n ==Methodologies for data quality in CISs and an Example of a Framework\n* Methodologies\no Types of Strategies\no Types of Methodologies\no General Overview of 4 Methodologies\no Relevant Steps in Methodologies\no Comparison of Methodologies\n* Frameworks and Services for CISs\no Data Quality Broker\no Rating Service\no Quality Notification Service\no Data Quality Factory\n* References\n\nThe tutorial it describes several existing methodologies for assessing and improving quality of data. The methodologies are described in terms of the steps composing them and a comparison framework is also illustrated.\n----\n ==The COMET Methodology - Business and Requirements Modelling\n* Part 1a: Methodology Overview\no Motivation\n- Why system development methodology?\no Software system development methodologies\no COMET\n- A medium-sized methodology for developing Web services in a Service-Oriented Architecture (SOA)\no References\n* Part 1b: Business Modelling\n* Part 1c: Requirements Modelling\n\nThis tutorial presents the COMET Web services modelling which provides guidelines for the design and implementation of Web services based on the COMET business, requirements and architecture models. The tutorial covers the following Web services technologies: XML (eXtensible Markup Language), XSD (XML Schema Definition), WSDL (Web Services Description Language), SOAP (Simple Object Access Protocol), UDDI (Universal Description, Discovery and Invocation) and BPEL (Business Process Execution Language).\n----\n----\n ==[[http://elearning.interop-vlab.eu/course/view.php?id=26|INTEROPERABILITY ]]\nEnterprise Modelling for Interoperability\n* Introduction: Concepts\n* Enterprise modelling supports interoperability\no Enterprise Architectures Aligning Processes with IT\no Business Process Models Integration\n- Unified Approach\n- Interoperability Aspects\n- UEML\n\nIn this tutorial, we are going to define a series of concepts, which will be followed by the explanation of the enterprise architectures aligning processes with IT and the business process models integration.\n----\n ==Ontology for Interoperability\n* Introduction to interoperability\n* Ontology-based solution\no Semantic mismatch analysis\no Semantic annotation\no Reconciliation\n* Three levels of interoperability\no Information interoperability\no Process Interoperability\no Service Interoperability\n* Ontology based architectures for interoperability\n\nIn this tutorial, we will give an overview of the way ontologies can be used to support the interoperability of enterprise software applications. We will therefore expose what the interoperability problem is.\n----\n ==Introduction to Architecture and Platforms for Enterprise system Interoperability\n* Introduction to Software Architecture\n* Architectural styles for interoperability\n* Existing platforms Special Issues:\n* Process Brokers & Integration\n* Design of Processes for Integration\n* Service Oriented Architecture\n\nIn this tutorial, we will introduce you to some of the underlying technical architecture in interoperable systems. First, we will define software architecture. Then, we will see different styles of interoperability architecture. These styles will be Process brokers and Service Oriented Architecture. You will also see some examples of existing platforms.\n----\n ==Ontology Interoperability\n* Ontology Interoperability Pitfalls\n* Solutions for Interoperability among ontologies\no Ontology merging\no Ontology alignment\no Ontology mapping\no Ontology transformation\n* Some solutions in the State of the Art\no Methods for ontology mapping/merging\no A brief overview of some existing systems\n- Mapping Frameworks/tools\n- Similarity reasoning approaches/ solutions\no Summary table of the collected material\n* Conclusions\n\nThe objective of this tutorial is to give an overview of the problem of Ontology Interoperability. We will therefore see what the common problems encountered are when comparing two or more ontologies describing the same domain, ontology pitfalls, then what the basic operations used to solve the differences among such ontologies are and will give some details about a number of State of the Art solutions, methods and tools.\n----\n ==Model Driven Architecture: General Overview\n* Problem\n* MDA Framework\n* MDA Development Life Cycle\n* MDA Benefits\n* Inside the MDA Framework\n* Model Transformation\n\nThis tutorial gives an overview on the Model Driven Architecture (MDA) proposed by the Object Management Group (OMG). It presents the general framework of MDA in terms of development life cycle and abstraction levels.After presenting the benefits of this approach a focus is performed on model transformations.", "recorded": "2009-02-13T10:11:05", "title": "Course Syllabus - Interoperability"}, {"url": "mlsb2012_stojanova_ppi", "desc": "**Motivation:** Catalogs, such as Gene Ontology (GO) and MIPS-FUN, assume that functional classes\r\nare organized hierarchically (general functions include more specific functions). This has recently\r\nmotivated the development of several machine learning algorithms under the assumption that instances\r\nmay belong to multiple hierarchy organized classes. Besides relationships among classes,\r\nit is also possible to identify relationships among examples. Although such relationships have been\r\nidentified and extensively studied in the in the area of protein-to-protein interaction (PPI)\r\nnetworks, they have not received much attention in hierarchical protein function prediction. The\r\nuse of such relationships between genes introduces autocorrelation and violates the assumption\r\nthat instances are independently and identically distributed, which underlines most machine\r\nlearning algorithms. While this consideration introduces additional complexity to the learning\r\nprocess, we expect it would also carry substantial benefits.\\\\\r\n**Results:** This article demonstrates the benefits (in terms of predictive accuracy) of considering autocorrelation\r\nin multi-class gene function prediction. We develop a tree-based algorithm for considering\r\nnetwork autocorrelation in the setting of Hierarchical Multi-label Classification (HMC). The\r\nempirical evaluation of the proposed algorithm, called NHMC, on 24 yeast datasets using MIPSFUN\r\nand GO annotations and exploiting three different PPI networks, clearly shows that taking\r\nautocorrelation into account improves performance.\\\\\r\n**Conclusions:** Our results suggest that explicitly taking network autocorrelation into account increases\r\nthe predictive capability of the models, especially when the underlying PPI network is\r\ndense. Furthermore, NHMC can be used as a tool to assess network data and the information it\r\nprovides with respect to the gene function.", "recorded": "2012-09-08T16:30:00", "title": "Using PPI Networks in hierarchical multi-label classification trees for gene function prediction"}, {"url": "solomon_rousu_sopef", "desc": "Enzyme function prediction is an important problem in post-genomic\r\nbioinformatics. There are two general methods for solving the problem:\r\ntransfer of annotation from a similar, already annotated protein, and\r\nmachine learning approaches that treat the problem as classification\r\nagainst a fixed taxonomy, such as Gene Ontology or the EC hierarchy.\r\nThese methods are suitable in cases where the function has been\r\npreviously characterized and included in the taxonomy. However, given a\r\nnew function that is not previously described, existing approaches\r\narguably do not offer adequate support for the human expert.\r\n\r\nIn this presentation, we I will present a structured output learning\r\napproach, where the enzyme function, an enzymatic reaction, is described\r\nin fine-grained fashion with so called reaction kernels which allow\r\ninterpolation and extrapolation in the output (reaction) space. A\r\nstructured output model is learned to predict enzymatic reactions from\r\nsequence motifs. We bring forward several choices for constructing\r\nreaction kernels and experiment with them in the remote homology case\r\nwhere the functions in the test set have not been seen in the training\r\nphase. Our experiments demonstrate the viability of our approach.", "recorded": "2009-09-04T10:00:00", "title": "Structured Output Prediction of Enzyme Function via Reaction Kernels"}, {"url": "eswc2010_bechhofer_sppf", "desc": "SKOS (Simple Knowledge Organisation System) is a common data model for sharing and linking knowledge organization systems via the Web. Many knowledge organization systems, such as thesauri, taxonomies, classification schemes and subject heading systems, share a similar structure, and are used in similar applications. SKOS captures much of this similarity and makes it explicit, enabling data and technology sharing across diverse applications.\r\nThe SKOS data model provides a standard, low-cost migration path for porting existing knowledge organization systems to the Semantic Web. SKOS also provides a light weight, intuitive language for developing and sharing new knowledge organization systems. It may be used on its own, or in combination with formal knowledge representation languages such as the Web Ontology language (OWL). SKOS was published as a W3C Recommendation in August 2009 and is seeing growing take-up in a number of fields including (among others) cultural heritage, economics, astronomy, and local government. SKOS also looks set to play a key role in providing vocabularies for the Data Web through its use in Open Linked Data.", "recorded": "2010-06-03T09:04:00", "title": "SKOS: Past, Present, Future - and a little bit of history, architecture and engineering "}, {"url": "iswc08_klinov_oerpdl", "desc": "This paper describes the first steps towards developing a methodology for testing and evaluating the performance of reasoners for the probabilistic description logic P-  ${\\ensuremath{\\mathcal{SHIQ}}(D)}$ . Since it is a new formalism for handling uncertainty in DL ontologies, no such methodology has been proposed. There are no sufficiently large probabilistic ontologies to be used as test suites. In addition, since the reasoning services in P-  ${\\ensuremath{\\mathcal{SHIQ}}(D)}$ are mostly query oriented, there is no single problem (like classification or realization in classical DL) that could be an obvious candidate for benchmarking. All these issues make it hard to evaluate the performance of reasoners, reveal the complexity bottlenecks and assess the value of optimization strategies. This paper addresses these important problems by making the following contributions: First, it describes a probabilistic ontology that has been developed for the real-life domain of breast cancer which poses significant challenges for the state-of-art P-  ${\\ensuremath{\\mathcal{SHIQ}}(D)}$ reasoners. Second, it explains a systematic approach to generating a series of probabilistic reasoning problems that enable evaluation of the reasoning performance and shed light on what makes reasoning in P-  ${\\ensuremath{\\mathcal{SHIQ}}(D)}$ hard in practice. Finally, the paper presents an optimized algorithm for the non-monotonic entailment. Its positive impact on performance is demonstrated using our evaluation methodology.", "recorded": "2008-10-28T17:00:00", "title": "Optimization and Evaluation of Reasoning in Probabilistic Description Logic: Towards a Systematic Approach"}, {"url": "solomon_skunca_pcifa", "desc": "Phylogenetic profiling is a genomic context method that predicts gene function by correlating gene occurrence patterns in selected organisms [1]. The intuition behind phylogenetic profiling is that genes found and lost together (i. e. inherited together) in different genomes are likely to share function, either by 1) being involved in the same biological pathway (which is therefore incomplete without all members in a given genome), or 2) being crucial for survival in a particular environment, so their presence is mandatory throughout the phenotype. \r\nWe have used a recently developed machine learning approach based on decision trees for Hierarchical Multi \u2013 label Classification (HMC) [2] to predict Gene Ontology (GO) assignments of Orthologous Matrix (OMA) groups [3]. The HMC extension of the decision tree classifier takes into account the hierarchical layout of GO and considerably improves computational efficiency and accuracy by taking into account a set of class labels simultaneously when constructing the decision trees, instead of learning each class label separately. A standard decision tree would recursively split the training data into subsets (\u2018branches\u2019) on values of an attribute in such a manner as to decrease a measure of entropy of a class label within the subsets after the split. The HMC approach has to deal with multiple class labels, and would compute a weighted average of decrease in entropy over all labels when deciding on a split point. The weights here are inversely proportional to the depth of a class in the GO, giving more significance to high-level, more general GO terms.\r\nWe have inspected the effects of stepwise addition of putative paralogs on computational learning of orthologous groups\u2019 (and consequentially gene) function. By introducing paralogous genes in the learning process, we substantially increase its success and show that gene function prediction from sequence information alone, when encoded as a paralog-containing phylogenetic profile, is a promising approach in narrowing of possible function space for a particular protein.", "recorded": "2010-02-15T13:00:00", "title": "Paralogs considerably improve accuracy of phylogenetic profiling for in silico functional annotation"}]