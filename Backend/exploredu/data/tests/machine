[{"url": "bmvc09_london", "desc": "The British Machine Vision Conference is the main UK conference on machine vision and the related areas. Organized by the British Machine Vision Association, the 20'th BMVC was held in London and jointly run by Queen Mary and UCL.\n\n----\nThe conference homepage can be found at [[http://www.bmva.org/bmvc/2009/|British Machine Vision Conference 2009]]\n----", "recorded": "2009-09-07T14:30:00", "title": "British Machine Vision Conference (BMVC), London 2009"}, {"url": "mit21l448jf2010_paradis_lec18", "desc": "Topics: Alan Turing and the Thinking Machine\r\n\r\n    Alan Turing\r\n    Thinking Machine\r\n    von Neumann\r\n    Stored-Program Computer\r\n    ENIAC\r\n    The Imitation Game\r\n    Coding\r\n    Discrete State Machines\r\n    Digital Computers\r\n    Machine Learning\r\n    Joseph Weizenbaum\r\n    Chatterbots\r\n", "recorded": "2010-12-25T09:00:00", "title": " Lecture 18: Alan Turing and the Thinking Machine "}, {"url": "mit21l448jf2010_paradis_lec22", "desc": "Topics: H. G. Wells The Time Machine and the final utopia\r\n\r\n    Naturalistic and Evolutionary model of the human condition\r\n    Time Machine\r\n", "recorded": "2011-01-07T09:00:00", "title": "Lecture 22: H. G. Wells \"The Time Machine\" and The Final Utopia "}, {"url": "mlss2010_lawrence_mlfcs", "desc": "", "recorded": "2010-05-06T09:03:21", "title": "Machine learning for cognitive science 1: What is machine learning?"}, {"url": "bootcamp07_vilanova", "desc": "Pascal Boot camp is meant to be a crossroad between a summer school and a strong workshop session.\r\n;And the following professors kindly accepted to participate: \r\n:Isabelle Guyon, Ulrike von Luxburg, Mark Girolami, Colin de la Higuera, Joaqu\u00edn Qui\u00f1onero, Florence d'Alche-Buc, William Triggs, Mikaela Keller, Amir Saffari, Cecilio Angulo, Mario Mart\u00edn, Llu\u00eds Belanche. \r\n\r\nThe main topics developed in this summercamp will be:\r\n * Basic Math and TCS for Machine Learning\r\n * Useful existing software for Machine Learning\r\n * Introduction to Machine Learning\r\n * Theoretical frameworks and foundations\r\n * Experimental Machine Learning\r\n * Feature extraction and model selection\r\n * Graphical models\r\n * Kernel methods and linear predictors\r\n * Clustering\r\n * General view of application areas\r\n * Machine learning in vision\r\n * Machine learning in user interfaces\r\n * Machine learning for data mining", "recorded": "2007-07-02T09:00:00", "title": "PASCAL Bootcamp in Machine Learning, Vilanova 2007"}, {"url": "is2012_sammut_child_machine", "desc": "I think of machine learning research as building two different types of entities: Turing\u2019s Child Machine and H.G. Wells\u2019 World Brain. The former is a machine that learns incrementally by receiving instruction from a trainer or by its own trial-and-error. The latter is a permanent repository that makes all human knowledge accessible to anyone in the world. While machine learning began following the Child Machine model, recent research has been more focussed on \u201corganising the world\u2019s information\u201d. Both are\r\nimportant endeavours, however, incremental learning has been neglected and, we argue, should be revived.", "recorded": "2012-10-11T14:15:15", "title": "The Child's Machine vs. the World's Brain "}, {"url": "mlss2010_sardinia", "desc": "Cognitive science aims to reverse engineer human intelligence; machine learning provides one of our most powerful sources of insight into how machine intelligence is possible. Cognitive science therefore raises challenges for, and draws inspiration from, machine learning; and insights about the human mind may help inspire new directions for machine learning. This summer school brings together leading researchers from both fields, and those working at the interface between them. It is aimed at graduate students, post-docs and established researchers from both the cognitive science and machine learning communities, interested in exploring the interface between human and machine intelligence.\r\n\r\nMore about the event at http://www.mlss.cc/sardinia10", "recorded": "2010-05-06T09:00:00", "title": "Cognitive Science and Machine Learning Summer School (MLSS), Sardinia 2010"}, {"url": "icml2015_lille", "desc": "[[http://icml.cc/2015/|ICML 2015]] is the leading international machine learning conference and is supported by the [[http://www.machinelearning.org/|International Machine Learning Society (IMLS)]].", "recorded": "2015-07-06T00:00:00", "title": "32nd International Conference on Machine Learning (ICML), Lille 2015"}, {"url": "stanfordcs229f08_ng_lec01", "desc": "The Motivation & Applications of Machine Learning, The Logistics of the Class, The Definition of Machine Learning, The Overview of Supervised Learning, The Overview of Learning Theory, The Overview of Unsupervised Learning, The Overview of Reinforcement Learning\r\n", "recorded": "2009-04-17T10:00:00", "title": "Lecture 1 - The Motivation & Applications of Machine Learning"}, {"url": "icml05_bonn", "desc": "The International Conference on Machine Learning (ICML) is worldwide the largest international conference on machine learning research and applications. ICML is supported by IMLS (International Machine Learning Society). ICML papers have been included in the ACM International Conference Proceedings Series, i.e., the proceedings are available at the ACM Digital Library.", "recorded": "2005-08-07T00:00:00", "title": "22nd Annual International Conference on Machine Learning (ICML), Bonn 2005"}, {"url": "mlcs07_goldsmith_mtm", "desc": "The title of this talk is \u201cYour Turing Machine or Mine?\u201d. What I am alluding\r to with this title is the universality of a universal Turing machine, and at\r the same time, to the fact that there are many different universal Turing machines\r with somewhat different properties. Universal Turing machines are both\r universal and individual, in different senses. A universal Turing machine is one\r that can emulate (or imitate) any other Turing machine, and thus in a sense can\r undertake to compute any of a very large class of computable functions. But\r there are an indefinitely large number of universal Turing machines that can be\r defined.", "recorded": "2007-06-21T09:00:00", "title": "My Turing Machine or Yours?"}, {"url": "ecmlpkdd08_pedersen_emlu", "desc": "The tutorial on embedded machine learning will present a case study of implementing and using a binary support vector machine in wireless sensor networks. It will use a very popular operating system for wireless sensor networks called TinyOS and the new exciting open hardware/software platform Lego Mindstorms NXT from LEGO. Outline of the tutorial (structured list of topics):\r\nThe tutorial provides an overview of embedded machine learning and an Overview of wireless sensor networks, TinyOS and the programming language nesC. It provides an introduction to LEGO MINDSTORMS and the main hardware items needed to engage the problem in a meaningful way. The mapping of the binary support vector machine to the constraints of the embedded machine learning problem is given (memory, battery, little CPU).", "recorded": "2008-09-14T09:00:00", "title": "Embedded Machine Learning: Using Support Vector Machines in Wireless Sensor Networks using TinyOS and Lego Mindstorms NXT"}, {"url": "bbci2012_sugiyama_machine_learning", "desc": "In statistical machine learning, avoiding density estimation is essential because it is often more difficult than solving a target machine learning problem itself.  This is often referred to as Vapnik's principle, and the support vector machine is one of the successful realizations of this principle.  Following this spirit, a new machine learning framework based on the ratio of probability density functions has been introduced recently.  This density-ratio framework includes various important machine learning tasks such as transfer learning, outlier detection, feature selection, clustering, and conditional density estimation.  All these tasks can be effectively and efficiently solved in a unified manner by direct estimating the density ratio without going through density estimation. In this lecture, I give an overview of theory, algorithms, and application of density ratio estimation.", "recorded": "2012-09-25T11:00:00", "title": "Density Ratio Estimation in Machine Learning"}, {"url": "mlws04_sheffield", "desc": "Machine Learning is the study of computer algorithms that improve automatically through experience. Applications range from datamining programs that discover general rules in large data sets, to information filtering systems that automatically learn users' interests. (Machine Learning, Tom Mitchell, McGraw Hill, 1997)", "recorded": "2004-09-07T00:00:00", "title": "Machine Learning Workshop, Sheffield 2004"}, {"url": "single_mladenic_semanticweb", "desc": "Machine Learning and Semantic web are covering conceptually different sides of the same story - Semantic Web\u2019s typical approach is top-down modeling of knowledge and proceeding down towards the data while Machine Learning is almost entirely data-driven bottom-up approach trying to discover the structure in the data and express it in the more abstract ways and rich knowledge formalisms. The talk will discuss possible interaction and usage of Machine Learning and Knowledge discovery for Semantic Web with emphases on ontology construction. In the second half of the talk we will take a look at some research using machine learning for Semantic Web and demos of the corresponding prototype systems.", "recorded": "2011-03-11T10:12:25", "title": "Machine Learning and Knowledge Discovery for Semantic Web"}, {"url": "smartdw09_cancedda_welcome", "desc": "Statistical Machine Learning has many points of contact with Statistical Machine Translation. This workshop presents recent work within this intersection. It is sponsored both by the Pascal Network of Excellence and the EU-funded SMART project. This is a 3 years project aimed at addressing a range of problems in Machine Translation and Cross-Language Information Retrieval using modern Statistical Learning methods.\r\n\r\nOne aim of this workshop is to disseminate scientific results and shared experiences from the SMART project to the larger technical and scientific community. More importantly, it is an opportunity to strenghten the connections between the Machine Translation and Machine Learning communities.", "recorded": "2009-05-13T09:30:00", "title": "Welcome to the Statistical Multilingual Analysis for Retrieval and Translation - SMART Dissemination Workshop "}, {"url": "mlas06_pittsburgh", "desc": "Machine learning approaches to natural language processing problems such as information retrieval, document classification, and information extraction have developed rapidly over recent years. Even more recently, the joint analysis of text and images has become a significant focus for machine learning. This autumn school will summarize the state of the art in machine learning for text analysis and for joint text/image analysis, as presented by researchers active in these fields. It is intended for students who already have a familiarity with machine learning, and is designed for software developers, graduate students, and advanced researchers with an interest in learning more about this area.", "recorded": "2006-09-26T00:00:00", "title": "Autumn School 2006: Machine Learning over Text and Images - Pittsburgh"}, {"url": "ijcai2011_etzioni_webscale", "desc": "Research interests include: fundemental problems in the study of intelligence, Web search, Machine Reading, and Machine Learning.", "recorded": "2011-07-22T00:00:00", "title": "Open Information Extraction at Web Scale"}, {"url": "epsrcws08_stern_aml", "desc": "This presentation, based on his PhD from the University of Cambridge, describes a number of applications of machine learning to the game of Go.", "recorded": "2008-01-25T09:30:55", "title": "Applications of Machine Learning to the Game of Go"}, {"url": "machine_learning", "desc": "As a broad subfield of artificial intelligence, machine learning is concerned with the development of algorithms and techniques that allow computers to \"learn\". At a general level, there are two types of learning: inductive, and deductive. Inductive machine learning methods extract rules and patterns out of massive data sets.\r\n \r\n Some parts of machine learning are closely related to data mining and statistics. Machine learning research is focused on the computational properties of the statistical methods, such as their computational complexity.\r\n \r\n Machine learning has a wide spectrum of applications including natural language processing,syntactic pattern recognition, search engines, medical diagnosis, bioinformatics and cheminformatics, detecting credit card fraud, stock market analysis, classifying DNA sequences, speech and handwriting recognition, object recognition in computer vision, game playing and robot locomotion.\r\n \\\\ \r\n **From Wikipedia, the free encyclopedia**", "recorded": "2007-04-12T15:29:16", "title": "Machine Learning Course"}, {"url": "icml08_helsinki", "desc": "The 25th International Conference on Machine Learning (ICML 2008) was organized in Helsinki, Finland on July 5-9, 2008. ICML is the leading international machine learning conference, attracting annually about 500 participants from all over the world. ICML is supported by the [[http://www.machinelearning.org|International Machine Learning Society(IMLS)]]. Check also [[http://videolectures.net/icml07_corvallis/ | last years videos - ICML07]]... Event home page: > http://icml2008.cs.helsinki.fi/ ----", "recorded": "2008-07-05T09:00:00", "title": "25th International Conference on Machine Learning (ICML), Helsinki 2008"}, {"url": "ecmlpkdd2012_de_raedt_declarative_modeling", "desc": "Despite the popularity of machine learning and data mining today, it remains challenging to develop applications and software that incorporates machine learning or data mining techniques. This is because machine learning and data mining have focussed on developing high-performance algorithms for solving particular tasks rather than on developing general principles and techniques.\r\nI propose to alleviate these problems by applying the constraint programming methodology to machine learning and data mining and to specify machine learning and data mining problems as constraint satisfaction and optimization problems. What is essential is that the user be provided with a way to declaratively specify what the machine learning or data mining problem is rather than having to outline how that solution needs to be computed.  This corresponds to a model + solver-based approach to machine learning and data mining, in which the user specifies the problem in a high level modeling language and the system automatically transforms such models into a format that can be used by a solver to efficiently generate a solution.   This should be much easier for the user than having to implement or adapt an algorithm that computes a particular solution to a specific problem.\r\nThroughout the talk, I shall use illustrations from our work on constraint programming for itemset mining and probabilistic programming.", "recorded": "2012-09-28T09:05:17", "title": "Declarative Modeling For Machine Learning and Data Mining"}, {"url": "iswc2011_salonen_prototyping", "desc": "Creation of virtual machine laboratories \u2013 simulated planning and learning environments demonstrating function and structure of working machines \u2013 often involve a lot of manual labor. A notable source of the labor is the programming required due to changes in structural and functional models of a system. As a result, rapid prototyping of a virtual machine laboratory becomes difficult, if not impossible. We argue that by using a combination of semantic modeling and prototyping with a web-based system, more rapid\r\ndevelopment of virtual machine laboratories can be achieved. In this paper, we present the design and implementation of a semantic, web-based virtual machine laboratory prototyping environment. Application of the environment to a case example is also described and discussed.", "recorded": "2011-10-27T14:30:00", "title": "An Implementation of a Semantic, Web-Based Virtual Machine Laboratory Prototyping Environment"}, {"url": "icml2010_haifa", "desc": "ICML is the leading international machine learning conference, attracting annually some 500 participants from all over the world. ICML is supported by the [[http://www.machinelearning.org/|International Machine Learning Society]] (IMLS). \r\n\r\nMore about the conference at [[http://www.icml2010.org/index.html|ICML 2010]]", "recorded": "2010-06-21T09:00:00", "title": "27th International Conference on Machine Learning (ICML), Haifa 2010"}, {"url": "mlss08au_hutter_fund", "desc": "Machine learning is usually taught as a bunch of methods that can\r solve a bunch of problems (see above).\r \r The second part of the tutorial takes a step back and asks about the\r foundations of machine learning, in particular the (philosophical)\r problem of inductive inference, (Bayesian) statistics, and\r artificial intelligence.\r \r It concentrates on principled, unified, and exact methods.", "recorded": "2008-03-10T08:35:43", "title": "Foundations of Machine Learning"}, {"url": "opt08_wright_oimlr", "desc": "The use of optimization as a framework for formulating machine learning problems has become much more widespread in recent years. In some cases, the demands of the machine learning problems go beyond the scope of traditional optimization paradigms. While existing optimization formulations and algorithms serve as an good starting point for the solution strategies, important work must be carried out at the interface of optimization and machine learning to devise strategies that exploit the special features of the application and that perform well on very large data sets. This talk reviews recent developments from an optimization perspective, focusing on activity during the past three years, and looking in particular at problems where the machine learning application has motivated novel algorithms or analysis in the optimization domain. We also discuss some current challenges, highlighting several recent developments in optimization that may be useful in machine learning applications.\r\n\r\n", "recorded": "2008-12-12T07:30:00", "title": " Optimization in Machine Learning: Recent Developments and Current Challenges"}, {"url": "mlss09uk_cambridge", "desc": "The 13th Machine Learning Summer School was held in Cambridge, UK. This year's edition was organized by the University of Cambridge, Microsoft Research and PASCAL. The school offered an overview of basic and advanced topics in machine learning through theoretical and practical lectures given by leading researchers in the field. We hope to attract international students, young researchers and industry practitioners with a keen interest in machine learning and a strong mathematical background.\r\n\r\n----\r\nThe Summer school homepage can be found at http://mlg.eng.cam.ac.uk/mlss09/index.html\r\n----", "recorded": "2009-08-27T15:01:37", "title": "Machine Learning Summer School (MLSS), Cambridge 2009"}, {"url": "bmvc2012_surrey", "desc": "The British Machine Vision Conference is one of the major international conferences on computer vision and related areas. Organized by the [[http://www.bmva.org/|British Machine Vision Association]], the 23rd BMVC was held at the [[http://www.surrey.ac.uk/|University of Surrey]]. \r\n\r\nTo find out more please visit the [[http://bmvc2012.surrey.ac.uk/index.php|BMVC 2012]] website.", "recorded": "2012-09-03T13:30:00", "title": "British Machine Vision Conference (BMVC), Surrey 2012"}, {"url": "cmuseminars", "desc": "The [[http://www.ml.cmu.edu/index.html|Machine Learning Department]] is an academic department within Carnegie Mellon University's [[http://www.cs.cmu.edu/|School of Computer Science]]. They focus on research and education in all areas of statistical machine learning. \r\n\r\nFor more information visit the department\u00b4s [[http://www.ml.cmu.edu/Seminars%20and%20talks/index.html|Seminars and Talks website]].", "recorded": "2015-02-10T00:00:00", "title": "Machine Learning Seminars at Carnegie Mellon University"}, {"url": "bmvc2013_bristol", "desc": "The British Machine Vision Conference is one of the major international conferences on computer vision and related areas. Organized by the [[http://www.bmva.org/|British Machine Vision Association]], the 23rd BMVC was held at the [[http://www.bristol.ac.uk/|University of Bristol]].\r\n\r\nTo find out more please visit the [[http://bmvc2013.bristol.ac.uk/|BMVC 2013]] website.", "recorded": "2013-09-09T09:00:00", "title": "British Machine Vision Conference (BMVC), Bristol 2013"}, {"url": "aibootcamp2011_sanchez_att", "desc": "Machine Translation has evolved notably in the last two decades. This talk introduces the main concepts related to Machine Translation. First, IBM word alignment models are described. Then, phrase-based models are explained as a natural extension of word alignment models. Finally, syntactic approaches are presented in order to deal with language pairs with different syntactic structure.", "recorded": "2011-02-22T09:00:00", "title": "Introduction to Machine Translation"}, {"url": "slsfs05_mladenic_drfsm", "desc": "Dimensionality reduction is a commonly used step in machine learning, especially when dealing with a high dimensional space of features. The original feature space is mapped onto a new, reduced dimensioanllyity space and the examples to be used by machine learning algorithms are represented in that new space. The mapping is usually performed either by selecting a subset of the original features or/and by constructing some new features. This persentation deals with the first approach, feature subset selection. We provide a brief overview of the feature subset selection techniques that are commonly used in machine learning and give a more detailed description of feature subset selection used in machine learning on text data. Performance of some methods used is document categorization is illustrated by providing experimental comparison on real-world data collected from the Web.", "recorded": "2005-02-24T00:00:00", "title": "Dimensionality Reduction by Feature Selection in Machine Learning"}, {"url": "ssll09_hutter_isml", "desc": "This course provides a brief overview of the methods and practice of statistical machine learning, which is concerned with the development of algorithms and techniques that learn from observed data by constructing stochastic models that can be used for making predictions and decisions. The idea of the course is to (a) give a mini-introduction and background to logicians interested in the AI courses, and (b) to summarize the core concepts covered by the machine learning courses during this week.", "recorded": "2009-01-30T09:00:00", "title": "Introduction To Statistical Machine Learning"}, {"url": "mloss08_sonnenburg_iao", "desc": "We believe that the wide-spread adoption of open source software policies will have a tremendous ipact\r\non the \ufb01eld of machine learning. The goal of this workshop is to further support the current dvelopments\r\nin this area and give new impulses to it. Following the success of the inaugural NIPS-MLOSS workshop\r\nheld at NIPS 2006, the Journal of Machine Learning Research (JMLR) has started a new track for machine\r\nlearning open source software initiated by the workshop\u2019s organizers. Many prominent machine learning\r\nresearchers have co-authored a position paper advocating the need for open source software in machine\r\nlearning. Furthermore, the workshop\u2019s organizers have set up a community website mloss.org where people\r\ncan register their software projects, rate existing projects and initiate discussions about projects and related\r\ntopics. This website currently lists 156 such projects including many prominent projects in the area of\r\nmachine learning. The main goal of this workshop is to bring the main practitioners in the area of machine\r\nlearning open source software together in order to initiate processes which will help to further improve the\r\ndevelopment of this area. In particular, we have to move beyond a mere collection of more or less unrelated\r\nsoftware projects and provide a common foundation to stimulate cooperation and interoperability between\r\ndi\ufb00erent projects. An important step in this direction will be a common data exchange format such that\r\ndi\ufb00erent methods can exchange their results more easily.", "recorded": "2008-12-12T07:30:00", "title": "Introduction and overwiew of the Machine Learning Open Source Software workshop"}, {"url": "nips2010_wright_oaml", "desc": "Optimization provides a valuable framework for thinking about,\r\nformulating, and solving many problems in machine learning. Since\r\nspecialized techniques for the quadratic programming problem arising\r\nin support vector classification were developed in the 1990s, there\r\nhas been more and more cross-fertilization between optimization and\r\nmachine learning, with the large size and computational demands of\r\nmachine learning applications driving much recent algorithmic research\r\nin optimization. This tutorial reviews the major computational\r\nparadigms in machine learning that are amenable to optimization\r\nalgorithms, then discusses the algorithmic tools that are being\r\nbrought to bear on such applications. We focus particularly on such\r\nalgorithmic tools of recent interest as stochastic and incremental\r\ngradient methods, online optimization, augmented Lagrangian methods,\r\nand the various tools that have been applied recently in sparse and\r\nregularized optimization.", "recorded": "2010-12-06T13:00:00", "title": "Optimization Algorithms in Machine Learning"}, {"url": "icml09_beygelzimer_zadrozny_langford_riml", "desc": "Machine learning reductions are about reusing solutions to simple, core problems in order to solve more complex problems. A basic difficulty in applying machine learning in practice is that we often need to solve problems that don't quite match the problems solved by standard machine learning algorithms. Reductions are techniques that transform such practical problems into core machine learning problems. These can then be solved using any existing learning algorithm whose solution can, in turn, be used to solve the original problem.\r\nThe material that we plan to cover is both algorithmic and analytic. We will discuss existing and new algorithms, along with the methodology for analyzing and creating new reductions. We will also discuss common design flaws in folklore reductions. In our experience, this approach is an effective tool for designing empirically successful, automated solutions to learning problems.", "recorded": "2009-06-14T09:00:00", "title": "Reductions in Machine Learning"}, {"url": "nipsworkshops09_xu_fpga", "desc": "Machine learning algorithms are becoming increasingly important in our daily life. However, training on very large scale datasets is usually very slow. FPGA is a reconfigurable platform that can achieve high parallelism and data throughput. Many works have been done on accelerating machine learning algorithms on FPGA. In this paper, we adapt Google's MapReduce model to FPGA by realizing an on-chip MapReduce framework for machine learning algorithms. A processor scheduler is implemented for the maximum computation resource utilization and load balancing. In accordance with the characteristics of many machine learning algorithms, a common data access scheme is carefully designed to maximize data throughput for large scale dataset. This framework hides the task control, synchronization and communication away from designers to shorten development cycles. In a case study of RankBoost acceleration, up to 31.8x speedup is achieved versus CPU-based design, which is comparable with a fully manually designed version. We also discuss the implementations of two other machine learning algorithms, SVM and PageRank, to demonstrate the capability of the framework.", "recorded": "2009-12-11T16:23:00", "title": "FPGA-based MapReduce Framework for Machine Learning"}, {"url": "mloss08_whistler", "desc": "We believe that the wide-spread adoption of open source software policies will have a tremendous impact on the field of machine learning. The goal of this workshop is to further support the current developments in this area and give new impulses to it. Following the success of the inaugural NIPS-MLOSS workshop held at NIPS 2006, the Journal of Machine Learning Research (JMLR) has started a new track for machine learning open source software initiated by the workshop's organizers. Many prominent machine learning researchers have co-authored a position paper advocating the need for open source software in machine learning. Furthermore, the workshop's organizers have set up a community website mloss.org where people can register their software projects, rate existing projects and initiate discussions about projects and related topics. This website currently lists 132 such projects including many prominent projects in the area of machine learning.\r\n\r\nThe main goal of this workshop is to bring the main practitioners in the area of machine learning open source software together in order to initiate processes which will help to further improve the development of this area. In particular, we have to move beyond a mere collection of more or less unrelated software projects and provide a common foundation to stimulate cooperation and interoperability between different projects. An important step in this direction will be a common data exchange format such that different methods can exchange their results more easily.\r\n\r\nMore information about workshop - http://mloss.org/workshop/nips08/", "recorded": "2008-12-12T07:30:00", "title": "NIPS Workshop on Machine Learning Open Source Software, Whistler 2008"}, {"url": "nipsworkshops09_gray_lsml", "desc": "To seed discussion, I will attempt to organize research efforts in large-scale machine learning by looking at common computational problems across all of machine learning, and the challenges of creating efficient parallel algorithms for them. I'll begin by identifying four common types of computational bottlenecks that occur across all of machine learning, or prototype algorithmic problems: N-body problems, graph operations, linear algebra, and optimization. Within each category, I'll discuss what we can or cannot learn from the existing body of work in scientific computing, highlight a few of the most successful and recent specific serial algorithms that have been developed for concreteness, and discuss what makes them easy or hard to parallelize. I'll synthesize some of these observations to obtain a list of desiderata for parallel machine learning algorithms research and software toolkits.", "recorded": "2009-12-11T15:30:00", "title": "Large-Scale Machine Learning: The Problems, Algorithms, and Challenges"}, {"url": "ecmlpkdd09_ben_david_tpim", "desc": "Theoretical analysis has played a major role in some of the most prominent practical successes of statistical machine learning. However, mainstream machine learning theory assumes some strong simplifying assumptions which are often unrealistic. In the past decade, the practice of machine learning has led to the development of various heuristic paradigms that answer the needs of a vastly growing range of applications. Many useful such paradigms fall beyond the scope of the currently available analysis. Will theory play a similar pivotal role in the newly emerging sub areas of machine learning?\r\nIn this talk, I will survey some such application-motivated theoretical challenges. In particular, I will discuss recent developments in the theoretical analysis of semi-supervised learning, multi-task learning, \u201clearning to learn\u201d, privacy-preserving learning and more.", "recorded": "2009-09-08T09:02:00", "title": "Theory-Practice Interplay in Machine Learning \u2013 Emerging Theoretical Challenges"}, {"url": "nipsworkshops2010_mahoney_lam", "desc": "Very large informatics graphs such as large social and information networks typically have properties that render many popular machine learning and data analysis tools largely inappropriate. While this is problematic for these applications, it also suggests that these graphs may be useful as a test case for the development of new algorithmic tools that may then be applicable much more generally. Many of the popular machine learning and data analysis tools rely on linear algebra, and they are typically used by calling traditional numerical linear algebra code as a black box. After briefly reviewing some of the structural properties of large social and information networks that are responsible for the inapplicability of traditional linear algebra and machine learning tools, I will describe several examples of \"new linear algebra\" and \"new machine learning\" that arise from the analysis of such informatics graphs. These new directions involve looking \"inside\" the black box, and they place very different demands on the linear algebra than are traditionally placed by numerical, scientific computing, and small-scale machine learning applications.", "recorded": "2010-12-11T17:20:00", "title": "Linear Algebra and Machine Learning of Large Informatics Graphs"}, {"url": "clspss09_lopez_mt", "desc": "", "recorded": "2009-06-11T09:00:00", "title": "Machine translation"}, {"url": "clspss09_denero_ml", "desc": "", "recorded": "2009-06-11T10:40:00", "title": "Machine translation"}, {"url": "icml09_montreal", "desc": "The 26th International Conference on Machine Learning (ICML 2009) will be organized in Montreal, Canada on June 14-18, 2009. ICML is the leading international machine learning conference, attracting annually about 500 participants from all over the world. ICML is supported by the International Machine Learning Society (IMLS).\r\n\r\nICML is co-located with two closely related conferences, the 25th Conference on Uncertainty in Artificial Intelligence (UAI) and the 22nd Annual Conference on Learning Theory (COLT). A Multidisciplinary Symposium on Reinforcement Learning will also be co-located.\r\n\r\n----\r\nMore about the Conference can be found at: http://www.cs.mcgill.ca/~icml2009/\r\n----", "recorded": "2009-06-14T08:33:02", "title": "26th International Conference on Machine Learning (ICML), Montreal 2009"}, {"url": "ecmlpkdd08_zavrel_mlce", "desc": "Machine learning methods are widely used as a heuristic knowledge acquisition method for building commercial text mining and document understanding systems, even though learning from data inherently delivers imperfect results. In this talk I will look at the practical value of imperfect solutions in document understanding systems, and at how machine learning is effective in this context, in particular in domains which are transaction oriented. We will point out practical issues from an industrial perspective, illustrated by cases from our practice at Textkernel. Machine Learning is proving to be a viable commercial methodology for knowledge acquisition and offers a principled way towards progress in systems engineering for Language Technology.", "recorded": "2008-09-16T14:00:00", "title": " \tMachine Learning Considered Effective for Commercial Document Understanding Systems"}, {"url": "mlss04_berderisland", "desc": "The fourth Machine Learning Summer School was held in Berder Island, France between the 12th and the 25th of September, 2004. More than 100 students and researchers from 20 countries interested in Machine Learning attended. This years' summer school presented some of the topics which are at the core of modern Learning Theory. 15 distinguished authorities from the field gave 14 courses in slots from 4 to 8 hours. In addition, many evening talks which were focused on additional topics were presented. There were also three practical sessions organized providing a 'hands-on' experience of working with Machine Learning algorithms.", "recorded": "2004-09-12T00:00:00", "title": "Machine Learning Summer School (MLSS), Berder Island 2004"}, {"url": "nipsworkshops09_machine_learning", "desc": "**Large-Scale Machine Learning: Parallelism and Massive Datasets**\r\n\r\nPhysical and economic limitations have forced computer architecture towards parallelism and away from exponential frequency scaling. Meanwhile, increased access to ubiquitous sensing and the web has resulted in an explosion in the size of machine learning tasks. In order to benefit from current and future trends in processor technology we must discover, understand, and exploit the available parallelism in machine learning. This workshop will achieve four key goals: *Bring together people with varying approaches to parallelism in machine learning to identify tools, techniques, and algorithmic ideas which have lead to successful parallel learning. *Invite researchers from related fields, including parallel algorithms, computer architecture, scientific computing, and distributed systems, who will provide new perspectives to the NIPS community on these problems, and may also benefit from future collaborations with the NIPS audience. *Identify the next key challenges and opportunities to parallel learning. *Discuss large-scale applications, e.g., those with real time demands, that might benefit from parallel learning. Prior NIPS workshops have focused on the topic of scaling machine learning, which remains an important developing area. We introduce a new perspective by focusing on how large-scale machine learning algorithms should be informed by future parallel architectures.\r\n----\r\nThe Workshop homepage can be found at http://www.select.cs.cmu.edu/meetings/biglearn09/.\r\n----", "recorded": "2009-12-11T07:30:00", "title": "Machine Learning"}, {"url": "mlss05us_chicago", "desc": "Machine learning is a field focused on making machines learn to make predictions from examples. It combines elements of mathematics, computer science, and statistics with applications in biology, physics, engineering and any other area where automated prediction is necessary. This short summer school is an intense introduction to the basics of machine learning and learning theory with various additional advanced topics covered. It is appropriate for anyone interested in learning this material.", "recorded": "2005-05-16T00:00:00", "title": "Machine Learning Summer School (MLSS), Chicago 2005"}, {"url": "ssll09_canberra", "desc": "The Summer Schools in Logic and Learning bring together two annual summer schools in the area of logic and machine learning: \r\n\r\n* **the Logic Summer School**\r\n* **the Machine Learning Summer School** \r\n\r\nThe summer schools are hosted by the Computer Sciences Laboratory in the Research School of Information Sciences and Engineering at The Australian National University, from the 26 January to 6 February 2009.\r\n\r\n#The Logic courses will consist of short courses on aspects of pure and applied logic. \r\n#The Machine Learning courses will consist of short courses on the theory and practice of machine learning, which combine deep theory from areas as diverse as Statistics, Mathematics, Engineering, and Information Technology with many practical and relevant real life applications. The courses will be taught by experts from Australia and overseas. \r\n#The summer schools this year will also include a special track on Artificial Intelligence (AI), which will feature courses on aspects of both logic and machine learning.", "recorded": "2009-01-26T09:00:00", "title": "Summer Schools in Logic and Learning, Canberra 2009"}, {"url": "colt2011_freeman_help", "desc": "I'll describe where computer vision needs advances from computer science and machine learning.  This talk will cover where computer vision works well: finding cars and faces, operating in controlled environments, and where it doesn't work well: in the uncontrolled settings of daily life.  Several aspects of the problem make it particularly appropriate for machine learning research: we have large datasets of high-dimensional data, so efficient processing is crucial for success.  The data are noisy, and we search and analyze images over Internet scales. I'll list a number of computer vision problems, describe their structure, and tell where we need help.  This talk was partially crowd-sourced: at recent computer vision conferences, I've asked my colleagues where they felt we needed help from computer science and machine learning, and I'll report on what they said. ", "recorded": "2011-07-11T11:10:00", "title": "Where machine vision needs help from machine learning"}, {"url": "eml07_whistler", "desc": "The ever increasing size of available data to be processed by machine learning algorithms has yielded several approaches, from online algorithms to parallel and distributed computing on multi-node clusters. Nevertheless, it is not clear how modern machine learning approaches can either cope with such parallel machineries or take into account strong constraints regarding the available time to handle training and/or test examples.\n\nThis workshop explores two alternatives:\n\n1. modern machine learning approaches that can handle real time processing at train and/or at test time, under strict computational constraints (when the flow of incoming data is continuous and needs to be handled), and\\\\\n2.  modern machine learning approaches that can take advantage of new commodity hardware such as multicore, GPUs, and fast networks.\n\nThis two-day workshop aims to set the agenda for future advancements by fostering a discussion of new ideas and methods and by demonstrating the potential uses of readily-available solutions. It brings together both researchers and practitioners to offer their views and experience in applying machine learning to large scale learning.\n\nFind out more at the [[http://bigml.wikispaces.com/cfp|Workshop website]].", "recorded": "2007-12-07T07:30:00", "title": "NIPS Workshop on Efficient Machine Learning, Whistler 2007"}, {"url": "mlss04_zien_mlb", "desc": "", "recorded": "2004-09-17T00:00:00", "title": "Machine Learning in Bioinformatics"}, {"url": "mlss2012_lawrence_machine_learning", "desc": "", "recorded": "2012-04-16T11:30:00", "title": "What is Machine Learning?"}, {"url": "bootcamp07_triggs_mlv", "desc": "", "recorded": "2007-07-12T10:00:00", "title": "Machine Learning in Vision"}, {"url": "icmi05_bengio_am", "desc": "", "recorded": "2005-10-07T00:00:00", "title": "The applications in Machine Learning"}, {"url": "stw07_kuhn_smt", "desc": "", "recorded": "2007-02-01T00:00:00", "title": "Statistical Machine Translation"}, {"url": "mlss07_gyorfi_mlaf", "desc": "", "recorded": "2007-08-29T17:13:51", "title": "Machine learning and finance"}, {"url": "bsciw08_schwaighofer_mld", "desc": "", "recorded": "2008-12-13T08:50:00", "title": "Machine Learning - Discussion"}, {"url": "aedml08_bratko_iml", "desc": "", "recorded": "2008-03-17T11:00:00", "title": "Introduction to machine learning "}, {"url": "clspss09_kingsbury_mrs", "desc": "", "recorded": "2009-06-15T10:40:00", "title": "Machine Recognition of Speech"}, {"url": "cern_evans_lhcms", "desc": "", "recorded": "2008-08-18T00:00:00", "title": "LHC machine status"}, {"url": "ktsymposium2013_colton_towards", "desc": "", "recorded": "2013-07-04T15:40:00", "title": "Towards the What-If Machine"}, {"url": "deeplearning2015_vincent_machine_learning", "desc": "", "recorded": "2015-08-03T09:00:00", "title": "Introduction to Machine Learning"}, {"url": "oiml05_lavin", "desc": "Optimization and inference are two important computational problems that arise in many machine learning and physical contexts. Bayesian inference consists of the computation of marginal probabilities in high dimensional probability models. It is at the core of many machine learning applications such as computer vision, robotics, expert systems and pattern recognition. Also optimization is found in many applications such as optimal control, Markov decision processes and expert systems.", "recorded": "2005-01-19T00:00:00", "title": "Workshop on Optimization and Inference in Machine Learning and Physics, Lavin 2005"}, {"url": "mloss08_reutemann_edml", "desc": "**Experiment Databases for Machine Learning**\\\\\nExperiment Databases for Machine Learning is a large public repository of machine learning experiments as\nwell as a framework for producing similar databases for speci\ufb01c goals. This projects aims to bring the infor-\nmation contained in many machine learning experiments together and organize it a way that allows everyone\nto investigate how learning algorithms have performed in previous studies. To share such information with\nthe world, a common language is proposed, dubbed ExpML, capturing the basic structure of a large range\nof machine learning experiments while remaining open for future extensions. This language also enforces\nreproducibility by requiring links to the used datasets and algorithms and by storing all details of the ex-\nperiment setup. All stored information can then be accessed by querying the database, creating a powerful\nway to collect and reorganize the data, thus warranting a very thorough examination of the stored results.\nThe current publicly available database contains over 500,000 classi\ufb01cation and regression experiments, and\nhas both an online interface, at http://expdb.cs.kuleuven.be, as well as a stand-alone explorer tool o\ufb00ering\nvarious visualization techniques. This framework can also be integrated in machine learning toolboxes to\nautomatically stream results to a global (or local) experiment database, or to download experiments that\nhave been run before.\n\n**BenchMarking Via Weka**\\\\\nBenchMarking Via Weka is a client-server architecture that supports interoperability between di\u000berent machine\nlearning systems. Machine learning systems need to provide mechanisms for processing data and\nevaluating generated models. In our system, the server hosts all the data and performs all the statistical\nanalyses, while the client performs all the pre-processing and model building. This separation of tasks\nopens up the possibility of o\u000bering a cross-platform and cross-language framework. By performing statistical\nanalyses on the host, we avoid unnecessary exchange and conversion of generated results.", "recorded": "2008-12-12T17:45:00", "title": "Experiment Databases for Machine Learning / BenchMarking Via Weka"}, {"url": "bsciw08_hartline_mlmda", "desc": "Given the complexity of preferences in markets such as key word advertising it is hard to believe that the de facto standard, decentralized, local, greedy algorithm\r\n(advertisers bid for clicks on keywords) is any where close to being optimal for any reasonable objective (welfare, profit, etc.). In this talk we consider the market design problem from a global perspective. We make connections between machine learning theory and market design theory, where machine learning design problems closely mirror game theoretic design problems. We reduce a general theoretical market design problem to a natural machine learning optimization problem. These theoretical results lead to a number of practical answers to advertising market design questions.", "recorded": "2008-12-13T16:00:00", "title": "Machine Learning, Market Design, and Advertising"}, {"url": "ecmlpkdd2011_bishop_embracing", "desc": "Over the last decade the number of deployed applications of machine learning has grown rapidly, with examples in domains ranging from recommendation systems and web search, to spam filters and voice recognition. Most recently, the Kinect 3D full-body motion sensor, which relies crucially on machine learning, has become the fastest-selling consumer electronics device in history. Developments such as the advent of widespread internet connectivity, with its centralisation of data storage, as well as new algorithms for computationally efficient probabilistic inference, will create many new opportunities for machine learning over the coming years. The talk will be illustrated with tutorial examples, live demonstrations, and real-world case studies.", "recorded": "2011-09-05T12:00:00", "title": "Embracing Uncertainty: Applied Machine Learning Comes of Age"}, {"url": "cidu2011_banerjee_intro_to_ml", "desc": "Over the past few decades, the field of Machine Learning has matured significantly, drawing ideas from several disciplines including Statistics, Optimization, and Artificial Intelligence. Applications of Machine Learning have led to important advances in a wide variety of domains ranging from Internet applications to scientific problems. This talk will give a gentle tutorial introduction to Machine Learning with broad overview on four families of models and methods: predictive models, graphical models, online learning, and exploratory data analysis. The talk will discuss the main idea behind some of key approaches in each family and the problems where they are applicable. A wide variety of applications including text analysis, recommendation systems, climate sciences, and finance will be discussed.", "recorded": "2011-10-19T15:00:00", "title": "Introduction to Machine Learning"}, {"url": "metaforum2012_soricut_translation", "desc": "", "recorded": "2012-06-20T12:00:00", "title": "Changing the Perspective on Machine Translation"}, {"url": "smartdw09_specia_cemt", "desc": "", "recorded": "2009-05-13T16:30:00", "title": "Confidence Estimation for Machine Translation"}, {"url": "icmi05_bengio_tfm", "desc": "", "recorded": "2005-10-07T00:00:00", "title": "The two faces of Machine Learning"}, {"url": "dmss06_cunningham_mlar", "desc": "", "recorded": "2006-06-15T00:00:00", "title": "Machine learning for access and retrieval I"}, {"url": "mithst512s04_ramoni_lec09", "desc": "", "recorded": "2004-03-08T09:00:00", "title": "Lecture 9: Machine-learning Approach"}, {"url": "icmi05_bengio_wfm", "desc": "", "recorded": "2005-10-07T00:00:00", "title": "What is the future of Machine Learning?"}, {"url": "deeplearning2015_blunsom_machine_translation", "desc": "", "recorded": "2015-08-10T09:00:00", "title": "From Language Modelling to Machine Translation"}, {"url": "mlss2012_lugosi_concentration_inequalities", "desc": "", "recorded": "2012-04-18T17:00:00", "title": "Concentration inequalities in machine learning"}, {"url": "wsdm08_etzioni_mrws", "desc": "", "recorded": "2008-02-12T09:00:00", "title": "Machine Reading at Web Scale"}, {"url": "psm08_ratsch_mls", "desc": "", "recorded": "2008-01-27T20:00:00", "title": "Machine Learning Summer Schools"}, {"url": "mlss09uk_tenenbaum_mlcs", "desc": "", "recorded": "2009-09-05T11:00:00", "title": " \t Machine Learning and Cognitive Science "}, {"url": "learning06_guyon_tmle", "desc": "", "recorded": "2006-10-04T00:00:00", "title": "Teaching Machine Learning from Examples"}, {"url": "icme2012_larson_introduction", "desc": "", "recorded": "2012-07-13T10:20:00", "title": "Time Machine Session: Introduction"}, {"url": "metaforum2011_pilos_machinetranslation", "desc": "", "recorded": "2011-06-28T09:35:00", "title": "Machine Translation at the European Commission"}, {"url": "icmi05_bengio_wum", "desc": "", "recorded": "2005-10-07T00:00:00", "title": "Where is Machine Learning used?"}, {"url": "pascal_mlcurricula", "desc": "", "recorded": "2011-03-30T22:37:32", "title": "PASCAL2 Machine Learning curricula"}, {"url": "sssc2011_grobelnik_machinelearning", "desc": "", "recorded": "2011-08-08T00:00:00", "title": "Machine learning for the semantic web"}, {"url": "translingeu2010_karger_verbmobil", "desc": "", "recorded": "2010-06-04T16:10:00", "title": "Verbmobil - A machine translation story"}, {"url": "ssll09_barbosa_gtiml", "desc": "This course covers diverse aspects of the role played by symmetry in pattern analysis and machine learning. It is designed to provide background knowledge using examples and to touch current research topics without over emphasizing formalizations and technical descriptions.", "recorded": "2009-01-28T14:00:00", "title": "Group Theory in Machine Learning"}, {"url": "icml07_corvallis", "desc": "The 24th Annual International Conference on Machine Learning was held in conjunction with the 2007 International Conference on Inductive Logic Programming at Oregon State University in Corvallis, Oregon.\n As a broad subfield of artificial intelligence, machine learning is concerned with the design and development of algorithms and techniques that allow computers to \"learn\". At a general level, there are two types of learning: inductive, and deductive.\n\nVisit the Conference website [[http://oregonstate.edu/conferences/event/icml2007/|here]].", "recorded": "2007-06-20T09:00:00", "title": "24th Annual International Conference on Machine Learning (ICML), Corvallis 2007"}, {"url": "opt08_whistler", "desc": "Classical optimization techniques have found widespread use in machine learning. Convex optimization has occupied the center-stage and significant effort continues to be still devoted to it. New problems constantly emerge in machine learning, e.g., structured learning and semi-supervised learning, while at the same time fundamental problems such as clustering and classification continue to be better understood. Moreover, machine learning is now very important for real-world problems with massive datasets, streaming inputs, the need for distributed computation, and complex models. \r\n\r\nThese challenging characteristics of modern problems and datasets indicate that we must go beyond the \"\"traditional optimization\"\" approaches common in machine learning. What is needed is optimization \"\"tuned\"\" for machine learning tasks. For example, techniques such as non-convex optimization (for semi-supervised learning, sparsity constraints), combinatorial optimization and relaxations (structured learning), stochastic optimization (massive datasets), decomposition techniques (parallel and distributed computation), and online learning (streaming inputs) are relevant in this setting. These techniques naturally draw inspiration from other fields, such as operations research, polyhedral combinatorics, theoretical computer science, and the optimization community.\r\n\r\nMore information about workshop - http://opt2008.kyb.tuebingen.mpg.de/", "recorded": "2008-12-12T07:30:00", "title": "NIPS Workshop on Optimization for Machine Learning, Whistler 2008"}, {"url": "mlcued08_cambridge", "desc": "**Machine Learning** is a multidisciplinary field which aims to understand and design algorithms that automatically extract useful information from data. Since real world data are typically noisy, ambiguous and occasionally erroneous, a central requirement of a learning system is that it must be able to handle uncertainty. Probability theory provides an ideal basis for representing and manipulating uncertain knowledge, so many successful algorithms in machine learning are based on probabilistic i.e. Bayesian inference. Bayesian inference provides a principled framework for machine learning, but exact inference is often intractable, so most algorithms rely on approximations such as variational methods or Markov chain Monte Carlo.\n----\n More > http://talks.cam.ac.uk/show/archive/9091 \n----", "recorded": "2008-03-03T13:53:26", "title": "Machine Learning seminars at the Cambridge University Engineering Department"}, {"url": "mlss03_zhou_asvmc", "desc": "", "recorded": "2003-08-08T11:00:00", "title": "Analysis of Support Vector Machine Classification"}, {"url": "tra08_pauzie_ahm", "desc": "", "recorded": "2008-04-21T17:00:00", "title": "Advances in Human Machine Interfaces (HMI)"}, {"url": "nipsworkshops09_discussion", "desc": "", "recorded": "2009-12-11T18:05:00", "title": "Discussion: Machine Learning in Computational Biology"}, {"url": "lsoldm2013_ghani_learning_powers", "desc": "", "recorded": "2013-09-23T14:35:41", "title": "Using Machine Learning Powers for Good"}, {"url": "mlss2010_kording_ncsam", "desc": "", "recorded": "2010-05-12T09:04:00", "title": "Neuroscience, cognitive science and machine learning"}, {"url": "nipsworkshops2011_marshall_panel", "desc": "", "recorded": "2011-12-16T18:00:00", "title": "Opportunities for cosmology to meet machine learning"}, {"url": "dmss06_li_mlari", "desc": "", "recorded": "2006-06-15T00:00:00", "title": "Machine learning for access and retrieval II"}, {"url": "stw07_kaariainen_tlmmt", "desc": "", "recorded": "2007-02-02T00:00:00", "title": "Thoughts on Language Models in Machine Translation"}, {"url": "strategymeeting2010_sanborn_chater_fcs", "desc": "", "recorded": "2010-03-16T16:00:00", "title": "Features in Cognitive Science and Machine Learning "}, {"url": "bootcamp07_saffari_mls", "desc": "", "recorded": "2007-07-02T17:00:00", "title": "Introduction to CLOP Machine Learning Toolbox"}, {"url": "nipsworkshops2013_hruschka_machine_reading", "desc": "", "recorded": "2013-12-10T07:30:00", "title": "Quick over of Machine Reading Approaches"}, {"url": "nips05_scholkopf_ocmlp", "desc": "", "recorded": "2005-12-10T00:00:00", "title": "Object Correspondence as a Machine Learning Problem"}, {"url": "mlss03_boucheron_cimla", "desc": "", "recorded": "2003-08-04T12:00:00", "title": "Concentration Inequalities with Machine Learning Applications"}, {"url": "psm08_sonnenburg_mlo", "desc": "", "recorded": "2008-01-27T10:00:00", "title": "Machine learning open source software"}, {"url": "mlss06tw_roweis_mlpgm", "desc": "", "recorded": "2006-07-24T00:00:00", "title": "Machine Learning, Probability and Graphical Models"}, {"url": "is02_proszeky_cammt", "desc": "", "recorded": "2002-10-14T00:00:00", "title": "Comprehension Assistance Meets Machine Translation"}, {"url": "ssspr2010_sanchis_trilles_basm", "desc": "", "recorded": "2010-08-20T10:05:00", "title": "Bayesian Adaptation for Statistical Machine Translation"}, {"url": "mlss04_achlioptas_mlfrm", "desc": "", "recorded": "2004-09-15T00:00:00", "title": "Machine Learning Flavor of Random Matrices"}, {"url": "icml05_ireson_emlie", "desc": "", "recorded": "2005-08-10T10:00:00", "title": "Evaluating Machine Learning for Information Extraction"}, {"url": "nipsworkshops2011_williamson_machine", "desc": "", "recorded": "2011-12-16T07:30:18", "title": "Relations Betweeen Machine Learning Problems"}, {"url": "clspss09_dredze_mlfpw", "desc": "", "recorded": "2009-06-09T09:00:00", "title": "Machine Learning - Finding Patterns in the World "}, {"url": "bioma06_dzeroski_oea", "desc": "", "recorded": "2007-05-09T09:20:00", "title": "Overview of Environmental applications of Machine Learning"}, {"url": "meeng2014_butala_numerical_control", "desc": "", "recorded": "2014-03-24T10:30:00", "title": "Computer Numerical Control of Machine-Tools"}, {"url": "aedml08_ljubljana", "desc": "**Area and goals of the seminar:**\r\n\r\nEnvironmental data often need to be analysed in order to obtain information necessary for environmental management decisions. Sometimes it is necessary to use the data to build a model of the environmental process that we want to manage. In other cases the analysis is necessary to understand the environmental processes studied by identifying and understanding the interrelationships of different parameters.\r\n\r\nMachine learning can be used to elicit regularities from data. In comparison to simple forms of regularities/dependencies treated by statistical methods, machine learning methods can find more complex regularities/dependencies that include both numerical and logical conditions.\r\n\r\nThe seminar will give an introduction to selected machine learning methods as well as illustrative case studies of using these methods to analyse environmental data. Applications in the areas of aquatic ecosystems, agriculture, forestry, environmental epidemiology, and disaster forecasting/relief will be covered in detail. The participants will learn to use selected machine learning tools and will have the opportunity for practical work with these tools on real environmental data.", "recorded": "2008-03-17T09:00:00", "title": "Analysis of Environmental Data with Machine Learning Methods "}, {"url": "lmcv04_grenoble", "desc": "The aim of the meeting is to encourage a closer interaction between the computer vision community and the machine learning and statistical pattern recognition communities.\n\nMore about the event can be found [[http://pascallin.ecs.soton.ac.uk/Workshops/LMCV04/|here]].", "recorded": "2004-05-03T00:00:00", "title": "Workshop on Pattern Recognition and Machine Learning in Computer Vision, Grenoble 2004"}, {"url": "icml2010_varaquaux_scik", "desc": "Scikits.learn is a Python module integrating classique machine learning algorithmes in the tightly-nit world of scientific Python packages\r\n\r\nIt aims to provide simple and efficient solutions to learning problems that are accessible to everybody and reusable in various contexts: machine-learning as a versatile tool for science and engineering.", "recorded": "2010-06-25T10:10:10", "title": "Scikitlearn"}, {"url": "acml2013_canberra", "desc": " The conference aims at providing a leading international forum for researchers in machine learning and related fields to share their new ideas and achievements.\r\n\r\nFor more informations please visit the [[http://acml2013.conference.nicta.com.au/|ACML 2013 website]].", "recorded": "2013-11-13T09:00:00", "title": "The 5th Asian Conference on Machine Learning (ACML), Canberra 2013"}, {"url": "etvc08_bach_mlakm", "desc": "Kernel methods are a new theoretical and algorithmic framework for machine learning. By representing data through well defined dot-products, referred to as kernels, they allow to use classical linear supervised machine learning algorithms to non linear settings and to non vectorial data. A major issue when applying these methods to image processing or computer vision is the choice of the kernel. I will present recent advances in the design of kernels for images that take into account the natural structure of images.", "recorded": "2008-11-20T12:00:00", "title": "Machine learning and kernel methods for computer vision"}, {"url": "iswc07_grobelnik_wsw", "desc": "The tutorial will cover basic topics from the field of Machine Learning\r\n explained in an intuitive way relevant for Semantic Web researchers and\r\n practitioners. In the first part the topics will cover brief top level\r\n overview of the Machine Learning field, its algorithms, and data types\r\n being analyzed. In the second part we will cover relation to Semantic\r\n Web and Web2.0. In the last part we will perform hands-on exercise with\r\n some of the tools for modeling text semantics and social networks in\r\n analytical way.", "recorded": "2007-11-11T09:00:00", "title": "What Semantic Web researchers need to know about Machine Learning?"}, {"url": "iswc07_mladenic_wsw", "desc": "The tutorial will cover basic topics from the field of Machine Learning\r explained in an intuitive way relevant for Semantic Web researchers and\r practitioners. In the first part the topics will cover brief top level\r overview of the Machine Learning field, its algorithms, and data types\r being analyzed. In the second part we will cover relation to Semantic\r Web and Web2.0. In the last part we will perform hands-on exercise with\r some of the tools for modeling text semantics and social networks in\r analytical way.", "recorded": "2007-11-11T09:30:00", "title": "What Semantic Web researchers need to know about Machine Learning?"}, {"url": "iswc07_fortuna_wsw", "desc": "The tutorial will cover basic topics from the field of Machine Learning\r explained in an intuitive way relevant for Semantic Web researchers and\r practitioners. In the first part the topics will cover brief top level\r overview of the Machine Learning field, its algorithms, and data types\r being analyzed. In the second part we will cover relation to Semantic\r Web and Web2.0. In the last part we will perform hands-on exercise with\r some of the tools for modeling text semantics and social networks in\r analytical way.", "recorded": "2007-11-11T10:30:00", "title": "What Semantic Web researchers need to know about Machine Learning?"}, {"url": "eswc2013_nebhi_nertis", "desc": "Recently, Machine Translation (MT) has become a quite\r\npopular technology in everyday use through Web services such as Google\r\nTranslate. Although the di\u000berent MT approaches provide good results,\r\nnone of them exploit contextual information like named entity to help\r\nuser comprehension.\r\nIn this paper, we present NERITS, a machine translation mashup system\r\nusing semantic annotation from Wikimeta. The goal of the application\r\nis to propose a cross-lingual translation by providing detailed informa-\r\ntion extracted from DBpedia about persons, locations and organizations\r\nin the mother tongue of the user. This helps at scaling the traditional\r\nmultilingual task of machine translation to cross-lingual applications.\r\n\r\nDemonstration: http://cms.unige.ch/lettres/linguistique/nebhi/nerits/", "recorded": "2013-05-28T18:22:27", "title": "NERITS - A Machine Translation Mashup System Using Wikimeta and Linked Open Data"}, {"url": "mlss07_scholkopf_intro", "desc": "", "recorded": "2007-08-20T09:00:29", "title": "Opening of the 9th Machine Learning Summer School"}, {"url": "mlss2010_clark_mlatcs", "desc": "", "recorded": "2010-05-11T13:32:29", "title": "Machine learning and the cognitive science of natural language"}, {"url": "workshops2012_mannor_activity_recognition", "desc": "", "recorded": "2012-03-29T10:35:00", "title": "Machine Learning for the Physical World: A Thematic Programme"}, {"url": "strategymeeting2010_peters_tml", "desc": "", "recorded": "2010-03-17T11:45:00", "title": "Towards Machine Learning of Motor Skills for Robotics"}, {"url": "ecmlpkdd09_vanschoren_acbp", "desc": "", "recorded": "2009-09-08T19:05:00", "title": "A Community-Based Platform for Machine Learning Experimentation"}, {"url": "psm08_gavalda_rpb", "desc": "", "recorded": "2008-01-27T20:00:00", "title": "Report on the 2007 PASCAL Bootcamp in Machine Learning"}, {"url": "translingeu2010_biani_tee", "desc": "", "recorded": "2010-06-07T16:15:00", "title": "The Evolution of the European Machine Translation Programme at the EPO"}, {"url": "icml2010_ben_david_buhmann_hancock_smola_inmgn", "desc": "", "recorded": "2010-06-25T16:00:00", "title": "Is non-(geo)metricity an issue for machine learning?"}, {"url": "aerfaiss08_koehn_pbfs", "desc": "", "recorded": "2008-06-25T08:30:00", "title": "Phrase-based and factored statistical machine translation"}, {"url": "solomon_moens_cavazza_muse_project", "desc": "", "recorded": "2013-10-01T13:00:00", "title": "Machine Understanding for Interactive Storytelling: The MUSE project"}, {"url": "bootcamp2010_eyraud_welcome", "desc": "", "recorded": "2010-07-05T14:00:00", "title": "Welcome PASCAL Bootcamp in Machine Learning 2010"}, {"url": "mlss05us_langford_wciml", "desc": "", "recorded": "2005-05-16T00:00:00", "title": "Welcome to Chicago, and a (brief!) introduction to machine learning"}, {"url": "icgi08_yvon_gi", "desc": "", "recorded": "2008-09-22T14:00:00", "title": "Grammatical Inference: news from the Machine Translation front"}, {"url": "mlss2010_chater_csfml2", "desc": "", "recorded": "2010-05-07T13:33:33", "title": "Cognitive science for machine learning 2: Empirical methods"}, {"url": "lsoldm2012_newnham_real_time_decisions", "desc": "Causata is a big data start-up that is using machine learning to allow clients to make real-time\r\ndecisions on customer data. In this presentation I will outline the business problem we are trying\r\nto solve and some of the challenges implementing a machine learning solution in the real world.", "recorded": "2012-09-18T09:30:00", "title": "Real-time Decisions in the Real World"}, {"url": "colt2013_arora_barrier", "desc": "One of the frustrations of machine learning theory is that many of the underlying algorithmic problems are provably intractable (e.g., NP-hard or worse) or presumed to be intractable (e.g., the many open problems in Valiant's model). This talk will suggest that this seeming intractability may arise because many models used in machine learning are more general than they need to be. Careful reformulation as well as willingness to consider new models may allow progress. We will use examples from recent work: Nonnegative matrix factorization, Learning Topic Models, ICA with noise, etc.\r\n", "recorded": "2013-06-14T14:00:00", "title": "Is Intractability a Barrier for Machine Learning?"}, {"url": "mlcued08_kondor_gtm", "desc": "**Machine Learning Tutorial Lecture** The use of algebraic methods\u2014specifically group theory, representation theory, and even some concepts from algebraic geometry\u2014is an emerging new direction in machine learning. The purpose of this tutorial is to give an entertaining but informative introduction to the background to these developments and sketch some of the many possible applications, including multi-object tracking, learning rankings, and constructing translation and rotation invariant features for image recognition. The tutorial is intended to be palatable by a non-specialist audience with no prior background in abstract algebra.", "recorded": "2007-10-18T16:00:00", "title": "Group Theory and Machine Learning"}, {"url": "ecmlpkdd2010_obozinski_smta", "desc": "Sparse methods such as regularization by the L1-norm has attracted a lot of interest in recent years in statistics, machine learning and signal processing. In the context of least-square linear regression, the problem is usually referred to as the Lasso or basis pursuit. The objective of the tutorial is to give a unified overview of the recent contributions of sparse convex methods to machine learning, both in terms of theory and algorithms. The course will be divided in three parts: in the first part, the focus will be on the regular L1-norm and variable selection, introducing key algorithms and key theoretical results. Then, several more structured machine learning problems will be discussed, on vectors (second part) and matrices (third part), such as multi-task learning, sparse principal component analysis, multiple kernel learning and sparse coding.", "recorded": "2010-09-20T09:00:00", "title": "Sparse methods for machine learning: Theory and algorithms"}, {"url": "nipsworkshops2011_storkey_markets", "desc": "Prediction markets show considerable promise for developing flexible mechanisms for machine learning. Here, machine learning markets for multivariate systems are defined, and a utility-based framework is established for their analysis. This differs from the usual approach of defining static betting functions. It is shown that such markets can implement model combination methods used in machine learning, such as product of expert and mixture of expert approaches as equilibrium pricing models, by varying agent utility functions. They can also implement models composed of local potentials, and message passing methods. Prediction markets also allow for more flexible combinations, by combining multiple different utility functions. Conversely, the market mechanisms implement inference in the relevant probabilistic models. This means that market mechanism can be utilized for implementing parallelized model building and inference for probabilistic modelling.", "recorded": "2011-12-16T08:00:00", "title": "Machine Learning Markets"}, {"url": "nipsworkshops2011_relations_between_ml_problems", "desc": "This workshop focuses on relations between machine learning problems. The idea is that by better understanding how different machine learning problems relate to each other, we will be able to better understand the field as a whole.\r\n\r\nThe idea of a relation is quite general. In includes such notions as reductions between learning problems, but is not restricted to that. Our goal can be explained by an analogy with functional analysis - rather than studying individual functions, functional analysis focusses on the transformations between different functions. This high level of abstraction led to enormous advances in mathematics.\r\n\r\nThe motivation for the workshop is several-fold:\r\n\r\n    * End users typically only care about solving their problem, not the technique used. Many machine learning techniques still require a detaile dunderstanding of their operation in order to use them\r\n    * ML as a service Much modern software is evolving to being delivered via the web as a service. What does it mean  for Machine Learning to be delivered as a service? One question that needs resolving is how to describe what the service does (ideally in a declarative manner). Understanding relations between machine learning problems can be thus seen as analogous to the composition of (Machine Learning) web services\r\n    * Reinvention Many machine learning solutions are reinvented / rediscovered. This is hardly surprising since the focus is often on techniques and not problems. If you can not describe your problem in a manner that others can easily understand and search, then it is hard to figure whether solutions to seemingly new problems already exist.\r\n    * Modularity A feature of mature engineering disciplines is modularity, which has enormous design and economic advantages. Understanding relations between problems seems important to achieve greater modularity.\r\n    * Conceptual simplicity Finally, if one can understand the field using a smaller number of primitives and combination operations, then this has an intrinsic appeal (apply Occam's razor at the meta-level!)\r\n\r\nWorkshop homepage: http://rml.anu.edu.au/", "recorded": "2011-12-16T07:30:00", "title": "Relations between machine learning problems \u2013 an approach to unify the field"}, {"url": "mlsvmlso05_thurnau", "desc": "Many modern machine learning algorithms reduce to solving large-scale linear, quadratic or semi-definite mathematical programming problems. Optimization has thus become a crucial tool for learning, and learning a major application of optimization. Furthermore, a systematic recasting of learning and estimation problems in the framework of mathematical programming has encouraged the use of advanced techniques from optimization such as convex analysis, Lagrangian duality and large scale linear algebra. This has allowed much sharper theoretical analyses, and greatly increased the size and range of problems that can be handled. Several key application domains have developed explosively, notably text and web analysis, machine vision, and speech all fuelled by ever expanding data resources easily accessible via the web.\n\nThis special topic is intended to bring closer optimization and machine learning communities for further algorithmic progress, particularly for developing large-scale learning methods capable of handling massive document and image datasets.\n\nTopics of interest include:\n\n    * Mathematical programming approaches to machine learning problems, like semi-definite programming, interior point methods, sequential convex programming, gradient-based methods, etc.\n    * Optimisation on graphical models for machine learning, belief propagation.\n    * Efficient training of Support Vector Machines, incremental SVMs, optimization over kernels.\n    * Convex relaxations of machine learning problems.\n    * Applications involving large scale databases, such as data mining, bioinformatics, multimedia. \n\nFind out more at the [[http://jmlr.csail.mit.edu/cfp/mllso.html|workshop website]].", "recorded": "2005-03-16T00:00:00", "title": "Workshop on Machine Learning, SVM and Large Scale Optimization, Thurnau 2005"}, {"url": "mda07_mueller_tmla", "desc": "", "recorded": "2007-06-28T16:30:00", "title": "The Machine Learning Approach to Brain-Computer Interfacing - Part 1"}, {"url": "mda07_krauledat_tmla", "desc": "", "recorded": "2007-06-28T17:00:00", "title": "The Machine Learning Approach to Brain-Computer Interfacing - Part 2"}, {"url": "nipsworkshops09_furlanello_mlpp", "desc": "", "recorded": "2009-12-11T15:45:00", "title": "A Machine Learning Pipeline for Phenotype Prediction from Genotype Data"}, {"url": "workshops2012_juan_multimodal_interaction", "desc": "", "recorded": "2012-03-29T09:15:00", "title": "Machine Learning for Multimodal Interaction - MLMI Thematic Programme"}, {"url": "strategymeeting2010_mannor_aru", "desc": "", "recorded": "2010-03-17T16:45:00", "title": "Activity Recognition/User Understanding: A Machine Learning Approach"}, {"url": "ecmlpkdd09_sebag_pcmlbar", "desc": "", "recorded": "2009-09-07T15:45:00", "title": "Panel: Challenges of Machine Learning-Based Abutonomouas Robotics"}, {"url": "mlcued08_briscoe_mlacnlp", "desc": "", "recorded": "2007-11-15T16:00:00", "title": "Machine Learning Applications / Challenges in Natural Language Parsing"}, {"url": "clsp_huang_forest", "desc": "", "recorded": "2008-04-29T13:00:00", "title": "Forest-Based Search Algorithms in Parsing Machine Translation"}, {"url": "workshops2012_lever_cognitive_arhitectures", "desc": "", "recorded": "2012-03-27T16:00:00", "title": "Composite learning systems using machine learning primitives"}, {"url": "metaforum2011_renals_moses", "desc": "", "recorded": "2011-06-27T11:45:00", "title": "Moses: The Success of an Open-Source Machine Translation Platform"}, {"url": "bootcamp07_balcazar_intro", "desc": "", "recorded": "2007-07-02T09:00:00", "title": "Introduction and Welcome to the PASCAL Bootcamp in Machine Learning 2007"}, {"url": "mlss2010_chater_csfml", "desc": "", "recorded": "2010-05-06T13:33:33", "title": "Cognitive science for machine learning 1:What is cognitive science?"}, {"url": "mlmi06_chen_mspml", "desc": "", "recorded": "2006-05-02T00:00:00", "title": "Content Analysis: Marriage of Signal Processing and Machine Learning"}, {"url": "icml2010_sonnenburg_braun_wao", "desc": "", "recorded": "2010-06-25T09:04:00", "title": "Welcome and Overview of the Workshop on Machine Learning Open Source"}, {"url": "icmi05_bengio_tsmla", "desc": "", "recorded": "2005-10-07T00:00:00", "title": "Tutorial on Statistical Machine Learning with Applications to Multimodal Processing"}, {"url": "translingeu2010_forcada_fos", "desc": "", "recorded": "2010-06-07T15:15:00", "title": "Free/Open-Source Machine Translation: The Apertium Platform"}, {"url": "stanfordcs229f08_ng_lec19", "desc": "Advice for Applying Machine Learning, Debugging Reinforcement Learning (RL) Algorithm, Linear Quadratic Regularization (LQR), Differential Dynamic Programming (DDP), Kalman Filter & Linear Quadratic Gaussian (LQG), Predict/update Steps of Kalman Filter, Linear Quadratic Gaussian (LQG)", "recorded": "2009-04-17T10:00:00", "title": "Lecture 19 - Advice for Applying Machine Learning "}, {"url": "mlss2012_lapalma", "desc": "The school addresses the following topics: Learning Theory, Kernel Methods, Bayesian Machine learning, Monte Carlo Methods , Bayesian Nonparametrics, Optimization, Graphical Models, Information theory and Dimensionality Reduction.\r\n\r\nDetailed information can be found [[http://mlss2012.tsc.uc3m.es/|here]].", "recorded": "2012-04-11T09:00:00", "title": "Machine Learning Summer School (MLSS), La Palma 2012"}, {"url": "mlss2011_bordeaux", "desc": "The school provides tutorials and practical sessions on basic and advanced topics of machine learning by leading researchers in the field. The summer school is intended for students, young researchers and industry practitioners with an interest in machine learning and a strong mathematical background.\r\n\r\nThe school addresses the following topics: Learning Theory, Bayesian inference, Monte Carlo Methods, Sparse Methods, Reinforcement Learning, Robot Learning, Boosting, Kernel Methods, Bayesian Nonparametrics, Convex Optimization and Graphical Models.\r\n\r\nDetailed information can be found at [[http://mlss11.bordeaux.inria.fr/|the summer school homepage]].", "recorded": "2011-09-04T09:00:00", "title": "Machine Learning Summer School (MLSS ), Bordeaux 2011"}, {"url": "aedml08_dzeroski_mle", "desc": "", "recorded": "2008-03-19T09:00:00", "title": "Machine learning and environmental/ecological modeling and An introduction to equation discovery"}, {"url": "workshops2012_herbster_data_dependent", "desc": "", "recorded": "2012-03-29T15:30:00", "title": "Data-Dependent Geometries and Structures: Analyses and Algorithms for Machine Learning"}, {"url": "pmsb06_ratsch_icega", "desc": "", "recorded": "2006-06-17T00:00:00", "title": "Improving the Caenorhabditis elegans Genome Annotation using Machine Learning"}, {"url": "icml05_fortuna_tuo", "desc": "", "recorded": "2005-08-07T00:00:00", "title": "The use of machine translation tools for cross-lingual text-mining "}, {"url": "nipsworkshops2013_sohn_image_labeling", "desc": "", "recorded": "2013-12-09T17:30:00", "title": "Augmenting CRFs with Boltzmann Machine Shape Priors for Image Labeling"}, {"url": "icml08_zaanen_chjsb", "desc": "", "recorded": "2008-07-09T15:46:17", "title": "Chorale Harmonization in the Style of J.S. Bach, A Machine Learning Approach"}, {"url": "mlss2010_griffiths_csfml3", "desc": "", "recorded": "2010-05-08T14:16:33", "title": "Cognitive science for machine learning 3: Models and theories in cognitive science"}, {"url": "mlss2010_scholkopf_mlfcs3", "desc": "", "recorded": "2010-05-08T09:04:00", "title": "Machine learning for cognitive science 3: Kernel methods and Bayesian methods"}, {"url": "oh06_szedmak_htsvm", "desc": "", "recorded": "2006-07-13T00:00:00", "title": "How to Teach Support Vector Machine to Learn Vector Outputs"}, {"url": "ssms08_hauptmann_mlpr", "desc": "", "recorded": "2008-09-01T09:00:00", "title": "Machine Learning, Pattern Recognition, Cross-modal analysis and fusion"}, {"url": "w3cworkshop2013_morgan_web", "desc": "Today's public organizations and institutions are faced with creating information for a digital world: information that is fluid rather than static, highly customized for individual needs, and available on-demand across multiple channels and geographies. As content volumes increase, new ways of delivering multilingual information without overstretching translation budgets must be found. Machine translation combined with human post-editing is an innovative new approach to help overcome these challenges, but can it really deliver the level of quality required for multilingual web content? Calling on real-life examples from some of the world leading private sector companies, this presentation will demonstrate how integrated machine translation and post-editing is already in use to considerably increase the amount of multilingual information that is being published on the web, without compromising on quality. Referencing SDL's automated translation survey 2010, ran in conjunction with the European Association of Machine Translation and the American Machine Translation Association, the presentation will also consider trends in the acceptance of machine translation combined with post-editing.", "recorded": "2013-03-12T16:16:00", "title": "How do you publish one thousand web pages, in 12 languages, at a high quality, 50% quicker than you can today?"}, {"url": "nipsworkshops2012_discrete_optimization", "desc": "Optimization problems with discrete solutions (e.g.,\r\ncombinatorial optimization) are becoming increasingly\r\nimportant in machine learning. The core of statistical\r\nmachine learning is to infer conclusions from data, and\r\nwhen the variables underlying the data are discrete,\r\nboth the tasks of inferring the model from data, as well\r\nas performing predictions using the estimated model are\r\ndiscrete optimization problems. Two factors complicate\r\nmatters: first, many discrete problems are in general\r\ncomputationally hard, and second, machine learning\r\napplications often demand solving such problems at very\r\nlarge scales.\r\n\r\nThe focus of this year\u2019s workshop lies on structures that\r\nenable scalability. Examples of important structures include\r\nsparse graphs, the marginal polytope, and submodularity.\r\nWhich properties of the problem make it possible to still\r\nefficiently obtain exact or decent approximate solutions?\r\nWhat are the challenges posed by parallel and distributed\r\nprocessing? Which discrete problems in machine learning\r\nare in need of more scalable algorithms? How can we\r\nmake discrete algorithms scalable while retaining quality?\r\nSome heuristics perform well but as of yet are devoid of a\r\ntheoretical foundation; what explains such good behavior?\r\n\r\nWorkshop homepage: http://discml.cc/", "recorded": "2012-12-07T07:45:00", "title": "Discrete Optimization in Machine Learning"}, {"url": "wapa2010_farinella_vpr", "desc": "Computer vision researchers are increasingly using algorithms from pattern recognition and machine learning to help build robust and reusable vision systems that act taking into account the visual content of images and videos. Just as learning is an essential component of biological visual systems, the design of machine vision systems that learn and adapt represent an important challenge in modern computer vision research. In this seminar I will focus on the task of recognition in computer vision by explaining how, why and where the techniques of pattern recognition and machine learning are used as essential ingredients to represent and recognize visual patterns in different application contexts.", "recorded": "2010-09-02T14:02:00", "title": "Visual Pattern Recognition"}, {"url": "nipsworkshops2011_reid_anatomy", "desc": "In order to relate machine learning problems we argue that we need to be able to articulate what is meant by a single machine learning problem. By attempting to name the various aspects of a learning problem we hope to clarify ways in which learning problems might be related to each other. We tentatively put forward a proposal for an anatomy of learning problems that will serve as scaffolding for posing questions about relations. After surveying the way learning problems are discussed in a range of repositories and services. We then argue that the terms used to describe problems to better understand a range of viewpoints within machine learning ranging from the theoretical to the practical.", "recorded": "2011-12-16T17:00:00", "title": "Anatomy of a Learning Problem"}, {"url": "uai2012_macready_quantum_annealing", "desc": "Quantum Computing offers the theoretical promise of dramatically faster computation through direct utilization of the underlying quantum aspects of reality. This idea, first proposed in the early 1980s, exploded in interest in 1994 with Peter Shor's discovery of a polynomial time integer factoring algorithm. Today the first experimental platforms realizing small-scale quantum algorithms are becoming commonplace. Interestingly, machine learning may be the \"killer app\" for quantum computing. We will introduce quantum algorithms, with focus on a recent quantum computational model that will be familiar to researchers with a background in graphical models. We will show how a particular quantum algorithm -- quantum annealing -- running on current quantum hardware can be applied to certain optimization problems arising in machine learning. In turn, we will describe a number of challenges to further progress in quantum computing, and suggest that machine learning researchers may be well-positioned to drive the first real-world applications of quantum annealing. ", "recorded": "2012-08-15T13:30:00", "title": "Quantum Annealing meets Machine Learning"}, {"url": "iswc2012_henson_constrained_devices", "desc": "The primary challenge of machine perception is to define efficient computational methods to derive high-level knowledge from low-level sensor observation data. Emerging solutions are using ontologies for expressive representation of concepts in the domain of sensing and perception, which enable advanced integration and interpretation of heterogeneous sensor data. The computational complexity of OWL, however, seriously limits its applicability and use within resource-constrained environments, such as mobile devices. To overcome this issue, we employ OWL to formally define the inference tasks needed for machine perception \u2013 explanation and discrimination \u2013 and then provide efficient algorithms for these tasks, using bit-vector encodings and operations. The applicability of our approach to machine perception is evaluated on a smart-phone mobile device, demonstrating dramatic improvements in both efficiency and scale.", "recorded": "2012-11-14T16:30:53", "title": "An Efficient Bit Vector Approach to Semantics-based Machine Perception in Resource-Constrained Devices"}, {"url": "ocwc2014_despiegelaere_my_machine", "desc": "Oskar is an eight year old boy with a mission: to build a machine to find and dig up treasures in his garden. Because, so he wonders, what if an ancient Roman treasure is hidden under the grass? Just think about it, what if he and his family moved away and they hadn\u2019t searched for the treasure? What if another family moved in who immediately would find the treasure? Wouldn\u2019t that be awful?\r\nSo Oskar invented a \u201cLook-For-Treasures-And-Dig-Them-Up-Machine\u201d. He thought about what the machine should be able to do, and made a beautiful drawing showing how it would look like. He even wrote a user manual. And now he wants his machine to be built as well. But how? His father is all thumbs (at least that\u2019s what his mother always says.). So what can he do?", "recorded": "2014-04-25T13:30:00", "title": "MyMachine"}, {"url": "machine_fraser_machine_learning", "desc": "", "recorded": "2013-04-11T15:22:00", "title": "Machine Learning in the Statistical Natural Language Processing Group of the University of Stuttgart"}, {"url": "icml07_panel_mltn", "desc": "", "recorded": "2007-06-21T10:40:00", "title": "ILP Invited Panel - Structured Machine Learning: The Next 10 Years"}, {"url": "bbci09_kincses_muller_bnbm", "desc": "", "recorded": "2009-07-10T14:30:45", "title": "Brain@Work: Neurotechnology-Based Man-Machine Interaction for Industrial Applications"}, {"url": "metaforum2011_hajic_meta", "desc": "", "recorded": "2011-06-27T12:00:00", "title": "META-RESEARCH: Leading-Edge Machine Translation Research for Multilingual Europe"}, {"url": "mlss2010_scholkopf_mlfcs2", "desc": "", "recorded": "2010-05-07T09:04:00", "title": "Machine learning for cognitive science 2: Bayesian methods and statistical learning theory"}, {"url": "mlas06_xing_iex", "desc": "Statistical machine learning theory and applications in computational biology are the **fields of interest of Eric Xing**. We asked him at this interview at Carnegie Mellon University what actually is statistical machine learning theory, where can solutions of this research be applied and if **being in the first ML Department** in the world does make his research any easier.", "recorded": "2006-09-26T00:00:00", "title": "Interview with Eric Xing"}, {"url": "mlmi06_washington", "desc": "MLMI is a joint workshop that brings together researchers from the different communities working on the common theme of advanced machine learning algorithms for processing and structuring multimodal human interaction. The motivation for creating this multi-disciplinary workshop arose from an actual need in several of the sponsoring projects.", "recorded": "2006-05-01T00:00:00", "title": "3rd Joint Workshop on Multimodal Interaction and Related Machine Learning Algorithms, Washington 2006"}, {"url": "snnsymposium2010_gavrila_ltsp", "desc": "The ability to recognize humans and their activities by vision is key for a machine to interact intelligently and effortlessly with a human-inhabited environment. This talk covers recent research at Daimler R&D and the Univ. of Amsterdam on the topic of \"Looking at People\"; I cover applications in the intelligent vehicle and smart surveillance domains, and emphasize the central role that machine learning plays herein. ", "recorded": "2010-11-17T15:30:00", "title": "Learning to see people"}, {"url": "machine_learning_video_tutorials_vol1", "desc": "Welcome to the first issue of the Journal of Machine Learning Video Tutorials. The tutorial constituting our inaugural issue consists of the invited lecture delivered by Prof Leslie P Kaelbling at the EMLC PKDD 2010 in Barcelona. Prof Kaelbling is an internationally recognized research leader in machine learning and artificial intelligence at large; in her tutorial, she shares with us her view of a research agenda geared towards the design of general-purpose agents that go beyond the current state of the art in their ability to act intelligently. Thus, follow the link, and enjoy this excellent tutorial! \r\n\r\nJose L. Balcazar, University of Cantabria", "recorded": "2011-06-08T12:22:32", "title": "Journal of Machine Learning Video Tutorials - Volume 1"}, {"url": "icml08_catanzaro_fsvm", "desc": "Recent developments in programmable, highly parallel Graphics Processing Units (GPUs) have enabled high performance implementations of machine learning algorithms. We describe a solver for Support Vector Machine training, using Platt's Sequential Minimal Optimization algorithm and an adaptive first and second order working set selection heuristic, which achieves speedups of 9-35x over LIBSVM running on a traditional processor. We also present a GPU-based system for SVM classification which achieves speedups of 81-138x over LibSVM (5-24x over our own CPU-based SVM classifier).", "recorded": "2008-07-08T14:50:00", "title": "Fast Support Vector Machine Training and Classification on Graphics Processors"}, {"url": "nipsworkshops09_optimization", "desc": "**Optimization for Machine Learning**\r\n\r\nIt is fair to say that at the heart of every machine learning algorithm is an optimization problem. It is only recently that this viewpoint has gained significant following. Classical optimization techniques based on convex optimization have occupied center-stage due to their attractive theoretical properties. But, new non-smooth and non-convex problems are being posed by machine learning paradigms such as structured learning and semi-supervised learning. Moreover, machine learning is now very important for real-world problems which often have massive datasets, streaming inputs, and complex models that also pose significant algorithmic and engineering challenges. In summary, machine learning not only provides interesting applications but also challenges the underlying assumptions of most existing optimization algorithms. Therefore, there is a pressing need for optimization \"tuned\" to the machine learning context. For example, techniques such as non-convex optimization (for semi-supervised learning), combinatorial optimization and relaxations (structured learning), non-smooth optimization (sparsity constraints, L1, Lasso, structure learning), stochastic optimization (massive datasets, noisy data), decomposition techniques (parallel and distributed computation), and online learning (streaming inputs) are relevant in this setting. These techniques naturally draw inspiration from other fields, such as operations research, theoretical computer science, and the optimization community. Motivated by these concerns, we would like to address these issues in the framework of this workshop.\r\n----\r\nThe Workshop homepage can be found at http://opt.kyb.tuebingen.mpg.de/\r\n----", "recorded": "2009-12-12T07:30:00", "title": "Optimization"}, {"url": "mlcs07_london", "desc": "Language acquisition and processing has been one of the central research issues in cognitive science. It is also an area in which the use of cognitive computational modelling has been especially intense. Language, and especially language acquisition, has been the key battleground for nativists and empiricists; and between advocates of rule-based, probabilistic, and connectionist models of thought. Yet the computational models proposed by CogSci researchers are often far behind, in scale and accuracy, the non-cognitively motivated models proposed by computational linguists, which are heavily based on machine learning techniques.\r\n\r\nThis workshop asks how far these techniques, and their theoretical underpinnings, provide tools for building richer theories of cognitive processes. For example, can powerful machine learning techniques (e.g. kernel methods) help build models of the cognitive operations involved in human language acquisition? Conversely, can insights from cognitive science help inform and focus computational linguistic and machine learning? Can evidence concerning the spectacular computational performance of the human language processor help inspire new generations of computational linguistic and machine learning tools?\r\n\r\nThis workshop brings together participants from all of the disciplines that address this problem to discuss a range of related topics from methodological issues in computational modelling of language acquisition, including evaluation of empirical learning models, to technical problems in machine learning and grammatical inference. The workshop includes invited talks by some of the leading researchers in these fields.", "recorded": "2007-06-21T09:00:00", "title": "Workshop on Machine Learning and Cognitive Science of Language Acquisition, London 2007"}, {"url": "icml09_natrajan_pml", "desc": "", "recorded": "2009-06-18T11:15:00", "title": "The Parallel Machine Learning (PML) Framework and Numerical Aspects of the Transform Regression Algorithm"}, {"url": "mlas06_xing_imlot", "desc": "", "recorded": "2006-09-26T00:00:00", "title": "Introduction to the Machine Learning over Text & Images - Autumn School by Eric Xing"}, {"url": "pcw05_ireson_grc", "desc": "", "recorded": "2005-04-13T00:00:00", "title": "Pascal Challenge on Evaluating Machine Learning for Information Extraction: Goals, Results and Conclusions"}, {"url": "aerfaiss08_honkela_aul", "desc": "", "recorded": "2008-06-26T08:30:00", "title": "Applying unsupervised learning in creating language models for information retrieval and machine translation"}, {"url": "wapa2010_okita_gbtp", "desc": "", "recorded": "2010-09-01T18:28:00", "title": "Gap Between Theory and Practice: Noise Sensitive Word Alignment in Machine Translation"}, {"url": "nipsworkshops2013_sheldon_bird_migration", "desc": "", "recorded": "2013-12-10T11:30:00", "title": "BirdCast: Novel Machine Learning Methods for Understanding Continent-Scale Bird Migration"}, {"url": "ecml03_adriaans_fkbs", "desc": "", "recorded": "2003-09-24T17:45:00", "title": "From Knowledge-based Systems to Skill-based Systems: Sailing as a Machine Learning Challenge"}, {"url": "pcw06_denoyer_mxdbg", "desc": "", "recorded": "2006-04-11T00:00:00", "title": "Mining XML documents - Bridging the gap between Machine Learning and Information Retrieval"}, {"url": "oh06_canisius_csanl", "desc": "", "recorded": "2006-07-11T00:00:00", "title": "Machine Learning for Sequential Data: A Comparative Study with Applications to Natural Language Processing"}, {"url": "mlss05au_graepel_mlg", "desc": "The course gives an introduction to the application of machine learning techniques to games. The course will consist of two parts, part I dealing with computer/video games, part II dealing with traditional board/strategy games. Alongside, I will introduce necessary background material including aspects of neural networks, reinforcement learning, and graphical models. 1. In recent years various aspects of computer games have been developed to near perfection. These include high-performance graphics, realistic surround sound, and detailed physical simulations. However, the control of non-player characters (NPCs), also known as game AI, has fallen behind to the point that the resulting gaming experience often suffers. Machine learning offers a framework for making NPCs adaptive to both the environment and the human player. This technology has therefore the potential to greatly enhance gaming experience. Furthermore, at development time machine learning techniques can be employed to automate the creation of (intelligent) NPC behavior, thereby replacing the current standard of scripting and trial-and-error. The examples presented include imitation learning for avatars and reinforcement learning in fighting games. 2. Classical board games such as Chess, Go, and Backgammon have been a traditional theme in artificial intelligence. While chess has essentially been solved by traditional AI approaches, world-class Backgammon engines could only be developed based on machine learning techniques, originally in the combination of neural networks and reinforcement learning. For the traditional board game Go, neither of the two approaches has been successful so far. In this part of the course I will explain and discuss the machine learning approach to Backgammon. I will then give an introduction to the game of Go and discuss what machine learning may be able to contribute to the field of computer Go with a particular focus on modeling the uncertainty that emerges from the game's overwhelming complexity.", "recorded": "2005-01-31T00:00:00", "title": "Machine Learning for Games"}, {"url": "mlss08au_hutter_isml", "desc": "The first part of his tutorial provides a brief overview of the fundamental methods\r and applications of statistical machine learning.\r The other speakers will detail or built upon this introduction.\r \r Statistical machine learning is concerned with\r the development of algorithms and techniques that learn from\r observed data by constructing stochastic models that can be used for\r making predictions and decisions.\r \r Topics covered include Bayesian inference and maximum likelihood\r modeling; regression, classification, density estimation,\r clustering, principal component analysis; parametric,\r semi-parametric, and non-parametric models; basis functions, neural\r networks, kernel methods, and graphical models; deterministic and\r stochastic optimization; overfitting, regularization, and\r validation.", "recorded": "2008-03-03T08:40:45", "title": "Introduction to Statistical Machine Learning"}, {"url": "bark08_ghahramani_samlbb", "desc": "I'll present some thoughts and research directions in Bayesian machine learning. I'll contrast black-box approaches to machine learning with model-based Bayesian statistics. Can we meaningfully create Bayesian black-boxes? If so what should the prior be? Is non-parametrics the only way to go? Since we often can't control the effect of using approximate inference, are coherence arguments meaningless? How can we convert the pagan majority of ML researchers to Bayesianism? If the audience gets bored of these philosophical musings, I will switch to talking about our latest technical work on Indian buffet processes.", "recorded": "2008-09-05T21:00:00", "title": "Should all Machine Learning be Bayesian? Should all Bayesian models be non-parametric?"}, {"url": "sumosummerschool2013_ohrid", "desc": "Within the project [[http://www.sumoproject.eu/|Super modeling by combining imperfect \r\nmodels]] funded by the ICT FET Open program of the EU, experts from non-linear \r\ndynamics, machine learning, and climate science will give \r\nlectures on a novel concept of improved modeling from three \r\ndifferent perspectives (nonlinear dynamics, machine learning, \r\nand climate modeling). This concept has been applied to \r\nimprove climate change projection. The concept is expected \r\nto be also applicable in problem areas other than climate \r\nmodeling where a small number of alternative models exist of \r\nthe same real-world complex system.", "recorded": "2013-09-01T09:00:00", "title": "SUMO Summer School on Non-Linear Dynamics Machine Learning and Climate Modeling, Ohrid 2013"}, {"url": "acl07_koehn_smt", "desc": "Statistical machine translation has matured in recent years into a viable challenge to traditional, more knowledge-driven methods, with a energetic research community and commercial interest. This tutorial serves as a basic introduction to the basic principles of current statistical machine translation methods, present the latest research in the field and point to the challenges and ideas beyond.\n\nSpecifically, the tutorial covers: * Foundations: available data and tools, generatice modeling, EM training, phrase-based models, evaluation. * Advanced Methods: log-linear models, discriminative training, specialized models, integration with speech * Outlook: Syntax-based and syntax-aided approaches", "recorded": "2006-04-04T08:00:00", "title": "Statistical Machine Translation: The Basic, the Novel, and the Speculative"}, {"url": "opt08_bottou_lsml", "desc": "The presentation stresses important differences between machine learning and conventional optimisation approaches and proposes some solutions. The first part discusses the the interaction of two kind of asympotic properties: those of the statistics and those of optimization algorithm. Unlikely optimization algorithm such as stochastic gradient show amazing performance for large-scale machine learning problems. The second part shows how the deeper causes of this performance suggests the theoretical possibility learn large-scale problems with a single pass over the data. Practical algorithms will be discussed: various second order stochastic gradients, averaging methods, dual methods with data reprocessing...", "recorded": "2008-12-12T15:38:55", "title": "Large-scale Machine Learning and Stochastic Algorithms"}, {"url": "aibootcamp2011_quinn_iml", "desc": "This talk gives an overview of machine learning from a practical perspective. Starting with examples of problems we might want to solve (in vision, signal processing, and geospatial inference), and the assumptions we have to make in order to get anywhere, it then covers a number of different supervised and unsupervised learning techniques. The talk concludes with ideas on how to evaluate a system, and when we should believe that a model is \"right\".", "recorded": "2011-02-17T00:09:00", "title": "Introduction to Machine Learning"}, {"url": "bsciw08_domingos_mlwuv", "desc": "Machine learning and the Web are a technology and an application area made for each other. The Web provides machine learning with an ever-growing stream of challenging problems, and massive data to go with them: search ranking, hypertext classification, information extraction, collaborative filtering, link prediction, ad targeting, social network modeling, etc. Conversely, seemingly just about every conceivable machine learning technique has been applied to the Web. Can we make sense of this vast jungle of techniques and applications? Instead of attempting an (impossible) exhaustive survey, I will instead try to distill a unified view of the field from our experience to date. By using the language of Markov logic networks - which has most of the statistical models used on the Web as special cases - and the state-of-the-art learning and inference algorithms for it, we will be able to cover a lot of ground in a short time, understand the fundamental structure of the problems and solutions, and see how to combine them into larger systems.", "recorded": "2008-12-13T09:30:00", "title": "Machine Learning for the Web: A Unified View"}, {"url": "mlss06au_canberra", "desc": "This school is suitable for all levels, both for people without previous knowledge in Machine Learning, and those wishing to broaden their expertise in this area. It will allow the participants to get in touch with international experts in this field. Exchange of students, joint publications and joint projects will result because of this collaboration. \\\\ For a research student, the summer school provides a unique, high-quality, and intensive period of study. It is ideally suited for students currently pursuing, or intending to pursue, research in Machine Learning or related fields. Limited scholarships are available for students to cover accommodation and registration costs. If funds are available partial travel support might also be provided. \\\\ IT professionals who use Machine Learning will find that the summer school provides relevant knowledge and exposure to contemporary techniques. In addition, they will benefit by direct interaction with top-notch researchers and knowledge workers. Previous experience indicates that personnel from both the industry as well as national laboratories like CSIRO, DSTO benefit immensely from the school. \\\\ For academics, the summer school is an excellent opportunity to help getting started in research on novel topics in Machine Learning. It provides an ideal forum for networking and discussions. Academics will also benefit from interaction with IT professionals which will lead to a deeper understanding of real life problems.", "recorded": "2006-02-06T00:00:00", "title": "Machine Learning Summer School (MLSS), Canberra 2006"}, {"url": "is2012_muggleton_ai_developement", "desc": "During the centennial year of his birth Alan Turing (1912-1954) has been widely celebrated as having laid the foundations for Computer Science, Automated Decryption, Systems Biology and the Turing Test. In this talk we investigate Turing's motivations and expectations for the development of Machine Intelligence, as expressed in his 1950 article in Mind. We show that many of the trends and developments within AI over the last 50 years were foreseen in this foundational paper. In particular, Turing not only describes the use of Computational Logic but also the necessity for the development of Machine Learning in order to achieve human-level AI within a 50 year time-frame. His description of the Child Machine (a machine which learns like an infant) dominates the closing section of the paper, in which he provides suggestions for how AI might be achieved. Turing discusses three alternative suggestions which can be characterised as: 1) AI by programming, 2) AI by ab initio machine learning and 3) AI using logic, probabilities, learning and background knowledge. He argues that there are inevitable limitations in the first two approaches and recommends the third as the most promising. We compare Turing's three alternatives to developments within AI, and conclude with a discussion of some of the unresolved challenges he posed within the paper.", "recorded": "2012-10-11T10:15:15", "title": "Alan Turing and the Development of Artificial Intelligence"}, {"url": "mlpmsummerschool2013_rosen_zvi_drugs_efficacy", "desc": "In this talk we focus on potential contribution of machine learning methods to healthcare and focus on the somewhat new trend called real world evidence or post launch monitoring. We review the machine learning paradigm of supervised learning. The value of ensemble methods and generative and discriminative prediction algorithms is discussed. Examples of how these methods can be utilized for making better informed medical decision will be reviewed. Finally, the talk includes results of studies performed on longitudinal patients' data of patients from three different disease areas. 1. Studies of data of 50K European HIV patients. 2. Studies of American diabetic patients and 3. An analysis of more than 1M epilepsy patients.", "recorded": "2014-04-16T10:16:31", "title": "Machine learning techniques for predicting complications and evaluating drugs efficacy"}, {"url": "mlss07_kohlbacher_ltsbp", "desc": "We demonstrate the application of machine learning methods to problems from biology, chemistry, and pharmacy, nameley the prediction of protein subcellular localization, prediction of chromatiographic separation of oligo nucleotides, and the prediction of percutaneous drug absorption. For these examples, we show how translating the primary data into problem-specific features is essential for solving classification and regression problems.\r ", "recorded": "2007-08-23T20:30:59", "title": "Lost in Translation -- Solving biological problems with machine learning"}, {"url": "mlss05au_canberra", "desc": "Machine Learning is a foundational discipline of the Information Sciences. It combines deep theory from areas as diverse as Statistics, Mathematics, Engineering, and Information Technology with many practical and relevant real life applications. The aim of the summer school is to cover the entire spectrum from theory to practice. It is mainly targeted at research students, IT professionals, and academics from all over the world.", "recorded": "2005-01-24T00:00:00", "title": "Machine Learning Summer School (MLSS), Canberra 2005"}, {"url": "mlss06tw_ratsch_sfpum", "desc": "Accurate ab initio gene finding is still a major challenge in computational biology. We employ state-of-the-art machine learning techniques based on Hidden Semi-Markov-SVMs to assay and improve the accuracy of genome annotations. We applied our system, called mSplicer, on the Caenorhabditis elegans genome and were able to drastically improve its annotation.", "recorded": "2006-08-01T00:00:00", "title": "Splice form prediction using Machine Learning"}, {"url": "acl2013_rosa_deepfix", "desc": "", "recorded": "2013-08-06T17:25:00", "title": "Deepfix: Statistical Post-editing of Statistical Machine Translation Using Deep Syntactic Analysis"}, {"url": "smartdw09_kaariainen_ssmt", "desc": "", "recorded": "2009-05-13T12:10:00", "title": "Sinuhe - Statistical Machine Translation with a Globally Trained Conditional Exponential Family Translation Model"}, {"url": "mlws04_lowe_mluii", "desc": "//`The only difference between a probabilistic classical world and the equations of the quantum world is that somehow or other it appears as if the probabilities would have to go negative ... that's the fundamental problem. I don't know the answer to it, but I wanted to explain that if I try my best to make the equations look as near as possible to what would be imitable by a classical probabilistic computer, I get into trouble'// \r\n These are the words of Richard Feynman in a famous keynote talk on Simulating Physics with Computers. He was pointing out that we have to face an intrinsic conceptual difficulty if we want to understand the world through mimicking its behaviour with computational systems. Actually, we do not have to go as esoteric as quantum physics. We see some of the same issues in Machine Learning and inference from probabilistic estimators in data-driven modelling. And in the same way that Feynman did not know the resolution to his problem, we are only just starting to become aware of some of our own problems in machine intelligence. The principled approach to Machine Intelligence that we have now come to accept is through a probabilistic viewpoint. The Bayesian view of inference is a subjective one and our knowledge of the universe derives from observation. But I will argue that the use of Machine Learning to represent or simulate the universe only allows generically non-positive probabilities! Of course, we can fudge some of the more uncomfortable aspects that some of these issues raise, but it still should make us think about whether we have got the correct working framework. In this talk I want to question parts of our working machinery we use in Machine Learning. At its heart I want to challenge the assumption that probabilities have to be positive. I want to give several arguments, descriptive and formal, to indicate why the use of positive probabilities is an ideal which is both overly restrictive and unrealisable. Indeed I will argue that the use of non-positive `probabilities' is both inevitable and natural. To do this I will need to use some old mathematical ideas from classical statistics and some more modern ideas from information theory. I will use some simple examples and proofs from Machine Learning applied to regression and classification tasks, and draw parallels with some basic quantum theory ideas. The core of the argument is that in modelling the universe through Machine Learning, we are obliged to make inferences based on finite and hence typically less-than-complete information. We can never know everything about a situation, and this gives us our link between quantum mechanics and statistical inference through machine learning. I will try to make a case that inference through any finite data-driven computation leads to this apparent problem with `probabilities'. So the issue is not just connected with quantum mechanics, but is a more generic problem related to trying to simulate even classical probabilities by Machine Learning ideas. If we have enough time, I will also discuss the consequences of this for information measures such as Entropy, and make the case for Fisher Information being a more appropriate measure for our state of knowledge about a system instead.", "recorded": "2004-09-07T11:30:00", "title": "Machine Learning, Uncertain Information, and the Inevitability of Negative `Probabilities'"}, {"url": "icml2010_schaul_pyb", "desc": "PyBrain is a versatile machine learning library for Python. Its goal is to provide flexible, easy-to-use yet still powerful algorithms for machine learning tasks, including a variety of predefined environments and benchmarks to test and compare algorithms. Implemented algorithms include Long Short-Term Memory (LSTM), policy gradient methods, (multidimensional) recurrent neural networks and evolution strategies like CMA-ES, NES or FEM.", "recorded": "2010-06-25T14:53:17", "title": "PyBrain"}, {"url": "mlss03_tipping_pp", "desc": "The aim of this course is two-fold: to convey the basic principles of Bayesian machine learning and to describe a practical implementation framework. Firstly, we will give an introduction to Bayesian approaches, focussing on the advantages of probabilistic modelling, the concept of priors, and the key principle of marginalisation. Secondly, we will exploit these ideas to realise practical algorithms for sparse linear regression and classification, as exemplified by models such as the \"relevance vector machine\".", "recorded": "2003-08-07T11:00:00", "title": "Bayesian Inference: Principles and Practice"}, {"url": "ecmlpkdd2010_panel", "desc": "A panel on the future research challenges and opportunities in data mining and machine learning from leading experts in industry.", "recorded": "2010-09-24T16:02:00", "title": "Panel"}, {"url": "newtonschw03s08_shawe_taylor_siml", "desc": "**Homepage Link**\r\n* http://www.newton.ac.uk/programmes/SCH/seminars/062510001.html", "recorded": "2008-06-25T10:00:00", "title": "Sparsity in machine learning: approaches and analyses"}, {"url": "w3cworkshop2012_honkela_semantic_resources", "desc": "", "recorded": "2012-03-16T11:30:00", "title": "Semantic Resources and Machine Learning for Quality, Efficiency and Personalisation of Accessing Relevant Information over Language Borders"}, {"url": "mlsb07_kontos_mlt", "desc": "", "recorded": "2007-09-24T16:00:00", "title": "Machine Learning Techniques to Identify Putative Genes Involved in Nitrogen Catabolite Repression in the Yeast Saccharomyces cerevisiae "}, {"url": "mlss07_tuebingen", "desc": "Machine Learning is a foundational discipline of the Information Sciences. It combines theory from areas as diverse as Statistics, Mathematics, Engineering, and Information Technology with many practical and relevant real life applications. The aim of the summer school is to cover the entire spectrum from theory to practice. It is mainly targeted at research students, academics, and IT professionals from all over the world. The program will feature introductory courses at the beginning to provide basic working knowledge of Machine Learning. Building on this introductory material, advanced topics will be covered progressively over the duration of the school. Subjects will be covered both in lectures (4-6 per topic) and in practical courses (where students will have the chance to implement methods for themselves); and are taught by world experts in their fields.\\\\", "recorded": "2007-08-20T09:00:00", "title": "Machine Learning Summer School (MLSS), T\u00fcbingen 2007"}, {"url": "cmulls08_palatucci_lpb", "desc": "Functional Magnetic Resonance Imaging (fMRI) has given neuroscientists and cognitive psychologists incredible power to analyze the deep mysteries of the human brain. With this powerful imaging technology, however, many new challenges have arisen for the statistics and machine learning communities. In this talk, I will present an overview of fMRI and some of the current machine learning challenges. I will discuss recent work on hierarchical Bayesian methods for dealing with high dimensional, sparse data. I will also discuss the application of classical order statistics to the problem of feature selection. Finally, I will show some of our latest results combining a large text corpus with fMRI to produce a generative model of neuro-activation for arbitrary words in the English language.", "recorded": "2008-05-05T12:00:00", "title": "Learning Patterns of the Brain: Machine Learning Challenges of fMRI Analysis"}, {"url": "aml08_whistler", "desc": "There has recently been a surge of interest in algebraic methods in machine learning. In no particular order, this includes: new approaches to ranking problems; the budding field of algebraic statistics; and various applications of non-commutative Fourier transforms.\r\n\r\nThe aim of the workshop is to bring together these distinct communities, explore connections, and showcase algebraic methods to the machine learning community at large. AML'08 is intended to be accessible to researchers with no prior exposure to abstract algebra. The program includes three short tutorials that will cover the basic concepts necessary for understanding cutting edge research in the field.\r\n\r\nMore information about workshop - http://www.gatsby.ucl.ac.uk/~risi/AML08/", "recorded": "2008-12-12T07:30:00", "title": "NIPS Workshop on Algebraic and Combinatorial Methods in Machine Learning, Whistler 2008"}, {"url": "mlss08au_lucey_linv", "desc": "This tutorial he will cover some of the core fundamentals in vision and demonstrate how they can be interpreted in terms of machine learning fundamentals. Unbeknownst to most researchers in the field of machine learning, the fundamentals of object registration and tracking such as optical flow, interest descriptors (e.g., SIFT), segmentation and correlation filters are inherently related to the learning topics of regression, regularization, graphical models, generative models and discriminative models. As a result many aspects of vision can be interpreted as applied forms of learning. From this discussion on fundamentals we shall also explore advanced topics in object registration and tracking such as non-rigid object alignment/ tracking and non-rigid structure from motion and how the application of machine learning is continuing to improve these technologies.", "recorded": "2008-03-12T08:30:48", "title": "Learning in Computer Vision"}, {"url": "mla09_marseille", "desc": "The aim of the workshop is to discuss various cutting edge design exploration techniques for practical design problems through invited lectures from the machine learning and aerospace disciplines. The workshop is mainly focused on how machine learning methods can be used for aerospace applications, which generally contain multi-objective and multi-disciplinary features.\r\n\r\nWorkshop Topics:\r\n\r\n#Multi-Objective Design Optimisation\r\n#Multi-Disciplinary Design Optimisation\r\n#Robust Design\r\n#Data Mining\r\n#Data Fusion\r\n\r\nFinally, the workshop aims to encourage researchers to apply their techniques and methods to the presented problems and to encourage continued research into similar problems from the field.\r\n\r\n----\r\nThe workshop homepage can be found at http://web.mac.com/davidrh/MLA09/Workshop.html\r\n----", "recorded": "2009-07-03T09:00:00", "title": "International Workshop on Machine Learning for Aerospace, Marseille 2009"}, {"url": "mlss08au_kioloa", "desc": "**This school is suitable for all levels**, both for people without previous knowledge in Machine Learning, and those wishing to broaden their expertise in this area. It will allow the participants to get in touch with international experts in this field. Exchange of students, joint publications and joint projects will result because of this collaboration. **For research students**, the summer school provides a unique, high-quality, and intensive period of study. It is ideally suited for students currently pursuing, or intending to pursue, research in Machine Learning or related fields. **For IT professionals **who use Machine Learning will find that the summer school provides relevant knowledge and exposure to contemporary techniques. In addition, they will benefit by direct interaction with top-notch researchers and knowledge workers. Previous experience indicates that personnel from both the industry as well as national laboratories like CSIRO, DSTO benefit immensely from the school. **For academics**, the summer school is an excellent opportunity to help getting started in research on novel topics in Machine Learning. It provides an ideal forum for networking and discussions. Academics will also benefit from interaction with IT professionals which will lead to a deeper understanding of real life problems. **Organizers,**\u00a0this summer school is organized by the Computer Sciences Laboratory of the Australian National University ([[http://csl.rsise.anu.edu.au/|CSL]]@[[http://www.anu.edu.au/|ANU]]) and the Statistical Machine Learning program of the National ICT Australia ([[http://sml.nicta.com.au/|SML]]@[[http://www.nicta.com.au/|NICTA]]), jointly with support from the Max-Planck-Institute for Biological Cybernetics in T\u00fcbingen and the Pascal Netwok. Please visit [[http://www.mlss.cc/|www.mlss.cc]] for more information about the [[http://www.mlss.cc/|previous summer schools]]. [[http://mlss08.rsise.anu.edu.au/committee|Local organizers]] are Li Cheng, Marcus Hutter, and Alex Smola.", "recorded": "2008-03-03T08:30:39", "title": "Machine Learning Summer School (MLSS), Kioloa 2008"}, {"url": "nipsworkshops2010_discrete_optimization", "desc": "Solving optimization problems with ultimately discrete solutions is\r\nbecoming increasingly important in machine learning: At the core\r\nof statistical machine learning is to infer conclusions from data,\r\nand when the variables underlying the data are discrete, both\r\nthe tasks of inferring the model from data, as well as performing\r\npredictions using the estimated model are discrete optimization\r\nproblems. Many of the resulting optimization problems are NPhard,\r\nand typically, as the problem size increases, standard off-the-shelf optimization procedures become intractable. Fortunately, most discrete optimization problems that arise in\r\nmachine learning have specific structure, which can be leveraged\r\nin order to develop tractable exact or approximate optimization\r\nprocedures. For example, consider the case of a discrete\r\ngraphical model over a set of random variables. For the task of\r\nprediction, a key structural object is the \"marginal polytope\",a\r\nconvex bounded set characterized by the underlying graph of\r\nthe graphical model. Properties of this polytope, as well as its\r\napproximations, have been successfully used to develop efficient\r\nalgorithms for inference. For the task of model selection, a key\r\nstructural object is the discrete graph itself.\r\n\r\nAnother problem structure is sparsity: While estimating a highdimensional\r\nmodel for regression from a limited amount of data\r\nis typically an ill-posed problem, it becomes solvable if it is known\r\nthat many of the coefficients are zero. Another problem structure,\r\nsubmodularity, a discrete analog of convexity, has been shown\r\nto arise in many machine learning problems, including structure\r\nlearning of probabilistic models, variable selection and clustering.\r\nOne of the primary goals of this workshop is to investigate how\r\nto leverage such structures. There are two major classes of\r\napproaches towards solving such discrete optimization problems\r\nmachine learning: Combinatorial algorithms and continuous\r\nrelaxations.\r\n\r\nWorkshop homepage: http://www.discml.cc/", "recorded": "2010-12-10T07:30:00", "title": "Discrete Optimization in Machine Learning"}, {"url": "bmvc2013_krause_machine_learning", "desc": "Numerous problems in machine learning and vision are inherently discrete. More often than not, these lead to challenging optimization problems. While convexity is an important property when solving continuous optimization problems, submodularity, often viewed as a discrete analog of convexity, is key to solving many discrete problems. Its characterizing property, diminishing marginal returns, appears naturally in a multitude of settings. While submodularity has long been recognized in combinatorial optimization and game theory, it has seen a recent surge of interest in theoretical computer science, machine learning and computer vision. This tutorial will introduce the concept of submodularity and its basic properties, and outline recent research directions -- such as new approaches towards large-scale optimization and sequential decision making tasks. We will discuss recent applications to challenging machine learning and vision problems such as high-order graphical model inference, structured sparse modeling, multiple object detection, active sensing etc. The tutorial will not assume any specific prior knowledge on the subject. ", "recorded": "2013-09-09T15:30:00", "title": "Submodularity in Machine Learning and Vision"}, {"url": "eswc2012_jiang_relation_prediction", "desc": "The three most common approaches for deriving or predicting instantiated relations, i.e. triple statements (s, p, o), are information extraction, reasoning and relational machine learning. Information extraction uses sensory information, typically in form of text, and extracts statements using various methods ranging from simple classifiers to the most sophisticated NLP approaches. Logical reasoning is based on a set of true statements and derives new statements via inference using higher-order logical axioms. Finally, machine learning exploits regularities in the data to predict the likelihood of new statements. In this paper we combine all three methods to exploit all sources of available information in a modular way, by which we mean that each approach, i.e., information extraction, reasoning, machine learning, can be optimized independently to be combined in an overall system. For relational machine learning, we present a novel approach based on hierarchical Bayesian multi-label learning which also sheds new light on common factorization approaches. We rank the probabilities for statements to be true in the sense that: given that we are forced to make a decision, what is the best option. We consider the fact that an entity can belong to more than one ontological class and discuss aggregation. We extend the approach to modeling nonlinear dependencies between relationships and for personalization. We validate our model using data from the Yago and the DBpedia ontology.", "recorded": "2012-05-31T17:00:00", "title": "Exploiting Information Extraction, Reasoning and Machine Learning for Relation Prediction"}, {"url": "workshops2012_hsu_bridging", "desc": "", "recorded": "2012-03-29T14:45:00", "title": "Bridging Human and Machine Learning: Using discrete Markov Chain Monte Carlo with People to explore human categories"}, {"url": "nipsworkshops09_freeman_smlptwcvcwlss", "desc": "From a user's perspective, I'll describe what solutions I'd like to see regarding the learning of large scale graphical models. Also, at a recent vision workshop, I asked (one by one) numerous leading researchers in computer vision what results they would like to see from computer scientists/machine learning folks. I'll present those responses.", "recorded": "2009-12-12T16:36:00", "title": "Some Machine Learning Problems that We in the Computer Vision Community Would Like to See Solved"}, {"url": "mlpmsummerschool2013_tuebingen", "desc": "The Marie Curie Initial Training Network on Machine Learning for Personalized Medicine held its first summer school in T\u00fcbingen (Germany) from September 23rd to September 27th, 2013. 70 participants from all over the world participated in this event.\r\n\r\nFor more information please visit the [[http://www.mlpm.eu/summer-school-2013/|1st MLPM Summer School]].\r\n", "recorded": "2013-09-23T09:00:00", "title": "1st Machine Learning for Personalized Medicine (MLPM) Summer School, T\u00fcbingen 2013"}, {"url": "mit600SCs2011_guttag_lec19", "desc": "This lecture continues to discuss optimization in the context of the knapsack problem, and talks about the difference between greedy approaches and optimal approaches. It then moves on to discuss supervised and unsupervised machine learning optimization problems. Most of the time is spent on clustering.\r\n\r\nTopics covered: Knapsack problem, local and global optima, supervised and unsupervised machine learning, training error, clustering, linkage, feature vectors.", "recorded": "2011-04-02T09:00:00", "title": "Lecture 19: More Optimization and Clustering"}, {"url": "smls09_cumberland_lodge", "desc": "Sparse estimation (or sparse recovery) is playing an increasingly important role in the statistics and machine learning communities. Several methods have recently been developed in both fields, which rely upon the notion of sparsity (e.g. penalty methods like the Lasso, Dantzig selector, etc.). Many of the key theoretical ideas and statistical analysis of the methods have been developed independently, but there is increasing awareness of the potential for cross-fertilization of ideas between statistics and machine learning.\r\n\r\nFurthermore, there are interesting links between lasso-type methods and boosting (particularly, LP-boosting); there has been a renewed interest in sparse Bayesian methods. Sparse estimation is also important in unsupervised method (sparse PCA, etc.). Recent machine learning techniques for multi-task learning and collaborative filtering have been proposed which implement sparsity constraints on matrices (rank, structured sparsity, etc.). At the same time, sparsity is playing an important role in various application fields, ranging from image and video reconstruction and compression, to speech classification, text and sound analysis, etc.\r\n\r\nThe overall goal of the workshop is to bring together machine learning researchers with statisticians working on this timely topic of research, to encourage exchange of ideas between both communities and discuss further developments and theoretical underpinning of the methods.\r\n\r\nFor detailed information visit the [[http://www.cs.ucl.ac.uk/staff/rmartin/smls09/|Workshops website]].", "recorded": "2009-04-01T09:00:00", "title": "Workshop on Sparsity in Machine Learning and Statistics, Cumberland Lodge 2009"}, {"url": "cmulls08_pittsburgh", "desc": "**The Machine Learning lunch** is a weekly seminar which has the goal of bringing together the different people at CMU working on related fields to discuss their work. In the past a broad range of topics has been discussed: reinforcement learning, machine learning in general, statistical AI, statistical learning theory, robot learning, text learning, etc. The talks have always been enjoyable and have ranged from quite informal to formal conference style talks. It is also a great forum to practice conference talks, bounce around new ideas and for guests from other universities and industry to speak. Currently the talks are sponsored by //**MLD - the Machine Learning Department of the School of Computer Science.\n\nThe goal of MLD is slightly broader than that of these talks - it brings together the many departments working on similar topics at CMU. The series has been going on for quite a few years. In earlier days it was called the Reinforcement Learning Lunch because of the emphasis on reinforcement learning. As the topics broadened, the name was changed to the **Machine Learning Lunch.**\n\nOrganizing committee: [[http://www.cs.cmu.edu/%7Eamahmed/|Amr Ahmed]], [[http://www.cs.cmu.edu/%7Edchau/|Polo Chau]], [[http://www.cs.cmu.edu/%7Eshanneke/|Steve Hanneke]], [[http://www.cs.cmu.edu/%7Esahong/|Sue Ann Hong]], [[http://www.cs.cmu.edu/%7Endr/|Nathan Ratliff ]]\n\n----\n{{http://l.yimg.com/a/i/ww/beta/y3.gif}} **This lecture series is being kindly sponsored by [[http://research.yahoo.com/Academic_Relations|Yahoo! Academic Relations]]**\n----", "recorded": "2008-01-21T12:00:00", "title": "Carnegie Mellon Machine Learning Lunch seminar"}, {"url": "icml09_dhillon_itmcml", "desc": "Matrix Computations are ubiquitous in all areas of science and engineering. In this talk, I will first survey some traditional problems in matrix computations and discuss issues that arise in solving them, such as, accuracy, algorithms and software. Then, I will discuss various matrix computation problems that arise in machine learning, especially specialized computations, such as non-negative matrix factorization, multilevel graph clustering and kernel learning. I will conclude with a pointer to resources and a discussion.", "recorded": "2009-06-18T14:00:00", "title": "Matrix Computations in Machine Learning"}, {"url": "mloss08_karatzoglou_kernlab", "desc": "kernlab is an R package providing kernel-based machine learning functionality. It is designed to provide\r\ntools for kernel algorithm development but also includes a range of popular machine learning methods for\r\nclassi\ufb01cation, regression, clustering, novelty detection, quantile regression and dimensionality reduction.\r\nAmong other algorithms included in the package are Support Vector Machines, Spectral Clustering, Kernel\r\nPCA, a QP solver and a range of kernels (Gaussian, Laplacian, string kernels etc.).", "recorded": "2008-12-12T09:00:10", "title": "Kernlab"}, {"url": "kdd2014_li_margin_distribution", "desc": "Support vector machine (SVM) has been one of the most popular learning algorithms, with the central idea of maximizing the minimum margin, i.e., the smallest distance from the instances to the classification boundary. Recent theoretical results, however, disclosed that maximizing the minimum margin does not necessarily lead to better generalization performances, and instead, the margin distribution has been proven to be more crucial. In this paper, we propose the Large margin Distribution Machine (LDM), which tries to achieve a better generalization performance by optimizing the margin distribution. We characterize the margin distribution by the first- and second-order statistics, i.e., the margin mean and variance. The LDM is a general learning approach which can be used in any place where SVM can be applied, and its superiority is verified both theoretically and empirically in this paper.", "recorded": "2014-08-25T10:30:00", "title": "Large Margin Distribution Machine"}, {"url": "wsdm2010_cambazoglu_eeo", "desc": "Some commercial web search engines rely on sophisticated machine learning systems for ranking web documents. Due to very large collection sizes and tight constraints on query response times, online efficiency of these learning systems forms a bottleneck. An important problem in such systems is to speedup the ranking process without sacrificing much from the quality of results. In this paper, we propose optimization strategies that allow short-circuiting score computations in additive learning systems. The strategies are evaluated over a state-of-the-art machine learning system and a large, real-life query log, obtained from Yahoo!. By the proposed strategies, we are able to speedup the score computations by more than four times with almost no loss in result quality.", "recorded": "2010-02-06T15:11:21", "title": "Early Exit Optimizations for Additive Machine Learned Ranking Systems "}, {"url": "mlss09us_lee_btrlsvm", "desc": "The support vector machine has been used successfully in a variety of applications. Also on the theoretical front, its statistical properties including Bayes risk consistency have been examined rather extensively. Taking another look at the method, we investigate the asymptotic behavior of the linear support vector machine through Bahadur type representation of the coefficients established under appropriate conditions. Their asymptotic normality and statistical variability are derived on the basis of the representation. Furthermore, direct theoretical comparison is made with likelihood based approach to classification such as linear discriminant analysis and logistic regression in terms of the asymptotic relative efficiency, where the efficiency of a classification procedure is defined using the excess risk from the Bayes risk.", "recorded": "2009-06-08T15:30:00", "title": "A Bahadur Type Representation of the Linear Support Vector Machine and its Relative Efficiency"}, {"url": "machine_schrouff_pattern_recognition", "desc": "In the past years, mass univariate statistical analyses of neuroimaging data have been complemented by the use of multivariate pattern analyses, especially based on machine learning models. While these allow an increased sensitivity for the detection of spatially distributed effects compared to univariate techniques, they lack an established and accessible software framework. The goal of this work was to build a toolbox comprising all the necessary functionalities for multivariate analyses of neuroimaging data, based on machine learning models. The \u201cPattern Recognition for Neuroimaging Toolbox\u201d (PRoNTo) is open-source, cross-platform and MATLAB-based, therefore being suitable for both cognitive and clinical neuroscience research. In addition, it is designed to facilitate novel contributions from developers, aiming to improve the interaction between the neuroimaging and machine learning communities.", "recorded": "2013-04-11T14:50:00", "title": "Pattern Recognition for Neuroimaging Toolbox"}, {"url": "mlss2010au_canberra", "desc": "Machine Learning is a foundational discipline of the Information Sciences. It combines deep theory from areas as diverse as Statistics, Mathematics, Engineering, and Information Technology with many practical and relevant real life applications. The aim of the summer school is to cover the entire spectrum from theory to practice. It is mainly targeted at research students, IT professionals, and academics from all over the world.\r\n\r\nThis school is suitable for all levels, both for people without previous knowledge in Machine Learning, and those wishing to broaden their expertise in this area. It will allow the participants to get in touch with international experts in this field. Exchange of students, joint publications and joint projects will result because of this collaboration.\r\n\r\nFor research students, the summer school provides a unique, high-quality, and intensive period of study. It is ideally suited for students currently pursuing, or intending to pursue, research in Machine Learning or related fields. Limited scholarships are available for students to cover accommodation and registration costs. If funds are available partial travel support might also be provided.\r\n\r\nIT professionals who use Machine Learning will find that the summer school provides relevant knowledge and exposure to contemporary techniques. In addition, they will benefit by direct interaction with top-notch researchers and knowledge workers. Previous experience indicates that personnel from both the industry as well as national laboratories like CSIRO, DSTO benefit immensely from the school.\r\n\r\nFor academics, the summer school is an excellent opportunity to help getting started in research on novel topics in Machine Learning. It provides an ideal forum for networking and discussions. Academics will also benefit from interaction with IT professionals which will lead to a deeper understanding of real life problems.\r\n\r\nDetailed information about the event can be found at [[http://mlss10.rsise.anu.edu.au/|MLSS2010]].\r\n\r\n\r\n**//Disclaimer:// VideoLectures.NET emphasizes we are not the authors of these recordings and that the overall quality of these videos was notably improved.", "recorded": "2010-09-27T09:00:00", "title": "Machine Learning Summer School (MLSS), Canberra 2010"}, {"url": "nipsworkshops2010_learning_cores", "desc": "In the current era of web-scale datasets, high throughput biology\r\nand astrophysics, and multilanguage machine translation,\r\nmodern datasets no longer fit on a single computer and traditional\r\nmachine learning algorithms often have prohibitively long running\r\ntimes. Parallelized and distributed machine learning is no longer\r\na luxury; it has become a necessity. Moreover, industry leaders\r\nhave already declared that clouds are the future of computing,\r\nand new computing platforms such as Microsoft\u2019s Azure and\r\nAmazon\u2019s EC2 are bringing distributed computing to the masses.\r\n\r\nThe machine learning community has been slow to react to these\r\nimportant trends in computing, and it is time for us to step up\r\nto the challenge. While some parallel and distributed machine\r\nlearning algorithms already exist, many relevant issues are yet to\r\nbe addressed. Distributed learning algorithms should be robust to\r\nnode failures and network latencies, and they should be able to\r\nexploit the power of asynchronous updates. Some of these issues\r\nhave been tackled in other fields where distributed computation\r\nis more mature, such as convex optimization and numerical\r\nlinear algebra, and we can learn from their successes and their\r\nfailures. \r\n\r\nThe workshop aims to draw the attention of machine\r\nlearning researchers to this rich and emerging area of problems\r\nand to establish a community of researchers that are interested in\r\ndistributed learning. We would like to define a number of common\r\nproblems for distributed learning (online/batch, synchronous/\r\nasynchronous, cloud/cluster/multicore) and to encourage future\r\nresearch that is comparable and compatible. We also hope to\r\nexpose the learning community to relevant work in fields such as\r\ndistributed optimization and distributed linear algebra. The daylong\r\nworkshop aims to identify research problems that are unique\r\nto distributed learning. The target audience includes leading\r\nresearchers from academia and industry that are interested in\r\ndistributed and large-scale learning.\r\n\r\nWorkshop homepage: http://lccc.eecs.berkeley.edu/", "recorded": "2010-12-10T07:30:00", "title": "Learning on Cores, Clusters, and Clouds"}, {"url": "mbc07_diethe_lpb", "desc": "Classification of musical genre from raw audio files is a fairly well researched area of music research, and as such provides a good starting point for testing a new algorithm. The Music Information Retrieval Evaluation eXchange (MIREX) is a yearly competition in a wide range of machine learning applications in music. MIREX 2005 included a genre classification task, the winner of which [1] was an application of the multiclass boosting algorithm AdaBoost.MH [2]. It is believed that Linear Programming Boosting (LPBoost) is a more appropriate algorithm for this application due to the higher degree of sparsity in the solutions [3]. The present study aims to improve on the [1] result by using a similar feature set and the multiclass boosting algorithm LPBoost.MC. \r \\\\ References: [1] J. Bergstra, N. Casagrande, D. Erhan, D. Eck, and K. Bal\u00b4azs. Aggregate features and ADABOOST for music classification. Machine Learning, 65 (2-3):473\u2013484, 2006. [2] R.E. Schapire and Y. Singer. Improved boosting algorithms using confidence-rated predictions. Machine Learning, 37:297\u2013336, 1999. [3] Ayhan Demiriz, Kristin P. Bennett, and John Shawe-Taylor. Linear programming boosting via column generation. Machine Learning, 46(1\u20133):225\u2013254, 2002.", "recorded": "2007-12-07T09:30:00", "title": "Linear Programming Boosting for Classi\ufb01cation of Musical Genre"}, {"url": "aedml08_dzeroski_mla", "desc": "", "recorded": "2008-03-20T10:00:00", "title": "Machine Learning Applications in Forestry: Brown bear habitat modeling / Predicting forest properties using LANDSAT and LIDAR data"}, {"url": "icmi05_trento", "desc": "This session on machine learning and the interviews below were recorded during this conference.", "recorded": "2005-10-07T00:00:00", "title": "International Conference on Multimodal Interfaces, Trento 2005"}, {"url": "gpip06_mackay_gpb", "desc": "How on earth can a plain old Gaussian distribution be useful for sophisticated regression and machine learning tasks?", "recorded": "2006-06-12T00:00:00", "title": "Gaussian Process Basics"}, {"url": "nipsworkshops2012_modern_nonparametric_methods", "desc": "Improvements in engineering and data acquisition techniques have rendered large amounts, of potentially high dimensional, data easily available. As a result, statistical analysis of big, high-dimensional data has become frequent in many scientific fields ranging from biology, genomics and health sciences to astronomy, economics and machine learning. Although nonparametric methods are better suitable to model complex systems underlying data generating processes and often achieve state of the art performance on a wide range of tasks, majority of the research in machine learning focuses on linear models. This may come from the common belief that nonparametric methods do not scale to big data problems.\r\n\r\nThe aim of this workshop is to bring together practitioners, who work on specialized applications, and theoreticians that are interested in providing sound methodology. It is important that we effectively communicate advances arising in different areas of machine learning and drawbacks of existing methods to develop methodology that matters. In particular, we hope to educate theoreticians of needs in real world applications and practitioners of new methodological advances. We will bring together machine learners and statisticians, since both communities work on the topic, but emphasise different aspects of the nonparametric learning and have complementary strengths. Furthermore, we hope to advertise recent successes of nonparametric methods in a number of domains, involving large scale high-dimensional problems, and to dismiss the common belief that nonparametric methods are not suitable for dealing with challenges arising from big data.\r\n\r\nWorkshop homepage: https://sites.google.com/site/nips2012modernnonparametric/home", "recorded": "2012-12-07T07:30:00", "title": "Modern Nonparametric Methods in Machine Learning"}, {"url": "mlsb09_noble_mlmfpa", "desc": "Computational biologists, and biologists more generally, spend a lot of time trying to more fully characterize proteins. In this talk, I will describe several of our recent efforts to use machine learning methods to gain a better understanding of proteins. First, we tackle one of the oldest problems in computational biology, the recognition of distant evolutionary relationships among protein sequences. We show that by exploiting a global protein similarity network, coupled with a latent space embedding, we can detect remote protein homologs more accurately than state-of-the-art methods such as PSI-BLAST and HHPred. Second, we use machine learning methods to improve our ability to identify proteins in complex biological samples on the basis of shotgun proteomics data. I will describe two quite different approaches to this problem, one generative and one discriminative.", "recorded": "2009-09-06T09:00:00", "title": "Machine Learning Methods For Protein Analyses"}, {"url": "is2012_dzeroski_biosciences", "desc": "The above title currently provides the best single label for the topics covered by my research group. Here I provide a brief summary of our current research and some of the research directions we intend to pursue. Systems biosciences study biological systems in a holistic manner, focusing on the interactions among system components rather than the components themselves. Large amounts of data of increasing complexity are generated in these sciences (which include systems ecology and systems biology): The use of machine learning to make sense of these data is thus a necessity. We discuss two machine learning tasks that appear in this context, predicting structured outputs and automated modeling of dynamic systems. We describe some techniques for solving these tasks and some example application of these techniques in systems ecology and systems biology.", "recorded": "2012-10-11T16:50:15", "title": "Machine Learning for Systems Biosciences"}, {"url": "ida07_likas_tre", "desc": "Estimating the reliability of individual classifications is very\r important in several applications such as medical diagnosis. Recently,\r the transductive approach to reliability estimation has been proved to be\r very efficient when used with several machine learning classifiers, such as\r Naive Bayes and decision trees. However, the efficiency of the transductive\r approach for state-of-the art kernel-based classifiers was not considered.\r In this work we deal with this problem and apply the transductive\r reliability methodology with sparse kernel classifiers, specifically the Support\r Vector Machine and Relevance Vector Machine. Experiments with\r medical and bioinformatics datasets demonstrate better performance of\r the transductive approach for reliability estimation compared to reliability\r measures obtained directly from the output of the classifiers. Furthermore,\r we apply the methodology in the problem of reliable diagnostics of\r the coronary artery disease, outperforming the expert physicians\u2019 standard\r approach.", "recorded": "2007-09-06T14:45:00", "title": "Transductive Reliability Estimation for Kernel Based Classifiers"}, {"url": "bbci09_vaadia_alpa", "desc": "Using useful signals from the brain, and useful computer algorithms to improve brain machine interfaces.\r\nI will talk about the physiology of motor cortex and the nature of activity of population of neurons in motor cortex during Sensorimotor learning, movement preparation and execution. I will present the approach of internal models of the brain as the basis for learning and perception and use all of the above to show how our current knowledge can facilitate approaches to adaptive brain machine interfaces.", "recorded": "2009-07-08T14:50:35", "title": "About Learning, Predictions and Adaptivity of Brains and Machines"}, {"url": "mlss06tw_taipei", "desc": "The two-week summer school program consists of about 40+ hours of lectures from 7/24 to 8/4. The late afternoon sessions will provide a chance for the participants to discuss the latest research problems with the invited speakers. Prior to the summer school, the National Taiwan University of Science and Technology will offer a month-long machine learning summer course for graduate credits. You may choose part of them that fit your need or attend everything if you are sufficiently energetic.", "recorded": "2006-07-24T00:00:00", "title": "Machine Learning Summer School (MLSS), Taipei 2006"}, {"url": "mlmi05uk_edinburgh", "desc": "The topics covered by the workshop are the following: * human-human communication modeling * speech and visual processing * multi-modal processing, fusion and fission * multi-modal dialog modeling * human-human interaction modeling * multi-modal data structuring and presentation * multimedia indexing and retrieval * meeting structure analysis * meeting summarizing * multimodal meeting annotation * machine learning applied to the above", "recorded": "2005-06-11T00:00:00", "title": "2nd Joint Workshop on Multimodal Interaction and Related Machine Learning Algorithms, Edinburgh 2005"}, {"url": "icml07_workshops", "desc": "**Workshop - \r Constrained Optimization and Learning with Structured Outputs\r \r ** \r \r In recent years, there has been a great deal of work relating constrained optimization problems with machine learning in structured output spaces. This has given rise to a number of novel and powerful approaches to solving some extremely difficult machine learning problems. We hope that this workshop will bring together researchers in both of these areas, thereby encouraging further collaboration and increasing awareness of the issues at hand in both communities.", "recorded": "2007-06-24T09:00:00", "title": "Workshop "}, {"url": "machine_learning_video_abstracts_vol4", "desc": "Welcome to the 4th issue of the Journal of Machine Learning Video Abstracts, published by Knowledge for All Foundation Ltd, London, with the goal of providing the legacy of the PASCAL2 Network of Excellence.\r\n\r\nThis issue is equally dedicated to presenting the flashtalks presentations at the 3rd EUCogIII Members Conference - EUCog - European Network for the Advancement of Artificial Cognitive Systems, Interaction and Robotics - in collaboration with Pascal2 Network of Excellence, held in Palma de Mallorca, 10-11 April 2013.", "recorded": "2013-04-09T14:58:42", "title": "Video Journal of Machine Learning Abstracts - Volume 4"}, {"url": "mlss04_hofmann_irtm", "desc": "This four hour course will provide an overview of applications of machine learning and statistics to problems in information retrieval and text mining. More specifically, it will cover tasks like document categorization, concept-based information retrieval, question-answering, topic detection and document clustering, information extraction, and recommender systems. The emphasis is on showing how machine learning techniques can help to automatically organize content and to provide efficient access to information in textual form.", "recorded": "2004-09-14T00:00:00", "title": "Information Retrieval and Text Mining"}, {"url": "mlss06au_hofmann_irtm", "desc": "This four hour course will provide an overview of applications of machine\r learning and statistics to problems in information retrieval and text\r mining. More specifically, it will cover tasks like document categorization,\r concept-based information retrieval, question-answering, topic detection and\r document clustering, information extraction, and recommender systems. The\r emphasis is on showing how machine learning techniques can help to\r automatically organize content and to provide efficient access to\r information in textual form.", "recorded": "2006-02-16T00:00:00", "title": "Information Retrieval and Text Mining"}, {"url": "nipsworkshops2011_computational_photography", "desc": "After the great success of last year's workshop on CP at NIPS, this workshop proposal tries to accommodate the strong interest in a follow-up workshop expressed by many workshop participants last year. The objectives of this workshop are: (i) to give an introduction to CP, present current approaches and report about the latest developments in this fast-progressing field, (ii) spot and discuss current limitations and present open problems of CP to the NIPS community, and (iii) to encourage scientific exchange and foster interaction between researchers from machine learning, neuro science and CP to advance the state of the art in CP.\r\n\r\nThe tight interplay between both hardware and software renders CP an exciting field of research for the whole NIPS community, which could contribute in various ways to its advancement, be it by enabling new imaging devices that are possible due to the latest machine learning methods or by new camera and processing designs that are inspired by our neurological understanding of natural visual systems.\r\n\r\nThus the target group of participants are researchers from the whole NIPS community (machine learning and neuro science) and researchers working on CP and related fields.\r\n\r\nWorkshop homepage: http://webdav.is.mpg.de/pixel/workshops/mlmcp-nips2011/index.html", "recorded": "2011-12-17T07:30:00", "title": "Machine Learning meets Computational Photography"}, {"url": "nipsworkshops2011_cosmology_meets_ml", "desc": "Many problems in modern cosmological data analysis are tightly related to fundamental problems in machine learning, such as classifying stars and galaxies and cluster finding of dense galaxy populations. Other typical problems include data reduction, probability density estimation, how to deal with missing data and how to combine data from different surveys. An increasing part of modern cosmology aims at the development of new statistical data analysis tools and the study of their behaviour and systematics often not aware of recent developments in machine learning and computational statistics.\r\n\r\nThe objectives of this workshop are two-fold:\r\n* To bring together experts from the Machine Learning and Computational Statistics community with experts in the field of cosmology to promote, discuss and explore the use of machine learning techniques in data analysis problems in cosmology and to advance the state of the art.\r\n    * By presenting current approaches, their possible limitations, and open data analysis problems in cosmology to the NIPS community, this workshop aims to encourage scientific exchange and to foster collaborations among the workshop participants.\r\n\r\nThe workshop is held as a one-day workshop organised jointly by experts in the field of empirical inference and cosmology. The target group of participants are researchers working in the field of cosmological data analysis as well as researchers from the whole NIPS community sharing the interest in real-world applications in a fascinating, fast-progressing field of fundamental research. Due to the mixed participation of computer scientists and cosmologists the invited speakers will be asked to give talks with tutorial character and make the covered material accessible for both computer scientists and cosmologists.\r\n\r\nWorkshop homepage: http://cmml-nips2011.wikispaces.com/", "recorded": "2011-12-16T07:30:00", "title": "Cosmology meets Machine Learning"}, {"url": "icml2010_sonnenburg_tnsau", "desc": "Recently, mloss.org has enabled machine learning researchers to register their software and allow other researchers to easily find, download, and reuse software matching their interests. Currently, more than 200 projects are listed. Furthermore, the Journal of Machine Learning Research now accepts papers to its special Open Source Software track, in which papers describing peer-reviewed software can be published, as a further incentive for researchers to publish their software under an open source license. Since its inception, in October 2007, seven papers have been published in this track with more papers currently under review. So far, the initiative has been highly successful, but has focused mostly on the \u201dmethod\u201d side of the problem to make machine learning research more reproducible. Hence we see the need to initiate a companion project to mloss.org which focuses on the free exchange and benchmarking of datasets. Additionally, this new repository will emphasise the precise specification of machine learning tasks: detailed definitions of datasets to be used (possibly including feature extraction or other preprocessing steps) together with the desired operation to be performed and the relevant performance metric. Finally, a solution to such a task would provide details of how to apply a general software package (such as on mloss.org) to this particular problem instance, as well as the obtained numerical performance measures. This project will thus focus on providing a platform for publishing, exchanging, collecting, and discussing such data sets, tasks, and solutions for challenging machine learning problems.", "recorded": "2010-06-25T12:00:00", "title": "The next steps after UCI - mldata.org"}, {"url": "rease_ciravegna_adauh1", "desc": "Tutorial given at the Third Semantic Web Summer School in Cercedilla, Spain,", "recorded": "2005-07-27T00:00:00", "title": "Automating Document Annotation using Human Language Technologies and Machine Learning"}, {"url": "nips09_madrid_sanchez_mla", "desc": "The presentation will describe the challenge of eInclusion in the technological design\r\nprocess, which impedes the complete integration of people with disabilities and elderly\r\nin Information Society. To face this challenge, the INREDIS project aims to face \r\nindividual needs of users instead of addressing the needs of the average user, by \r\nproposing basic technologies that enables the creation of personalized channels for\r\ncommunication and interaction with the technological environment. For this purpose, \r\nMachine Learning can help constructing effective methods to reflect user needs, \r\npreferences and expectations (and their evolution over time) on user interfaces,\r\nconsequently improving satisfaction and performance. In particular, academia and \r\nindustry within the INREDIS consortium explore together the potential of Machine \r\nLearning on multimodal services and ubiquitous assistive technologies, as well as \r\nadaptive user interfaces according to user and technological capabilities.", "recorded": "2009-12-10T15:15:00", "title": "Machine Learning Applied to Multi-Modal Interaction, Adaptive Interfaces and Ubiquitous Assistive Technologies"}, {"url": "turing100_brooks_humanoid_machines", "desc": "In his paper \"Intelligent Machinery\" Alan Turing suggested the idea of building a `thinking machine' by making a machine that emulated as many parts as possible of a person, and letting it \"roam the countryside\" finding things out for itself -a robot that was to learn from its experience in the ordinary world.  Turing rejected that idea as not practical at the time and moved on to more disembodied suggestions of how to build a thinking machine.  Now that we have experience with humanoid robots we can examine some of his briefly expressed ideas on such robots, and the challenges of what might be missing in such an enterprise, and see how it is playing out now that it is practical to build the machines he suggested.  But even further we can re-examine Turing's models of human behavior to produce a formalism for computation, and compare that to computational neuroscience which tries to explain human thought as computation.", "recorded": "2012-06-24T10:00:00", "title": "Turing's Humanoid Thinking Machines"}, {"url": "sip08_hussain_mpaim", "desc": "I will describe a generic matching pursuit algorithm that can be used in \r\nmachine learning for regression, subspace methods (kernel PCA and kernel \r\nCCA) and classi\ufb01cation (given time). I will also describe some generalisa- \r\ntion error bounds upper bounding their loss. Some of these bounds will \r\nbe formed using standard sample compression bounds whilst others will be \r\namalgamations of traditional learning theory techniques such as VC theory \r\nand Rademacher complexities. This is joint work with John Shawe-Taylor. \r\n\r\n", "recorded": "2008-12-06T12:00:00", "title": "Matching pursuit algorithms in machine learning"}, {"url": "mitworld_minsky_emoticon", "desc": "Contemporary artificial intelligence researchers (as well as neurologists and Karl Jung) are taken to task in this talk by one of the world\u2019s preeminent scholars of artificial intelligence.\n\nMarvin Minsky is worried that after making great strides in its infancy, AI has lost its way, getting bogged down in different theories of machine learning. Researchers \u201chave tried to invent single techniques that could deal with all problems, but each method works only in certain domains.\u201d Minsky believes we\u2019re facing an AI emergency, since soon there won\u2019t be enough human workers to perform the necessary tasks for our rapidly aging population.\n\nSo while we have a computer program that can beat a world chess champion, we don\u2019t have one that can reach for an umbrella on a rainy day, or put a pillow in a pillow case. For \u201ca machine to have common sense, it must know 50 million such things,\u201d and like a human, activate different kinds of expertise in different realms of thought, says Minsky.\n\nMinsky suggests that such a machine should, like humans, have a very high-level, rule-based system for recognizing certain kinds of problems. He labels these parts of the brain \u201ccritics.\u201d When one critic gets selected in a particular situation, the others get turned off. In the \u201ccloud of resources\u201d that comprises our mind, mental states, from emotions to reasoning, result from activating or suppressing the right resource. Minsky further refines his machine\u2019s reasoning architecture with six levels of thinking that attempt to emulate the different kinds of reasoning humans may engage in, often simultaneously: These include learned reactions, deliberative thinking, and reflective thinking, among others. A smart machine must have at least these levels, he says, because psychology, unlike physics, doesn\u2019t lend itself to a minimal number of laws. With at least 400 different areas of the brain operating, \u201cif a theory tries to explain everything by just 20 principles, it\u2019s doing something wrong.\u201d\n\nToday, while we have machines that can automatically assemble clothes, we don\u2019t have any that know how to sew together a tear in a shirt or a suit. Minsky proposes a new kind of AI that might eventually result in a \u201creally resourceful, clever thinking machine...with knowledge about how to do things,\u201d and which \u201ccan do the broad range of things children can do.\u201d", "recorded": "2007-09-12T14:52:39", "title": "Emotion Machine: Commonsense Thinking, Artificial Intelligence, and the Future of the Human Mind"}, {"url": "ecmlpkdd2010_belda_ip", "desc": "QSAR (Quantitative Structure-Activity Relationship) modelling is a usual step in drug discovery. QSAR methods use statistical and machine learning tools to draw out the significant relationships between the molecular structure of the drug candidates (the molecules) and its biological profile. To achieve such a goal, researchers usually describe the molecules with arrays of physico-chemical properties, such as total molecular charge, molecular weight, number of hydrogen bonds donors, etc. However, the predictive accuracy of statistical and machine learning tools in QSAR have been typically very low and more advanced tools are needed to achieve higher degrees of usage of QSAR drug discovery processes. For such a reason, at Intelligent Pharma, we have been researching in the field of functional data mining, that is, data mining of information described through functions, not only with fixed properties. By using functional data mining approaches, we can deal with physico-chemical parameters such as the volume of the molecules, which is a variable property that varies depending on the energy of the system and its flexibility. Therefore, more accurate predictive models can be built by using these approaches in the field of QSAR and drug discovery. The machine learning tool used in this research is support vector machines.", "recorded": "2010-09-24T13:25:00", "title": "Flexible QSAR: functional machine learning in computational chemistry"}, {"url": "nipsworkshops2011_discrete_optimization", "desc": "Fortunately, most discrete optimization problems that arise in machine learning have specific structure, which can be leveraged in order to develop tractable exact or approximate optimization procedures. For example, consider the case of a discrete graphical model over a set of random variables. For the task of prediction, a key structural object is the \"marginal polytope,\" a convex bounded set characterized by the underlying graph of the graphical model. Properties of this polytope, as well as its approximations, have been successfully used to develop efficient algorithms for inference. For the task of model selection, a key structural object is the discrete graph itself. Another problem structure is sparsity: While estimating a high-dimensional model for regression from a limited amount of data is typically an ill-posed problem, it becomes solvable if it is known that many of the coefficients are zero. Another problem structure, submodularity, a discrete analog of convexity, has been shown to arise in many machine learning problems, including structure learning of probabilistic models, variable selection and clustering. One of the primary goals of this workshop is to investigate how to leverage such structures.\r\n\r\nThe focus of this year\u00b4s workshop is on the interplay between discrete optimization and machine learning: How can we solve inference problems arising in machine learning using discrete optimization? How can one solve discrete optimization problems involving parameters that themselves are estimated from training data? How can we solve challenging sequential and adaptive discrete optimization problems where we have the opportunity to incorporate feedback (online and active learning with combinatorial decision spaces)? We will also explore applications of such approaches in computer vision, NLP, information retrieval etc.\r\n\r\nWorkshop homepage: http://discml.cc/", "recorded": "2011-12-17T07:30:00", "title": "Discrete Optimization in Machine Learning"}, {"url": "youtube_tom_mitchell_bmcs", "desc": "Google Tech Talks\nMarch 27, 3009\n\nABSTRACT\n\nPresented by\n\nTom M. Mitchell\nE. Fredkin Professor and Department Head\nMachine Learning Department\nCarnegie Mellon University\n\nHow does the human brain represent meanings of words and pictures in terms of the underlying neural activity? This talk will present our research using machine learning methods together with fMRI brain imaging to study this question. One line of our research has involved training classifiers that identify which word a person is thinking about, based on their neural activity observed using fMRI. A more recent line involves developing a computational model that predicts the neural activity associated with arbitrary English words, including words for which we do not yet have brain image data. This computational model is trained using a combination of fMRI data associated with several dozen concrete nouns, together with statistics gathered from a trillion-word text corpus. Once trained, the model predicts fMRI activation for any other concrete noun appearing in the text corpus, with highly significant accuracies over the 60 nouns for which we currently have fMRI data.\n\nTom M. Mitchell is the E. Fredkin Professor and head of the Machine Learning Department at Carnegie Mellon University. Mitchell is a past President of the American Association of Artificial Intelligence (AAAI), and a Fellow of the AAAS and of the AAAI. His general research interests lie in machine learning, artificial intelligence, and cognitive neuroscience. Mitchell believes the field of machine learning will be the fastest growing branch of computer science during the 21st century.\n\nMitchell's web home page is www.cs.cmu.edu/~tom", "recorded": "2009-03-27T00:00:00", "title": "Brains, Meaning and Corpus Statistics"}, {"url": "kdd2013_wallace_health_informatics", "desc": "We present novel machine learning and data mining methods that\r\nmake real-world learning systems more e\ufb03cient. We focus on the domain of clinical informatics, an archetypical example of a \ufb01eld overwhelmed with information. Due to properties inherent to clinical\r\ninformatics tasks \u2013 and indeed, to many tasks that require specialized domain knowledge \u2013 \u2018o\ufb00-the-shelf\u2019 machine learning technologies\r\ngenerally perform poorly in this domain.\r\n\r\nIf machine learning is to be successful in clinical science, novel methods must be developed to: mitigate the e\ufb00ects of class imbalance during model induction; exploit the wealth of domain knowledge highly\r\nskilled domain experts bring to the task; and to induce better models with less e\ufb00ort (fewer labels). We present new machine learning\r\nmethods that address each of these issues, and demonstrate their\r\ne\ufb03cacy in the task of abstract screening. In particular, we develop\r\nnew theoretical perspectives on class imbalance, novel methods for exploiting dual supervision (i.e., labels on both instances and features),\r\nand new active learning techniques that address issues inherent to\r\nreal-world applications (e.g., exploiting multiple experts in tandem).\r\nEach of these contributions aims to squeeze better classi\ufb01cation performance out of fewer labels, thereby making better use of domain\r\nexperts\u2019 time and expertise.\r\n\r\nThe immediate aim in this work is to reduce the workload involved\r\nin conducting systematic reviews, and to this end we demonstrate\r\nthat the developed methods can reduce reviewer workload by more\r\nthan half, without sacri\ufb01cing the comprehensiveness of reviews (i.e.,\r\nwithout missing any relevant published evidence). But this is only\r\nan exemplary task; the approaches presented here have wider application to many real-world learning problems, i.e., those that require\r\nspecialized expertise, exhibit class imbalance (and asymmetric costs)\r\nand for which limited human resources are available. We show that\r\nthe methods we have developed bring substantial improvements over previously existing machine learning approaches in terms of inducing\r\nbetter models with less e\ufb00ort.", "recorded": "2013-08-12T17:30:46", "title": "Machine Learning in Health Informatics: Making Better use of Domain Experts"}, {"url": "acml2013_holmes_weka_moa_databases", "desc": "This talk is in three parts. The first deals with an aspect of the Weka project that has received little attention, namely the use of machine learning in agricultural applications. I will outline our experiences in this field and present an application development  framework which is a direct result of this activity. In particular, one project has met one of the challenges proposed by Kiri Wagstaff at ICML 2012. Second, I will talk about our work in data stream mining with a focus on classification within the Massive  Online Analysis framework MOA. After a quick overview of what is in MOA I will present two recent results that indicate a need for caution and a statement of what constitutes state-of-the-art in data stream classification for practitioners.  Finally, I will present the idea of experiment databases, a framework for machine learning experimentation that saves effort and offers opportunities for meta learning and hypothesis generation.", "recorded": "2013-11-14T09:00:00", "title": "Weka, MOA and Experiment Databases: Frameworks for Machine Learning"}, {"url": "bootcamp07_guyon_itml", "desc": "\r This course covers feature selection fundamentals and applications. The students will first be reminded of the basics of machine learning algorithms and the problem of overfitting avoidance. In the wrapper setting, feature selection will be introduced as a special case of the model selection problem. Methods to derive principled feature selection algorithms will be reviewed as well as heuristic method, which work well in practice. One class will be devoted to feature construction techniques. Finally, a lecture will be devoted to the connections between feature section and causal discovery. The class will be accompanied by several lab sessions. The course will be attractive to students who like playing with data and want to learn practical data analysis techniques. The instructor has ten years of experience with consulting for startup companies in the US in pattern recognition and machine learning. Datasets from a variety of application domains will be made available: handwriting recognition, medical diagnosis, drug discovery, text classification, ecology, marketing.\r ", "recorded": "2007-07-02T00:09:10", "title": "Introduction to Machine Learning"}, {"url": "ecmlpkdd08_gasso_trfim", "desc": "Machine Learning algorithms often involve the joint optimization of several objective functions for achieving good generalization performance. Well known examples are Support Vector Machines for regression, classification and novelty detection or the Lasso problem where one objective function is related to the perfect fit of the data and the second one concerns particular desirable properties such as smoothness or sparsity of the target model. These two goals being antagonist, a trade-off needs to be achieved. Hence, the learning process can be cast in a multi-objective optimisation problem. The aim of this tutorial is to bridge the gap between the multi-objective optimization literature and the machine learning community by providing an insight on the Pareto frontier, the efficient computation of this frontier using regularization path algorithms. The connection between these algorithms and parametric optimisation problems will be highlighted as well as issues related to sparsity, model selection and numerical implementation.", "recorded": "2008-09-19T14:00:00", "title": "The Regularization Frontier in Machine Learning"}, {"url": "sikdd08_aleksovski_fpa", "desc": "Distance-based algorithms for both clustering and\nprediction are popular within the machine learning\ncommunity. These algorithms typically deal with attributevalue\n(single-table) data. The distance functions used are\ntypically hard-coded.\n\nWe are concerned here with generic distance-based\nlearning algorithms that work on arbitrary types of\nstructured data. In our approach, distance functions are not\nhard-coded, but are rather first-class citizens that can be\nstored, retrieved and manipulated. In particular, we can\nassemble, on-the-fly, distance functions for complex\nstructured data types from pre-existing components.\n\nTo implement the proposed approach, we use the\nstrongly typed functional language Haskell. Haskell allows\nus to explicitly manipulate distance functions. We have\nproduced a SW library/application with structured data types\nand distance functions and used it to evaluate the potential of\nHaskell as a basis for future work in the field of distancebased\nmachine learning.", "recorded": "2008-10-17T11:50:00", "title": "A Functional Programming Approach to Distance-based Machine Learning"}, {"url": "nips2011_scholkopf_inference", "desc": "Kernel methods in machine learning have expanded from tricks to construct nonlinear algorithms to general tools to assay higher order statistics and properties of distributions. They find applications also in causal inference, an intriguing field that examines causal structures by testing their probabilistic footprints. However, the links between causal inference and modern machine learning go beyond this and the talk will outline some initial thoughts how problems like covariate shift adaptation and semi-supervised learning can benefit from the causal methodology.", "recorded": "2011-12-14T09:30:00", "title": "From kernels to causal inference"}, {"url": "eml07_davenport_eml", "desc": "As an alternative to cumbersome nonlinear schemes for dimensionality reduction, the technique of random linear projection has recently emerged as a viable alternative for storage and rudimentary processing of high-dimensional data. We invoke new theory to motivate the following claim: the random projection method may be used in conjunction with standard algorithms for a multitude of machine learning tasks, with virtually no degradation in performance. Thus, random projections can been shown to result in both significant computational savings and provably good performance.", "recorded": "2007-12-08T10:05:00", "title": "Efficient Machine Learning using Random Projections"}, {"url": "ecmlpkdd09_christianini_awty", "desc": "Statistical approaches to Artificial Intelligence are behind most success stories of the field in the past decade. The idea of generating non-trivial behaviour by analysing vast amounts of data has enabled recommendation systems, search engines, spam filters, optical character recognition, machine translation and speech recognition. As we celebrate the spectacular achievements of this line of research, we need to assess its full potential, its limitations and its position within the larger scheme of things. What are the next steps to take towards machine intelligence?", "recorded": "2009-09-11T09:00:00", "title": "Are We There Yet?"}, {"url": "mlsb2010_edinburgh", "desc": "The aim of this workshop was to contribute to the cross-fertilization between the research in machine learning methods and their applications to systems biology (i.e., complex biological and medical questions) by bringing together method developers and experimentalists. We encourage submissions bringing forward methods for discovering complex structures (e.g. interaction networks, molecule structures) and methods supporting genome-wide data analysis.\r\n\r\nMore about the workshop can be found at [[http://mlsb10.ijs.si/|MLSB 2010 homepage]].", "recorded": "2010-10-15T09:00:00", "title": "4th International Workshop on Machine Learning in Systems Biology (MLSB), Edinburgh 2010"}, {"url": "w3cworkshop2011_herranz_translation", "desc": "The web is an open space and the standards by which it is \"governed\" must be open. However, one barrier clearly remains to make the web even more transnational and truly global. This has been called \"the language barrier\". Language Service Providers translation business model is clearly antiquated and it is increasingly being questioned when we face real translation needs by web users. Here, immediacy is paramount. This talk is about open standards in machine translation technologies and workflows, supporting a truly multilingual web.", "recorded": "2011-04-04T16:30:00", "title": "Open Standards in Machine Translation"}, {"url": "ecml03_dubrovnik", "desc": "The 14th European Conference on Machine Learning (ECML) and the 7th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD) were co-located in Cavtat, a small tourist town near Dubrovnik, Croatia, on September 22-26, 2003. Co-ordination of the two conferences provided ample opportunities for cross-fertilization between the two areas, and followed the success of jointly organized ECML/PKDD in 2001 and 2002.", "recorded": "2003-09-22T00:00:00", "title": "\tEuropean Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD), Dubrovnik 2003"}, {"url": "mlss07_teh_dp", "desc": "**The Bayesian approach** allows for a coherent framework for dealing with uncertainty in machine learning. By integrating out parameters, Bayesian models do not suffer from overfitting, thus it is conceivable to consider models with infinite numbers of parameters, aka Bayesian nonparametric models. An example of such models is the Gaussian process, which is a distribution over functions used in regression and classification problems. Another example is the Dirichlet process, which is a distribution over distributions. Dirichlet processes are used in density estimation, clustering, and nonparametric relaxations of parametric models. It has been gaining popularity in both the statistics and machine learning communities, due to its computational tractability and modelling flexibility.\r \r In the tutorial I shall introduce Dirichlet processes, and describe different representations of Dirichlet processes, including the Blackwell-MacQueen? urn scheme, Chinese restaurant processes, and the stick-breaking construction. I shall also go through various extensions of Dirichlet processes, and applications in machine learning, natural language processing, machine vision, computational biology and beyond.\r \r In the practical course I shall describe inference algorithms for Dirichlet processes based on Markov chain Monte Carlo sampling, and we shall implement a Dirichlet process mixture model, hopefully applying it to discovering clusters of NIPS papers and authors.", "recorded": "2007-08-27T11:15:44", "title": "Dirichlet Processes: Tutorial and Practical Course"}, {"url": "ecmlpkdd2012_abbeel_learning_robotics", "desc": "Robots are typically far less capable in autonomous mode than in tele-operated mode. The few exceptions tend to stem from long days (and more often weeks, or even years) of expert engineering for a specific robot and its operating environment. Current control methodology is quite slow and labor intensive.\r\nI believe advances in machine learning have the potential to revolutionize robotics.  In this talk, I will present new machine learning techniques we have developed that are tailored to robotics. I will describe in depth \u201cApprenticeship learning,\u201d  a new approach to high-performance robot control based on learning for control from ensembles of expert human demonstrations. Our initial work in apprenticeship learning has enabled the most advanced helicopter aerobatics to-date, including maneuvers such as chaos, tic-tocs, and auto-rotation landings which only exceptional expert human pilots can fly. Our most recent work in apprenticeship learning is providing traction on learning to perform challenging robotic manipulation tasks, such as knot-tying.   I will also briefly highlight three other machine learning for robotics developments: Inverse reinforcement learning and its application to quadruped locomotion, Safe exploration in reinforcement learning which enables robots to learn on their own, and Learning for perception with application to robotic laundry.", "recorded": "2012-09-25T09:05:17", "title": "Machine Learning for Robotics"}, {"url": "aedml08_kobler_mla", "desc": "", "recorded": "2008-03-20T12:00:00", "title": "Machine Learning Applications in Forestry: Forest stand mapping using LIDAR data / Habitat mapping from satellite images / Assesing the influence of climate change on habitats"}, {"url": "ijcai2011_koller_scene", "desc": "Research focuses on using probabilistic models and machine learning to understand complex domains that involve large amounts of uncertainty. ", "recorded": "2011-07-19T14:30:00", "title": "Rich Probabilistic Models for \u000bHolistic Scene Understanding"}, {"url": "nipsworkshops2012_probabilistic_numerics", "desc": "The traditional remit of machine learning is problems of inference on complex data. At the computational bottlenecks of our algorithms, we typically find a numerical problem: optimization, integration, sampling. These inner routines are often treated as a black box, but many of these tasks in numerics can be viewed as learning problems:\r\n\r\n* How can optimizers learn about the objective function,\r\nand how should they update their search direction?\\\\\r\n* How should a quadrature method estimate an integral\r\ngiven observations of the integrand, and where should\r\nthese methods put their evaluation nodes?\\\\\r\n* How should MCMC samplers adapt their proposal distributions given past evaluations of the unnormalised density?\\\\\r\n* Can approximate inference techniques be applied to\r\nnumerical problems?\r\n\r\nMany of these problems can be seen as special cases of decision theory, active learning, or reinforcement learning. Work along these lines was pioneered twenty years ago by Diaconis and O'Hagan. But modern desiderata for a numerical algorithm differ markedly from those common elsewhere in machine learning: Numerical methods are \"inner-loop\" algorithms, used as black boxes by large groups of users on wildly different problems. As such, robustness, computation and memory costs are more important here than raw prediction power or convergence speed. Availability of good implementations also matters. These kind of challenges can be well addressed by machine learning researchers, once the relevant community is brought together to discuss these topics.\r\n\r\nSome of the algorithms we use for numerical problems were developed generations ago. They have aged well, showing impressively good performance over a broad spectrum of problems. Of course, they also have a variety of shortcomings, which can be addressed by modern probabilistic models (see some of the work cited above). In the other direction, the numerical mathematics community, a much wider field than machine learning, is bringing experience, theoretical rigour and a focus on computational performance to the table. So there is great potential for cross-fertilization to both the machine learning and numerical mathematics community. The main goals of this workshop are\r\n\r\n* to bring numerical mathematicians and machine learning researchers into contact to discuss possible contributions of machine learning to numerical methods.\\\\\r\n* to present recent developments in probabilistic numerical methods.\\\\\r\n* to discuss future directions, performance measures, test problems, and code publishing standards for this young community.\r\n\r\nWorkshop homepage: http://www.probabilistic-numerics.org/", "recorded": "2012-12-08T07:30:00", "title": "Probabilistic Numerics"}, {"url": "stanfordcs229f07_machine_learning", "desc": "This course provides a broad introduction to machine learning and statistical pattern recognition.\n\nTopics include: supervised learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs; VC theory; large margins); reinforcement learning and adaptive control.\nThe course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.\nStudents are expected to have the following background:\n\nPrerequisites:\n* Knowledge of basic computer science principles and skills, at a level sufficient to write a reasonably non-trivial computer program.\n* Familiarity with the basic probability theory. (Stat 116 is sufficient but not necessary.)\n* Familiarity with the basic linear algebra (any one of Math 51, Math 103, Math 113, or CS 205 would be much more than necessary.)\n\n**Course Homepage:** [[http://see.stanford.edu/see/courseinfo.aspx?coll=348ca38a-3a6d-4052-937d-cb017338d7b1|SEE CS229 - Machine Learning (Fall,2007)]]\n\n**Course features at Stanford Engineering Everywhere page:**\n*[[http://see.stanford.edu/see/courseinfo.aspx?coll=348ca38a-3a6d-4052-937d-cb017338d7b1|Machine Learning]]\n*[[http://see.stanford.edu/see/lecturelist.aspx?coll=348ca38a-3a6d-4052-937d-cb017338d7b1|Lectures]]\n*[[http://see.stanford.edu/materials/aimlcs229/info.pdf|Syllabus]]\n*[[http://see.stanford.edu/see/materials/aimlcs229/handouts.aspx|Handouts]]\n*[[http://see.stanford.edu/see/materials/aimlcs229/assignments.aspx|Assignments]]\n*[[http://see.stanford.edu/see/materials/aimlcs229/resources.aspx|Resources]]", "recorded": "2008-07-22T09:00:00", "title": "Stanford Engineering Everywhere CS229 - Machine Learning"}, {"url": "mlss2012_scholkopf_kernel", "desc": "The course will cover some basic ideas of learning theory, elements of the theory of reproducing kernel Hilbert spaces, and some machine learning algorithms that build upon this.", "recorded": "2012-04-11T09:00:00", "title": "Kernel Methods"}, {"url": "aedml08_debeljak_mla", "desc": "", "recorded": "2008-03-20T14:30:00", "title": "Machine Learning Applications in Agriculture: Influence of farming practices on soil fauna / Predictive modeling for co-existence of GM and conventional crops (gene-flow, seedbank persistence, ferals)"}, {"url": "google_cox_cern", "desc": "Presentation of the [[http://lhc.web.cern.ch/lhc/|Large Hadron Collider]] and of the [[http://atlasexperiment.org/|Atlas Experiment]], happening under your feet here in Geneva.", "recorded": "2007-02-02T00:00:00", "title": "CERN's 27km Big Bang machine"}, {"url": "nipsworkshops2012_hennig_quasi_newton_methods", "desc": "This talk is a case-study about the utility of probabilistic formulations for numerical mathematics. I present a recent result showing that quasi-Newton methods can be interpreted as performing Gaussian (least-squares) regression on the Hessian of the objective function, using a particular noise process to keep uncertainty constant, and a non-obvious structured prior which ignores the duality between vectors and co-vectors. This insight connects these numerical methods to important areas of machine learning (regression) and control (Kalman filters). It allows cross-fertilization: Better numerical algorithms can be built using existing knowledge from machine learning, and machine learning can benefit from a new structured prior model allowing linear-cost inference on matrix-valued operators. Arguing for more and closer interaction between the fields of learning and numerical mathematics, I also point out some challenges arising from cultural differences between these communities.", "recorded": "2012-12-08T18:00:00", "title": "Probabilistic Interpretation of Quasi-Newton Methods"}, {"url": "emergingtrends2012_ljubljana", "desc": "The main goal of this one day event is to clarify how emerging technologies based on machine learning, machine translation, text mining, semantic web, open access, academic video journals, free video libraries, open lecture capture systems, OER and more can change and help co-create emerging publishing, curriculum, designation, filtration, validation and research trends in Academia in Europe and in general. For more information please visit the [[http://ct3.ijs.si/emerging_trends2012/|Emerging Trends Workshop website]].", "recorded": "2012-11-07T09:00:00", "title": "Workshop on Co-creation of Emerging Trends in Academia, Ljubljana 2012"}, {"url": "bbci09_blankertz_muller_mlasp", "desc": "We will first provide a brief overview of Brain-Computer Interface from a machine learning and signal processing perspective. In particular showing the wealth, the complexity and the difficulties of the data available, a truly enormous challenge: In real-time a multi-variate very strongly noise contaminated data stream is to be processed and neuroelectric activities are to be accurately differentiated. We will then in detail discuss the components of the data analysis chain employed in modern BCI systems, spanning all aspects from preprocessing and feature extraction, adaptive vs. fixed classification and feedback design.", "recorded": "2009-07-08T12:00:51", "title": "Machine Learning and Signal Processing Tools for BCI"}, {"url": "mlas06_li_iff", "desc": "**After a short stay at the ECE Dept. in UIUC, Fei-Fei Li is now back to Princeton as an assistant professor in the Computer Science Dept., where she is the PI of the Vision Lab.** The Videolectures.Net team spoke to her in Pittsburgh at CMU where we asked her the following questions: \r \r *What is your field of work?\r *What is your topic of research within the Machine Learning community?\r *Where do you see your group in 5 years time?\r *What are your goals?\r *Why are you moving to Princeton University?\r *If you would have a Machine Learning dream come true...", "recorded": "2006-09-26T00:00:00", "title": "Interview with Fei-Fei Li"}, {"url": "smartdw09_barcelona", "desc": "The aim of this workshop is to disseminate scientific results produced by the SMART project to the larger technical and scientific community working on Statistical Machine Translation. To facilitate this inter-exchange, it will be co-located with EAMT 2009 - 13th Annual Conference of the European Association for Machine Translation.\r\n\r\nA joint event of SMART project - Pascal Network.\r\n\r\nSMART (Statistical Multilingual Analysis for Retrieval and Translation) is a 3-year \"Specific Target Research Project\" (STReP) funded by the European Commission. SMART is an attempt to address different problems of Machine Translation and Cross-Language Information Retrieval and other shortcomings by the methods of modern Statistical Learning.\r\n\r\nIn the first two years of the project, the scientific focus has been on developing new and more effective statistical approaches while ensuring that existing know-how is duly taken into account. By was done by bringing together leading research institutions in Statistical Learning, Machine Translation and Textual Information Access.\r\n\r\nThe aim of this workshop is to disseminate scientific results and share experiences produced by the SMART project to the larger technical and scientific community. The SMART consortium considers this workshop to be a great opportunity for science investigations, creating both scientific and commercial opportunities as well as technological challenges to researchers.\r\n\r\n----\r\nThe workshop homepage can be found at http://patterns.enm.bris.ac.uk/smart-dissemination-workshop\r\n----", "recorded": "2009-05-13T00:09:00", "title": "Statistical Multilingual Analysis for Retrieval and Translation (SMART) Dissemination Workshop, Barcelona 2009"}, {"url": "nips2010_meng_mlhi", "desc": "With the ever increasing availability of quantitative information, especially data with complex spatial and/or temporal structures, two closely related fields are undergoing substantial evolution: Machine learning and Statistics. On a grand scale, both have the same goal: separating signal from noise. In terms of methodological choices, however, it is not uncommon to hear machine learners complain about statisticians\u2019 excessive worrying over modeling and inferential principles to a degree of being willing to produce nothing, and to hear statisticians express discomfort with machine learners\u2019 tendency to let ease of practical implementation trump principled justifications, to a point of being willing to deliver anything. To take advantage of the strengths of both fields, we need to train substantially more principled corner cutters. That is, we must train researchers who are at ease in formulating the solution from the soundest principles available, and equally at ease in cutting corners, guided by these principles, to retain as much statistical efficiency as feasible while maintaining algorithmic efficiency under time and resource constraints. This thinking process is demonstrated by applying the self-consistency principle (Efron, 1967; Lee, Li and Meng, 2010) to handling incomplete and/or irregularly spaced data with non-parametric and semi-parametric models, including signal processing via wavelets and sparsity estimation via the LASSO and related penalties. ", "recorded": "2010-12-07T14:00:00", "title": "Machine Learning with Human Intelligence: Principled Corner Cutting (PC2)"}, {"url": "iswc2014_reforgiato_recupero_sheldon", "desc": "SHELDON is the first true hybridization of NLP machine\r\nreading and Semantic Web. It is a framework that builds upon a ma-\r\nchine reader for extracting RDF graphs from text so that the output is\r\ncompliant to Semantic Web and Linked Data patterns. It extends the\r\ncurrent human-readable web by using Semantic Web practices and technologies in a machine-processable form. Given a sentence in any language, it provides different semantic functionalities (frame detection,\r\ntopic extraction, named entity recognition, resolution and coreference, terminology extraction, sense tagging and disambiguation, taxonomy induction, semantic role labeling, type induction, sentiment analysis, citation inference, relation and event extraction) as well as nice visualization tools which make use of the JavaScript infoVis Toolkit and\r\nRelFinder, as well as a knowledge enrichment component that extends\r\nmachine reading to Semantic Web data. The system can be freely used\r\nat http://wit.istc.cnr.it/stlab-tools/sheldon.", "recorded": "2014-10-22T14:40:00", "title": "SHELDON: Semantic Holistic framEwork for LinkeD ONtology data"}, {"url": "uai2012_gray_machine_learning", "desc": "There is much talk of \"big data\" these days, surrounding a seismic shift currently underway in industry. Much of what is being discussed today is reminiscent of, and perhaps can be informed by, efforts beginning at least two decades ago in astronomy -- which was forced into that pioneering position by some unique scientific constraints and motivating problems that we'll discuss. So what are the best ideas that have been developed over the years for doing machine learning on massive datasets? Toward scaling up the most popular textbook machine learning methods across seven basic types of statistical tasks, we'll first identify seven main types of computational bottlenecks. Then we'll consider seven cross-cutting classes of computational techniques which characterize the current fastest algorithms for these bottlenecks, discussing their strengths and weaknesses. We'll end with some exhortations regarding where more foundational research is needed.\r\n", "recorded": "2012-08-17T08:30:00", "title": "Machine Learning on (Astronomically) Large Datasets"}, {"url": "bootcamp2010_sanchez_mlnlp", "desc": "Probabilistic Context Free Grammars (PCFG) is a powerful formalism\r\nthat has been used for several applications in Computational Linguistics. One\r\nimportant problem of these models is the probabilistic estimation of the\r\nprobabilistic part of the models. This probabilistic estimation is based on\r\ntabular algorithms similar to the CKY algorithm. We review these estimation\r\nalgorithms and their properties. The use of these models for Language Modeling\r\nand Machine Translation is also introduced. Finally, an interactive-predictive\r\nframework for parsing is explained, that can be used for developing both on-line\r\nlearning techniques and active learning techniques.", "recorded": "2010-07-08T14:10:00", "title": "Machine Learning for Natural Languages Processing"}, {"url": "w3cworkshop2012_pilos_machine_translation", "desc": "The Directorate General for Translation (DGT) has been developing since October 2010 a new data-driven machine translation service for the European Commission. MT@EC should be operational in the second semester of 2013. One of the key requirements is for the service to be flexible and open: it should enable, on one hand, the use of any type of language resources and any type of MT technology and, on the other, facilitate easy access by any client (individual or service). The speaker will present the approach taken and highlight problems identified, as pointers to broader needs that should be addressed.", "recorded": "2012-03-15T14:00:00", "title": "The Machine Translation Service of the European Commission"}, {"url": "nipsworkshops09_vandenberghe_css", "desc": "Chordal graphs play a fundamental role in algorithms for sparse matrix factorization, graphical models, and matrix completion problems. In matrix optimization chordal sparsity patterns can be exploited in fast algorithms for evaluating the logarithmic barrier function of the cone of positive definite matrices with a given sparsity pattern and of the corresponding dual cone. We will give a survey of chordal sparse matrix methods and discuss two applications in more detail: linear optimization with sparse matrix cone constraints, and the approximate solution of dense quadratic programs arising in support vector machine training.", "recorded": "2009-12-12T07:30:00", "title": "Chordal Sparsity in Semidefinite Programming and Machine Learning"}, {"url": "aop07_gaal_sdf", "desc": "Similarity and differences by finite automata in HMMs, kernels, morphological analysers, compilers and image compressors. Constraint satisfaction solving by unweighted finite automata. Weighted finite automata, basics, semirings, examples. Weighted regular expressions. Extensions: multi-tape automata, join operation, symbol classes, relations among tapes. Linguistic examples: morphology, part-of-speech tagging, German compound analysis, edit distance, asymmetric term alignment (Machine Translation with Machine Learning). Image compression and manipulation. Tools used: XFST and WFSC of Xerox.", "recorded": "2007-10-23T14:30:00", "title": "Similarity and differences by finite automata"}, {"url": "eswc2013_aimashup", "desc": "The AI mashup challenge accepts and awards mashups that use AI technology, including but not restricted to machine learning and data mining, machine vision, natural language processing, reasoning, ontologies in the context of the semantic web. Imagine for example:\r\n\r\n*Information extraction or automatic text summarization to create a task-oriented overview mashup for mobile devices\r\n*Semantic Web technology and data sources adapting to user and task-specific configurations\r\n*Semantic background knowledge (such as ontologies, WordNet, Freebase or Cyc) to improve search and content combination\r\n*Machine translation for mashups that cross-language borders\r\n*Machine vision technology for novel ways of aggregating images, for instance mixing real and virtual environments\r\n*Intelligent agents taking over simple household planning tasks\r\n*Text-to-speech technology creating speech mashups with intelligent and emotional intonation\r\n*Speech-to-text technology for interactive speech mashups and multimodal services\r\n*The display of Pub Med articles on a map based on geographic entity detection referring to diseases or health centers\r\n*The integration of enterprise data - see Open Mashup Alliance.\r\n\r\nThe emphasis is not on providing and consuming semantic markup, but rather on using intelligence to mashup these resources in a more powerful way. For more ideas have a look at [[http://aimashup.org/aimashup13/doku.php?id=results|AI Mashup Challenge 2013 website]].", "recorded": "2013-05-28T17:00:00", "title": "AI Mashup Challenge 2013"}, {"url": "russir2012_ruiz_costa_khalilov_concept", "desc": "Our world is currently in an era of globalization, which implies increasing interaction and the intertwining of different language communities. Machine translation technology is one of the core components of efficient multilingual information environment and should be seen as a strategic issue in the framework of the modern multilingual community. Nowadays, statistical machine translation (SMT) is one of the most popular paradigms of MT. The aim of the course is to teach students the background, theory and implementation behind SMT systems. The course covers aspects of SMT technology from formal description to implementation. It is divided into three parts: theoretical background of SMT; introduction to the main SMT software; and SMT industry perspectives, along with open-source and commercial products. At the end of the course, students will be aware of the taxonomy of various approaches to SMT; the main research problems in the field; the MT industrial applications and they will be able to test and train real SMT systems using the open-source Moses toolkit.", "recorded": "2012-08-05T13:40:00", "title": "The concept and feasibility of modern statistical machine translation: theory, practice and industry applications"}, {"url": "mlss02_canberra", "desc": "Machine Learning is a foundational discipline of the Information Sciences. It combines deep theory from areas as diverse as Statistics, Mathematics, Engineering, and Information Technology with many practical and relevant real life applications. The aim of the summer school is to cover the entire spectrum from theory to practice. It is mainly targeted at research students, IT professionals, and academics from all over the world.\r\n\r\n**//Disclaimer:// VideoLectures.NET emphasizes that we are not authors of these videos and we are not taking responsibility for the low quality of these videos. They were uploaded due to their content value in the scientific field of Computer Science and published with the permission of the organizers.**", "recorded": "2002-02-11T00:00:00", "title": "Machine Learning Summer School (MLSS), Canberra 2002"}, {"url": "ecml07_warsaw", "desc": "The 18th European Conference on Machine Learning (ECML) and the 11th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD) were co-located in Warsaw, Poland, from September 17th to 21st, 2007. \n\nThe ECML/PKDD conference series intends to provide an international forum for the discussion of the latest high quality research results and is the major European scientific event in the field. The combined event comprised of presentations of contributed papers and invited talks, a wide program of workshops and tutorials, discovery challenge and industrial track.", "recorded": "2007-09-17T09:00:00", "title": "European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD), Warsaw 2007"}, {"url": "mlss03_tubingen", "desc": "Machine Learning is a foundational discipline of the Information Sciences. It combines deep theory from areas as diverse as Statistics, Mathematics, Engineering, and Information Technology with many practical and relevant real life applications.\r\n\r\nThe aim of the summer school is to cover the entire spectrum from theory to practice. It is mainly targeted at research students, IT professionals, and academics from all over the world.\r\n\r\n\r\n**//Disclaimer:// VideoLectures.NET emphasizes that we are not authors of these videos and we are not taking responsibility for the low quality of these videos. They were uploaded due to their content value in the scientific field of Computer Science and published with the permission of the organizers.**", "recorded": "2003-08-04T00:00:00", "title": "Machine Learning Summer School (MLSS), T\u00fcbingen 2003"}, {"url": "machine_learning_video_abstracts_vol5", "desc": "Welcome to the 5th issue of the Journal of Machine Learning Video Abstracts, published by Knowledge for All Foundation Ltd, London, with the goal of providing the legacy of the PASCAL2 Network of Excellence.\r\nThe fifth issue is dedicated to presenting the spotlight presentations of the 2013 Neural Information Processing Systems (NIPS) Conference held in Lake Tahoe, Nevada, United States, on December 5-10. \r\n\r\nThe presentations were lengthened to five minutes for this year's conference with an allowance of 5 slides for each paper. Microsoft recorded and post-processed the recordings whereas the VideoLectures.NET team incorporated the slides into the videos.\r\n", "recorded": "2013-12-06T00:00:00", "title": "Video Journal of Machine Learning Abstracts - Volume 5"}, {"url": "fmri06_mazaika_wcmll", "desc": "We describe three experiments combining neuroimaging and machine learning. The first\r experiment compares the performance of maximum likelihood and neural net classifiers for\r \"brain reading\" of fMRI data in the visual cortex. The second experiment applies the optimal\r classifier to measure the development of the face region in children and adolescents. While the\r previous experiments used block designs, the third experiment describes an event-related\r experiment where the classification algorithm learned something real, but not what was planned.\r The corroboration and validation of the classification results with brain images will be\r demonstrated.", "recorded": "2006-12-08T00:00:00", "title": "Classifying single trial fMRI: What can machine learning learn?"}, {"url": "ecmlpkdd2011_sundaresan_data", "desc": "Large Social Commerce Network sites like eBay have to constantly grapple with building scalable machine learning algorithms for search ranking, recommender systems, classification, and others. Large data availability is both a boon and curse. While it offers a lot more diverse observation, the same diversity with sparsity and lack of reliable labeled data at scale introduces new challenges. Also, availability of large data helps take advantage of correlational factors while requiring creativity in discarding irrelevant data. In this talk we will discuss all of this and more from the context of eBay\u2019s large data problems.", "recorded": "2011-09-09T09:00:00", "title": "Data Science and Machine Learning at Scale"}, {"url": "ecmlpkdd2010_peters_rsl", "desc": "Autonomous robots that can assist humans in situations of daily life have been a long standing vision of artificial intelligence, robotics, and cognitive sciences. A first step towards this goal is to create robots that can learn tasks triggered by environmental context or higher level instruction. Hence, it is essential that more machine learning researchers dare to face the challenge of robot learning. To enable scientists an easier entry into this field, this tutorial will present an overview over the state of the art. It will cover both real-world problems and research opportunities for a machine learning researchers who are willing to enter the area of robot learning. Empirical evaluations on a several robot systems illustrate the effectiveness and applicability of the presented methods for anthropomorphic robotsAutonomous robots that can assist humans in situations of daily life have been a long standing vision of artificial intelligence, robotics, and cognitive sciences. A first step towards this goal is to create robots that can learn tasks triggered by environmental context or higher level instruction. Hence, it is essential that more machine learning researchers dare to face the challenge of robot learning. To enable scientists an easier entry into this field, this tutorial will present an overview over the state of the art. It will cover both real-world problems and research opportunities for a machine learning researchers who are willing to enter the area of robot learning. Empirical evaluations on a several robot systems illustrate the effectiveness and applicability of the presented methods for anthropomorphic robots.", "recorded": "2010-09-20T09:00:00", "title": "Robot skill Learning"}, {"url": "bmvc2013_coates_machine_vision", "desc": "Machine learning algorithms have freed practitioners from many error-prone, hand-engineered components for making decisions in common machine vision tasks such as object recognition. A major source of difficulty, however, is that such learning systems still rely on many hand-built components like sophisticated feature extractors that attempt to identify higher-level patterns in images that typical learning algorithms cannot discover on their own. \"Deep learning\" and \"representation learning\" algorithms aim to remove this hurdle by learning higher-level representations automatically from data and have led to recent successes in vision, speech, and language tasks. This tutorial will introduce the basic components of deep learning algorithms and practical techniques for debugging and applying these methods to machine vision problems. The first part of the tutorial will cover neural network models and basic training approaches including error back-propagation and numerical optimization methods, with image classification as a motivating application. The second part will cover additional (sometimes domain-specific) techniques to improve the performance of these algorithms and apply them to other vision tasks including detection and image segmentation. With these tools, audience members will understand how deep learning algorithms work and how they are used in practical applications with sufficient knowledge to complete a hands-on tutorial available on the web. We will conclude with a brief high-level overview of other important topics and results in deep learning research.", "recorded": "2013-09-09T13:30:00", "title": "Deep Learning for Machine Vision"}, {"url": "ijcai09_feigenbaum_cata", "desc": "The Computers and Thought Award is presented at IJCAI conferences to outstanding young scientists in artificial intelligence. The award was established with royalties received from the book, Computers and Thought, edited by Edward Feigenbaum and Julian Feldman; it is currently supported by income from IJCAI funds.  Past recipients of this honor have been: Terry Winograd (1971), Patrick Winston (1973), Chuck Rieger (1975), Douglas Lenat (1977), David Marr (1979), Gerald Sussman (1981), Tom Mitchell (1983), Hector Levesque (1985), Johan de Kleer (1987), Henry Kautz (1989), Rodney Brooks (1991), Martha Pollack (1991), Hiroaki Kitano (1993), Sarit Kraus (1995), Stuart Russell (1995), Leslie Kaelbling (1997), Nicholas Jennings (1999), Daphne Koller (2001), Tuomas Sandholm (2003), and Peter Stone (2007). \r\n\r\nThere are two winners of the 2009 IJCAI Computers and Thought Award: Carlos Guestrin, Assistant Professor in the Machine Learning Department and the Computer Science Department of Carnegie Mellon University, and Andrew Ng, Assistant Professor in the Computer Science Department of Stanford University. Professor Guestrin is recognized for significant contributions to machine learning, probabilistic reasoning, and intelligent distributed sensor networks. Professor Ng is recognized for fundamental contributions to the application of machine learning to robot perception and control, for leadership in constructing robots that perform unscripted tasks in real environments, and for major contributions to machine learning.", "recorded": "2009-07-14T19:05:00", "title": "IJCAI-09 Computers and Thought Award"}, {"url": "aaai08_sermanet_plalrv", "desc": "New York University uses machine learning to extend the vision of the DARPA LAGR robots.", "recorded": "2008-06-02T11:00:00", "title": "10. DARPA LAGR Program: Learning Applied to Long-Range Vision using a Collision-Free Navigation Platform"}, {"url": "turing100_kasparov_friedel_paper_machine", "desc": "It is an amazing fact that the very first chess program in history was written a few years before computers had been invented. It was designed by a visionary man who knew that programmable computers were coming and that, once they were built, they would be able to play chess. The man, of course, was Alan Turing, one of the greatest mathematicians who ever lived. Soon after the war he wrote the instructions that would enable a machine to play chess. Since there was as yet no machine that could execute the instructions he did so himself, acting as a human CPU and requiring more than half an hour per move. A single game is recorded, one in which Turing's \"paper machine\" lost to a colleague.\r\n\r\nGarry Kasparov will sketch the historical context of Turing\u2019s involvement in chess and then go on to describe how the chess computer experts reconstructed the paper machine to run on a modern day computer. In the process they encountered a problem: the chess engine refused to duplicate all of Turing\u2019s moves as recorded in the historical game. The debugging process, in which computer chess pioneer Ken Thompson was involved, left the programmers baffled. Then someone called Donald Michie, a colleague from Bletchley, who advocated debugging not the program but Turing! \u201cAlan did not care about details; he was interested in general principle.\u201d Kasparov\u2019s lecture will discuss the points of deviation from the recorded game.\r\n\r\nIn the second part of the lecture Kasparov will describe a number of Turing Tests that have been performed for chess. For a while it was impossible to reliably tell computer games from those of humans. However, today the task has become simpler because of the ruthless precision of computer play, which has reached a level of many hundreds of Elo points above the best human players.", "recorded": "2012-06-24T09:00:00", "title": "The Reconstruction of Turing's \"Paper Machine\""}, {"url": "icml09_seeger_igps", "desc": "Most machine learning (ML) algorithms rely fundamentally on concepts of numerical mathematics. Standard reductions to black-box computational primitives do not usually meet real-world demands and have to be modified at all levels. The increasing complexity of ML problems requires layered approaches, where algorithms are components rather than stand-alone tools fitted individually with much human effort. In this modern context, predictable run-time and numerical stability behavior of algorithms become fundamental. Unfortunately, these aspects are widely ignored today by ML researchers, which limits the applicability of ML algorithms to complex problems.\r\nBackground and Objectives\r\n\r\nOur workshop aims to address these shortcomings, by trying to distill a compromise between inadequate black-box reductions and highly involved complete numerical analysis. We will invite speakers with interest in *both* numerical methodology *and* real problems in applications close to machine learning. While numerical software packages of ML interest will be pointed out, our focus will rather be on how to best bridge the gaps between ML requirements and these computational libraries. A subordinate goal will be to address the role of parallel numerical computation in ML. Examples of machine learning founded on numerical methods include low level computer vision and image processing, non-Gaussian approximate inference, Gaussian filtering / smoothing, state space models, approximations to kernel methods, and many more.\r\nImpact and Expected Outcome\r\n\r\nWe will call the community's attention to the increasingly critical issue of numerical considerations in algorithm design and implementation. A set of essential rules for how to use and modify numerical software in ML is required, for which we aim to lay the groundwork in this workshop. These efforts should lead to an awareness of the problems, as well as increased focus on efficient and stable ML implementations. We will encourage speakers to point out useful software packages, together with their caveats, asking them to focus on examples of ML interest. Raising awareness about the increasing importance of stability and predictable run-time behaviour of numerical machine learning algorithms and primitives. Establishing a code of conduct for how to best select and modify existing numerical mathematics code for machine learning problems. Learning about developments in current numerical mathematics, a major backbone of most machine learning methods.", "recorded": "2009-06-18T08:00:00", "title": "Introduction and General Problem Statement"}, {"url": "w3cworkshop2012_lewis_metadata_interoperability", "desc": "January 2012 saw the kick off of the MLW-LT (\"Multilingual Web - Language Related Technologies\") Working Group at the W3C as part of the Internationalization Activity. This WG will define meta-data for web content (mainly HTML5) and \"deep Web\" content (CMS or XML files from which HTML pages are generated) that facilitates content interaction with multilingual language technologies such as machine translation, and localization processes. The WG brings together localisation and content management companies with content and language meta-data research expertise, including strong representation from the Centre for Next Generation Localisation. This talk will present three concrete business use cases that span CMS, localisation and machine translation functions. It discusses the challenges in addressing these cases with existing metadata (e.g. ITS tags) and the technical requirements for additional standardised metadata. This talk will be complemented by a breakout session to allow attendees to voice their comments and requirements in more detail, in order to better inform the working group.", "recorded": "2012-03-15T14:00:00", "title": "Meta-data interoperability between CMS, localisation and machine translation: Use Cases and Technical Challenges"}, {"url": "prib2010_widmer_nmlm", "desc": "MHC class I molecules are key players in the human immune system. They bind small peptides derived from intracellular proteins and present them on the cell surface for surveillance by the immune system. Prediction of such MHC class I binding peptides is a vital step in the design of peptide-based vaccines and therefore one of the major problems in computational immunology. Thousands of different types of MHC class I molecules exist, each displaying a distinct binding specificity. The lack of sufficient training data for the majority of these molecules hinders the application of Machine Learning to this problem.\r\n\r\nWe propose two approaches to improve the predictive power of kernel-based Machine Learning methods for MHC class I binding prediction: First, a modification of the Weighted Degree string kernel that allows for the incorporation of amino acid properties. Second, we propose an enhanced Multitask kernel and an optimization procedure to fine-tune the kernel parameters. The combination of both approaches yields improved performance, which we demonstrate on the IEDB benchmark data set.", "recorded": "2010-09-23T11:15:00", "title": "Novel Machine Learning Methods for MHC Class I Binding Prediction"}, {"url": "kdd07_rao_lcad", "desc": "We present LungCAD, a computer aided diagnosis (CAD) system that employs a classification algorithm for detecting solid pulmonary nodules from CT thorax studies. We briefly describe some of the machine learning techniques developed to overcome the real world challenges in this medical domain. The most significant hurdle in transitioning from a machine learning research prototype that performs well on an in-house dataset into a clinically deployable system, is the requirement that the CAD system be tested in a clinical trial. We describe the clinical trial in which LungCAD was tested: a large scale multi-reader, multi-case (MRMC) retrospective observational study to evaluate the effect of CAD in clinical practice for detecting solid pulmonary nodules from CT thorax studies. The clinical trial demonstrates that every radiologist that participated in the trial had a significantly greater accuracy with LungCAD, both for detecting nodules and identifying potentially actionable nodules; this, along with other findings from the trial, has resulted in FDA approval for LungCAD in late 2006.", "recorded": "2007-08-14T14:45:00", "title": "LungCAD: A Clinically Approved, Machine Learning System for Lung Cancer Detection"}, {"url": "iswc07_bloehdorn_kmmi", "desc": "The amount of ontologies and meta data available on the Web is constantly growing. The successful application of machine learning techniques for learning of ontologies from textual data, i.e. mining for the Semantic Web, contributes to this trend. However, no principal approaches exist so far for mining from the Semantic Web. We investigate how machine learning algorithms can be made amenable for directly taking advantage of the rich knowledge expressed in ontologies and associated instance data. Kernel methods have been successfully employed in various learning tasks and provide a clean framework for interfacing between non-vectorial data and machine learning algorithms. In this spirit, we express the problem of mining instances in ontologies as the problem of defining valid corresponding kernels. We present a principled framework for designing such kernels by means of decomposing the kernel computation into specialized kernels for selected characteristics of an ontology which can be flexibly assembled and tuned. Initial experiments on real world Semantic Web data enjoy promising results and show the usefulness of our approach.", "recorded": "2007-11-15T11:00:00", "title": "Kernel Methods for Mining Instance Data in Ontologies"}, {"url": "ecmlpkdd08_antwerp", "desc": "The European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD) will be located in Antwerp, Belgium, from September 15th to 19th, 2008. This event builds upon a very successful series of 18 ECML and 11 PKDD conferences, which have been jointly organized for the past seven years. It has become the major European scientific event in these fields and in 2008 it will comprise presentations of contributed papers and invited speakers, a wide program of workshops and tutorials, a discovery challenge, a demo track and an industrial track.", "recorded": "2008-09-15T09:00:00", "title": "European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD), Antwerp 2008"}, {"url": "icml2015_ward_randomization", "desc": "A wide variety of problems in machine learning, including exemplar clustering, document summarization, and sensor placement, can be cast as constrained submodular maximization problems. Unfortunately, the resulting submodular optimization problems are often too large to be solved on a single machine. We consider a distributed, greedy algorithm that combines previous approaches with randomization. The result is an algorithm that is embarrassingly parallel and achieves provable, constant factor, worst-case approximation guarantees. In our experiments, we demonstrate its efficiency in large problems with different kinds of constraints with objective values always close to what is achievable in the centralized setting.", "recorded": "2015-07-09T14:46:53", "title": "The Power of Randomization: Distributed Submodular Maximization on Massive Datasets"}, {"url": "mlss08au_webers_mll", "desc": "The first laboratory has not been recorded but has featured some hands on experiments with Elefant ([[http://elefant.developer.nicta.com.au/]]) mainly concentrating on installing, using, and developing machine learning algorithms within the Elefant framework. We will walk through examples of implementing a simple stochastic gradient descent algorithm as a part of this tutorial. This is the first part of the second session which is split with [[mlss08au_vishwanathan_mll|//S.V.N. Vishwanathan's// \"%title\"]] and will feature hands on experiments with BNRM (Bundle Methods for Regularized Risk Minimization) ([[http://users.rsise.anu.edu.au/~chteo/BMRM.html]]). The emphasis here will be on developing various loss function modules which can then be plugged into the BMRM solver.", "recorded": "2008-03-14T15:30:00", "title": "Machine Learning Laboratory"}, {"url": "mlss08au_vishwanathan_mll", "desc": "The first laboratory has not been recorded but has featured some hands on experiments with Elefant ([[http://elefant.developer.nicta.com.au]]) mainly concentrating on installing, using, and developing machine learning algorithms within the Elefant framework. We will walk through examples of implementing a simple stochastic gradient descent algorithm as a part of this tutorial. This is the second part of the second session which is split with [[mlss08au_webers_mll|//Christfried Webers's// \"%title\"]] and will feature hands on experiments with BNRM (Bundle Methods for Regularized Risk Minimization) ([[http://users.rsise.anu.edu.au/~chteo/BMRM.html]]). The emphasis here will be on developing various loss function modules which can then be plugged into the BMRM solver.", "recorded": "2008-03-14T18:00:00", "title": "Machine Learning Laboratory"}, {"url": "nips2010_larochelle_lcf", "desc": "We describe a model based on a Boltzmann machine with third-order connections that can learn how to accumulate information about a shape over several fixations. The model uses a retina that only has enough high resolution pixels to cover a small area of the image, so it must decide on a sequence of fixations and it must combine the \"glimpse\" at each fixation with the location of the fixation before integrating the information with information from other glimpses of the same object. We evaluate this model on a synthetic dataset and two image classification datasets, showing that it can perform at least as well as a model trained on whole images.", "recorded": "2010-12-09T10:00:00", "title": "Learning to combine foveal glimpses with a third-order Boltzmann machine"}, {"url": "epsrcws08_blake_amv", "desc": "This presentation describes novel approaches to spatial inference \r problems in vision and image processing. Markov random field models are \r described for image restoration, foreground segmentation, graph cutting \r and stereo matching.", "recorded": "2008-01-25T13:30:47", "title": "Applications to Machine Vision"}, {"url": "stanfordcs229f08_ng_lec06", "desc": "Multinomial Event Model, Non-linear Classifiers, Neural Network, Applications of Neural Network, Intuitions about Support Vector Machine (SVM), Notation for SVM, Functional and Geometric Margins", "recorded": "2009-04-17T10:00:00", "title": "Lecture 6 - Multinomial Event Model "}, {"url": "sikdd08_dali_tes", "desc": "In this paper we present a machine learning approach to\nextract subject-predicate-object triplets from English\nsentences. \n\nSVM is used to train a model on human\nannotated triplets, and the features are computed from\nthree parsers.", "recorded": "2008-10-17T10:00:00", "title": "Triplet Extraction from Sentences"}, {"url": "pesb07_manchester", "desc": "Systems Biology models often have numerous parameters, such as kinetic constants, decay rates and drift/diffusion terms, which are unknown or only weakly constrained by existing experimental knowledge. A crucial problem for Systems Biology is that these parameters are often very difficult to measure directly. Furthermore, they may vary greatly according to their in vivo context. As a result, methods for the estimation of these parameters are of great interest. Standard approaches include maximum likelihood or least squares methods, using various optimisation heuristics such as simulated annealing and evolutionary algorithms. Although these approaches have had some success, it is very difficult to estimate the parameters when there are many interactions in the system under consideration. In this case the likelihood surface may have many local optima, the parameters may be poorly determined because there is not enough data, or there may be ambiguities brought about by symmetries or redundancy in the system.\r\n\r\nThese same issues arise in machine learning. However, the problems in Systems Biology have an additional facet. In machine learning the models of interest are typically general function approximators, whereas in Systems Biology models are intended to provide a mechanistic description of the system, often using ordinary or stochastic differential equations.\r\n\r\nRecently, attention in machine learning and statistical inference has turned to parameter estimation in these models. The main goal of this workshop is to bridge the divide between the fields by bringing together experts in machine learning and statistics with systems biologists and bioinformaticians.", "recorded": "2007-03-28T09:00:00", "title": "Workshop on Parameter Estimation in Systems Biology, Manchester 2007"}, {"url": "nipsworkshops2010_computational_photography", "desc": "Computational photography (CP) is a new field that explores\r\nand is about to redefine how we take photographs and videos.\r\nApplications of CP are not only \"everyday\" photography but\r\nalso new methods for scientific imaging, such as microscopy,\r\nbiomedical imaging, and astronomical imaging, and can thus be\r\nexpected to have a significant impact in many areas. There is\r\nan apparent convergence of methods, what we have traditionally\r\ncalled \"image processing\", and recently many works in machine\r\nvision, all of which seem to be addressing very much the same, if\r\nnot tightly related problems. These include deblurring, denoising,\r\nand enhancement algorithms of various kinds. \r\n*What do we learn from this convergence and its application to CP? \r\n*Can we create more contact between the practitioners of these fields, who often\r\ndo not interact? \r\n*Does this convergence mean that the fields are intellectually shrinking to the same point, or expanding and hence overlapping with each other more? \r\n\r\nBesides discussing such questions, the goal of this workshop is two-fold: \r\n*(i) to present the current approaches, their possible limitations, and\r\nopen problems of CP to the NIPS community, and \r\n*(ii) to foster interaction between researchers from machine learning, neuro\r\nscience and CP to advance the state of the art in CP. \r\n\r\nThe key of the existing CP approaches is to combine (i) creative hardware\r\ndesigns with (ii) sophisticated computations, such as e.g. new\r\napproaches to blind deconvolution. This interplay between both\r\nhardware and software is what makes CP an ideal real-world\r\ndomain for the whole NIPS community, who could contribute in\r\nvarious ways to its advancement, be it by enabling new imaging\r\ndevices that are possible due to the latest machine learning\r\nmethods or by new camera and processing designs that are\r\ninspired by our neurological understanding of natural visual\r\nsystems. Thus the target group of participants are researchers\r\nfrom the whole NIPS community (machine learning and neuro\r\nscience) and researchers working on CP and related fields.\r\n\r\nWorkshop homepage: http://people.kyb.tuebingen.mpg.de/mhirsch/mlmcp/mlmcp.html", "recorded": "2010-12-10T07:30:00", "title": "Machine Learning meets Computational Photography"}, {"url": "fmri06_hardoon_ufa", "desc": "Recently machine learning methodology has been used increasing to analyze the relationship\r between stimulus categories and fMRI responses [2, 14, 15, 11, 13, 8, 9, 1, 12, 7].\r Here, we introduce a new unsupervised machine learning approach to fMRI analysis approach,\r in which the simple categorical description of stimulus type (e.g. type of task)\r is replaced by a more informative vector of stimulus features. We compared this new\r approach with a standard Support Vector Machine (SVM) analysis of fMRI data using a\r categorical description of stimulus type.\r The following study differs from conventional unsupervised approaches in that we make\r use of the stimulus characteristics. We use kernel Canonical Correlation Analysis (KCCA)\r to learn the correlation between the fMRI volume and the corresponding stimulus features\r presented at a particular time point. CCA can be seen as the problem of finding basis\r vectors for two sets of variables such that the correlation of the projections of the variables\r onto these basis vectors are mutually maximised. KCCA first projects the data into a higher\r dimensional feature space before performing CCA in the new feature space.", "recorded": "2006-12-08T00:00:00", "title": "Unsupervised fMRI Analysis"}, {"url": "lsoldm2013_diethe_machine_learning", "desc": "Model-Based Machine Learning using deterministic approximations is well suited to large scale environments, since it allows practitioners to develop sophisticated models that can still be solved efficiently. Infer.NET is a framework for performing inference on Bayesian Factor Graphs, and has been designed from the ground up to be computationally efficient. The compiler architecture means that the generated inference code often approaches the efficiency of hand-written code. Infer.NET also supports batch-processing of large datasets by sharing variables between models and the use of sparse messages when using Expectation Propagation. Customised message operators can also be implemented to overcome particular performance bottlenecks.", "recorded": "2013-09-24T17:35:18", "title": "Large Scale Model-Based Machine Learning"}, {"url": "iccc2014_kantosalo_machine_creativity", "desc": "This  paper  investigates  how  to  transform machine  creativity  systems  into  interactive  tools  that \r\nsupport human-computer co-creation. We use three case studies to identify common issues in this \r\ntransformation, under the perspective of User-Centered Design. We also analyse the interactivity \r\nand creative behavior of the three platforms in terms of Wiggins\u2019 formalization of creativity as a \r\nsearch.  We  arrive  at  the  conclusion  that  adapting  creative  software  for  supporting  human-\r\ncomputer co-creation requires redesigning some major aspects of the software, which guides our \r\non-going project of building an interactive poetry composition tool. ", "recorded": "2014-06-10T09:10:00", "title": "From Isolation to Involvement: Adapting Machine Creativity Software to Support Human-Computer Co-Creation"}, {"url": "sicgt07_belkin_gmagod", "desc": "In recent years graph-based methods have seen success in different machine learning applications, including clustering, dimensionality reduction and semi-supervised learning. In these methods a graph is associated to a data set, after which certain aspects of the graph are used for various machine learning tasks. It is, however, important to observe that such graphs are empirical objects corresponding to a randomly chosen set of data points. In my talk I will discuss some of our work on using spectral graph methods for dimensionality reduction and semi-supervised learning and certain theoretical aspects of these methods, in particular, when data is sampled from a low-dimensional manifold.", "recorded": "2007-09-07T18:39:35", "title": "Graph methods and geometry of data"}, {"url": "nipsworkshops2010_alpaydin_sld", "desc": "We propose a supervised and localized dimensionality reduction method that combines\r\nmultiple feature representations or kernels. Each feature representation or\r\nkernel is used where it is suitable through a parametric gating model in a supervised\r\nmanner for efficient dimensionality reduction and classification, and local\r\nprojection matrices are learned for each feature representation or kernel. The kernel\r\nmachine parameters, the local projection matrices, and the gating model parameters\r\nare optimized using an alternating optimization procedure composed of\r\nkernel machine training and gradient-descent updates. Empirical results on benchmark\r\ndata sets validate the method in terms of classification accuracy, smoothness\r\nof the solution, and ease of visualization.", "recorded": "2010-12-11T16:35:00", "title": "Supervised and Localized Dimensionality Reduction from Multiple Feature Representations or Kernels"}, {"url": "mit21l448jf2010_paradis_lec17", "desc": "Topics: Evolution and Cybernetics\r\n\r\n    Norbert Wiener, the golem, and the problem of control\r\n    The key issue of cybernetics\r\n    Evolution as Phylogenetic Learning\r\n    Biological versus machine reproduction, Sorcery versus science\r\n    An increasingly automated world\r\n", "recorded": "2010-12-19T09:00:00", "title": " Lecture 17: Evolution and Cybernetics "}, {"url": "colt2015_frongillo_property_elicitation", "desc": "The elicitation of a statistic, or property of a distribution, is the task of devising proper scoring rules, equivalently proper losses, which incentivize an agent or algorithm to truthfully estimate the desired property of the underlying probability distribution. Exploiting the connection of such elicitation to convex analysis, we address the vector-valued property case, which has received relatively little attention in the literature despite its applications to both machine learning and statistics.\r\nWe first provide a very general characterization of linear and ratio-of-linear properties, the first of which resolves an open problem by unifying and strengthening several previous characterizations in machine learning and statistics. We then ask which vectors of properties admit nonseparable scores, which cannot be expressed as a sum of scores for each coordinate separately, a natural desideratum for machine learning. We show that linear and ratio-of-linear do admit nonseparable scores, and provide evidence for the conjecture that these are the only such properties (up to link functions). Finally, we provide a general method for producing identification functions and address an open problem by showing that convex maximal level sets are insufficient for elicitability in general.", "recorded": "2015-07-05T10:20:00", "title": "Vector-Valued Property Elicitation"}, {"url": "mlsb2012_basel", "desc": "**MLSB12**, the Sixth International Workshop on Machine Learning in Systems Biology was held in Basel, Switzerland on September 8-9, 2012.\n\nThe aim of this workshop is to contribute to the cross-fertilization between the research in machine learning methods and their applications to systems biology (i.e., complex biological and medical questions) by bringing together method developers and experimentalists. We encourage submissions bringing forward methods for discovering complex structures (e.g. interaction networks, molecule structures) and methods supporting genome-wide data analysis.\n\nMost of the workshop proceedings are available at the **[[http://bioinformatics.oxfordjournals.org/content/28/18.toc#MLSB2012PAPERS|Bioinformatics Journal, Volume 28 Issue 21 November 1, 2012]]**.\n\nThe Workshop was organized as \"Satellite Meeting\" of the 11th European Conference on Computational Biology (ECCB) by **[[http://webdav.tuebingen.mpg.de/u/karsten/group/index.html?page=employee&employee=Karsten|Karsten Borgwardt]]** and **[[http://cbio.mskcc.org/research/ratsch-research-group/|Gunnar R\u00e4tsch]]**. MLSB 2012 is supported by the **PASCAL2 Network of Excellence**.\n\nTo find out more please visit the [[http://www.mlsb.cc/index.html|MLSB 2012 website]].", "recorded": "2012-09-08T09:00:00", "title": "6th International Workshop on Machine Learning in Systems Biology (MLSB), Basel 2012"}, {"url": "ecmlpkdd2010_graepel_mlm", "desc": "Machine Learning plays a crucial role in Microsoft's online services. In this talk, I will describe three powerful applications of machine learning.\r\n\r\nTrueSkill is Xbox Live's Ranking and Matchmaking system and ensures that gamers online have balanced and exciting matches with equally skilled opponents. \r\nAdPredictor is the system that estimates click-through rates (CTR) for ad selection and pricing within Microsoft's search engine Bing. \r\nMatchbox is a large scale Bayesian recommender system that combines aspects of collaborative filtering and content-based recommendation. It is currently being used for tweet recommendation within projectemporia.com. \r\n\r\nAll three systems have in common that they are based on techniques from graphical models and approximate Bayesian inference, yet operate at large scale. I will discuss the underlying models and algorithms as well as application-specific insights and findings. Time permitting, I will show the three systems in action. This is based on joint work with Ralf Herbrich, David Stern, Thomas Borchert, Tom Minka, and Joaquin Qui\u00f1onero Candela.TrueSkill is Xbox Live's Ranking and Matchmaking system and ensures that gamers online have balanced and exciting matches with equally skilled opponents.", "recorded": "2010-09-24T15:27:00", "title": "Machine Learning in Microsoft's Online Services: TrueSkill, AdPredictor, and Matchbox "}, {"url": "icml09_leskovec_msain", "desc": "Emergence of the web, social media and online social networking websites gave rise to detailed traces of human social activity. This offers many opportunities to analyze and model behaviors of millions of people. For example, we can now study ''planetary scale'' dynamics of a full Microsoft Instant Messenger network of 240 million people, with more than 255 billion exchanged messages per month. \r\nMany types of data, especially web and \"social\" data, come in a form of a network or a graph. This tutorial will cover several aspects of such network data: macroscopic properties of network data sets; statistical models for modeling large scale network structure of static and dynamic networks; properties and models of network structure and evolution at the level of groups of nodes and algorithms for extracting such structures. I will also present several applications and case studies of blogs, instant messaging, Wikipedia and web search. \r\nMachine learning as a topic will be present throughout the tutorial. The idea of the tutorial is to introduce the machine learning community to recent developments in the area of social and information networks that underpin the Web and other on-line media.", "recorded": "2009-06-14T16:10:00", "title": "Modeling Social and Information Networks: Opportunities for Machine Learning"}, {"url": "amlcf09_london", "desc": "During recent years, the use of intelligent systems in the financial and economic industries have increased substantially, providing a new perspective to the agenda of finance and economics by their ability to handle large amounts of financial data and simulate complex models. This field of research is known as computational finance. The most common applications of computational finance are within the area of investment banking and financial risk management, and currently employ learning methods such as Support Vector Machines, Bayesian approaches, Regression, Neural Network, Fuzzy Logic and Genetic Algorithms.\r\n\r\nThe aim of the workshop is to open discussion and stimulate interaction between the disciplines of computational finance and machine learning geared towards the development of new methods that will answer specific complex questions in finance. The workshop is targeted towards academics and professionals alike.\r\n\r\nThe workshop is organised by the centre for Computational Statistical and Machine Learning (CSML) and by the Financial Computing Team at University College London under the sponsorship of the Patterns Analysis, Statistical Modelling and Computational Learning (PASCAL) Network of Excellence 2.\r\n\r\nDetailed information can be found at the [[http://web.mac.com/davidrh/AMLCF09/Workshop.html|Workshop homepage]].\r\n", "recorded": "2009-07-20T09:00:00", "title": "International Workshop on Advances in Machine Learning for Computational Finance (AMLCF), London 2009"}, {"url": "icwsm2011_pennacchiotti_classification", "desc": "This paper addresses the task of user classification in social media such as Twitter, i.e. automatically inferring the values of user attributes (e.g. political orientation, ethnicity) by leveraging observable information such as the user behavior, network structure and the linguistic content of the users Twitter feed.We solve the task using a machine learning approach that leverages a large set of features. We report experimental results on 3 tasks with different characteristics (political affiliation detection, ethnicity identification and detecting affinity for a particular business). We show that such an automatic analysis of user accounts can be done with good results and that rich linguistic features prove consistently valuable across tasks.", "recorded": "2011-07-20T15:30:00", "title": "A Machine Learning Approach to Twitter User Classification"}, {"url": "aibootcamp2011_balcazar_dmml", "desc": "The purpose of these lectures today is to review a few rather basic Machine Learning algorithms, while trying to see them from a Data Mining perspective. Thus, we will discuss the very notion of modelling, its role within the process of Knowledge Discovery from Data, and some of the particularities of this specific context. We will go through two \"descriptive modelling\" processes, namely k-means clustering and association rule mining; we will discuss some generalities about \"predictive modelling\", such as ROC-based evaluation and the bias-variance trade-off, and discuss some specific simple classifiers: na\u00efve Bayes, nearest neighbours, linear classifiers, their extension using kernels, and the Adaboost metapredictor.", "recorded": "2011-02-18T09:00:00", "title": "Data mining and Machine learning algorithms"}, {"url": "prib2010_fedonin_mldb", "desc": "We studied 1372 LacI-family transcription factors and their 4484 DNA binding sites using machine learning algorithms and feature selection techniques. The Naive Bayes classifier and Logistic Regression were used to predict binding sites given transcription  factor sequences. Prediction accuracy was estimated using 10-fold cross-validation. Experiments showed that the best prediction of nucleotide densities at selected site positions is obtained using only a few key protein sequence positions. These positions are stably selected by the forward feature selection based on the mutual information of factor-site position pairs. ", "recorded": "2010-09-23T11:15:00", "title": "Machine Learning Study of DNA Binding by Transcription Factors from the LacI family "}, {"url": "iswc2012_coppens_next_web", "desc": "In this paper, we provide a view on the future Web as a\r\nSemantic read-write Web. Given a number of prerequisites for enabling\r\na fully read-write Web for machines, we predict the following. First,\r\ndatasets and data in general will be driven by machine updates. Second,\r\nlots of human-created machine-readable schemas will become obsolete.\r\nThird, the Web of Services and the Web of Data will be fully integrated\r\ninto aWeb of read-write Data. Fourth, reasoning on theWeb stays mono-\r\ntonic and will be optimized for versioned data. Additionally, two application areas are discussed where the read-write Semantic Web will have\r\na deep impact: the Web of Things and advanced personalisation.", "recorded": "2012-11-11T12:00:53", "title": "A truly Read-Write Web for machines as the next generation Web?"}, {"url": "nipsworkshops09_lewis_fbp", "desc": "Finding the number of groups in a data set, k, is an important problem in the\r\nfield of unsupervised machine learning with applications across many scientific\r\ndomains. The problem is difficult however, because it is ambiguous and hierarchical,\r\nand current techniques for finding k often produce unsatisfying results.\r\nHumans are adept at navigating ambiguous and hierarchical situations, and this\r\npaper measures human performance on the problem of finding k across a wide\r\nvariety of data sets. We find that humans employ multiple strategies for choosing\r\nk, often simultaneously, and the number of possible interpretations of even simple\r\ndata sets with very few (N < 20) samples can be quite high. In addition, two\r\nleading machine learning algorithms are compared to the human results.", "recorded": "2009-12-11T09:30:00", "title": "Finding a Better k: A Psychophysical Investigation of Clustering"}, {"url": "mlpmsummerschool2014_paris", "desc": "The 2nd MLPM summer school is open to students, researchers and professionals interested in bringing together machine learning and statistical genetics to contribute to the development of data-driven, personalized medicine. The summer school mixes invited talks, scientific lectures and complementary skills courses to educate interdisciplinary experts who develop and employ the computational and statistical tools that are necessary to enable personalized medical treatment of patients according to their genetic and molecular properties and who are aware of the scientific, clinical and industrial implications of this research.\r\n\r\nFor more information please visit the [[http://mlpm.eu/summer-school/summer-school-2014/|2nd MLPM Summer School]].", "recorded": "2014-09-11T00:00:00", "title": "2nd Machine Learning for Personalized Medicine (MLPM), Paris 2014 "}, {"url": "solomon_dzeroski_mlsb", "desc": "The talk will discuss two topics  that have recently attracted a lot of attention within machine learning and data mining, namely structured-output prediction and constraint-based data mining. It will present some recently developed approaches addressing these topics. It will also discuss applications of these approaches in  the area of system biosciences, namely systems biology and ecology. Example applications from systems biology include gene function prediction, predicting the response of genes to stress and analyzing image data from genome and compound screens. Ecological applications include predicting the state of ecosystems, such as forests, from satellite images, predicting the composition of biological communities from environmental data and taxonomical classification from images.", "recorded": "2010-12-21T13:00:00", "title": "Machine Learning and Systems Biosciences"}, {"url": "nips09_bach_smm", "desc": "Regularization by the L1-norm has attracted a lot of interest in recent years in statistics, machine learning and signal processing. In the context of least-square linear regression, the problem is usually referred to as the Lasso or basis pursuit. Much of the early effort has been dedicated to algorithms to solve the optimization problem efficiently, either through first-order methods, or through homotopy methods that leads to the entire regularization path (i.e., the set of solutions for all values of the regularization parameters) at the cost of a single matrix inversion. A well-known property of the regularization by the L1-norm is the sparsity of the solutions, i.e., it leads to loading vectors with many zeros, and thus performs model selection on top of regularization. Recent works have looked precisely at the model consistency of the Lasso, i.e., if we know that the data were generated from a sparse loading vector, does the Lasso actually recover the sparsity pattern when the number of observations grows? Moreover, how many irrelevant variables could we consider while still being able to infer correctly the relevant ones? The objective of the tutorial is to give a unified overview of the recent contributions of sparse convex methods to machine learning, both in terms of theory and algorithms. The course will be divided in three parts: in the first part, the focus will be on the regular L1-norm and variable selection, introducing key algorithms and key theoretical results. Then, several more structured machine learning problems will be discussed, on vectors (second part) and matrices (third part), such as multi-task learning, sparse principal component analysis, multiple kernel learning and sparse coding.", "recorded": "2009-12-07T13:00:00", "title": "Sparse Methods for Machine Learning: Theory and Algorithms"}, {"url": "russir09_karpovich_gfollr", "desc": "Greedy function approximation and boosting algorithms are well suited for solving practical machine learning tasks. We will describe well-known boosting algorithms and their modifications used for solving learning to rank problems. ", "recorded": "2009-09-12T16:45:00", "title": "Greedy Function Optimization in Learning to Rank"}, {"url": "siso08_whistler", "desc": "Structured data emerges rapidly in a large number of disciplines: bioinformatics, systems biology, social network analysis, natural language processing and the Internet generate large collections of strings, graphs, trees, and time series. Designing and analysing algorithms for dealing with these large collections of structured data has turned into a major focus of machine learning over recent years, both in the input and output domain of machine learning algorithms, and is starting to enable exciting new applications of machine learning.\n\nThe goal of this workshop is to bring together experts on learning with structured input and structured output domains and its applications, in order to exchange the latest developments in these growing fields. The workshop will include one session on learning with structured inputs, featuring a keynote by Prof. Eric Xing from Carnegie Mellon University. A second session will focus on learning with structured outputs, with a keynote by Dr. Yasemin Altun from the MPI for Biological Cybernetics. A third session will present novel applications of structured input-structured output learning to real-world problems.\n\nMore information about workshop can be found [[http://agbs.kyb.tuebingen.mpg.de/wikis/bg/siso2008/FrontPage|here]].", "recorded": "2008-12-12T07:30:00", "title": "NIPS Workshop on Structured Input - Structured Output, Whistler 2008"}, {"url": "kdd07_chaovalitwongse_sfmc", "desc": "In this study, a novel multidimensional time series classification technique, namely support feature machine (SFM), is proposed. SFM is inspired by the optimization model of support vector machine and the nearest neighbor rule to incorporate both spatial and temporal of the multi-dimensional time series data. This paper also describes an application of SFM for detecting abnormal brain activity. Epilepsy is a case in point in this study. In epilepsy studies, electroencephalograms (EEGs), acquired in multidimensional time series format, have been traditionally used as a gold-standard tool for capturing the electrical changes in the brain. From multi-dimensional EEG time series data, SFM was used to identify seizure pre-cursors and detect seizure susceptibility (pre-seizure) periods. The empirical results showed that SFM achieved over 80% correct classification of per-seizure EEG on average in 10 patients using 5-fold cross validation. The proposed optimization model of SFM is very compact and scalable, and can be implemented as an online algorithm. The outcome of this study suggests that it is possible to construct a computerized algorithm used to detect seizure pre-cursors and warn of impending seizures through EEG classification.", "recorded": "2007-08-14T14:00:00", "title": "Support Feature Machine for Classification of Abnormal Brain Activity "}, {"url": "iswc2014_aroyo_crowd_truth", "desc": "In this paper we introduce the CrowdTruth open-source software framework for machine-human computation, that implements a novel approach to gathering human annotation data for a variety of media (e.g. text, image, video). The CrowdTruth approach embodied in the software captures human semantics through a pipeline of four processes: a) combining various machine processing of media in order to better understand the input content and optimize its suitability for micro-tasks, thus optimize the time and cost of the crowdsourcing process; b) providing reusable human-computing task templates to collect the maximum diversity in the human interpretation, thus collect richer human semantics; c) implementing \u2019disagreement metrics\u2019, i.e. CrowdTruth metrics, to support deep analysis of the quality and semantics of the crowdsourcing data; and d) providing an interface to support data and results visualization. Instead of the traditional inter-annotator agreement, we use their disagreement as a useful signal to evaluate the data quality, ambiguity and vagueness. We demonstrate the applicability and robustness of this approach to a variety of problems across multiple domains. Moreover, we show the advantages of using open standards and the extensibility of the framework with new data modalities and annotation tasks.", "recorded": "2014-10-22T12:20:00", "title": "Crowd Truth: Machine-Human Computation Framework for Harnessing Disagreement in Gathering Annotated Data"}, {"url": "machine_pinter_natural_language", "desc": "We outline a Web-based architecture that serves the construction of\r\ntransdisciplinary knowledge and is supported by machine intelligence. At the\r\nheart of the architecture is the interaction between a repository of concepts\r\nand the machine intelligence. The repository is an ontology integrated with a\r\nWiki; each concept in the ontology is grounded in the natural language text of\r\nthe Wiki. The machine intelligence exploits structured sparse coding and helps\r\nusers interact with the concept repository. Most importantly, it helps\r\npractitioners of different fields understand each other by explaining unknown\r\nterminology in the Wiki, and it facilitates creating and maintaining content.\r\nThese two components evolve together: as the concept repository grows, the\r\nintelligence performs better. Furthermore, extending the concept repository\r\nsemi-automatically becomes easier.\r\n\r\nTo support wide contribution and ensure the high quality of the accumulated\r\nknowledge, content is divided into two types: drafts and articles. Anyone who\r\nregisters can create and edit drafts, but only drafts that pass a voting\r\nprocedure and are approved by an Editorial Board can become articles. Articles\r\nare very similar to scientific papers: they undergo peer-review and contain\r\nverified knowledge. Two communities have already started using these portals in\r\nthe domains of robotic surgery and education.", "recorded": "2013-04-11T14:54:00", "title": "Natural language processing supported transdisciplinary crowdsourcing"}, {"url": "mitworld_copeland_at", "desc": "The code-breaking work at Bletchley Park, which helped save Britain from Nazi Germany, qualifies as one of the greatest stories of World War II, and the misunderstood genius, Alan Turing, stands at the center of this tale. Perhaps no one understands Turing\u2019s role during this period -- and his larger impact on mathematics and computing -- like B. Jack Copeland. In this lecture, Copeland contends that Turing should be celebrated as the father of artificial intelligence.\n\nHe dates the origins of AI \u201cto the dark days in 1940, 1941\u201d when Turing and fellow code-breakers were pitted against the Nazi code machine Enigma. Copeland describes how Turing broke the previously inviolable indicator system of Enigma, and helped design electromechanical machines to read thousands of German radio intercepts. These devices employed heuristic searching, which is now a central idea of AI. They found the right answer \u201coften enough and fast enough to be read in real time.\u201d Without such machines, German U-boats would have decimated the North Atlantic convoys providing a lifeline to Britain.\n\nCopeland notes that Turing was hooked on this idea of heuristic problem solving, and that he speculated on building sophisticated machines by \u201cmaking use of guided search.\u201d Well before the breakout of war, Turing had conceived of a general computing machine that stored programs in memory. The world\u2019s first large-scale, electronic computer, Colossus was used at Bletchley to break other Nazi codes, and Turing found additional inspiration for pursuing ideas of machine intelligence. \u201cNowadays when nearly everyone owns the physical realization of a universal Turing machine, Turing\u2019s idea of a one-stop shop computing machine is apt to seem as obvious as the wheel,\u201d says Copeland. But in 1936, engineers \u201cthought in terms of building specific machines for particular purposes.\u201d Turing\u2019s idea was revolutionary.\n\nAfter the war, Turing joined the National Physical Laboratory and developed the schematics for an Automatic Computing Engine, intended as the first electronic stored-program general-purpose digital computer. AI was central to his vision, believes Copeland. He described this project as \u201cbuilding a brain.\u201d Turing speculated about constructing networks of unorganized, artificial neurons that could be trained to carry out particular tasks. He also published an account of the Turing Test, a method for determining whether a computer \u201cthinks.\u201d Finally, Copeland defends the Turing Test against its many detractors, and predicts that \u201cthe probability of AI getting its act together in 50 years to produce a machine that can pass the Turing Test is pretty good.\u201d", "recorded": "2006-11-30T14:52:00", "title": "Alan Turing: Codebreaker and AI Pioneer"}, {"url": "iccc2014_deussen_getting_physical", "desc": "Computer  graphics  traditionally  focusses  on  creating  photorealistic  images.  However,  for more \r\nthan  20  years,  computer  graphics  researchers  have  also  worked  on  creating  abstract  visual \r\nrepresentations. In my  talk, I will give an overview of our research in this field. I will show how \r\nwe  apply  such  techniques  in different  fields  ranging  from  landscape  visualization  to CAD.  I will \r\ndescribe  the  projects  related  to  our  painting  robot  e-David, which  is  the  basis  from which we \r\nstudy human and machine painting. Using this machine, we create paintings with different media \r\nand in different styles completely automatically using a simple visual feedback loop. I will discuss \r\nthe role of creativity in this process and how we want to incorporate artistic freedom and higher-\r\norder styles in the future. ", "recorded": "2014-06-10T14:30:00", "title": "Non-photorealistic Rendering Getting Physical"}, {"url": "nipsworkshops2013_jialin_pan_transfer_learning", "desc": "Transfer learning has attracted increasingly attention in artificial intelligence, machine learning and many other application areas. Different from traditional machine learning methods which assume the training and testing data come from the same task or domain, transfer learning aims to extract common knowledge across domains or tasks, such that a model trained on one domain or task can be adapted to other domains or tasks. In this talk, I will first give an overview of transfer learning and discuss the relationships between transfer learning and other learning areas, and then summarize various transfer learning approaches into several categories, and introduce some representative methods, finally discuss research challenges and future directions in this area.", "recorded": "2013-12-10T07:30:00", "title": "An Overview of Transfer Learning"}, {"url": "bootcamp2010_marseille", "desc": "The LIF organised the second bootcamp of the European Network of Excellence PASCAL (Pattern Analysis, Statistical modelling and ComputAtional Learning). This school was held during summer 2010 on the campus of St. Charles in Marseille.\r\n\r\nThe bootcamp is an international event which consists of a one week courses (lectures, pratical and lab sessions) designed for Master students or students just about to start a PhD. It aims at training good-level students from various scientific fieds to major thematics of PASCAL network, particularly machine learning.\r\n\r\nThe first PASCAL bootcamp was held in Barcelona during summer 2007.\r\n\r\nMore about the event at [[http://bootcamp.lif.univ-mrs.fr:8080/mainpage|Bootcamp 2010:Home]]", "recorded": "2010-07-05T09:00:00", "title": "PASCAL Bootcamp in Machine Learning, Marseille 2010"}, {"url": "icml08_collins_spp", "desc": "Modeling language at the syntactic or semantic level is a key problem in natural language processing, and involves a challenging set of structured prediction problems. In this talk I'll describe work on machine learning approaches for syntax and semantics, with a particular focus on lexicalized grammar formalisms such as dependency grammars, tree adjoining grammars, and categorial grammars. \n;I'll address key issues in the following areas: \n:1) the design of learning algorithms for structured linguistic data;\n:2) the design of representations that are used within these learning algorithms;\n:3) the design of efficient approximate inference algorithms for lexicalized grammars, in cases where exact inference can be very expensive.\n\nIn addition, I'll describe applications to machine translation, and natural language interfaces.", "recorded": "2008-07-07T16:05:27", "title": "Structured Prediction Problems in Natural Language Processing"}, {"url": "smartdw09_gimenez_emtaie", "desc": "Our goal was twofold. On the one side, we studied the problem of Automatic MT evaluation. We analyzed the main deficiencies of the current evaluation methodology and suggested several complementary improvements. On the other side, we built an empirical MT system and have analyzed several of its limitations. We incorporated linguistic knowledge into the system with the aim to improve overall translation quality. In particular, we addressed the problem of lexical selection in Statistical Machine Translation. As a side question, we also studied one of the main criticisms against Empirical MT systems, i.e., their strong domain dependence, and how its negative effects may be mitigated by properly combining outer knowledge sources when porting a system into a new domain.", "recorded": "2009-05-13T10:00:00", "title": "Empirical Machine Translation and its Evaluation"}, {"url": "turing100_matiyasevich_number_theory", "desc": "Beside well-known revolutionary contributions, Alan Turing had a number of significant results in \"traditional\" mathematics. In particular he was very much interested in the famous Riemann Hypothesis. This hypothesis, stated by Berhard Riemann in 1859 and included by David Hilbert in his 8th problem in 1990, still remains open, being now one of the Millennium Problems. The Riemann Hypothesis predicts positions of zeros of so called zeta function, and Alan Turing developed a rigorous method for verifying the Hypothesis for the initial zeros. He also invented a machine for calculating the values of the zeta function. In contrast to celebrated imaginable Turing machines, Turing started to implement this machine but never finished because of the War.", "recorded": "2012-06-23T11:30:00", "title": "Alan Turing and Number Theory"}, {"url": "mlss05us_long_eib", "desc": "Biologists frequently use databases; for example, when a biologist encounters some unfamiliar proteins, s/he will use databases to get a preliminary idea of what is known about them. The databases can be often interpreted as lists of assertions. An example is a protein-protein interaction database: each entry is a pair of proteins that are asserted to interact, along with the supporting evidence. Often a candidate for inclusion in such a database can be supported in a variety of fundamentally different ways. A methodological challenge is how to effectively combine these different sources of evidence to make accurate aggregate predictions. Ideas from machine learning are useful for this. I will describe some of the special properties of problems like this, and relevant methods from machine learning, including algorithms based on bayesian networks, boosting and SVMs.", "recorded": "2005-05-19T00:00:00", "title": "Evidence Integration in Bioinformatics"}, {"url": "nipsworkshops2013_laketahoe", "desc": "The Post-Conference Workshop Program covered a wide range of topics from Neuroscience to Machine Learning.\r\n\r\nDetailed information can be found at [[http://nips.cc/Conferences/2013/Program/schedule.php?Session=Workshops|NIPS 2013 Workshops homepage]].", "recorded": "2013-12-09T09:00:00", "title": "NIPS Workshops, Lake Tahoe 2013"}, {"url": "mlss2011_scholkopf_kernel", "desc": "The course will start with basic ideas of machine learning, followed by some elements of learning theory. It will also introduce positive definite kernels and their associated feature spaces, and show how to use them for kernel mean embeddings, SVMs, and kernel PCA.", "recorded": "2011-09-05T09:00:00", "title": "Kernel Methods"}, {"url": "eswc2011_fu_mapping", "desc": "While ontologies are widely accepted internationally as knowledge management mechanism across disciplines, the ability to reason over knowledge bases regardless of the natural languages used in them has become a pressing issue in digital content management. To enable knowledge sharing and reuse, ontology mapping techniques must be able to work with otherwise isolated ontologies that are labelled in diverse natural languages. Machine translation techniques are often employed by cross-lingual ontology mapping approaches to turn a cross-lingual mapping problem into a monolingual mapping problem which can then be solved by state of the art monolingual ontology matching tools. However in the process of doing so, complications introduced by machine translation tools can compromise the performance of the subsequent monolingual matching techniques. In this paper, a novel approach to improve the quality of cross-lingual ontology mapping is presented and evaluated. The proposed approach adopts the pseudo feedback technique that is similar to the well understood relevance feedback mechanism used in the field of information retrieval. It is shown through the evaluation that the pseudo feedback feature can enhance the effectiveness of machine translation and monolingual matching techniques in a cross-lingual ontology mapping scenario.", "recorded": "2011-05-31T16:30:00", "title": "Using Pseudo Feedback to Improve Cross-Lingual Ontology Mapping"}, {"url": "wapa2010_graepel_tals", "desc": "Probabilistic Graphical Models play a crucial role in Microsoft's online services. In this talk, I will describe two powerful applications of machine learning in practice. TrueSkill is Xbox Live's Ranking and Matchmaking system and ensures that gamers online have balanced and exciting matches with equally skilled opponents. AdPredictor is the system that estimates click-through rates (CTR) for ad selection and pricing within Microsoft's search engine Bing. The two systems have in common that they are based on factor graph models and approximate Bayesian inference. They operate at a very large scale involving millions of gamers and billions of ad impressions, respectively. However, in this talk, I will put particular emphasis on those aspects of these applications that are not part of the generic machine learning setting: a) The difficulties that arise because these are closed-loop systems in which the predictions determine the future composition of the training sample. b) The consequences of the fact that these systems make decisions that have an impact on more or less rational agents (advertisers, users, gamers) with the ability to influence the training sample. Time permitting, I will show the two systems in action. This is based on joint work with Ralf Herbrich, Thomas Borchert, Tom Minka, and Joaquin Qui\u0144onero Candela.", "recorded": "2010-09-01T14:00:00", "title": "TrueSkill and AdPredictor: Large Scale Machine Learning in the Wild"}, {"url": "machine_learning_video_abstracts_vol2", "desc": "Welcome to the 2nd issue of the Journal of Machine Learning Video Abstracts, published by Knowledge for All Foundation Ltd, London, with the goal of providing the legacy of the PASCAL2 Network of Excellence.\r\n\r\nThis issue is equaly dedicated to presenting the spotlight presentations of the 2011 Neural Information Processing Systems (NIPS) Conference held in Granada, Spain, on December 12, as the [[http://videolectures.net/machine_learning_video_abstracts_vol1/|Volume 1]] of this Journal. The VideoLectures.NET team were on site to record the presentations and they post-processed the recordings to incorporate the slides into the video, they also provide the technical infrastructure for the journal pages.\r\n\r\nWe are publishing this Volume rather late, as the video specific publishing model is still experimental and it makes harder to organize all the necessary stages of the publishing cycle, therefore the review process took longer than expected. We are delighted to have been able to record these and make them available to the wider Machine Learning and Artificial Intelligence community together with the wider audience. A special note of thankfulness goes also to our reviewers and spotlight chairs.\r\n\r\n[[http://videolectures.net/peter_l_bartlett/|Peter L. Bartlett]], [[http://videolectures.net/fernando_pereira/|Fernando Pereira]] and Davor Orlic\r\n\r\nEditors of the 2nd Issue and Programme Chairs of the 2010 NIPS Conference", "recorded": "2011-12-12T18:45:00", "title": "Video Journal of Machine Learning Abstracts - Volume 2"}, {"url": "ecmlpkdd2011_cheng_learning", "desc": "The learning of predictive models that guarantee monotonicity\r\nin the input variables has received increasing attention in machine\r\nlearning in recent years. While the incorporation of monotonicity constraints\r\nis rather simple for certain types of models, it may become a more\r\nintricate problem for others. By trend, the difficulty of ensuring monotonicity\r\nincreases with the flexibility or, say, nonlinearity of a model. In\r\nthis paper, we advocate the so-called Choquet integral as a tool for learning\r\nmonotone nonlinear models. While being widely used as a flexible\r\naggregation operator in different fields, such as multiple criteria decision\r\nmaking, the Choquet integral is much less known in machine learning so\r\nfar. Apart from combining monotonicity and flexibility in a mathematically\r\nsound and elegant manner, the Choquet integral has additional\r\nfeatures making it attractive from a machine learning point of view.\r\nNotably, it offers measures for quantifying the importance of individual\r\npredictor variables and the interaction between groups of variables. As\r\na concrete application of the Choquet integral, we propose a generalization\r\nof logistic regression. The basic idea of our approach, referred\r\nto as choquistic regression, is to replace the linear function of predictor\r\nvariables, which is commonly used in logistic regression to model the log\r\nodds of the positive class, by the Choquet integral.", "recorded": "2011-09-06T09:00:00", "title": "Learning Monotone Nonlinear Models using the Choquet Integral"}, {"url": "nipsworkshops2011_bayesian_nonparametrics", "desc": "Bayesian nonparametric methods are an expanding part of the machine learning landscape. Proponents of Bayesian nonparametrics claim that these methods enable one to construct models that can scale their complexity with data, while representing uncertainty in both the parameters and the structure. Detractors point out that the characteristics of the models are often not well understood and that inference can be unwieldy. Relative to the statistics community, machine learning practitioners of Bayesian nonparametrics frequently do not leverage the representation of uncertainty that is inherent in the Bayesian framework. Neither do they perform the kind of analysis --- both empirical and theoretical --- to set skeptics at ease. In this workshop we hope to bring a wide group together to constructively discuss and address these goals and shortcomings.\r\n\r\nWorkshop homepage: http://people.seas.harvard.edu/~rpa/nips2011npbayes.html", "recorded": "2011-12-17T07:30:00", "title": "Bayesian Nonparametric Methods: Hope or Hype?"}, {"url": "roks2013_wiering_vector", "desc": "In this paper we describe a novel extension of the support vector machine, called\r\nthe deep support vector machine (DSVM). The original SVM has a single layer with kernel\r\nfunctions and is therefore a shallow model. The DSVM can use an arbitrary number of layers,\r\nin which lower-level layers contain support vector machines that learn to extract relevant\r\nfeatures from the input patterns or from the extracted features of one layer below. The\r\nhighest level SVM performs the actual prediction using the highest-level extracted features\r\nas inputs. The system is trained by a simple gradient ascent learning rule on a min-max\r\nformulation of the optimization problem. A two-layer DSVM is compared to the regular SVM\r\non ten regression datasets and the results show that the DSVM outperforms the SVM.", "recorded": "2013-07-09T17:00:00", "title": "Deep Support Vector Machines"}, {"url": "bbci09_lebedev_bmib", "desc": "Brain-machine interfaces (BMIs) have experienced an explosive development during the last decade. Current state of the art BMIs convert neuronal ensemble activity recorded in nonhuman primates or human subjects into reaching and grasping movements performed by artificial actuators. BMIs that enact movements of lower extremeties are less explored. Additionally, most BMI implementations do not have somatosensory feedback from the actuator. I will review our recent experiments in which we (i) extracted bipedal locomotion patterns from monkey cortical activity and (ii) used spatiotemporal patterns of intracortical microstimulation to deliver information back to the brain. These results bring us closer to building clinical neuroprosthetic devices for restoration of both sensory and motor functions in paralyzed people.", "recorded": "2009-07-09T10:30:40", "title": "Brain-Machine Interfaces Based on Neuronal Ensemble Recordings "}, {"url": "icml08_shamir_lcm", "desc": "After a classifier is trained using a machine learning algorithm and put to use in a real world system, it often faces noise which did not appear in the training data. Particularly, some subset of features may be missing or may become corrupted. We present two novel machine learning techniques that are robust to this type of classification-time noise. First, we solve an approximation to the learning problem using linear programming. We analyze the tightness of our approximation and prove statistical risk bounds for this approach. Second, we define the online-learning variant of our problem, address this variant using a modified Perceptron, and obtain a statistical learning algorithm using an online-to-batch technique. We conclude with a set of experiments that demonstrate the effectiveness of our algorithms.", "recorded": "2008-07-08T15:15:00", "title": "Learning to Classify with Missing and Corrupted Features"}, {"url": "mlg07_baldi_laccs", "desc": "Informatics methods and computers have not yet become as pervasive in chemistry as they have in physics and biology. Drawing analogies from bioinformatics, key ingredients for progress in chemoinformatics are the availability of large, annotated databases of compounds and reactions, data structures and algorithms to efficiently search these databases, and computational methods to predict the physical, chemical, and biological properties of new compounds and reactions. We will describe how graph-based methods play a key role in the development of: (1) a large public database of compounds and reactions (ChemDB) and the underlying algorithms and representations; (2) machine learning kernel methods to predict molecular properties; and (3) the applications of these methods to drug screening/design problems and the identification of new drug leads against a major disease.\r \r ", "recorded": "2007-08-01T09:00:00", "title": "Learning and Charting Chemical Space with Strings and Graphs: Challenges and Opportunities for AI and Machine Learning"}, {"url": "mlpm", "desc": "MLPM is a [[http://ec.europa.eu/research/mariecurieactions/|Marie Curie Initial Training Network]], funded by the European Union within the 7th Framework Programme. MLPM has started on January 1, 2013 and will be carried out over a period of four years. MLPM is a consortium of several universities, research institutions and companies located in Spain, France, Germany, Belgium, UK, and in the USA. MLPM involves the predoctoral training of 14 young scientists in the research field at the interface of Machine Learning and Medicine. Its goal is to educate interdisciplinary experts who will develop and employ the computational and statistical tools that are necessary to enable personalized medical treatment of patients according to their genetic and molecular properties and who are aware of the scientific, clinical and industrial implications of this research.", "recorded": "2015-01-29T17:45:32", "title": "MLPM - Machine Learning for Personalized Medicine"}, {"url": "mlss09us_spielman_fgvd", "desc": "We ask \"What is the right graph to fit to a set of vectors?\" We propose one solution that provides good answers to standard Machine Learning problems, that has interesting combinatorial properties, and that we can compute efficiently. Joint work with Jonathan Kelner and Samuel Daitch.", "recorded": "2009-06-08T16:30:00", "title": "Fitting a Graph to Vector Data "}, {"url": "mloss08_collobert_tor", "desc": "Torch provides a Matlab-like environment for state-of-the-art machine learning algorithms. It is easy to use\r\nand very e\ufb03cient, thanks to a simple-yet-powerful fast scripting language (Lua), and a underlying C/C++\r\nimplementation. Torch is easily extensible and has been shown to scale to very large applications.", "recorded": "2008-12-12T08:00:00", "title": "Torch"}, {"url": "eswc09_hersonissos", "desc": "The vision of the Semantic Web is to enhance today's Web by exploiting machine-processable metadata. The explicit representation of the semantics of data, enriched with domain theories (ontologies), will enable a web that provides a qualitatively new level of service. It will weave together a large network of human knowledge and makes this knowledge machine-processable. Various automated services will help the users to achieve their goals by accessing and processing information in machine-understandable form. This network of knowledge systems will ultimately lead to truly intelligent systems, which will be employed for various complex decision-making tasks. Semantic Web research can benefit from ideas and cross-fertilization with many other areas: Artificial Intelligence, Natural Language Processing, Databases and Information Systems, Information Retrieval, Multimedia, Distributed Systems, Social Networks and Web Engineering. Many advances within these areas can contribute towards the realization of the Semantic Web.\r\n\r\nThe 6th Annual European Semantic Web Conference (ESWC2009) presented the latest results in research and applications of Semantic Web technologies. ESWC2009 also featured a tutorial program, system descriptions and demos, a poster track, a PhD Symposium and a number of collocated workshops.\r\n\r\n----\r\nThe ESWC 2009 homepage can be found at http://www.eswc2009.org/\r\n----", "recorded": "2009-05-31T10:00:00", "title": "6th Annual European Semantic Web Conference (ESWC), Hersonissos 2009"}, {"url": "nips2012_dietterich_sustainability", "desc": "Research in computational sustainability seeks to develop and apply methods from computer science to the many challenges of managing the earth's ecosystems sustainably. Viewed as a control problem, ecosystem management is challenging for two reasons. First, we lack good models of the function and structure of the earth's ecosystems. Second, it is difficult to compute optimal management policies because ecosystems exhibit complex spatio-temporal interactions at multiple scales. This talk will discuss some of the many challenges and opportunities for machine learning research in computational sustainability. These include sensor placement, data interpretation, model fitting, computing robust optimal policies, and finally executing those policies successfully. Examples will be discussed on current work and open problems in each of these problems. All of these sustainability problems involve spatial modeling and optimization, and all of them can be usefully conceived in terms of facilitating or preventing flows along edges in spatial networks. For example, encouraging the recovery of endangered species involves creating a network of suitable habitat and encouraging spread along the edges of the network. Conversely, preventing the spread of diseases, invasive species, and pollutants involves preventing flow along edges of networks. Addressing these problems will require advances in several areas of machine learning and optimization.", "recorded": "2012-12-05T09:00:00", "title": "Challenges for Machine Learning in Computational Sustainability"}, {"url": "sicgt07_workshop", "desc": "The workshop focuses on the fundamentals of graph theory relevant to learning, with emphasis on the applications of spectral clustering, visualisation and transductive learning.\n\nMethods from graph theory have made an impact in Machine Learning recently through two avenues. The first arises when we view the data samples as the vertices of the graph with the similarity between the examples encoded by the weights on the edges. This view of the data can be used to motivate a number of techniques, including spectral clustering, nonlinear dimensionality reduction, visualisation, transductive and semi-supervised classification.\n\nThe second reason for involving graph theory is through the representation of complex objects by graphs. This could be for objects that have a natural graph structure such as molecules or gene networks, or for cases where a feature extraction phase constructs a graph, as for example in natural language processing or computer vision. A key development in this area has been the realisation that feature spaces involving exponentially many features can be used implicitly via kernels that compute in polynomial time inner products between projections into the feature space. This use of graph representations is becoming common in many applications of machine learning making a focus on this topic relevant to a number of application areas, particularly bioinformatics and natural language processing.\n\nFor more information visit the [[http://conferences.imfm.si/conferenceDisplay.py?confId=11|Workshop website]].", "recorded": "2007-06-25T09:00:00", "title": "Workshop on Graph Theory and Machine Learning"}, {"url": "icml07_li_scm", "desc": "For large-scale classification problems, the training samples can be clustered beforehand as a downsampling pre-process, and then only the obtained clusters are used for training. Motivated by such assumption, we proposed a classification algorithm, Support Cluster Machine (SCM), within the learning framework introduced by Vapnik. For the SCM, a compatible kernel is adopted such that a similarity measure can be handled not only between clusters in the training phase but also between a cluster and a vector in the testing phase. We also proved that the SCM is a general extension of the SVM with the RBF kernel. The experimental results confirm that the SCM is very effective for largescale classification problems due to significantly reduced computational costs for both training and testing and comparable classification accuracies. As a by-product, it provides a promising approach to dealing with privacy-preserving data mining problems.", "recorded": "2007-06-22T11:00:00", "title": "Support Cluster Machine"}, {"url": "mloss08_raeder_mm", "desc": "Common practice in Machine Learning often implicitly assumes a stationary distribution, meaning that the\r\ndistribution of a particular feature does not change over time. In practice, however, this assumption is often\r\nviolated and real-world models have to be retrained as a result. It would be helpful, then, to be able to\r\nanticipate and plan for changes in distribution in order to avoid this retraining. Model Monitor is a Java\r\ntoolkit that addresses this problem. It provides methods for detecting distribution shifts in data, comparing\r\nthe performance of multiple classi\ufb01ers under shifts in distribution, and evaluating the robustness of individual\r\nclassi\ufb01ers to distribution change. As such, it allows users to determine the best model (or models) for their\r\ndata under a number of potential scenarios. Additionally, Model Monitor is fully integrated with the WEKA\r\nmachine learning environment, so that a variety of commodity classi\ufb01ers can be used if desired.", "recorded": "2008-12-12T17:15:00", "title": "Model Monitor"}, {"url": "computationaltopology2013_zitnik_methods", "desc": "Fast growth in the amount of data emerging from studies across various scientific disciplines and engineering requires alternative approaches to understand large and complex data sets in order to turn data into useful knowledge. Topological methods are making an increasing contribution in revealing patterns and shapes of high-dimensional data sets. Ideas, such as studying the shapes in a coordinate free ways, compressed representations and invariance to data deformations are important when one is dealing with large data sets. In this talk we consider which key concepts make topological methods appropriate for data analysis and survey some machine learning techniques proposed in the literature, which exploit them. We illustrate their utility with examples from computational biology, text classification and data visualization.", "recorded": "2013-07-02T16:00:00", "title": "Topological Methods in Machine Learning"}, {"url": "nipsworkshops2011_boyd_multipliers", "desc": "Problems in areas such as machine learning and dynamic optimization on a large network lead to extremely large convex optimization problems, with problem data stored in a decentralized way, and processing elements distributed across a network. We argue that the alternating direction method of multipliers is well suited to such problems. The method was developed in the 1970s, with roots in the 1950s, and is equivalent or closely related to many other algorithms, such as dual decomposition, the method of multipliers, Douglas-Rachford splitting, Spingarn's method of partial inverses, Dykstra's alternating projections, Bregman iterative algorithms for $\\ell_1$ problems, proximal methods, and others. After briefly surveying the theory and history of the algorithm, we discuss applications to statistical and machine learning problems such as the lasso and support vector machines.", "recorded": "2011-12-16T09:30:00", "title": "Alternating Direction Method of Multipliers"}, {"url": "mlss2012_hogg_astronomy", "desc": "Astronomy is a prime user community for machine learning and probabilistic modeling.  There are very large, public data sets (mostly but not entirely digital imaging), there are simple but effective models of many of the most important phenomena (stars, quasars, and galaxies), and there are very good models of telescopes, cameras, and detectors.  I will show in detail some examples of problems we were able to solve in astrophysics by bringing probabilistic inference and decision theory to astronomy.  I will discuss why many \"supervised\" methods are not nearly as useful in astronomy as those that involve generative modeling.  I hope to leave the audience with real research problems, the solutions to which would be (a) achievable with contemporary machine-learning methods, and at the same time (b) very exciting within the astrophysics community.", "recorded": "2012-04-20T11:30:00", "title": "Probabilistic decision-making, data analysis, and discovery in astronomy"}, {"url": "reasecs_maynard_hltsw", "desc": "This tutorial covers the use of Human Language Technology for the Semantic Web and Web Services. It includes material on an introduction to Information Extraction, Evaluation, Language Engineering and Machine Learning approaches, Semantic Metadata Creation, and Language Generation.\r\n\r\nDocuments:\r\n;[[Human_Language_Technology.pdf]]\r\n;[[Human_Language_Technology.ppt]]\r\n;[[Human_Language_Technology.zip]]", "recorded": "2004-10-06T00:00:00", "title": "Human Language Technology for the Semantic Web"}, {"url": "stanfordcs229f08_ng_lec11", "desc": "Bayesian Statistics and Regularization, Online Learning, Advice for Applying Machine Learning Algorithms, Debugging/fixing Learning Algorithms, Diagnostics for Bias & Variance, Optimization Algorithm Diagnostics, Diagnostic Example - Autonomous Helicopter, Error Analysis, Getting Started on a Learning Problem\r\n", "recorded": "2009-04-17T10:00:00", "title": "Lecture 11 - Bayesian Statistics and Regularization "}, {"url": "eswc2012_rettinger_rdf_data", "desc": "The increasing availability of structured data in Resource Description Framework (RDF) format poses new challenges and opportunities for data mining. Existing approaches to mining RDF have focused on one specific data representation, one specific machine learning algorithm or one specific task, only. Kernels, however, promise a more flexible approach by providing a powerful framework for decoupling the data representation from the learning task. This paper focuses on how the well established family of kernel-based machine learning algorithms can be readily applied to instances represented as RDF graphs. We first review the problems that arise when conventional graph kernels are used for RDF graphs. We then introduce two versatile families of RDF graph kernels based on intersection graphs and intersection trees. These kernels can better exploit the inherent properties of RDF, while providing an easy to use interface between any RDF graph (including vocabulary extensions such as RDFS and OWL) and any kernel-based learning algorithm (which are available for solving many machine learning tasks). The flexibility of the approach is demonstrated on two common relational learning tasks: entity classification and link prediction. The results show that our novel RDF graph kernels with standard SVMs achieve competitive predictive performance when compared to specialized techniques for both tasks.", "recorded": "2012-05-31T16:30:00", "title": "Graph Kernels for RDF data"}, {"url": "mlsb2012_heinonen_metabolite", "desc": "**Motivation:** Metabolite identification from tandem mass spectra is an important problem in\r\nmetabolomics, underpinning subsequent metabolic modelling and network analysis. Yet, currently\r\nthis task requires matching the observed spectrum against a database of reference spectra\r\noriginating from similar equipment and closely matching operating parameters, a condition that is\r\nrarely satisfied in public repositories. Furthermore, the computational support for identification of\r\nmolecules not present in reference databases is lacking. Recent efforts in assembling large public\r\nmass spectral databases such as MassBank have opened the door for the development of a new\r\ngenre of metabolite identification methods.\\\\\r\n**Results:** We introduce a novel framework for prediction of molecular characteristics and\r\nidentification of metabolites from tandem mass spectra using machine learning with the support\r\nvector machine (SVM). Our approach is to first predict a large set of molecular properties of the\r\nunknown metabolite from salient tandem mass spectral signals, and in the second step to use the\r\npredicted properties for matching against large molecule databases, such as PubChem. We\r\ndemonstrate that several molecular properties can be predicted to high accuracy, and that they\r\nare useful in de novo metabolite identification, where the reference database does not contain any\r\nspectra of the same molecule.\\\\\r\n**Availability:** An Matlab/Python package of the FingerID tool is freely available on the web at\r\nhttp://www.sourceforge.net/p/fingerid.", "recorded": "2012-09-09T13:00:00", "title": "Metabolite identification and molecular fingerprint prediction via machine learning"}, {"url": "machine_learning_video_abstracts_vol1", "desc": "Welcome to the first issue of the Journal of Machine Learning Video Abstracts. The first issue is dedicated to presenting the spotlight presentations of the 2010 Neural Information Processing Systems (NIPS) Conference held in Vancouver, Canada, on December 7-9. The presentations were lengthened to four minutes for this year's conference with an allowance of 4 slides for each paper. The\r\nVideoLectures.NET team were on site to record the presentations and they post-processed the recordings to incorporate the slides into the video. The VideoLectures.NET portal provides the infrastructure for the\r\njournal pages. All of the abstracts have been reviewed for quality of recording and appropriateness of presentation, while the papers they describe were accepted to the highly prestigious and competitive NIPS\r\nevent. At the conference itself there was also a poster session at which researchers could discuss the results with the authors and some of the presentations refer to these sessions at the end of their presentations. We are very conscious of the enormous effort that the authors have put into these presentations in order to communicate the results and main contributions of their papers to the community. We are therefore delighted to have been able to record them and make them available to a wider audience as a special issue of the Journal\r\nof Machine Learning Video Abstracts.\r\n\r\n[[http://videolectures.net/john_shawe_taylor/|John Shawe-Taylor]] and Richard Zemel\r\n;Editors of the Special Issue and Programme Chairs of the 2010 NIPS Conference", "recorded": "2010-12-06T18:30:00", "title": "Video Journal of Machine Learning Abstracts - Volume 1"}, {"url": "ocwc2014_gordon_open_education", "desc": "One of the big promises of open and massively online education is easy data collection: we can record everything from students\u2019 habits in reading and viewing lectures, to their participation in discussion groups, to their timing and performance on exercises. So, open education is a natural fit for machine learning - for example, we can use ML to predict future student performance, to select and sequence learning activities, and even to help grade some types of assignments. But there\u2019s a lot more left to do: I\u2019ll argue that even-bigger gains can come from ML that\u2019s focused on understanding educational content and how students learn it, and on communicating this understanding to human educators. To achieve such understanding and communication, we need to take advantage of ML techniques including representation learning, structured learning, and exploration / experimentation.", "recorded": "2014-04-25T09:00:00", "title": "What can machine learning do for open education?"}, {"url": "solomon_duane_supermodeling", "desc": "Computational models of an ongoing objective process, as in weather\r\nforecasting, must continually assimilate new observational data as they run.\r\nBoth \"truth\" and \"model\" are chaotic systems that thus synchronize through\r\na limited exchange of information in one direction - a phenomenon that can\r\nbe characterized as machine perception. A recent suggestion has been to\r\nenvision the fusion of different models analogously, as 3-way synchronization\r\nof the different models with reality. This phenomenon may be useful for\r\nimproving climate projection by combining a few different models that differ\r\nin regard to the magnitude of global warming and regional predictions.\r\nSeveral machine learning approaches have been proposed to train the\r\nconnections linking corresponding variables in the different models.\r\nStochastic approaches can avoid non-global local optima, but it seems likely\r\nthat an intelligent \"expert system\" approach would improve the supermodel. ", "recorded": "2011-10-26T10:00:00", "title": "Supermodeling:  Consensus by Synchronization of Alternative Models"}, {"url": "bbci2012_wichmann_system_identification", "desc": "Understanding perception and the underlying cognitive processes on a behavioral level requires a solution to the feature identification problem: Which are the features on which sensory systems base their computations? What techniques can we use to identify them? Thus one of the central challenges in psychophysics is System Identification: We need to infer the critical features, or cues, human observers make use of when they see or hear. What aspect of the visual or auditory stimulus actually influences behaviour if faced with real-world, complex stimuli? In my laboratory we have developed exploratory, data-driven system identification techniques based on modern machine learning methods to infer the critical features from human behavioural judgments. I will present these methods and show what their benefits are over the traditional \u201creverse-correlation\u201d approach and the \u201cbubbles technique\u201d.", "recorded": "2012-09-25T09:00:00", "title": "System Identification Using Machine Learning Methods"}, {"url": "mlss06tw_zhang_stai", "desc": "The talk is divided into two parts. The first part focuses on web-search ranking, for which I discuss training relevance models based on DCG (discounted cumulated gain) optimization. Under this metric, the system output quality is naturally determined by the performance near the top of its rank-list. I will mainly focus on various theoretical issues for this learning problem. The second part discusses related algorithmic issues in the context of optimizing the scoring function of a statistical machine translation system according to the BLEU metric (standard measure of translation quality). Our approach treats machine translation as a black-box, and can optimize millions of system parameters automatically. This has not been attempted before in this context. I will present our method and some initial results.", "recorded": "2006-07-26T00:00:00", "title": "Large Scale Ranking Problem: some theoretical and algorithmic issues"}, {"url": "mlpmsummerschool2014_bourque_human_non_coding_DNA", "desc": "In this presentation we will present an overview of the functional genomics datasets and tools that have been made available by consortiums such as ENCODE, the NIH Roadmap and now the International Human Epigenome Consoritum (IHEC). These data have been generated in a collection of reference and disease cell-types and include information on protein-DNA interactions or on histone marks (ChIP-Seq), transcriptome (RNA-Seq), methylation (Mehyl-Seq) and open chromatin (DNase-Seq). We will explain how these data can used to interpret human non-coding DNA and help identify detrimental DNA variants or mutations. Finally, we will show how machine-approaches can be used to go beyond the simple annotation of non-coding DNA and to mine these functional genomics data even further.", "recorded": "2014-09-17T09:00:00", "title": "Deciphering human non-coding DNA using machine learning approaches"}, {"url": "icgi08_coste_opening", "desc": "ICGI-2008 is the ninth in a series of successful biennial international conferences in the area of grammatical inference.\r\n  Grammatical inference has been extensively addressed by researchers in information theory, automata theory, language acquisition, computational linguistics, machine learning, pattern recognition, computational learning theory and neural networks.", "recorded": "2008-09-22T09:00:00", "title": "Opening Remarks"}, {"url": "nipsworkshops2011_sierranevada", "desc": "The Post-Conference Workshop Program covered a wide range of topics from Neuroscience to Machine Learning.\r\n\r\nDetailed information can be found at [[http://nips.cc/Conferences/2011/Program/schedule.php?Session=Workshops|NIPS 2011 Workshops homepage]].\r\n\r\n----\r\n**Click on the picture for the videos from 2011 NIPS Conference.**\r\n||[[:nips2011_granada]]||\r\n----", "recorded": "2011-12-16T07:30:00", "title": "NIPS Workshops, Sierra Nevada 2011"}, {"url": "bootcamp07_alche_bio", "desc": "After a brief introduction to the use of machine learning in computational biology, we focus on the problem of biological networks inference.\r We define the problem as a problem of kernel learning using prediction in kernelized output spaces. Methods based on Output kernel Tree are presented to solve the problem. Results on two benchmarks are shown.\r ", "recorded": "2007-07-13T09:00:00", "title": "ML in Bioinformatics "}, {"url": "mloss08_maes_nieme", "desc": "Nieme is a machine learning library for large-scale classi\ufb01cation, regression and ranking. It relies on the framework of energy-based models which uni\ufb01es several learning algorithms ranging from simple perceptrons\r\nto recent models such as the Pegasos support vector machine or L1-regularized maximum entropy models.\r\nThis framework also uni\ufb01es batch and stochastic learning which are both seen as energy minimization\r\nproblems. Nieme can hence be used in a wide range of situations, but is particularly interesting for very-\r\nlarge-scale learning tasks where both the examples and the features are processed incrementally. Being able\r\nto deal with new incoming features at any time within the learning process is another key feature of the\r\nNieme toolbox. Nieme is released under the GPL license. It is e\ufb03ciently implemented in C++ and works\r\non Linux, MacOS and Windows. Interfaces are available for C++, Java and Python.", "recorded": "2008-12-12T16:15:00", "title": "Nieme"}, {"url": "eswc2015_mutharaju_scalable_reasoning", "desc": "OWL 2 EL is one of the tractable profiles of the Web Ontology\r\nLanguage (OWL) which is a W3C-recommended standard. OWL 2\r\nEL provides sufficient expressivity to model large biomedical ontologies\r\nas well as streaming data such as traffic, while at the same time allows\r\nfor efficient reasoning services. Existing reasoners for OWL 2 EL, however,\r\nuse only a single machine and are thus constrained by memory and\r\ncomputational power. At the same time, the automated generation of\r\nontological information from streaming data and text can lead to very\r\nlarge ontologies which can exceed the capacities of these reasoners. We\r\nthus describe a distributed reasoning system that scales well using a cluster\r\nof commodity machines. We also apply our system to a use case on\r\ncity traffic data and show that it can handle volumes which cannot be\r\nhandled by current single machine reasoners.", "recorded": "2015-06-02T12:00:00", "title": "Distributed and Scalable OWL EL Reasoning"}, {"url": "kdd08_borgwardt_gmgk", "desc": "Social and biological networks have led to a huge interest in data analysis on graphs. Various groups within the KDD community have begun to study the task of data mining on graphs, including researchers from database-oriented graph mining, and researchers from kernel machine learning. Their approaches are often complementary, and we feel that exciting research problems and techniques can be discovered by exploring the link between these different approaches to graph mining.\r\n\r\nThis tutorial presents a comprehensive overview of the techniques developed in graph mining and graph kernels and examines the connection between them. The goal of this tutorial is i) to introduce newcomers to the field of graph mining, ii) to introduce people with database background to graph mining using kernel machines, iii) to introduce people with machine learning background to database-oriented graph mining, and iv) to present exciting research problems at the interface of both fields. ", "recorded": "2008-08-24T14:00:00", "title": "Graph Mining and Graph Kernels"}, {"url": "amlcf09_dhar_mlbp", "desc": "Forecasting models generally do a very poor job in predicting future returns in financial markets which\r\nare characterized by high levels of noise. It is commonly known that predictions of typical autoregressive\r\ntime series forecasting models, for example, are usually very conservative \u2013 meaning close to the mean \u2013\r\nbecause of the inherent difficulty of making accurate forecasts. This situation is particularly discouraging\r\nsince it is the large future values that we especially care about predicting accurately. Machine learning\r\nmethods provide a potential solution to this problem by providing multiple models that each of which\r\nmakes forecasts of different magnitudes. The obvious problem with this approach is overfitting,\r\nespecially if the learner has no constraints on how complex its models can be. In this talk, I present the\r\ntradeoffs between increased the model complexity and forecast accuracy based on a mix of trading results\r\nand current research.", "recorded": "2009-07-21T09:00:00", "title": "Machine Learning Based Prediction in Financial Markets"}, {"url": "nips09_muller_tbc", "desc": "Brain Computer Interfacing (BCI) aims at making use of brain signals\r\nfor e.g. the control of objects, spelling, gaming and so on. This talk\r\nwill first provide a brief overview of Brain Computer Interface\r\nfrom a machine learning and signal processing perspective. In\r\nparticular it shows the wealth, the complexity and the difficulties of\r\nthe data available, a truly enormous challenge: In real-time a\r\nmulti-variate very strongly noise contaminated data stream is to be\r\nprocessed and neuroelectric activities are to be accurately\r\ndifferentiated in real time.\r\n\r\nFinally, I report in more detail about the Berlin Brain Computer\r\n(BBCI) Interface that is based on EEG signals and take the audience\r\nall the way from the measured signal, the preprocessing and filtering,\r\nthe classification to the respective application. \u00c2 BCI as a new\r\nchannel for man-machine communication is discussed in a clinical\r\nsetting and for gaming.", "recorded": "2009-12-10T13:30:00", "title": "Towards Brain Computer Interfacing: Algorithms for on-line Differentiation of Neuroelectric Activities"}, {"url": "mlsb09_king_asci", "desc": "The basis of science is the hypothetico-deductive method and the recording of experiments in sufficient detail to enable reproducibility. We report the development of the Robot Scientist \"Adam\" which advances the automation of both. Adam has autonomously generated functional genomics hypotheses about the yeast Saccharomyces cerevisiae, and experimentally tested these hypotheses using laboratory automation. We have confirmed Adam's conclusions through manual experiments. To describe Adam's research we have developed an ontology and logical language. The resulting formalization involves over 10,000 different research units in a nested tree-like structure, ten levelsdeep, that relates the 6.6 million biomass measurements to their logical description. This formalization describes how a machine discovered new scientific knowledge. Describing scientific investigations in this way opens up new opportunities to apply machine learning and data-mining to discover new knowledge.", "recorded": "2009-09-06T16:00:00", "title": "Automating Science"}, {"url": "aibootcamp2011_accra", "desc": "The Ghana-India Kofi Annan Centre of Excellence in ICT (AITI-KACE) in collaboration with PASCAL and ACTIVE Knowledge Powered Enterprise hosted, from February 14-22, 2011 Ghana's first capacity transfer bootcamp in the areas of Machine Learning and ACTIVE (Advanced Technologies for Knowledge-Intensive Enterprises).\n\nThe bootcamp is an international event which consists of 10 days of intensive training (lectures, practical and lab sessions) designed for Masters level students, researchers and lecturers interested in upgrading their knowledge and skills in the areas of Machine Learning and **[[http://videolectures.net/active/|ACTIVE]]**. This workshop aims at developing participants scientific skills to enable them to collaborate on research in these areas. Similar bootcamps, sponsored by **[[http://videolectures.net/pascal/|PASCAL]]** were held in Europe with the most recent in Marseilles 2010.", "recorded": "2011-02-14T09:00:00", "title": "AI Bootcamp 2011 @ AITI-KACE, Accra, Ghana"}, {"url": "mlmi04uk_rienks_addmu", "desc": "We show that, using a Support Vector Machine classifier, it is possible to determine with a 75% success rate who dominated a particular meeting on the basis of a few basic features. We discuss the corpus we have used, the way we had people judge dominance and the details of the classifier and features that were used.", "recorded": "2005-06-11T12:00:00", "title": "Automatic Dominance Detection In Meetings Using Support Vector Machines"}, {"url": "pcw06_kouylekov_eci", "desc": "The focus of our participation in PASCAL RTE2 was estimating the cost of the information of the hypothesis which is missing in the text and can not be matched with entailment rules. We have tested different system settings for calculating the importance of the words of the hypothesis and investigated the possibility of combining them with machine learning algorithm.", "recorded": "2006-04-10T00:00:00", "title": "Tree Edit Distance for Recognizing Textual Entailment: Estimating the Cost of Insertion"}, {"url": "nips2011_gilbert_analysis", "desc": "The last 15 years we have seen an explosion in the role of\r\nsparsity in mathematical signal and image processing, signal and image\r\nacquisition and reconstruction algorithms, and myriad applications.\r\nIt is also central to machine learning. I will present an overview of\r\nthe mathematical theory and several fundamental\r\nalgorithmic results, including a fun application to solving Sudoku puzzles.", "recorded": "2011-12-13T16:00:00", "title": "Sparsity: algorithms, approximations, and analysis"}, {"url": "russir2012_talbot_machine", "desc": "Google Translate is a research driven product that bridges the language barrier and makes it possible to explore the multilingual web in 63 languages. This talk will highlight some of the key research efforts that have made this possible: large-scale language models, large-scale parallel data mining, syntactic reordering and targeted evaluation metrics.", "recorded": "2012-08-21T10:00:00", "title": "Machine Translation at Google"}, {"url": "nipsworkshops2010_optimization", "desc": "Our workshop focuses on optimization theory and practice that is\r\nrelevant to machine learning. This proposal builds on precedent\r\nestablished by two of our previously well-received NIPS\r\nworkshops:\r\n*(@NIPS*08) http://opt2008.kyb.tuebingen.mpg.de/\r\n*(@NIPS*09) http://opt.kyb.tuebingen.mpg.de/\r\n\r\nBoth these workshops had packed (often overpacked) attendance almost\r\nthroughout the day. This enthusiastic reception reflects the strong\r\ninterest, relevance, and importance enjoyed by optimization in\r\nthe greater ML community. One could ask why does optimization\r\nattract such continued interest? The answer is simple but telling:\r\noptimization lies at the heart of almost every ML algorithm. For\r\nsome algorithms textbook methods suffice, but the majority require\r\ntailoring algorithmic tools from optimization, which in turn depends\r\non a deeper understanding of the ML requirements. In fact, ML\r\napplications and researchers are driving some of the most cuttingedge\r\ndevelopments in optimization today. The intimate relation\r\nof optimization with ML is the key motivation for our workshop,\r\nwhich aims to foster discussion, discovery, and dissemination\r\nof the state-of-the-art in optimization, especially in the context\r\nof ML. The workshop should realize its aims by: \r\n*Providing a platform for increasing the interaction between researchers from\r\noptimization, operations research, statistics, scientific computing,\r\nand machine learning; \r\n*Identifying key problems and challenges that lie at the intersection of optimization and ML; \r\n*Narrowing the gap between optimization and ML, to help reduce rediscovery,\r\nand thereby accelerating new advances.\r\n\r\nWorkshop homepage: http://opt.kyb.tuebingen.mpg.de/", "recorded": "2010-12-10T07:30:00", "title": "Optimization for Machine Learning"}, {"url": "mlss05us_mcallester_lsd", "desc": "Discriminative learning framework is one of the very successful fields of machine learning. The methods of this paradigm, such as Boosting, and Support Vector Machines have significantly advanced the state-of-the-art for classification by improving the accuracy and by increasing the applicability of machine learning methods. One of the key benefits of these methods is their ability to learn efficiently in high dimensional feature spaces, either by the use of implicit data representations via kernels or by explicit feature induction. However, traditionally these methods do not exploit dependencies between class labels where more than one label is predicted. Many real-world classification problems involve sequential, temporal or structural dependencies between multiple labels. We will investigate recent research on generalizing discriminative methods to learning in structured domains. These techniques combine the efficiency of dynamic programming methods with the advantages of the state-of-the-art learning methods.", "recorded": "2005-05-26T08:30:00", "title": "Learning on Structured Data"}, {"url": "mcslw04_christmann_rpsvm", "desc": "The talk brings together methods from two disciplines: machine learning theory and robust statistics. We argue that robustness is an important aspect and we show that many existing machine learning methods based on convex risk minimization have - besides other good properties - also the advantage of being robust if the kernel and the loss function are chosen appropriately. Our results cover classification and regression problems. Assumptions are given for the existence of the influence function and for bounds on the influence function. Kernel logistic regression, support vector machines, least squares and the AdaBoost loss function are treated as special cases. We also consider Robust Learning from Bites, a simple method to make some methods from convex risk minimization applicable for huge data sets for which currently available algorithms are much to slow. As an example we use a data set from 15 German insurance companies.", "recorded": "2005-10-05T10:15:00", "title": "Robustness properties of support vector machines and related methods"}, {"url": "russir2012_nie_information", "desc": "Cross-language information retrieval (CLIR) aims to find relevant documents that are written in a different language than the query. When there is not enough relevant information in the language of the user, the user may be interested in using CLIR to find more information originally written in other languages. In this talk, we will describe the problems of CLIR and its differences with general the machine translation task. The traditional approaches will be described, namely the ones based on machine translation, bilingual dictionaries and parallel and comparable texts.\r\nAlthough reasonable effectiveness can be obtained using these approaches, problems remain. We will further analyze the remaining problems and describe the current efforts to solve them. Finally, we will show that the approaches developed for CLIR can be adequately used for other IR tasks such as monolingual IR. This talk will provide a general introduction to the area of CLIR and suggest some interesting research problems for the future.", "recorded": "2012-08-21T10:00:00", "title": "Cross-Language Information Retrieval and Beyond"}, {"url": "mlss05us_altun_lsd", "desc": "Discriminative learning framework is one of the very successful fields of machine learning. The methods of this paradigm, such as Boosting, and Support Vector Machines have significantly advanced the state-of-the-art for classification by improving the accuracy and by increasing the applicability of machine learning methods. One of the key benefits of these methods is their ability to learn efficiently in high dimensional feature spaces, either by the use of implicit data representations via kernels or by explicit feature induction. However, traditionally these methods do not exploit dependencies between class labels where more than one label is predicted. Many real-world classification problems involve sequential, temporal or structural dependencies between multiple labels. We will investigate recent research on generalizing discriminative methods to learning in structured domains. These techniques combine the efficiency of dynamic programming methods with the advantages of the state-of-the-art learning methods.", "recorded": "2005-05-26T08:30:00", "title": "Learning on Structured Data"}, {"url": "w3cworkshop2012_gornostay_services", "desc": "Today terminology plays an extremely important role in multilingual Europe in ensuring efficient and precise communication. This presentation focuses on Tilde\u2019s projects and advances in establishing cloud-based platforms for acquiring, sharing, and reusing language resources to improve automated natural language data processing (e.g., machine translation_. Multilingual consolidated and harmonized terminology is already utilized as data in the process of human translation and now it is also being developed as a web-based service with machines (e.g. machine translation systems, indexing systems, and search engines) as users. This development has the potential to vastly enhance the degree of automation for linked open data technologies and reveal synergies between Linked Open Data and Multilingual Language Technologies.\r\n\r\n**The transcript of the Q&A session \"Linked Open Data and the Lexicon\" is available [[http://www.w3.org/2012/06/11-mlwDub-minutes#qa_3|here]].**", "recorded": "2012-06-11T12:20:00", "title": "Extending the Use of Web-Based Terminology Services"}, {"url": "um05_guerrero_iixum", "desc": "Infoville XXI is a citizen web portal of Valencia (Spain). This paper presents several approaches based on Machine Learning that help in improving the site. Three ways of improvement are taken into account: (i) user clickstream forecasting, (ii) user profiling by clustering and (iii) recommendation of services to users, being the last two techniques part of a methodological framework with general applicability that tries to be useful for a range of web sites as wide as possible. Results obtained with data sets from this web portal show that the most appropriate techniques for user clickstream forecasting become Support Vector Machines and Multilayer Perceptrons, whilst Adaptive Resonance Theory and Self-Organizing Maps appear to be the most suitable techniques for clustering. Final recommendation and adaptation of the recommender system is currently being developed by using Learning Vector Quantization and Reinforcement Learning.", "recorded": "2005-07-25T00:00:00", "title": "Improving Infoville XXI using Machine Learning Techniques"}, {"url": "bootcamp07_guyon_lwo", "desc": " This course covers feature selection fundamentals and applications. The students will first be reminded of the basics of machine learning algorithms and the problem of overfitting avoidance. In the wrapper setting, feature selection will be introduced as a special case of the model selection problem. Methods to derive principled feature selection algorithms will be reviewed as well as heuristic method, which work well in practice. One class will be devoted to feature construction techniques. Finally, a lecture will be devoted to the connections between feature section and causal discovery. The class will be accompanied by several lab sessions. The course will be attractive to students who like playing with data and want to learn practical data analysis techniques. The instructor has ten years of experience with consulting for startup companies in the US in pattern recognition and machine learning. Datasets from a variety of application domains will be made available: handwriting recognition, medical diagnosis, drug discovery, text classification, ecology, marketing.", "recorded": "2007-07-04T09:00:00", "title": "Learning without overlearning"}, {"url": "bootcamp07_guyon_ifs", "desc": " This course covers feature selection fundamentals and applications. The students will first be reminded of the basics of machine learning algorithms and the problem of overfitting avoidance. In the wrapper setting, feature selection will be introduced as a special case of the model selection problem. Methods to derive principled feature selection algorithms will be reviewed as well as heuristic method, which work well in practice. One class will be devoted to feature construction techniques. Finally, a lecture will be devoted to the connections between feature section and causal discovery. The class will be accompanied by several lab sessions. The course will be attractive to students who like playing with data and want to learn practical data analysis techniques. The instructor has ten years of experience with consulting for startup companies in the US in pattern recognition and machine learning. Datasets from a variety of application domains will be made available: handwriting recognition, medical diagnosis, drug discovery, text classification, ecology, marketing.\r ", "recorded": "2007-07-04T10:00:00", "title": "Introduction to feature selection"}, {"url": "icml09_smith_spn", "desc": "This tutorial will discuss the use of structured prediction methods from machine learning in natural language processing. The field of NLP has, in the past two decades, come to simultaneously rely on and challenge the field of machine learning. Statistical methods now dominate NLP, and have moved the field forward substantially, opening up new possibilities for the exploitation of data in developing NLP components and applications. However, formulations of NLP problems are often simplified for computational or practical convenience, at the expense of system performance. This tutorial aims to introduce several structured prediction problems from NLP, current solutions, and challenges that lie ahead. Applications in NLP are a mainstay at ICML conferences; many ML researchers view NLP as a primary or secondary application area of interest. This tutorial will help the broader ML community understand this important application area, how progress is measured, and the trade-offs that make it a challenge.", "recorded": "2009-06-14T16:00:00", "title": "Structured Prediction for Natural Language Processing"}, {"url": "bootcamp07_guyon_emb", "desc": " This course covers feature selection fundamentals and applications. The students will first be reminded of the basics of machine learning algorithms and the problem of overfitting avoidance. In the wrapper setting, feature selection will be introduced as a special case of the model selection problem. Methods to derive principled feature selection algorithms will be reviewed as well as heuristic method, which work well in practice. One class will be devoted to feature construction techniques. Finally, a lecture will be devoted to the connections between feature section and causal discovery. The class will be accompanied by several lab sessions. The course will be attractive to students who like playing with data and want to learn practical data analysis techniques. The instructor has ten years of experience with consulting for startup companies in the US in pattern recognition and machine learning. Datasets from a variety of application domains will be made available: handwriting recognition, medical diagnosis, drug discovery, text classification, ecology, marketing.", "recorded": "2007-07-03T12:30:00", "title": "Embedded Methods"}, {"url": "nipsworkshops2011_bach_optimization", "desc": "Submodular functions are relevant to machine learning for mainly two reasons: (1) some problems may be expressed directly as the optimization of submodular functions and (2) the Lovasz extension of submodular functions provides a useful set of regularization functions for supervised and unsupervised learning. In this talk, I will present the theory of submodular functions from a convex analysis perspective, presenting tight links between certain polyhedra, combinatorial optimization and convex optimization problems. In particular, I will show how submodular function minimization is equivalent to solving a wide variety of convex optimization problems. This allows the derivation of new efficient algorithms for approximate submodular function minimization with theoretical guarantees and good practical performance. By listing examples of submodular functions, I will also review various applications to machine learning, such as clustering or subset selection, as well as a family of structured sparsity-inducing norms that can be derived and used from submodular functions.", "recorded": "2011-12-17T09:15:00", "title": "Learning with Submodular Functions: A Convex Optimization Perspective"}, {"url": "machine_learning_video_abstracts_vol3", "desc": "Welcome to the 3rd issue of the Journal of Machine Learning Video Abstracts, published by Knowledge for All Foundation Ltd, London, with the goal of providing the legacy of the PASCAL2 Network of Excellence.\nThis issue is equally dedicated to presenting the spotlight presentations of the 2012 Neural Information Processing Systems (NIPS) Conference held in December 3, 2012, Lake Tahoe, Nevada, United States as the [[http://videolectures.net/machine_learning_video_abstracts_vol1/|Volume 2]] and [[http://videolectures.net/machine_learning_video_abstracts_vol1/|Volume 3]] of this Journal. The original papers which were submitted to NIPS 2012 were also added to the video abstracts.\n\n[[http://videolectures.net/chris_burges/|Chris J.C. Burges]], [[http://videolectures.net/leon_bottou/|Leon Bottou]], [[http://www2.research.att.com/~haffner/|Patrick Haffner]] and Davor Orlic\n\nEditors of the 3rd Issue and Programme Chairs of the 2010 NIPS Conference\n\n----\nNIPS Oral Sessions videos are available at **[[nips2012_laketahoe]]**\n\nNIPS Workshops videos are available at **[[nipsworkshops2012_laketahoe]]**", "recorded": "2012-12-04T09:00:00", "title": "Video Journal of Machine Learning Abstracts - Volume 3"}, {"url": "aaai2013_bennett_fighting_tuberculosis", "desc": "Tuberculosis (TB) infects one third of the world's population and is the second leading cause of death from a single infectious agent worldwide. The emergence of drug resistant TB remains a constant threat. We examine how machine learning methods can help control tuberculosis. DNA fingerprints of Mycobacterium tuberculosis complex bacteria (Mtb) are routinely gathered from TB patient isolates for every tuberculosis patient in the United States to support TB tracking and control efforts. We develop learning models to predict the genetic lineages of Mtb based on DNA fingerprints. Mining of tuberculosis patient surveillance data with respect to these genetic lineages helps discover outbreaks, improve TB control, and reveal Mtb phenotype differences. We discuss learning- and visualization-based tools to support public health efforts towards TB control in development for the New York City Health Department.", "recorded": "2013-07-17T09:00:00", "title": "Fighting the Tuberculosis Pandemic Using Machine Learning"}, {"url": "iswc2012_bernstein_knowledge_computation", "desc": "Before the Internet most collaborators had to be sufficiently close by to work together towards a certain goal. Now, the cost of collaborating with anybody anywhere on the world has been reduced to\r\nalmost zero. As a result large-scale collaboration between humans and computers has become technically feasible. In these collaborative setups humans can carry the part of the weight of processing. Hence, people and computers become a kind of \\global brain\" of distributed interleaved human-machine computation (often called collective intelligence, social computing, or various other terms). Human computers as part of computational processes, however, come with their own strengths and issues. In this paper we take the underlying ideas of Bernstein et al. regarding three traits on human computation|motivational diversity, cognitive diversity, and error diversity|and discuss them in the light of a Global Brain Semantic Web.", "recorded": "2012-11-11T09:30:01", "title": "Interleaving Human-Machine Knowledge and Computation"}, {"url": "mlpmsummerschool2014_varoquaux_brain_imaging", "desc": "In this talk, I would like to showcase a few examples of machine learning problems that arise when using brain imaging to understand brain function and its pathologies. I'll first introduce brain images and to derive statistical features. Then I'll discuss how prediction from these images is useful for diagnostic purposes, but also as a windows to understand the brain. I'll highlight specific challenges that arise when learning predictive models from brain maps, and details solutions put forward by our group, namely spatial penalties. Moving beyond well-posed statistical maps, I'll show how a combination of unsupervised modeling and supervised learning can predict phenotypic traits from spontaneous brain activity, recorded without controlling the subjects behavior. Finally, I'll detail how our work builds upon and nourishes a Python software stack that we leverage to interact with practitioners.", "recorded": "2014-09-18T16:00:00", "title": "Machine learning for brain imaging"}, {"url": "xlimekickoff2013_karlsruhe", "desc": "**[[http://xlime.eu/|xLiMe]]** proposes to extract knowledge from different media channels and languages and relate it to cross-lingual, cross-media knowledge bases. By doing this in near real-time we will provide a continuously updated and comprehensive view on knowledge diffusion across media, e.g., from European communities like Catalonia to worldwide content in English. By combining speech recognition, natural language processing, machine learning and semantic technologies we will advance key open research problems, by: \r\n\r\n# extracting machine-readable knowledge (entities, sentiment, events and opinions) from multilingual, multimedia and social media content and integrate it with cross-lingual, cross-media knowledge bases, \r\n# searching this knowledge with structured and unstructured queries in near real-time, \r\n# monitoring its provenance, consumption and diffusion and \r\n# analysing the interdependency between media exposure and behavioural patterns", "recorded": "2013-11-25T09:00:00", "title": "Project crossLingual crossMedia knowledge extraction (xLiMe) Kick Off Meeting, Karlsruhe 2013"}, {"url": "reasecs_shvaiko_sbsm", "desc": "Knowledge representation and ontologies: Describing what can be achieved using knowledge representation, how knowledge can be represented in machine-understandable forms and what are the challenges in making machines effectively communicate on the basis of ontologies.\n\nPresentation at the workshop 'Semantic Web Technology Showcase' at the European Semantic Web Technology Conference (ESTC 2007).\n\n\nDocuments:\n;[[Semantic_Matching.pdf]]", "recorded": "2007-08-07T00:00:00", "title": "Schema-based Semantic Matching"}, {"url": "mlss06tw_lin_svm", "desc": "Support vector machines (SVM) and kernel methods are important machine learning techniques. In this short course, we will introduce their basic concepts. We then focus on the training and optimization procedures of SVM. Examples demonstrating the practical use of SVM will also be discussed. Basically we focus on classification. If time is allowed, we will also touch SVM regression.", "recorded": "2006-07-27T00:00:00", "title": "Support Vector Machines"}, {"url": "solomon_wheeler_immunesystems", "desc": "Artificial immune systems are a newly emerging paradigm for\r\ndistributed machine learning that takes its organisational and\r\nrepresentational cues from the human immune system and applies it to\r\ndifficult error and anomaly detection across a variety of problems.\r\nThe talk will be general in nature, and introduce basic concepts and\r\napplications including the use of AIS in network and physical\r\nsecurity.\r\n", "recorded": "2011-06-01T13:00:00", "title": "An overview of artificial immune systems"}, {"url": "pcw06_burchardt_atelf", "desc": "We present a baseline system for modeling textual entailment that combines deep syntactic analysis with structured lexical meaning descriptions in the FrameNet paradigm. Textual entailment is approximated by degrees of structural and semantic overlap of text and hypothesis, which we measure in a match graph. The encoded measures of similarity are processed in a machine learning setting.", "recorded": "2006-04-10T00:00:00", "title": "Approaching Textual Entailment with LFG and FrameNet Frames"}, {"url": "oh06_denoyer_xsm", "desc": "A key problem for automating the processing of semi-structured resources is the format heterogeneity among data sources. For dealing with heterogeneous semi-structured data, the correspondence between the different formats has to be established. The multiplicity and the rapid growth of information sources have motivated researchers to develop machine learning technologies for helping to automate those transformations.", "recorded": "2006-07-14T00:00:00", "title": "XML structure mapping"}, {"url": "aop07_bertinoro", "desc": "Automatic pattern analysis of data is a pillar of modern science, technology and business, with deep roots in statistics, machine learning, pattern recognition, theoretical computer science, and many other fields. A unified conceptual understanding of this strategic field is of utmost importance for researchers as well as for users of this technology. Following the successful workshop held in Erice in 2005\u00a0- [[ref:aop05_erice]] \u00a0this new edition will combine an emphasis on the common principles of this discipline and a focus on its impact on modern science and technology. Automatically finding trends, anomalies, similarities and any other relation of interest in a dataset is a crucial task for theory and applications, where statistical and algorithmic ideas are intertwined, as well as ideas and methods from information theory, optimization, data mining and machine learning. Very often, different communities focus on different aspects or approaches, so that a general view of the problem is difficult to achieve. The goal of this course is precisely this: to bring together and compare different approaches to the problem of detecting and analyzing any type of patterns in any type of data. This course will deal with general themes arising from the analysis of patterns in different disciplines, theor impact on science and technology. The intended audience are students and researchers in statistcs, computer science, data mining, neural networks and data intensive sciences, interested in pattern analysis. The focus will be on unifying principles that underlie classic disciplines such as sequence pattern matching, pattern recognition by means of machine learning systems, etc.\r\n\r\n[[http://www.analysis-of-patterns.net/bertinoro_07/index.shtml]]", "recorded": "2007-10-21T09:00:00", "title": "The Analysis of Patterns, Bertinoro 2007"}, {"url": "rss2010_zaragoza", "desc": "The ability to adapt to changing environment autonomously will be essential for future robots. While this need is well-recognized, most machine learning research focuses largely on perception and static data sets. Instead, future robots need to interact with the environment to generate the data that is needed to foster real-time adaptation based on all information collected in previous interactions and observations. In other words, we need to close the loop between the robot acting, robot sensing and robot learning. Novel active methods need to outperform passive methods by a margin that compensates the potential the extra computational burden and the cost of the active data sampling.\n\nDuring the last years, there has been an increasing interest in related techniques that could potentially become applicable in this context. These include techniques from statistics such as adaptive sensing or sequential experimental design as well novel reinforcement learning methods that have the potential to scale into robotics. In this context, we would like to bring together researchers from both the robotics and active machine learning in order to discuss for which problems the autonomous learning loop can be closed using learning, and to identify the machine learning methods that can be used to close it.\n\nDetailed information can be found at [[http://users.isr.ist.utl.pt/~rmcantin/pmwiki/pmwiki.php/RSS10/RSS10|RSS 2010 Workshop on Active Learning for Robotics]].\n\n\n**//Disclaimer:// VideoLectures.NET emphasizes we are not the authors of these recordings and that the overall quality of these videos was notably improved.", "recorded": "2010-06-27T09:45:00", "title": "Towards Closing the Loop: Active Learning for Robotics - RSS'10 Workshop"}, {"url": "w3cworkshop2012_jones_linking", "desc": "We introduce Localisation & Language Linked Data (L3D) as a way to connect content creators, consumers, language service providers and translators. Our Drupal-based platform, CMS-LION, provides text segmentation, machine translation, crowd-sourced post-editing, and content review. The output of each component is inter-connected through Linked Data, which is enriched using provenance data. This allows for post-edits to be extracted and used in the re-training of Statistical Machine Translation (SMT) engines in close collaboration with the Panacea (http://www.panacea-lr.eu/) research project. Panacea offers web service based work flows for executing Natural Language Processing (NLP) processes. This talk will introduce the platform, its features, and collaboration with Panacea, as well as discussing our current and future research agenda.\r\n\r\n**The transcript of the Q&A session \"Linking Resources\" is available [[http://www.w3.org/2012/06/11-mlwDub-minutes#qa_2|here]].**", "recorded": "2012-06-11T11:00:00", "title": "Linking Localisation and Language Resources"}, {"url": "iswc2011_ngonga_ngomo_scms", "desc": "The migration to the Semantic Web requires from CMS that they integrate human- and machine-readable data to support their seamless integration into the Semantic Web. Yet, there is still a blatant need for frameworks that can be easily integrated into CMS and allow to transform their content into machine-readable knowledge with high accuracy.\r\nIn this paper, we describe the SCMS (Semantic Content Management Systems) framework, whose main goals are the extraction of knowledge from unstructured data in any CMS and the integration of the extracted knowledge into the same CMS. Our framework integrates a highly accurate knowledge extraction pipeline. In addition, it relies on the RDF and HTTP standards for communication and can thus be integrated in virtually any CMS. We present how our framework is being used in the energy sector. We also evaluate our approach and show that our framework outperforms even commercial software by reaching up to 96% F-score.", "recorded": "2011-10-27T10:30:00", "title": "SCMS - Semantifying Content Management Systems"}, {"url": "ecmlpkdd09_bled", "desc": "The European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD) took place in Bled, Slovenia, from September 7th to 11th, 2009. This event builds upon a very successful series of 19 ECML and 12 PKDD conferences, which have been jointly organized for the past eight years. It has become the major European scientific event in these fields and in 2009 it will comprise presentations of contributed papers and invited speakers, a wide program of workshops and tutorials, a discovery challenge, a demo track and an industrial track.\n\nThe Conference homepage can be found [[http://www.ecmlpkdd2009.net/|here]].\n\nCheck previous editions of the ECML PKDD conference series here\n*[[http://videolectures.net/ecml03_dubrovnik/|ECML PKDD 2003 - Dubrovnik]]\n*[[http://videolectures.net/ecml07_warsaw/|ECML PKDD 2007 - Warsaw]]\n*[[http://videolectures.net/ecmlpkdd08_antwerp/|ECML PKDD 2008 - Antwerp]]", "recorded": "2009-09-07T09:00:00", "title": "European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD), Bled 2009"}, {"url": "bbci2012_kawanabe_brain_machine_interfaces", "desc": "Thanks to the huge efforts by researchers in this field, basic technologies of brain-computer interfaces (BCIs) has been established and practical applications, for instance, BCI rehabilitations are being developed. Based on our previous achievements of non-invasive brain-machine interfaces (BMIs) in experimental rooms, ATR-BICR has started the network BMI project in order to develop real-world BMI for supporting elderly and disable people in daily life. We will tackle this challenging problem by simultaneous measurement of human behavior and brain activities and also by parallel and distributed processing of large-scale data. For conducting real-world BMI experiments and acquiring daily-life brain activities, we have prepared in the premises of ATR the BMI smart house with various ambient sensors. In this talk, I will introduce our network BMI project and show some pilot results in the first year.", "recorded": "2012-09-19T14:45:00", "title": "Challenges Towards Brain-Machine Interfaces for Supporting Elderly and Disabled People in Daily Life"}, {"url": "ecmlpkdd2011_spiliopoulou_melodic", "desc": "Modelling the real world complexity of music is a challenge for machine learning. We address the task of modeling melodic sequences from the same music genre. We perform a comparative analysis of two probabilistic models; a Dirichlet Variable Length Markov Model (Dirichlet-VMM) and a Time Convolutional Restricted Boltzmann Machine (TC-RBM). We show that the TC-RBM learns descriptive music features, such as underlying chords and typical melody transitions and dynamics. We assess the models for future prediction and compare their performance to a VMM, which is the current state of the art in melody generation. We show that both models perform significantly better than the VMM, with the Dirichlet-VMM marginally outperforming the TC-RBM. Finally, we evaluate the short order statistics of the models, using the Kullback-Leibler divergence between test sequences and model samples, and show that our proposed methods match the statistics of the music genre significantly better than the VMM.", "recorded": "2011-09-07T15:30:00", "title": "Comparing Probabilistic Models for Melodic Sequences"}, {"url": "machine_muandet_machines", "desc": "This paper presents a kernel-based discriminative learning framework on probability measures. Rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data. By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space (RKHS), we are able to apply many standard kernel-based learning techniques in straightforward fashion. To accomplish this, we construct a generalization of the support vector machine (SVM) called a support measure machine (SMM). Our analyses of SMMs provides several insights into their relationship to traditional SVMs. Based on such insights, we propose a flexible SVM (Flex-SVM) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework.", "recorded": "2012-12-05T10:22:00", "title": "Learning from Distributions via Support Measure Machines"}, {"url": "cernacademictraining08_saban_tavian_mess_clhc", "desc": "The operation of the Large Hadron Collider relies many systems with technologies often beyond the start of the art and in particular on hundreds of superconducting magnets operating in superfluid He at 1.9K powered by more than 1700 power converters. A sophisticated magnet protection system is crucial to detect a quench and safely extract the energy stored in the circuits (about 1GJ only in one of the dipole circuits of each sector) after a resistive transition. \r\n\r\nIn order to ensure safe operation, these systems depend on each other and on the infrastructure systems (controls, electricity distribution, water cooling, ventilation, communication systems, etc.). The commissioning of the technical systems together with the associated infrastructures is therefore mandatory.\r\n\r\nThe complexity of operating this machine stems from the dependence and interplay of the systems, their nominal performance which is often at the technological frontier and on their geographical distribution.\r\n", "recorded": "2008-05-13T10:56:00", "title": "Commissioning of the LHC super-conducting magnets systems - Why an LHC Hardware Commissioning? Specificity and complexity of this machine"}, {"url": "icml09_wingate_tarlc", "desc": "Building on last year's competition and the benchmarking events that preceded it, this event will be a forum for reinforcement learning researchers to rigorously compare the performance of their methods on a suite of challenging domains. The competition finals and workshop will take place during the Multidisciplinary Symposium on Reinforcement Learning, a part of the 2009 International Conference on Machine Learning (ICML'09) in Montreal, Canada. To encourage student participation, we will be awarding travel scholarships for student competitors.\r\n\r\nIn order to encourage even greater participation, our technical committee has been working hard to lower the bar for entry. The competition software is easy to install, and contains sample code so that you can get started with minimal effort. In addition to travel scholarships there are a number of exciting prizes for grabs. We are also including resources to make it easy for instructors to use the competition software as a part of a course on Reinforcement Learning, Machine Learning, or Artificial Intelligence.\r\n\r\n[[http://2009.rl-competition.org/index.php]]", "recorded": "2009-06-18T18:10:00", "title": "Third Annual Reinforcement Learning Competition"}, {"url": "nips2011_reichert_inference", "desc": "It has been argued that perceptual multistability reflects probabilistic inference performed by the brain when sensory input is ambiguous. Alternatively, more traditional explanations of multistability refer to low-level mechanisms such as neuronal adaptation. We employ a Deep Boltzmann Machine (DBM) model of cortical processing to demonstrate that these two different approaches can be combined in the same framework. Based on recent developments in machine learning, we show how neuronal adaptation can be understood as a mechanism that improves probabilistic, sampling-based inference. Using the ambiguous Necker cube image, we analyze the perceptual switching exhibited by the model. We also examine the influence of spatial attention, and explore how binocular rivalry can be modeled with the same approach. Our work joins earlier studies in demonstrating how the principles underlying DBMs relate to cortical processing, and offers novel perspectives on the neural implementation of approximate probabilistic inference in the brain.", "recorded": "2011-12-13T12:40:00", "title": "Neuronal Adaptation for Sampling-Based Probabilistic Inference in Perceptual Bistability"}, {"url": "mlsb07_evry", "desc": "Molecular biology and also all the biomedical sciences are undergoing a true revolution as a result of the emergence and growing impact of a series of new disciplines/tools sharing the \"-omics\" suffix in their name. These include in particular genomics, transcriptomics, proteomics and metabolomics devoted respectively to the examination of the entire systems of genes, transcripts, proteins and metabolites present in a given cell or tissue type. The availability of these new, highly effective tools for biological exploration is dramatically changing the way one performs research in at least two respects. First of all, the amount of available experimental data is not at all a limiting factor any more; on the contrary, there is a plethora of it. The challenge has shifted towards identifying the relevant pieces of information given the question, and how to make sense out of it (a \"data mining\" issue). Secondly, rather than to focus on components in isolation, we can now try to understand how biological systems behave as the result of the integration and interaction between the individual components that one can now monitor simultaneously (so called \"systems biology\"). Taking advantage of this wealth of \"genomic\" information has become a conditio sine qua non for whoever ambitions to remain competitive in molecular biology and more generally in biomedical sciences. Machine learning naturally appears as one of the main drivers of progress in this context, where most of the targets of interest deal with complex structured objects: sequences, 2D and 3D structures or interaction networks. At the same time bioinformatics and systems biology have already induced significant new developments of general interest in machine learning, for example in the context of learning with structured data, graph inference, semi-supervised learning, system identification, and novel combinations of optimization and learning algorithms. The aim of this workshop is to contribute to the cross-fertilization between the research in machine learning methods and their applications to complex biological and medical questions by bringing together method developers and experimentalists. We encourage submissions bringing forward methods for discovering complex structures (e.g. interaction networks, molecule structures) and methods supporting genome-wide data analysis.", "recorded": "2007-09-24T09:00:00", "title": "1st International Workshop on Machine Learning in Systems Biology (MLSB), Evry 2007"}, {"url": "mlsb09_ljubljana", "desc": "Molecular biology and all the biomedical sciences are undergoing a true revolution as a result of the emergence and growing impact of a series of new disciplines/tools sharing the \u201c-omics\u201d suffix in their name. These include in particular genomics, transcriptomics, proteomics and metabolomics, devoted respectively to the examination of the entire systems of genes, transcripts, proteins and metabolites present in a given cell or tissue type.\r\n\r\nThe availability of these new, highly effective tools for biological exploration is dramatically changing the way one performs research in at least two respects. First, the amount of available experimental data is not a limiting factor any more; on the contrary, there is a plethora of it. Given the research question, the challenge has shifted towards identifying the relevant pieces of information and making sense out of it (a \u201cdata mining\u201d issue). Second, rather than focus on components in isolation, we can now try to understand how biological systems behave as a result of the integration and interaction between the individual components that one can now monitor simultaneously (so called \u201csystems biology\u201d).\r\n\r\nTaking advantage of this wealth of \u201cgenomic\u201d information has become a \u2018conditio sine qua non\u2019 for whoever ambitions to remain competitive in molecular biology and in the biomedical sciences in general. Machine learning naturally appears as one of the main drivers of progress in this context, where most of the targets of interest deal with complex structured objects: sequences, 2D and 3D structures or interaction networks. At the same time bioinformatics and systems biology have already induced significant new developments of general interest in machine learning, for example in the context of learning with structured data, graph inference, semi-supervised learning, system identification, and novel combinations of optimization and learning algorithms.\r\n\r\nThe aim of this workshop is to contribute to the cross-fertilization between the research in machine learning methods and their applications to systems biology (i.e., complex biological and medical questions) by bringing together method developers and experimentalists.\r\n\r\nThe Workshop is organized as \"core - event\" of Patterns Analysis, Statistical Modelling and Computational Learning - Network of Excellence 2 (PASCAL 2).\r\n\r\n----\r\nMore about the workshop can be found at: http://mlsb09.ijs.si/index.html\r\n----", "recorded": "2009-09-05T08:00:00", "title": "3rd International Workshop on Machine Learning in Systems Biology (MLSB), Ljubljana 2009"}, {"url": "icgi08_saint_malo", "desc": "ICGI-2008 is the ninth in a series of successful biennial international conferences in the area of grammatical inference.\r\n\r\nGrammatical inference has been extensively addressed by researchers in information theory, automata theory, language acquisition, computational linguistics, machine learning, pattern recognition, computational learning theory and neural networks.", "recorded": "2008-09-22T09:00:00", "title": "9th International Colloquium on Grammatical Inference (ICGI), Saint-Malo 2008"}, {"url": "ida07_tutorials", "desc": "At the IDA-2007 conference we propose an interesting agenda of events that include several tutorial tracks, open panel discussions, and keynote talks, all based on the following topics of interest: # Algorithms and Techniques (Machine Learning, Data Mining, Statistics) # Theoretical Contributions (Data Analysis Principles, KDD, Data Modeling) # Application Fields (Practical, Applied and Industrial Data Analysis)", "recorded": "2007-09-06T09:00:00", "title": "Tutorials"}, {"url": "reasecs_ppr", "desc": "The Personal Publication Reader (PPR) provides a personalized access to publications and allows navigating through publications within an embedded context. The PPR gathers information about publications from distributed, heterogenous sources, extracts machine-readable semantics and enriches them with knowledge about authors, etc. so that rule-based reasoning can be used to provied context-adapted access.", "recorded": "2007-11-08T00:00:00", "title": "Personal Publication Reader"}, {"url": "mit600SCs2011_guttag_lec04", "desc": "This lecture introduces the notion of decomposition and abstraction by specification. It also covers Python modules, functions, parameters, and scoping. Finally, it uses the Python assert statement and type 'str'.\r\n\r\nTopics covered: Decomposition, module, function, abstraction, formal parameter, actual parameter, argument, assert, scope, mapping, stack, last in first out, LIFO, strings, slicing.", "recorded": "2011-01-27T09:00:00", "title": "Lecture 4: Machine Interpretation of a Program "}, {"url": "w3cworkshop2013_mccathienevile_bits", "desc": "Yandex has more than two decades of experience in developing linguistic analysis tools. We have machine-assisted translation systems, localisation systems, content management systems, front-end development modularisation and more. So what goes wrong in practice? And why doesn't all this technology always lead to perfect results?", "recorded": "2013-03-12T16:35:00", "title": "Localization in a Big Company: The Hard Bits"}, {"url": "mlss06tw_meila_co", "desc": "Clustering, or finding groups in data, is as old as machine learning itself, if not older. However, as more people use clustering in a variety of settings, the last few years we have brought unprecedented developments in this field. This tutorial will survey the most important clustering methods in use today from a unifying perspective. I will then present some of the current paradigms shifts in data clustering.", "recorded": "2006-08-02T00:00:00", "title": "Clustering - An overview"}, {"url": "nipsworkshops2012_laketahoe", "desc": "The Post-Conference Workshop Program covered a wide range of topics from Neuroscience to Machine Learning.\n\nDetailed information can be found at [[http://nips.cc/Conferences/2012/Program/schedule.php?Session=Workshops|NIPS 2012 Workshops homepage]].\n\n----\nNIPS Oral Sessions videos are available at **[[nips2012_laketahoe]]**\n\nNIPS Spotlight Sessions videos are available at **[[machine_learning_video_abstracts_vol3/]]**", "recorded": "2012-12-07T07:30:00", "title": "NIPS Workshops, Lake Tahoe 2012"}, {"url": "ida07_plenary", "desc": "At the IDA-2007 conference we propose an interesting agenda of events that include several tutorial tracks, open panel discussions, and keynote talks, all based on the following topics of interest: # Algorithms and Techniques (Machine Learning, Data Mining, Statistics) # Theoretical Contributions (Data Analysis Principles, KDD, Data Modeling) # Application Fields (Practical, Applied and Industrial Data Analysis)", "recorded": "2007-09-07T09:00:00", "title": "Plenaries"}, {"url": "nipsworkshops09_christoudias_blmk", "desc": "Many problems in machine learning involve datasets that are comprised of multiple views. The \r\nseparate views can be de\ufb01ned over a single input (e.g., multiple image feature types), or from multiple information sources (e.g., audio and video). In this context, each view can provide a redundant \r\nindication of the underlying class or event of interest, useful for classi\ufb01cation.", "recorded": "2009-12-12T16:40:00", "title": "Bayesian Localized Multiple Kernel Learning "}, {"url": "nipsworkshops2010_whistler", "desc": "The Post-Conference Workshop Program covered a wide range of topics from Neuroscience to Machine Learning.\r\n\r\nDetailed information can be found at [[http://nips.cc/Conferences/2010/Program/schedule.php?Session=Workshops|NIPS 2010 Workshops homepage]].\r\n\r\n----\r\n\r\n**Click on the picture for the videos from 2010 NIPS Conference.**\r\n||[[:nips2010_vancouver]]||\r\n**Click on the picture for the videos from 2010 NIPS Spotlights.**\r\n||[[:machine_learning_video_abstracts_vol1]]||\r\n----", "recorded": "2010-12-10T09:00:00", "title": "NIPS Workshops, Whistler 2010"}, {"url": "brownbag_ijs", "desc": "The BrownBag seminar is held every week at the Department of Knowledge Technologies at the Jo\u017eef Stefan Institute. The goal is to advance cutting-edge research and applications of knowledge technologies, including data, text and web mining, machine learning, decision support, language technologies, knowledge management, and other information technologies that support the acquisition, management, modelling and use of knowledge and data.", "recorded": "2010-01-11T09:46:43", "title": "IJS BrownBag Seminar"}, {"url": "eswc08_tenerife", "desc": "**The vision of the Semantic Web** \r\n is to enhance today's Web by exploiting machine-processable metadata. The explicit representation of the semantics of data, enriched with domain theories (ontologies), will enable a web that provides a qualitatively new level of service. It will weave together a large network of human knowledge and makes this knowledge machine-processable. Various automated services will help the users to achieve their goals by accessing and processing information in machine-understandable form. This network of knowledge systems will ultimately lead to truly intelligent systems, which will be employed for various complex decision-making tasks. Semantic Web research can benefit from ideas and cross-fertilization with many other areas: Artificial Intelligence, Natural Language Processing, Databases and Information Systems, Information Retrieval, Multimedia, Distributed Systems, Social Networks and Web Engineering. Many advances within these areas can contribute towards the realization of the Semantic Web. \r\n \r\n **The continuing success of ESWC** as a premier publication place in the area of semantics has culminated in a record number of high-quality submissions for this year (275 submissions, 19% acceptance rate).\r\n \r\n **The 5th Annual European Semantic Web Conference (ESWC 2008)** will present the latest results in research and applications of Semantic Web technologies. ESWC 2008 will also feature a tutorial program, system descriptions and demos, a posters track, a Ph.D. symposium and a number of collocated workshops. \r\n \r\n //ESWC 2008 is sponsored by STI2, Semantic Technology Institutes International. For more information on STI2, please visit [[http://www.sti2.org/|www.sti2.org]].//", "recorded": "2008-06-01T09:00:00", "title": "5th Annual European Semantic Web Conference (ESWC), Tenerife 2008"}, {"url": "mloss08_tanner_rlgcg", "desc": "RL-Glue is a protocol and software implementation for evaluating reinforcement learning algorithms. Our\r\nsystem facilitates the comparison of alternative algorithms and can greatly accelerate research progress as\r\nthe UCI database has accelerated progress in supervised machine learning. Creating a comparable bench-\r\nmarking resource for reinforcement learning is challenging because of the temporal nature of reinforcement\r\nlearning. Reinforcement learning agents interact with a dynamic process (the environment) which gener-\r\nates observations and rewards. The observations and rewards received by the learning agent depend on the actions; training data cannot simply be stored in a \ufb01le as they are in supervised learning. Instead, the rein-\r\nforcement learning agent and environment must be interacting programs. RL-Glue agents and environments\r\ncan be written in Java, C/C++, Matlab, Python, and Lisp and can all run on one machine, or can connect\r\nacross the Internet. In this seminar, we will introduce the design principles that helped shape RL-Glue\r\nand demonstrate some of the interesting extensions that have been created by the reinforcement learning\r\ncommunity.", "recorded": "2008-12-12T17:25:00", "title": "RL Glue and Codecs Glue"}, {"url": "smartdw09_turchi_ltt", "desc": "In this talk, an extensive experimental study of a Statistical Machine Translation system, Moses, from the point of view of its learning capabilities is presented. Very accurate Learning Curves are obtained, by using high-performance computing, and extrapolations of the projected performance of thesystem under different conditions are provided. Our experiments suggest:\r\n\r\n   1. The representation power of the system is not currently a limitation to its performance,\\\\\r\n   2. The inference of its models from finite sets of i.i.d. data is responsible for current performance limitations,\\\\\r\n   3. It is unlikely that increasing dataset sizes will result in significant improvements (at least in traditional i.i.d. setting),\\\\\r\n   4. It is unlikely that novel statistical estimation methods will result in significant improvements.\\\\\r\n\r\nThe current performance wall is mostly a consequence of Zipf's law, and this should be taken into account when designing a statistical machine translation system. A few possible research directions are discussed as a result of this investigation, most notably  the  integration  of  linguistic  rules  into the  model  inference  phase, and the development of active learning procedures.", "recorded": "2009-05-13T14:00:00", "title": "Learning to Translate: statistical and computational analysis"}, {"url": "lsoldm2012_ghavamzadeh_feature_selection", "desc": "Feature selection is an important problem in many areas of machine learning including\r\nreinforcement learning (RL). A possible approach to feature selection is to solve the machine\r\nlearning problem in a high dimensional feature space in the hope that relevant features lie there.\r\nHowever, this approach may suffer from overfitting and have poor prediction performance. Two\r\nmethods that have been used in regression to overcome this problem are regularization (adding\r\nl-2 and/or l-1 penalization terms to the objective function) and random projections (solving the\r\nproblem in a randomly generated low dimensional space). In this talk, we study the use of these\r\ntwo methods in value function approximation in RL In particular, we study the widely-used\r\nleast-squares temporal difference (LSTD) learning algorithm. We first provide a thorough\r\ntheoretical analysis of LSTD with random projections and derive performance bounds for the\r\nresulting algorithm. We then analyze the performance of Lasso-TO, a modification of LSTD in\r\nwhich the projection operator is defined as a Lasso problem.", "recorded": "2012-09-18T15:30:00", "title": "A Top-down Approach to Feature Selection in Reinforcement Learning"}, {"url": "is2012_demsar_orange", "desc": "Orange (http://orange.biolab.si) is a general-purpose machine learning and data mining tool. It features a multilayer architecture suitable for different kinds of users, from inexperienced data mining beginners to programmers who prefer to access the tool through its scripting interface. In the paper we outline the history of Orange's development and present its current state, achievements and the future \r\nchallenges.", "recorded": "2012-10-11T17:30:15", "title": "ORANGE: Data Mining Fruitful and Fun"}, {"url": "cidu2011_schmidt_climate", "desc": "I will discuss the challenges in analyzing data in climate science with ever-increasing observational streams from satellites and increasing complexity and resolution in climate models. I will describe specific challenges in climate model parameterization, dealing with multi-model ensembles, the rise of process-based diagnostics and data mining techniques, and the potential for computer science and machine learning to contribute to solving those problems.", "recorded": "2011-10-20T08:45:00", "title": "The Data Challenge in Climate Science"}, {"url": "iswc2014_patel_schneider_analyzing_schema", "desc": "Schema.org is a way to add machine-understandable information to web pages that is processed by the major search engines to improve search performance. The definition of schema.org is provided as a set of web pages plus a partial mapping into RDF triples with unusual properties, and is incomplete in a number of places. This analysis of and formal semantics for schema.org provides a complete basis for a plausible version of what schema.org should be.", "recorded": "2014-10-23T14:40:00", "title": "Analyzing Schema.org"}, {"url": "sikdd2014_rei_large_scale", "desc": "We present a semi-automatic data exploration and \r\norganization tool. The system integrates machine learning \r\nand text mining algorithms into an simple user interface and \r\na Client/Server architecture. The main features of the \r\nsystems include unsupervised and supervised methods for \r\nconcept suggestion, visualization and ability to make both \r\ndata and methods available to other applications as a \r\nservice. ", "recorded": "2014-10-06T10:50:00", "title": "A System For Large Scale Data Exploration and Organization"}, {"url": "ecmlpkdd2012_seldin_laviolette_shawe_taylor_pac", "desc": "PAC-Bayesian analysis is a basic and very general tool for data-dependent analysis in machine learning. By now, it has been applied in such diverse areas as supervised learning, unsupervised learning, and reinforcement learning, leading to state-of-the-art algorithms and accompanying generalization bounds. PAC-Bayesian analysis, in a sense, takes the best out of Bayesian methods and PAC learning and puts it together: (1) it provides an easy way to exploit prior knowledge (like Bayesian methods); (2) it provides strict and explicit generalization guarantees (like VC theory); and (3) it is data-dependent and provides an easy and strict way of exploiting benign conditions (like Rademacher complexities). In addition, PAC-Bayesian bounds directly lead to efficient learning algorithms. Thus, it is a key and basic subject for machine learning. While the first papers on PAC-Bayesian analysis were not easy to read, subsequent simplifications made it possible to explain it literally in three slides. We will start with a general introduction to PAC-Bayesian analysis, which should be accessible to an average student, who is familiar with machine learning at the basic level. Then, we will survey multiple forms of PAC-Bayesian bounds and their numerous applications in different fields, including supervised and unsupervised learning, finite and continuous domains, and the very recent extension to martingales and reinforcement learning. Some of these applications will be explained in more details, while others will be surveyed at a high level. We will also describe the relations and distinctions between PAC-Bayesian analysis, Bayesian learning, VC theory, and Rademacher complexities. We will discuss the role, value, and shortcomings of frequentist bounds that are inspired by Bayesian analysis.", "recorded": "2012-09-28T10:30:14", "title": "PAC-Bayesian Analysis and Its Applications"}, {"url": "nips09_hill_mlb", "desc": "Brain-computer interfaces (BCI) aim to be the ultimate in assistive technology: \r\ndecoding a user's intentions directly from brain signals without involving any \r\nmuscles or peripheral nerves.  Thus, some classes of BCI potentially offer hope \r\nfor users with even the most extreme cases of paralysis, such as in late-stage \r\nAmyotrophic Lateral Sclerosis, where nothing else currently allows communication \r\nof any kind.   Other lines in BCI research aim to restore lost motor function in as \r\nnatural a way as possible, reconnecting and in some cases re-training motor-cortical \r\nareas to control prosthetic, or previously paretic, limbs. Research and development \r\nare progressing on both invasive and non-invasive fronts, although BCI has yet to \r\nmake a breakthrough to widespread clinical application.\r\n\r\nThe high-noise high-dimensional nature of brain-signals, particularly in non-invasive\r\napproaches and in patient populations, make robust decoding techniques a necessity. \r\nGenerally, the approach has been to use relatively simple feature extraction techniques, \r\nsuch as template matching and band-power estimation, coupled to simple linear classifiers. \r\nThis has led to a prevailing view among applied BCI researchers that (sophisticated) \r\nmachine-learning is irrelevant since \"it doesn't matter what classifier you use once you've \r\ndone your preprocessing right and extracted the right features.\"  I shall show a few examples \r\nof how this runs counter to both the empirical reality and the spirit of what needs to be done \r\nto bring BCI into clinical application.  Along the way I'll highlight some of the interesting \r\nproblems that remain open for machine-learners.", "recorded": "2009-12-10T14:00:00", "title": "Machine Learning for Brain-Computer Interfaces"}, {"url": "stw07_grenoble", "desc": "More than half of the EU citizens are not able to hold a conversation in a language\r\n other than their mother tongue, let alone to \r\n conduct a negotiation, or interpret a law. In a time of wide availability of\r\n communication technologies, language barriers are a \r\n serious bottleneck to European integration and to economic and cultural exchanges in\r\n general. More effective tools to overcome such \r\n barriers, in the form of software for machine translation and other cross-lingual\r\n textual information access tasks, are in strong \r\n demand.\r\n \r\n Statistical methods are promising, in that they achieve performances equivalent or\r\n superior to those of rule-based systems, at a \r\n fraction of the development effort. There are, however, some identified shortcomings\r\n in these methods, preventing their broad \r\n diffusion. As an example, even though lexical choice is usually more accurate with\r\n Statistical Machine Translation (SMT) systems \r\n than with their rule-based counterparts, the text they produce tends to be less\r\n fluent. As a second example, SMT systems are trained \r\n in batch mode and do not adapt by taking user feedback into account. Finally, in\r\n Cross-Language Information Retrieval tasks, query \r\n words are most often translated independent of one another, thus giving up possibly\r\n relevant contextual clues.\r\n \r\n SMART is an attempt to address these and other shortcomings by the methods of modern\r\n Statistical Learning. The scientific focus is \r\n on developing new and more effective statistical approaches while ensuring that\r\n existing know-how is duly taken into account. By \r\n bringing together leading research institutions in Statistical Learning, Machine\r\n Translation and Textual Information Access, the \r\n SMART consortium is well positioned to achieve this goal.", "recorded": "2007-01-31T00:00:00", "title": "SMART Workshop, Grenoble 2007"}, {"url": "ecmlpkdd2010_gartner_vembu_apsd", "desc": "Structured prediction is the problem of predicting multiple outputs with complex internal structure and dependencies among them. Algorithms and models for predicting structured data have been in use for a long time. For example, recurrent neural networks and hidden Markov models have found interesting applications in temporal pattern recognition problems such as speech recognition. With the introduction of support vector machines in the 1990s, there has been a lot of interest in the machine learning community in discriminative models of learning. In this tutorial, we plan to cover recent developments in discriminative learning algorithms for predicting structured data.\r\n\r\nWe believe this tutorial will be of interest to machine learning researchers including graduate students who would like to gain an understanding of structured prediction and state-of-the-art approaches to solve this problem. Structured prediction has several applications in the areas of natural language processing, computer vision and computational biology, just to name a few. We believe the material presented in this tutorial will also be of interest to researchers working in the aforementioned application areas.", "recorded": "2010-09-20T14:00:00", "title": "Algorithms for Predicting Structured Data"}, {"url": "nipsworkshops2012_poczos_learning", "desc": "Low-dimensional embedding, manifold learning, clustering,\r\nclassification, and anomaly detection are among the most important\r\nproblems in machine learning. The existing methods usually consider\r\nthe case when each instance has a fixed, finite-dimensional feature\r\nrepresentation. Here we consider a different setting. We assume that\r\neach instance corresponds to a continuous probability distribution.\r\nThese distributions are unknown, but we are given some i.i.d. samples\r\nfrom each distribution. Our goal is to estimate the distances between\r\nthese distributions and use these distances to perform low-dimensional\r\nembedding, clustering/classification, or anomaly detection. We present\r\nestimation algorithms and prove when the effective dimension is small\r\nenough (as measured by the doubling dimension), then the excess\r\nprediction risk in the regression problem converges to zero with a\r\npolynomial rate. We demonstrate the power of our methods by\r\noutperforming the best published results on several computer vision\r\nbenchmarks. We also show how our perspective on learning from\r\ndistributions allows us to define new analyses in astronomy and fluid\r\ndynamics simulations.", "recorded": "2012-12-07T16:24:00", "title": "Machine Learning on Distributions"}, {"url": "oiml05_kappen_piaso", "desc": "Many problems in machine learning use a probabilistic description. Examples are pattern recognition methods and graphical models. As a consequence of this uniform description, one can apply generic approximation methods such as mean field theory and sampling methods. Another important class of machine learning problems are the reinforcement learning problems, aka optimal control problems. Here, also a probabilistic description is used, but up to now efficient mean field approximations have not been obtained. In this presentation, I consider linear-quadratic control of an arbitrary dynamical system and show, that for this class of stochastic control problems the non-linear Hamilton-Jacobi-Bellman equation can be transformed into a linear equation. The transformation is similar to the transformation used to relate the Schr\u00f6dinger equation to the Hamilton-Jacobi formalism. The computation can be performed efficiently by means of a forward diffusion process that can be computed by stochastic integration or that can be described by a path integral. For this path integral it is expected that a variational mean field approximation could be derived.", "recorded": "2005-01-23T00:00:00", "title": "A path integral approach to stochastic optimal control"}, {"url": "nips2011_bach_machine", "desc": "We consider the minimization of a convex objective function defined on a Hilbert space, which is only available through unbiased estimates of its gradients. This problem includes standard machine learning algorithms such as kernel logistic regression and least-squares regression, and is commonly referred to as a stochastic approximation problem in the operations research community. We provide a non-asymptotic analysis of the convergence of two well-known algorithms, stochastic gradient descent (a.k.a.~Robbins-Monro algorithm) as well as a simple modification where iterates are averaged (a.k.a.~Polyak-Ruppert averaging). Our analysis suggests that a learning rate proportional to the inverse of the number of iterations, while leading to the optimal convergence rate in the strongly convex case, is not robust to the lack of strong convexity or the setting of the proportionality constant. This situation is remedied when using slower decays together with averaging, robustly leading to the optimal rate of convergence. We illustrate our theoretical results with simulations on synthetic and standard datasets.", "recorded": "2011-12-13T13:30:00", "title": "Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine Learning"}, {"url": "cidu2011_el_ghaoui_text_corpora", "desc": "Sparse machine learning has recently emerged as powerful tool to obtain models of\r\nhigh-dimensional data with high degree of interpretability, at low computational cost. This paper posits that these methods can be extremely useful for understanding large collections of text documents, without requiring user expertise in machine learning. Our approach relies on three main ingredients: (a) multi-document text summarization and (b) comparative summarization of two corpora, both using sparse regression or classi\ffication; (c) sparse principal components and sparse graphical models for unsupervised analysis and visualization of large text corpora. We validate our approach using a corpus of Aviation Safety Reporting System (ASRS) reports and demonstrate that the methods can reveal causal and contributing factors in runway incursions. Furthermore, we show that the methods automatically discover four main tasks that pilots perform during flight, which can aid in further understanding the causal and contributing factors to runway incursions and other drivers for aviation safety incidents.", "recorded": "2011-10-21T11:25:00", "title": "Topic Summarization for Large Text Data Sets"}, {"url": "iswc2013_ngonga_ngomo_data_streams", "desc": "The vision behind the Web of Data is to extend the current document-oriented Web with machine-readable facts and structured data, thus creating a representation of general knowledge. However, most of the Web of Data is limited to being a large compendium of encyclopedic knowledge describing entities. A huge challenge, the timely and massive extraction of RDF facts from unstructured data, has remained open so far. The availability of such knowledge on the Web of Data would provide significant benefits to manifold applications including news retrieval, sentiment analysis and business intelligence. In this paper, we address the problem of the actuality of the Web of Data by presenting an approach that allows extracting RDF triples from unstructured data streams. We employ statistical methods in combination with deduplication, disambiguation and unsupervised as well as supervised machine learning techniques to create a knowledge base that reflects the content of the input streams. We evaluate a sample of the RDF we generate against a large corpus of news streams and show that we achieve a precision of more than 85%.", "recorded": "2013-10-24T12:30:01", "title": "Real-time RDF extraction from unstructured data streams"}, {"url": "is2012_pogacnik_photo_aesthetics", "desc": "In this paper we propose a method for automatic assessment of aesthetic appeal of photographs. We identify signifi\fcant parameters that distinguish high quality photography from low quality snapshots. On the basis of these parameters, we de\ffined calculable features for automatic assessment of photography aesthetics using machine learning methods. The calculation of features depends heavily on the identifi\fcation of the subject in photographs. With the subject identi\ffied, we defi\fned and implemented various features to analyze various aspects of a photograph. The features were tested on two datasets. First dataset was obtained from Flickr and manually labeled for evaluation. Second dataset was based on photographs from DPChallenge portal where subjects were identi\fed with a face detection algorithm. Both experiments showed some promising results. In this article we specify the features which contribute to a successful classi\fcation of photographs, analyze their in influence and discuss the results. In conclusion, we off\u000ber some suggestions for further research.", "recorded": "2012-10-08T16:30:58", "title": "Evaluating Photo Aesthetics Using Machine Learning"}, {"url": "iccc2014_gamback_poetic_machine", "desc": "The paper  reports  an  initial  study on  computational poetry generation  for Bengali. Bengali  is  a \r\nmorpho-syntactically rich language and partially phonemic. The poetry generation  task has been \r\ndefined  as  a  follow-up  rhythmic  sequence generation based on user  input. The design process involves  rhythm  understanding  from  the  given  input  and  follow-up  rhyme  generation  by \r\nleveraging  syllable/phonetic  mapping  and  natural  language  generation  techniques.  A \r\nsyllabification engine based on grapheme-to-phoneme mapping has been developed  in order  to \r\nunderstand  the given  input  rhyme. A Support Vector Machine-based  classifier  then predicts  the \r\nfollow-up  syllable/phonetic  pattern  for  the  generation  and  candidate  words  are  chosen \r\nautomatically,  based  on  the  syllable  pattern.  The  final  rhythmic  poetical  follow-up  sentence  is \r\ngenerated  through  n-gram  matching  with  weight-based  aggregation.  The  quality  of  the \r\nautomatically  generated  rhymes  has  been  evaluated  according  to  three  criteria:  poeticness, \r\ngrammaticality, and meaningfulness. ", "recorded": "2014-06-12T12:30:00", "title": "Poetic Machine: Computational Creativity for Automatic Poetry Generation in Bengali"}, {"url": "iccc2014_ventura_musical_discovery", "desc": "Many music composition algorithms attempt to compose music in a particular style. The resulting \r\nmusic is often impressive and indistinguishable from the style of the training data, but it tends to \r\nlack  significant  innovation.  In  an  effort  to  increase  innovation  in  the  selection  of  pitches  and \r\nrhythms,  we  present  a  system  that  discovers  musical  motifs  by  coupling  machine  learning \r\ntechniques  with  an  inspirational  component.  Unlike many  generative models,  the  inspirational \r\ncomponent  allows  the  composition  process  to  originate  outside  of  what  is  learned  from  the \r\ntraining data. Candidate motifs are extracted from non-musical media such as images and audio. \r\nMachine  learning  algorithms  select  and  return  the motifs  that most  resemble  the  training data. \r\nThis  process  is  validated  by  running  it  on  actual  music  scores  and  testing  how  closely  the discovered  motifs  match  the  expected  motifs.  We  examine  the  information  content  of  the \r\ndiscovered  motifs  by  comparing  the  entropy  of  the  discovered  motifs,  candidate  motifs,  and \r\ntraining data. We measure  innovation by comparing  the probability of  the  training data and  the \r\nprobability of the discovered motifs given the model. ", "recorded": "2014-06-11T11:10:00", "title": "Musical Motif Discovery in Non-musical Media"}, {"url": "roks2013_jaggi_connections", "desc": "We investigate the relation of two fundamental tools in machine learning and\r\nsignal processing, that is the support vector machine (SVM) for classi\ufb01cation, and the Lasso\r\ntechnique used in regression. We show [7] that the resulting optimization problems are equivalent, in the following sense: Given any instance of one of the two problems, we construct an\r\ninstance of the other, having the same optimal solution.\r\n\r\nIn consequence, many existing optimization algorithms for both SVMs and Lasso can also\r\nbe applied to the respective other problem instances. Also, the equivalence allows for many\r\nknown theoretical insights for SVM and Lasso to be translated between the two settings. One\r\nsuch implication gives a simple kernelized version of the Lasso, analogous to the kernels used\r\nin the SVM setting. Another consequence is that the sparsity of a Lasso solution is equal\r\nto the number of support vectors for the corresponding SVM instance, and that one can use\r\nscreening rules to prune the set of support vectors. Furthermore, we can relate sublinear time\r\nalgorithms for the two problems, and give a new such algorithm variant for the Lasso.", "recorded": "2013-07-08T14:50:00", "title": "Connections between the Lasso and Support Vector Machines"}, {"url": "mlg08_helsinki", "desc": "Driven by application areas ranging from biology to the World Wide Web, research in Data Mining and Machine Learning is nowadays increasingly focusing on the analysis of structured data. Of particular interest is data that consists of interrelated parts or is characterized by collections of objects that are interrelated and linked together into complex graphs and structures. Following in the footsteps of the highly successful MLG workshops in the past, MLG 2008 again will be the premier forum for bringing together different sub-disciplines within Machine Learning and Data Mining that focus on the analysis of structured data. The workshop is actively seeking contributions dealing with all forms of structured data, including but not limited to graphs, trees, sequences, relations and networks.\r\n\r\nContributions are invited from all relevant disciplines, such as for example\r\n * Statistical Relational Learning\r\n * Inductive Logic Programming\r\n * Kernel Methods for Structured Data\r\n * Probabilistic Models for Structured Data\r\n * Graph Mining\r\n * (Multi-)relational Data Mining\r\n * Methods for Structured Outputs\r\n * Network Analysis", "recorded": "2008-07-04T09:00:00", "title": "6th International Workshop on Mining and Learning with Graphs (MLG), Helsinki 2008"}, {"url": "w3cworkshop2011_dohmen_smt", "desc": "Statistical Machine Translation systems are a welcome development for news analytics. They enable topic-specific translation services, but are not without problems. The SMT system that is developed for the Let'sMT (FP7) project is trained and used to translate financial news for SemLab's news sentiment analysis platform. This talk will give an example of the benefits and problems of integrating such systems.", "recorded": "2011-10-21T14:45:00", "title": "The use of SMT in financial news sentiment analysis"}, {"url": "aistats2011_fortlauderdale", "desc": "AISTATS is an interdisciplinary gathering of researchers at the intersection of computer science, artificial intelligence, machine learning, statistics, and related areas. Since its inception in 1985, the primary goal of AISTATS has been to broaden research in these fields by promoting the exchange of ideas among them. \r\n\r\nMore about the conference at [[http://www.aistats.org/|AISTATS 2011]].\r\n", "recorded": "2011-04-11T09:00:00", "title": "14th International Conference on Artificial Intelligence and Statistics (AISTATS), Ft. Lauderdale 2011"}, {"url": "mlss05us_steinwart_salrs", "desc": "We present some learning rates for support vector machine classification. In particular we discuss a recently proposed geometric noise assumption which allows to bound the approximation error for Gaussian RKHSs. Furthermore we show how a noise assumption proposed by Tsybakov can be used to obtain learning rates between 1/sqrt(n) and 1/n. Finally, we describe the influence of the approximation error on the overall learning rate.", "recorded": "2005-05-16T00:00:00", "title": "Some Aspects of Learning Rates for SVMs"}, {"url": "icml08_hsu_ctja", "desc": "The triple jump extrapolation method is an effective approximation of Aitken\u2019s acceleration for accelerating the convergence of many machine learning algorithms that can be formulated as fixedpoint iteration. In the remainder of this abstract, we briefly review the general idea of the triple jump method and then describe how to apply it to accelerate stochastic gradient descent (SGD) for training linear support vector machines (SVM).", "recorded": "2008-07-09T14:00:00", "title": "CTJLSVM: Componentwise Triple Jump Acceleration for Training Linear SVM"}, {"url": "w3cworkshop2011_grunwald_translation", "desc": "GTS has developed a plugin for websites developed using the open-source Wordpress CMS. It is the only solution that supports post-editing MT and allows content publishers to create their own translation community. This talk will present our system and describe some of the challenges in translation of dynamic web content and the potential rewards that our concept holds.", "recorded": "2011-04-04T16:30:00", "title": "Website translation using post-edited machine translation and crowdsourcing"}, {"url": "icml2010_sonnenburg_shog", "desc": "The SHOGUN machine learning toolbox's focus is on large scale kernel methods and especially on Support Vector Machines (SVM). It comes with a generic interface for kernel machines and features 15 different SVM implementations that all access features in a unified way via a general kernel framework or in case of linear SVMs so called \"DotFeatures\", i.e., features providing a minimalistic set of operations (like the dot product).", "recorded": "2010-06-25T11:30:49", "title": "Shogun"}, {"url": "pcw06_penas_upr2c", "desc": "This paper reports the description of the developed system and the results obtained in the participation of the UNED in the Second Recognizing Textual Entailment (RTE) Challenge. New techniques and tools have been added: enriched queries to WordNet, detection of numeric expresions and their entailment, and Support Vector Machine classification (SVM) are the more relevant. The accuracy performed is slightly higher than the one from the previous edition system.", "recorded": "2006-04-10T00:00:00", "title": "UNED at PASCAL RTE-2 Challenge"}, {"url": "aibootcamp2011_sanchez_mnlp", "desc": "Natural Language Processing combines concepts from different research fields, like Formal Languages, Speech Recognition, Computational Linguistics and Machine Learning. This talk describes syntactic approaches to deal with Natural Language Processing problems. Hidden Markov Models and Probabilistic Context Free Grammars are usual concepts in NLP. The probabilistic estimation of these models is also described in this talk.", "recorded": "2011-02-21T12:00:00", "title": "Syntactic Approaches for Natural Language Processing"}, {"url": "nipsworkshops09_whistler", "desc": "The Post-Conference Workshop Program covered a wide range of topics from Neuroscience to Machine Learning. For more workshop schedule information please consult the Workshop URL on the workshop's page.\r\n\r\nDetailed information can be found at [[http://nips.cc/Conferences/2009/Program/schedule.php?Session=Workshops|NIPS 2009 Workshops Homepage]].\r\n\r\n----\r\n\r\n**Click on the picture for the videos from 2009 NIPS Conference in Vancouver**.\r\n||[[:nips09_vancouver]]||\r\n----", "recorded": "2009-12-11T07:30:00", "title": "NIPS Workshops, Whistler 2009"}, {"url": "kdd2014_srikant_aggarwal_programming_skills", "desc": "The automatic evaluation of computer programs is a nascent area of research with a potential for large-scale impact. Extant program assessment systems score mostly based on the number of test-cases passed, providing no insight into the competency of the programmer. In this paper, we present a system to grade computer programs automatically. In addition to grading a program on its programming practices and complexity, the key kernel of the system is a machine-learning based algorithm which determines closeness of the logic of the given program to a correct program. This algorithm uses a set of highly-informative features, derived from the abstract representations of a given program, that capture the program's functionality. These features are then used to learn a model to grade the programs, which are built against evaluations done by experts. We show that the regression models provide much better grading than the ubiquitous test-case-pass based grading and rivals the grading accuracy of other open-response problems such as essay grading . We also show that our novel features add significant value over and above basic keyword/expression count features. In addition to this, we propose a novel way of posing computer-program grading as a one-class modeling problem and report encouraging preliminary results. We show the value of the system through a case study in a real-world industrial deployment. To the best of the authors' knowledge, this is the first time a system using machine learning has been developed and used for grading programs. The work is timely with regard to the recent boom in Massively Online Open Courseware (MOOCs), which promises to produce a significant amount of hand-graded digitized data.", "recorded": "2014-08-26T16:00:00", "title": "Grading Computer Programming Skills using Machine Learning"}, {"url": "nipsworkshops2012_yen_procedure", "desc": "Concave-Convex Procedure (CCCP) has been widely used to solve nonconvex d.c.(difference of convex function) programs occur in learning problems, such as sparse support vector machine (SVM), transductive SVM, sparse principal com- ponenent analysis (PCA), etc. Although the global convergence behavior of CC- CP has been well studied, the convergence rate of CCCP is still an open problem. Most of d.c. programs in machine learning involve constraints or nonsmooth objective function, which prohibits the convergence analysis via differentiable map. In this paper, we approach this problem in a different manner by connecting CC- CP with more general block coordinate decent method. We show that the recent convergence result of coordinate gradient descent on nonconvex, nonsmooth problem can also apply to exact alternating minimization. This implies the convergence rate of CCCP is at least linear, if in d.c. program the nonsmooth part is piecewise-linear and the smooth part is strictly convex quadratic. Many d.c. programs in SVM literature fall in this case.", "recorded": "2012-12-08T09:30:00", "title": "On Convergence Rate of Concave-Convex Procedure"}, {"url": "icml2015_ghoshdastidar_tensor_spectral_method", "desc": "Matrix spectral methods play an important role in statistics and machine learning, and most often the word `matrix\u2019 is dropped as, by default, one assumes that similarities or affinities are measured between two points, thereby resulting in similarity matrices. However, recent challenges in computer vision and text mining have necessitated the use of multi-way affinities in the learning methods, and this has led to a considerable interest in hypergraph partitioning methods in machine learning community. A plethora of \u201chigher-order\u201d algorithms have been proposed in the past decade, but their theoretical guarantees are not well-studied. In this paper, we develop a unified approach for partitioning uniform hypergraphs by means of a tensor trace optimization problem involving the affinity tensor, and a number of existing higher-order methods turn out to be special cases of the proposed formulation. We further propose an algorithm to solve the proposed trace optimization problem, and prove that it is consistent under a planted hypergraph model. We also provide experimental results to validate our theoretical findings.", "recorded": "2015-07-09T14:46:53", "title": "A Provable Generalized Tensor Spectral Method for Uniform Hypergraph Partitioning"}, {"url": "bootcamp07_guyon_fcon", "desc": " This course covers feature selection fundamentals and applications. The students will first be reminded of the basics of machine learning algorithms and the problem of overfitting avoidance. In the wrapper setting, feature selection will be introduced as a special case of the model selection problem. Methods to derive principled feature selection algorithms will be reviewed as well as heuristic method, which work well in practice. One class will be devoted to feature construction techniques. Finally, a lecture will be devoted to the connections between feature section and causal discovery. The class will be accompanied by several lab sessions. The course will be attractive to students who like playing with data and want to learn practical data analysis techniques. The instructor has ten years of experience with consulting for startup companies in the US in pattern recognition and machine learning. Datasets from a variety of application domains will be made available: handwriting recognition, medical diagnosis, drug discovery, text classification, ecology, marketing.\r \r \r Play with the Gisette dataset of the feature selection challenge. See how with simple feature extraction methods, performances can be improved over the pure \u201cagnostic\u201d approach.", "recorded": "2007-07-05T09:00:00", "title": "Feature construction"}, {"url": "eml07_vishwanathan_nqm", "desc": "The BFGS quasi-Newton method and its limited-memory variant LBFGS revolutionized nonlinear optimization, and dominate it to this day. Their application to large-scale machine learning, however, has been hindered by the fact that they assume a smooth, strictly convex, and deterministic objective function in a finite-dimensional vector space. Here we relax these assumptions one by one, and present (L)BFGS variants newly developed in our group that perform well on non-convex smooth, quasi-convex non-smooth, and non-deterministic objectives. Paradigmatic applications include parameter estimation in MLPs (non-convex smooth) and SVMs (convex non-smooth), and stochastic approximation of gradients (non-deterministic) for efficient online learning on large data sets. \\\\ \r We are also able to lift LBFGS to an RKHS for online SVM training. In all these cases our BFGS variants outperform previous methods on a wide variety of models and data sets, from toy problems to large-scale data-mining tasks.", "recorded": "2007-12-08T15:30:00", "title": "New Quasi-Newton Methods for Efficient Large-Scale Machine Learning"}, {"url": "colt2013_princeton", "desc": "The conference is a single track meeting that includes invited talks as well as oral presentations of all refereed papers.\r\nWe invited submissions of papers addressing theoretical aspects of machine learning and related topics. We strongly support a broad definition of learning theory, including, but not limited to:\r\n\r\n# Design and analysis of learning algorithms and their generalization ability\r\n# Computational complexity of learning\r\n# Optimization procedures for learning\r\n# Unsupervised, semi-supervised learning and clustering\r\n# Online learning\r\n# Active learning\r\n# High dimensional and non-parametric empirical inference, including sparsity methods\r\n# Planning and control, including reinforcement learning\r\n# Learning with additional constraints: E.g. privacy, time or memory budget, communication\r\n# Learning in other settings: E.g. social, economic, and game-theoretic\r\n# Analysis of learning in related fields: natural language processing, neuroscience, bioinformatics, privacy and security, machine vision, data mining, information retrieval.\r\n\r\nFor more information visit the [[http://orfe.princeton.edu/conferences/colt2013/|COLT 2013 website]].", "recorded": "2013-06-12T08:30:00", "title": "26th Annual Conference on Learning Theory (COLT), Princeton 2013"}, {"url": "nipsworkshops2010_machine_learning", "desc": "By inviting numerical mathematics researchers with interest in\r\nboth numerical methodology and real problems in applications\r\nclose to machine learning, we probe realistic routes out of the\r\nprototyping sandbox. Our aim is to strengthen dialog between\r\nNM, signal processing, and ML. Speakers are briefed to provide\r\nspecific high-level examples of interest to ML and to point out\r\naccessible software. We initiate discussions about how to\r\nbest bridge gaps between ML requirements and NM interfaces\r\nand terminology.\r\n\r\nThe workshop reinforces the community\u2019s awakening attention\r\ntowards critical issues of numerical scalability and robustness\r\nin algorithm design and implementation. Further progress on\r\nmost real-world ML problems is conditional on good numerical\r\npractices, understanding basic robustness and reliability issues,\r\nand a wider, more informed integration of good numerical\r\nsoftware. As most real-world applications come with reliability\r\nand scalability requirements that are by and large ignored by\r\nmost current ML methodology, the impact of pointing out tractable\r\nways for improvement is substantial.\r\n\r\nWorkshop homepage: http://numml.kyb.tuebingen.mpg.de/", "recorded": "2010-12-10T07:30:00", "title": "Numerical Mathematics Challenges in Machine Learning"}, {"url": "cidu2011_mallya_classification", "desc": "Current methods of drought assessment utilize drought indices, such as the standardized precipitation index and Palmer drought severity index, that rely on subjective thresholds and hence cannot be universally applied across different climatic regions. In addition, most of the existing drought indices are not amenable to probabilistic treatment which is essential for quantifying model uncertainties in drought classification. This study applies a machine learning tool, the hidden Markov model (HMM), for probabilistic drought classification. The HMM-based drought index (HMM-DI) developed in this study, does not require specification of subjective\r\nthresholds and model parameters are determined from historical data during parameter estimation. The drought classifications obtained using HMM-DI are compared with SPI results. The HMM-DI reveals new insights into the frequency and severity of droughts and their spatio-temporal variations. The effectiveness of HMM-DI is assessed by its application to monthly precipitation data over India. The results suggest that HMM-DI can be a promising alternative to conventional drought indices.", "recorded": "2011-10-21T16:25:00", "title": "A Machine Learning Approach for Probabilistic Drought Classification"}, {"url": "lsoldm2013_portugaly_learning_systems", "desc": "In complex real world systems, machine learning is used to influence actions, rather than just provide predictions. Those actions in turn influence the environment of the system. The goal of machine learning in these systems is therefore causal rather than correlational. e.g. what would be the survival chance of patient A if we gave them drug B (causal question); what is the survival chance of the patient A knowing that they were given drug B (correlational question). By injecting noise into actions taken by the system, we can collect data that allows us to infer causality, and predict the consequences of changes to the system. Such predictions allow both humans and algorithms to select changes that improve both the short-term and long-term performance of such systems. This work provides a framework (which can be viewed as a generalization of the A/B testing framework) for counterfactual causal inference in complex systems. Parts of this framework were implemented in Microsoft\u2019s bing search advertising system, and I will show data from this implementation. This is work by L\u00e9on Bottou, which I had the good fortune of participating in. ", "recorded": "2013-09-25T12:30:18", "title": "Causal Reasoning and Learning Systems"}, {"url": "ecmlpkdd2011_tsamardinos_discovery", "desc": "The tutorial presents an introduction to basic assumptions and techniques for causal discovery from observational data with the use of graphs that represent conditional independence models. It first presents the basic theory of causal discovery such as the Causal Markov Condition, the Faithfulness Condition, and the d-separation criterion, graphical models for representing causality such as Causal Bayesian Networks, Maximal Ancestral Graphs and Partial Ancestral Graphs. It presents prototypical and state-of-the-art algorithms such as the PC, FCI and HITON for learning such models (global learning) or parts of such models (local learning) from data. The tutorial also discusses the connections of causality to feature selection and present causal-based feature selection techniques. Finally, case-studies of applications of causal discovery algorithms are presented, with a focus on applications to biomedical data.\r\n\r\nThe tutorial is designed for a wide audience with a general Machine Learning, Data Mining, and Statistical background..\r\n\r\nThe tutorial aims to:\r\n\r\n    * Familiarize the audience with the field and increase comprehension of the problem of causal induction as it pertains to everyday data analysis tasks; familiarize the audience with formalisms that represent causal relations among variables and provide a language for thinking about causality and causal discovery\r\n    * Increase understanding of the basic principles of causal induction and familiarity with prototypical and state-of-the-art algorithms in the field; enable the correct interpretation of the output of such algorithms\r\n    * Enable the correct application of causal-discovery algorithms in practical data mining, machine learning, or statistical analysis tasks\r\n\r\nMore specifically, it aims to clarify the following issues  that are important to every researcher and practitioner of data analysis:\r\n\r\n    * While most machine learning techniques assume identically and independently distributed data (i.i.d. data) quite often in many fields the data do not follow this assumption. The data may be experimental (e.g., after knocking out a gene) or under selection bias, e.g., in case-control studies. The tutorial helps understanding the differences and how they arise due to the causal structure of the domain\r\n    * It is often the case that the purpose of the analysis is to identify important variables (a.k.a feature selection), called biomarkers in biology, risk factors in medicine, etc. The tutorial helps understanding the connection between the selected variables and the causal structure.\r\n    * It is often the case that prediction models are not the final goal, but instead the goal is to control a system, e.g., treat a patient, design a drug with desired properties, etc. Causal modeling and induction is necessary to build machine learning models that can predict the outcome in a system that is being manipulated (e.g., under different experimental conditions).\r\n    * The tutorial provides a deeper understanding in standard (non-causal) Bayesian Networks that have been proven important in Machine Learning, reasoning with Uncertainty in Artificial Intelligence, and Decision Support Systems for over two decades.\r\n    * Causal discovery has already led to important discoveries, thus knowledge of these methods and their potential is important for the data analysts of the future.\r\n\r\nThe tutorial outline is shown below:\r\n\r\n   1. Representing Causality\r\n   2. Inducing Causal Models from Data\r\n   3. Case Studies and Practical Issues", "recorded": "2011-09-05T09:00:00", "title": "Introduction to causal discovery: A Bayesian Networks approach"}, {"url": "nips2011_chazelle_naturalalgorithms", "desc": "I will discuss the merits of an algorithmic approach to the analysis of\r\ncomplex self-organizing systems. I will argue that computer science,\r\nand algorithms in particular, offer a fruitful perspective on\r\nthe complex dynamics of multiagent systems: for example,\r\nopinion dynamics, bird flocking, and firefly synchronization.\r\nI will give many examples and try to touch on some of the theory\r\nbehind them, with an emphasis on their algorithmic nature\r\nand the particular challenges to machine learning that\r\nan algorithmic approach to dynamical systems raises.", "recorded": "2011-12-14T16:00:00", "title": "Natural Algorithms"}, {"url": "rldm2015_littman_computational_reinforcement", "desc": "In machine learning, the problem of reinforcement learning is concerned with using experience gained through interacting with the world and evaluative feedback to improve a system\u2019s ability to make behavioral decisions. This tutorial will introduce the fundamental concepts and vocabulary that underlie this field of study. It will also review recent advances in the theory and practice of reinforcement learning, including developments in fundamental technical areas such as generalization, planning, exploration and empirical methodology.", "recorded": "2015-06-07T09:15:00", "title": "Basics of Computational Reinforcement Learning"}, {"url": "lsoldm2014_symington_thijssen_path_integral", "desc": "The key objective of the CompLACS project\r\nis to solve real-world problems by decomposing them into\r\nsmaller sub-problems, each of which can be addressed by\r\na body of theory from the machine learning domain. This\r\npresentation illustrates this goal by showing how path\r\nintegral control can be used to coordinate quadrotors\r\nto achieve a complex task. It discusses the problem\r\ndecomposition, simulation and hardware architecture and\r\npresents experimental results.", "recorded": "2014-09-10T16:30:00", "title": "Multi-agent quadrotor Path Integral Control"}, {"url": "cernacademictraining09_evans_rossi_nessi_stc", "desc": "After a general introduction to the motivations for a LHC upgrade, the lectures will discuss the beam dynamics and technological challenges of the increase of the LHC luminosity, and the possible scenarios. Items such as a stronger final focus with larger aperture magnets, crab cavities, electron cloud issues, beam-beam interaction, machine protection and collimation will be discussed.\r\n", "recorded": "2009-06-08T10:09:00", "title": "Scenarios and Technological Challenges for a LHC Luminosity Upgrade: Introduction to the LHC Upgrade Program and Summary of Physics Motivations"}, {"url": "nipsworkshops2013_bareinboim_limited_experiments", "desc": "Most scientific explorations are concerned with generalizing empirical findings to new environments, settings, or populations, a problem that the machine learning literature labeled \"transfer learning.\" Our talk focuses on a particular type of generalizability, called \u201ctransportability\u201d, defined as a license to transfer causal effects learned in experimental studies to a new population, in which only observational studies can be conducted. We introduce a formal representation called \u201cselection diagrams\u201d.", "recorded": "2013-12-09T08:30:00", "title": "Causal Transportability with Limited Experiments"}, {"url": "mlss05au_patrick_tc", "desc": "This course will cover the principal topics important to creating a working text categorization system. It will focus on the components of such a system and processes required to create it based on the practical experiences of the Scamseek project. The role of machine learning will be the center of the discussion but the surrounding tasks of language modeling, computational linguistics and software engineering will all be discussed to varying degrees. Discussion of some aspects of the Scamseek project are restricted under secrecy agreements with ASIC.", "recorded": "2005-01-28T00:00:00", "title": "Text Categorization"}, {"url": "bootcamp2010_murray_iml", "desc": "How can we represent data on a computer and use it to learn to perform\r\nuseful tasks? This lecture reviews some simple classification and\r\nregression rules, discusses under- and over-fitting and emphasises the\r\nutility of defining objective functions for learning. There is also a\r\nshort overview of Bayesian learning, and some practical tips for\r\npre-processing and visualizing data. The lecture ends with a brief\r\nmention of unsupervised learning and related topics.", "recorded": "2010-07-05T14:03:00", "title": "Introduction to Machine Learning"}, {"url": "mlas06_mitchell_itm", "desc": "**Tom Mitchell is the first Chair of Department of the first Machine Learning Department in the World**, based at Carnegie Mellon. The Videolectures.Net team spoke to him in Pittsburgh at CMU where we discussed about how he started the department, what was the response of the broader community and its past, present and future. //\"The university said you can only have a department if you have a discipline that is going to be here in one hundred years otherwise you can not have a department.\"//", "recorded": "2006-09-26T00:00:00", "title": "Interview with Tom Mitchell"}, {"url": "single_fortuna_ontology", "desc": "This thesis addresses the task of formalizing and implementing the process of semi-automatic ontology construction. We propose a theoretical framework for formalizing the ontology construction process. The process is described as a sequence of operators applied to the ontology. Several types of common operators are identified and each type is abstracted so it can be discovered by a combination of machine learning algorithms and user interactions. The proposed ontology learning framework is generic and can handle various domains. The requirement is, that domain data can be provided in a format supported by the learning algorithms.\\\\\r\nOperators defined as part of the ontology construction process are implemented using several machine learning algorithms. Clustering, active learning and large-scale classifications are used to learn operators for adding concepts and relations. A novel visualization approach for visualizing instances, concepts and ontologies is developed, using a combination of dimensionality reduction techniques. The ability to incorporate additional background data is implemented using a novel feature weighting schema, and the addition of new instances to the ontology is translated to a standard classification task.\\\\\r\nWe also developed a system, which implements the framework, together with the proposed machine learning algorithms. The system takes domain data on the input, and guides the user through the process of constructing the ontology for the given domain. The developed system was applied in several use-cases, where domain data was provided as a text corpus or a social network, to showcase the capabilities.\\\\\r\nThe system was also evaluated in two user studies, to evaluate the user interface and to compare developed ontologies against manually constructed ones. The results of the users studies show, that the system is user friendly enough to be used by domain experts. The users can construct ontologies that are comparable to manually constructed ontology and can do so in a shorter amount of time.", "recorded": "2011-10-12T13:00:00", "title": "Semi-Automatic Ontology Construction"}, {"url": "mbc07_whistler", "desc": "Music is one of the most widespread of human cultural activities, existing in some form in all cultures throughout the world. The definition of music as organised sound is widely accepted today but a na\u00efve interpretation of this definition may suggest the notion that music exists widely in the animal kingdom, from the rasping of crickets' legs to the songs of the nightingale. However, only in the case of humans does music appear to be surplus to any obvious biological purpose, while at the same time being a strongly learned phenomenon and involving significant higher order cognitive processing rather than eliciting simple hardwired responses.\n\nA two day workshop takes place at NIPS 07 (Whistler, Canada) and spans topics from signal processing and musical structure to the cognition of music and sound. In the first day the workshop provides a forum for cutting edge research addressing the fundamental challenges of modeling the structure of music and analysing its effect on the brain. It also provides a venue for interaction between the machine learning and the neuroscience/brain imaging communities to discuss the broader questions related to modeling the dynamics of brain activity. During the second day the workshop focuses on the modeling of sound, music perception and cognition. These have provide, with the crucial role of machine learning, a break through in various areas of music technology, in particular: Music Information Retrieval (MIR), expressive music synthesis, interactive music making, and sound design.\n\nUnderstanding of music cognition in its implied top-down processes can help to decide which of the many descriptors in MIR are crucial for the musical experience and which are irrelevant. The target group is of researchers within the fields of (Music) Cognition, Music Technology, Machine Learning, Psychology, Sound Design, Signal Processing and Brain Imaging.", "recorded": "2007-12-07T07:30:00", "title": "NIPS Workshop on Music, Brain and Cognition, Whistler 2007 "}, {"url": "nipsworkshops2013_tenenbaum_learning", "desc": "People can learn a new visual concept almost perfectly from just a single example, yet machine learning algorithms typically require hundreds of examples to perform similarly.  Humans can also use their learned concepts in richer ways than conventional machine learning systems, to parse objects into parts, generate new instances,recognize abstract types of concepts and even imagine novel concepts of a given type. I will discuss two computational approaches we have explored that aim to capture these human learning abilities.  The first (with Ruslan Salakhutdinov and Antonio Torralba) is the hierarchical-deep or \"HD\" architecture, which learns a hierarchical nonparametric Bayesian model (specifically, a hierarchical Dirichlet process topic model) on top of a deep Boltzmann machine feature extractor.  This approach is appealingly general, but I will argue it is insufficiently structured to capture the conceptual knowledge humans learn in real-world domains.  I will then present a new architecture (with Brendan Lake and Ruslan Salakhutdinov) that represents concepts as simple programs, embodying basic principles of hierarchy, compositionality and causality, and constructs programs that best explain observed examples under a Bayesian criterion.  This approach requires more domain-specific engineering in choosing the form of the programs to be learned, but it is still very flexible.  On a challenging one-shot classification task, the Bayesian program learner is the first to achieve human-level performance, substantially outperforming a range of other approaches including both deep Boltzmann machines and our HD models.  I will also illustrate several \"visual Turing test\" experiments probing the program learning model's more creative parsing, generalization and generation abilities, and show that in many cases it is indistinguishable from the performance of humans.", "recorded": "2013-12-09T09:00:00", "title": "Two architectures for one-shot learning"}, {"url": "promo_movement_man_machine", "desc": "The group Analysis and synthesis v movement in man and machine has long-standing excellence in the field of man and machine movement analysis and artificial and natural motor control. The group has wide experience in robotics research, robotic applications, rehabilitation and clinical work.\r\n\r\nIn past we worked hand by hand with companies like TRIMO on construction assembly robot, for this was group praised with The 2010 EUROP/EURON Robotics Technology Transfer Award, 3rd prize. Our knowledge is further exploited by Eta Cerkno, Yaskawa, ABB and others. We have traditionally strong collaboration with number of clinics at UKC in particular with the University Rehabilitation Institute. \r\n\r\nThe vision of the research team is based on the international openness, co-operation on equal basis with eminent EU groups (ETH, TUM) and worldwide. The group has been a partner in the EU funded STREP projects SENSATIONS, GENTLE/S, I-Match, Alladin, MIMICS and Evryon.\r\n\r\nMain original scientific achievements are: evaluation of the movements of the upper extremity, novel robotic devices for training of the upper extremities, advanced interactive control of complex robot devices, training of standing, standing-up, and walking, psychophysiological feedback loop and haptics in MRI environment. \r\n\r\nThe excellence of the group can be judged also by the number of publications in peer reviewed journals (57 in the last 5 years), the number of Ph. D. (9 in last 5 years, currently there are 9 Ph.D. students), to mention, our group financing is 1,8 FTE. Currently we serve as guest editors in two IF Journals and as members of several boards. Our track is showing notable number of book chapters in international books, robotic textbooks in Slovenia and internationally. During last five years were filled five patent applications. \r\n\r\nLectures are given for several courses in robotics, microprocessors and biomedical engineering. It is worth to notice traditional student industrial workshop Days of Robotics.\r\n\r\nOur doctors can be met at Berkeley, University of Illinois and in companies as Instrumentation Technologies, Avtoelektrika, Avtenta, Silica, Lastinski in\u017e., University of Split.\r\n\r\nMembers of group received national Zois research award as well as Zois award for life achievements, prestigous Swiss Technology Award, are IEEE fellows, AIMBE fellows, members of SAZU and IAS. The program \u201cAnalysis and synthesis of movement in man and machine\u201d was according to the evaluation of ARRS in past placed among five best in the area of engineering in Slovenia and recently classified among those with six year financing.", "recorded": "2011-09-08T14:04:46", "title": "Analysis and synthesis v movement in man and machine"}, {"url": "is2012_mladenic_grobelnik_text_data", "desc": "Text is one of the traditional ways of communication between people. With the growing availability of text data in electronic form, handling and analysis of text by means of computers gained popularity. Handling text data with machine learning methods brought interesting challenges to the area that got further extended by incorporation of some natural language specifics. As the methods were capable of addressing more complex problems related to text data, the expectations got bigger calling for a combination of methods from different research areas including information retrieval,\r\nmachine learning, statistical data analysis, data mining, natural language processing, semantic technologies. Nowadays automatic text analysis is an integral part of many systems, pushing boundaries of research capabilities towards artificial intelligence dream on never ending learning from\r\ntext aiming at mimicking ways of human learning. The paper presents development of text analysis research in Slovenian that we have been personally involved in, pointing out interesting research problems that have been and are still addressed by the research, example tasks that have been addressed and some challenges on the way.", "recorded": "2012-10-11T12:20:15", "title": "Artificial Intelligence Handling Text Data"}, {"url": "eswc2011_allocca_automatic", "desc": "When different versions of an ontology are published online, the links between them are often lost as the standard mechanisms (such as owl:versionInfo and owl:priorVersion) to expose these links are rarely used. This generates issues in scenarios where people or applications are required to make use of large scale, heterogenous ontology collections, implicitly containing multiple versions of ontologies. In this paper, we propose a method to detect automatically versioning links between ontologies which are available online through a Semantic Web search engine. Our approach is based on two main steps. The first step selects candidate pairs of ontologies by using versioning information expressed in their identifiers. In the second step, these candidate pairs are characterized through a set of features, including similarity measures, and classified by using Machine Learning Techniques, to distinguish the pairs that represent versions from the ones that do not. We discuss the features used, the methodology employed to train the classifiers and the precision obtained when applying this approach on the collection of ontologies of the Watson Semantic Web search engine.", "recorded": "2011-05-31T17:00:00", "title": "Automatic Identi\ufb01cation of Ontology Versions Using Machine Learning Techniques"}, {"url": "ripd07_whistler", "desc": "When dealing with distributions it is in general infeasible to estimate them explicitly in high dimensional settings, since the associated learning rates can be arbitrarily slow. On the other hand, a great variety of applications in machine learning and computer science require distribution estimation and/or comparison. Examples include testing for homogeneity (the \"two-sample problem\"), independence, and conditional independence, where the last two can be used to infer causality; data set squashing / data sketching / data anonymisation; domain adaptation (the transfer of knowledge learned on one domain to solving problems on another, related domain) and the related problem of covariate shift; message passing in graphical models (EP and related algorithms); compressed sensing; and links between divergence measures and loss functions.\r\n \r\n The purpose of this workshop is to bring together statisticians, machine learning researchers, and computer scientists working on representations of distributions for various inference and testing problems, to discuss the compromises necessary in obtaining useful results from finite data. In particular, what are the capabilities and weaknesses of different distribution estimates and comparison strategies, and what negative results apply?", "recorded": "2007-12-08T07:30:00", "title": "NIPS Workshop on Representations and Inference on Probability Distributions, Whistler 2007"}, {"url": "icml2015_le_histograms", "desc": "Many applications in machine learning handle bags of features or histograms rather than simple vectors. In that context, defining a proper geometry to compare histograms can be crucial for many machine learning algorithms. While one might be tempted to use a default metric such as the Euclidean metric, empirical evidence shows this may not be the best choice when dealing with observations that lie in the probability simplex. Additionally, it might be desirable to choose a metric adaptively based on data. We consider in this paper the problem of learning a Riemannian metric on the simplex given unlabeled histogram data. We follow the approach of Lebanon(2006), who proposed to estimate such a metric within a parametric family by maximizing the inverse volume of a given data set of points under that metric. The metrics we consider on the multinomial simplex are pull-back metrics of the Fisher information parameterized by operations within the simplex known as Aitchison(1982) transformations. We propose an algorithmic approach to maximize inverse volumes using sampling and contrastive divergences. We provide experimental evidence that the metric obtained under our proposal outperforms alternative approaches.", "recorded": "2015-07-08T14:46:53", "title": "Unsupervised Riemannian Metric Learning for Histograms Using Aitchison Transformations"}, {"url": "nipsworkshops09_computational_biology", "desc": "**Machine Learning in Computational Biology**\r\n\r\nThe field of computational biology has seen dramatic growth over the past few years, both in terms of new available data, new scientific questions, and new challenges for learning and inference. In particular, biological data are often relationally structured and highly diverse, well-suited to approaches that combine multiple weak evidence from heterogeneous sources. These data may include sequenced genomes of a variety of organisms, gene expression data from multiple technologies, protein expression data, protein sequence and 3D structural data, protein interactions, gene ontology and pathway databases, genetic variation data (such as SNPs), and an enormous amount of textual data in the biological and medical literature. New types of scientific and clinical problems require the development of novel supervised and unsupervised learning methods that can use these growing resources. Furthermore, next generation sequencing technologies are yielding terabyte scale data sets that require novel algorithmic solutions. The goal of this workshop is to present emerging problems and machine learning techniques in computational biology.\r\n----\r\nThe Workshop homepage can be found at http://www.mlcb.org/.\r\n----", "recorded": "2009-12-11T07:30:00", "title": "Computational Biology"}, {"url": "mit24262s04_singer_lec03", "desc": "* Creativity and the nature of the universe / religion. What was the nature of the creativity of the creation? \r\n* The phenomenon of the \"robber baron\" - people who amassed enormous fortunes and then bestowed the money upon foundations, relatives, etc. \t\r\n* The difference between intuition and instinct - Bergson and the theory of creativity. Creativity as a search for perfection? Metaphysics of Plato vs. imperfect love. \t\r\n* Having a \"good\" life as a creative act. Nietzsche's idea of the Superman (the artist). \t\r\n* Is learning a creative process? Can learning science / engineering be a creative process? \t\r\n* Minsky's opinions on the \"emotion machine\" - can people learn about their own emotions by thinking about machine emotions? \t\r\n* Margaret Boden - what is creativity? Inventiveness of two sorts - superficial (exploratory), and inventive. \t\r\n* What is existence in relation to the Catholic idea of \"ensoulment\"? What is it never to have existed? What happens when the light of consciousness goes out? \t\r\n* Anthony's student presentation on the week's reading - summary of the reading, interpretations. \t\r\n* Freud as a reformer in sexual politics. \t\r\n* Imagination as a personal perception of the boundlesness of creativity. \r\n* Humans are bounded by their imagination. How imagination changes the concept of the world.\r\n* Discussion of Proust.", "recorded": "2004-02-24T09:00:00", "title": "Session 3 - Creativity, the nature of the universe and religion"}, {"url": "kdd09_srihari_omnmlswome", "desc": "Merchants selling products on the Web often ask their customers to share their opinions and hands-on experiences on products they have purchased. Unfortunately, reading through all customer reviews is difficult, especially for popular items, the number of reviews can be up to hundreds or even thousands. This makes it difficult for a potential customer to read them to make an informed decision. The OpinionMiner system designed in this work aims to mine customer reviews of a product and extract high detailed product entities on which reviewers express their opinions. Opinion expressions are identified and opinion orientations for each recognized product entity are classified as positive or negative. Different from previous approaches that employed rule-based or statistical techniques, we propose a novel machine learning approach built under the framework of lexicalized HMMs. The approach naturally integrates multiple important linguistic features into automatic learning. In this paper, we describe the architecture and main components of the system. The evaluation of the proposed method is presented based on processing the online product reviews from Amazon and other publicly available datasets.\r\n", "recorded": "2009-07-01T11:35:00", "title": "OpinionMiner: A Novel Machine Learning System for Web Opinion Mining and Extraction"}, {"url": "ilpmlgsrl09_eisner_wdal", "desc": "The field of AI has become implementation-bound. We have plenty of ideas, but it is increasingly laborious to try them out, as our models become more ambitious and our datasets become larger, noisier, and more heterogeneous. The software engineering burden makes it hard to start new work; hard to reuse and combine existing ideas; and hard to educate our students. In this talk, I'll propose to hide many common implementation details behind a new level of abstraction that we are developing. Dyna is a declarative programming language that combines logic programming with functional programming. It also supports modularity. It may be regarded as a kind of deductive database, theorem prover, truth maintenance system, or equation solver. I will illustrate how Dyna makes it easy to specify the combinatorial structure of typical computations needed in natural language processing, machine learning, and elsewhere in AI. Then I will sketch implementation strategies and program transformations that can help to make these computations fast and memory-efficient. Finally, I will suggest that machine learning should be used to search for the right strategies for a program on a particular workload. ", "recorded": "2009-07-04T14:00:00", "title": "Weighted Deduction\u000bas an Abstraction Level for AI"}, {"url": "womenscience2012_bristol", "desc": "Even though there is considerable number of women in computer science\r\nresearch, the proportion compared to the number of men is still much lower\r\nthan one would wish for. \r\nWe have conducted interviews with women at different stage of their research\r\ncarrier in machine learning and data mining to promote their research and in\r\ngeneral women in science.\r\n\r\nThe interviews were partially supported by the **[[http://videolectures.net/pascal/|EU PASCAL2 Network of Excellence]]**.", "recorded": "2012-09-24T14:53:22", "title": "Interviews with Women in Science, Bristol 2012"}, {"url": "mlss04_bishop_gmvm", "desc": "In this course I will discuss how exponential families, a standard tool in statistics, can be used with great success in machine learning to unify many existing algorithms and to invent novel ones quite effortlessly. In particular, I will show how they can be used in feature space to recover Gaussian Process classification for multiclass discrimination, sequence annotation (via Conditional Random Fields), and how they can lead to Gaussian Process Regression with heteroscedastic noise assumptions.", "recorded": "2004-09-16T00:00:00", "title": "Graphical Models and Variational Methods"}, {"url": "mloss08_glasmachers_shark", "desc": "Shark is a C++ machine learning library. Tutorials and html documentation make Shark easy to learn.\r\nThe installation of Shark is straightforward, it does not depend on any third party software and compiles\r\nunder Linux, Solaris, MacOS, and Windows. Various example programs serve as starting points for own\r\nprojects. Shark provides methods for linear and nonlinear optimization, in particular evolutionary and\r\ngradient-based algorithms. It comes with di\ufb00erent types of arti\ufb01cial neural networks ranging from standard", "recorded": "2008-12-12T08:30:00", "title": "Shark"}, {"url": "nips09_zolyomi_perkins_pgc", "desc": "This presentation will discuss goals, methodology, and research for creating accessible user experiences in today's cutting-edge software technology.  Using a framework for mapping user requirements to technology solutions, we will discuss the complex market for accessible technology and the opportunity for innovation in technology solutions. Demos will be given on new accessibility solutions in Windows and on the Internet. The presentation will highlight the complexities of the accessible technology space, with the hope of inspiring research and application of machine learning.", "recorded": "2009-12-10T16:45:00", "title": "Perspective on the Goals and Complexities of Inclusive Design"}, {"url": "mlss09us_chicago", "desc": "The theme of this year's summer school is Theory and Practice of Computational Learning, to be held in conjunction with a research workshop on the same topic, during the period June 1-11, 2009 at International House, University of Chicago. The program will consist of a mixture of tutorial lectures and research talks. 3-4 hours of tutorial lectures will be held in the morning, and the reserarch talks in the afternoon.\r\n\r\n----\r\nThe Summer school homepage can be found at http://www.cse.ohio-state.edu/mlss09/\r\n----", "recorded": "2009-06-01T09:00:00", "title": "Machine Learning Summer School (MLSS), Chicago 2009"}, {"url": "mlmi04ch_willks_ac", "desc": "What will an artificial Companion be like? Who will need them and how much good or harm will they do? Will they change our lives and social habits in the radical way technologies have in the past: just think of trains, phones and television? Will they force changes in the law so that things that are not people will be liable for damages; up till now, it is the case that if a machine goes wrong, it is always the maker or the programmer, or their company, which is at fault.", "recorded": "2004-06-23T00:00:00", "title": "Artificial Companions"}, {"url": "icml05_getoor_srl2", "desc": "Statistical machine learning is in the midst of a \"relational revolution\". After many decades of focusing on independent and identically-distributed (iid) examples, many researchers are now studying problems in which the examples are linked together into complex networks. These networks can be a simple as sequences and 2-D meshes (such as those arising in part-of-speech tagging and remote sensing) or as complex as citation graphs, the world wide web, and relational data bases.", "recorded": "2005-08-10T16:00:00", "title": "Statistical Relational Learning - Part 2"}, {"url": "mlss04_smola_effs", "desc": "In this course I will discuss how exponential families, a standard tool in statistics, can be used with great success in machine learning to unify many existing algorithms and to invent novel ones quite effortlessly. In particular, I will show how they can be used in feature space to recover Gaussian Process classification for multiclass discrimination, sequence annotation (via Conditional Random Fields), and how they can lead to Gaussian Process Regression with heteroscedastic noise assumptions.", "recorded": "2004-09-20T00:00:00", "title": "Exponential Families in Feature Space"}, {"url": "cmulls08_ratliff_ssmmt", "desc": "Traditionally there has been a mismatch between the requirements of \r nontrivial applications and the prediction tools offered by machine \r learning. Applications such as natural language processing, optical \r character recognition, and path planning are often implemented in terms \r combinatorial inference algorithms, such as parsing algorithms, Viterbi \r decoding, and A* planning. These inference algorithms necessarily \r utilize the inherent structure of the problem to efficiently navigate an \r exponential number of target elements such as the set of all parse trees \r for a sentence, the set of possible words of a particular length, or the \r set of all paths between two points in a graph. On the other hand, \r research into supervised learning techniques in machine learning and \r statistics has focused primarily on regression and classification \r algorithms which at best handle only a handful of classes. These \r techniques cannot be applied directly to most applications. Typically, \r engineers are required to meticulously define learnable subproblems by \r inducing independence assumptions which are often strongly violated in \r practice.\r \r \r In recent years, however, the advent of conditional random fields, and \r then maximum margin structured classification, has changed the way the \r machine learning community views these problems. Researchers have found \r ways in which the inherent structure in the problems can be used to \r directly train these combinatorial inference procedures. Dubbed \r structured prediction, this class of algorithms utilizes the same \r implicit structural properties that make the inference algorithms efficient.\r \r \r In this presentation, after introducing structured prediction at a high \r level, I will cover in detail one of the two most cited formalisms of \r structured prediction: maximum margin structured classification. With a \r particular emphasis placed on functional gradient techniques, I will \r present a number of algorithms for solving these problems along with \r their results on various applications and a discussion of relative \r trade-offs.", "recorded": "2008-01-21T12:00:00", "title": "Structured Prediction: Maximum Margin Techniques"}, {"url": "fgconference2015_cbar", "desc": "Unconsciously, humans evaluate situations based on environment and social parameters when recognizing emotions in social interactions. Without context, even humans may misunderstand the observed facial, vocal or body behavior. Contextual information, such as the ongoing task, the identity and natural expressiveness of the individual, and the intra- and inter-personal context, help us interpret and respond to social interactions. These considerations suggest that attention to context information can deepen our understanding of affect communication for a reliable real-world affect related applications. \r\n     \r\nBuilding upon the success of previous CBAR workshops, the key aim of this third workshop is to explore how computer vision can address the challenging task of automatic extraction and recognition of context information in real-world applications. Specifically, we wish to exploit advances in computer vision and machine learning for real time scene analysis and understanding that include, tracking and recognition of human actions, gender recognition, age estimation, and objects recognition and tracking for real-time context based visual, vocal, or audiovisual affect recognition. \r\n    \r\nThe key aim of the workshop is to explore the challenges, benefits, and drawbacks of integrating context information on affect production, interpretation and recognition. We wish to investigate cutting-edge methods and methodologies in computer vision and machine learning that can be applied to (1) detect and interpret context information in social interaction and/or human-machine interaction, and (2) train and validate classifiers to create a fully automatic multimodal and context based affect recognition. \r\n    \r\nThe workshop is relevant to FG given the challenging area of research on context based affect recognition and its wide range of applications such as but not limited to intelligent video surveillance, human-computer interaction, intelligent humanoid robots, clinical diagnosis (e.g., pain and depression assessment). The workshop focuses on making affect recognition more robust and useful in real-world situations (e.g., work, home, school, and health care environment). We solicit high quality papers from a variety of fields such as computer vision and pattern recognition, behavioral and automatic methodologies that uses innovative and promising approaches to extract, interpret and/or include contextual information in audiovisual affect recognition and how it can improve existing frameworks for human-centered affect recognition. \r\n    \r\nFor its third year, the workshop aims at inviting scientists working in related areas of machine learning and computer vision, scene analysis, ambient computing, smarts environments to share their expertise and achievements in the emerging field of automatic and context based visual, audio, and multimodal affect analysis and recognition.", "recorded": "2015-06-04T09:10:37", "title": "Third International Workshop on Context Based Affect Recognition (CBAR) 2015"}, {"url": "tcmm2014_leuven", "desc": "Recent advances have seen the proliferation of GPUs and multicore computers as well as the development of new libraries that fully exploit the new hardware capabilities. On the algorithmic side, the increased demand for processing large-scale and structured datasets has stimulating the development of new solutions as well as leading to the revival of old scalable and distributed algorithms, including proximal, stochastic and generalized Frank-Wolfe methods.\r\n\r\nThe workshop provides a venue for researchers and practitioners to interact on the latest developments in technical computing in relation to machine learning and mathematical engineering problems and methods (including also optimization, system identification, computational statistics, signal processing, data visualization, deep learning, compressed sensing and big-data). A special attention is paid to implementations on high-level high-performance modern programming languages suitable for large-scale, parallel and distributed computing and capable to efficiently handle structured data. The emphasis is especially on the open-source alternatives, including but not limited to Julia, Python, Scala and R.\r\n\r\nFor more information visit the [[http://www.esat.kuleuven.be/stadius/tcmm2014/index.php|TCMM 2014 website]].", "recorded": "2014-09-08T09:00:00", "title": "International Workshop on Technical Computing for Machine Learning and Mathematical Engineering (TCMM), Leuven 2014"}, {"url": "fgconference2015_morency_human_communication", "desc": "Human face-to-face communication is a little like a dance, in that participants continuously adjust their behaviors based on verbal and nonverbal cues from the social context. Today\u2019s computers and interactive devices are still lacking many of these human-like abilities to hold fluid and natural interactions. Multimodal machine learning addresses this challenge of creating algorithms and computational models able to analyze, recognize and predict human subtle communicative behaviors in social context. I formalize this new research endeavor with a Human Communication Dynamics framework, addressing four key computational challenges: behavioral dynamic, multimodal dynamic, interpersonal dynamic and societal dynamic. Central to this research effort is the introduction of new probabilistic models able to learn the temporal and fine-grained latent dependencies across behaviors, modalities and interlocutors. In this talk, I will present some of our recent achievements modeling multiple aspects of human communication dynamics, motivated by applications in healthcare (depression, PTSD, suicide, autism), education (learning analytics), business (negotiation, interpersonal skills) and social multimedia (opinion mining, social influence).", "recorded": "2015-05-07T08:30:00", "title": "Multimodal Machine Learning: Modeling Human Communication Dynamics"}, {"url": "nips09_goldberg_ttp", "desc": "It is estimated that more that 2 million people in the United States have significant communication\r\nimpairments that result in them relying on methods other than natural speech alone for communication\r\n[2]. One type of commonly used augmentative and alternative communication (AAC) system is\r\npictorial communication software such as SymWriter [8], which uses a lookup table to transliterate\r\neach word (or common phrase) in a sentence into an icon. This is an example of converting information\r\nbetween modalities. However, the resulting sequence of icons can be difficult to understand.\r\nWe have been developing general-purpose Text-to-Picture (TTP) synthesis algorithms [10, 5] to\r\nimprove understandability using machine learning techniques. Our goal is to help users with special\r\nneeds, such as the elderly or those with disabilities, to rapidly browse documents through pictorial\r\nsummaries (e.g., Figure 5). Our TTP system targets general English. This differs from other pictorial\r\nconversion systems that require hand-crafted narrative descriptions of a scene [1, 9], 3D models [3],\r\nor special domains [6]. Instead, we use a concatenative or \u201ccollage\u201d approach. In this talk, we\r\ndiscuss how machine learning enables the key components of our TTP system.", "recorded": "2009-12-10T16:00:00", "title": "Toward Text-to-Picture Synthesis"}, {"url": "livingbitsandthings2013_bled", "desc": "Living bits and things is positioned as the first and still the only IoT/M2M event targeting Central and South East Europe with focus on \u00bbdelivering value and benefits for business and life\u00ab.\r\nAs a \u00bbpragmatic\u00ab event it is primarily recognised for its:\r\n \r\n*Open space sessions with dynamic networking and evening \u00bbbreak-the-ice\u00ab events\r\n*Simplified open approach \u2013 presentations only, no papers needed\r\n*Exciting business vs. engineering \u00bbone-on-one\u00ab discussions \u2013 two sides of the same coin  \r\n*Discussions on social, ethic, safety and privacy impacts of the IoT\r\n*Case studies and existing applications for business and communities\r\n \r\nAt this year's event we are going to chase the ideas, thoughts and answers to a simple and basic question: \u00bbWould you offer me one touch application, please?\u00ab. We will primarily focus to simplicity and usability. \r\n \r\nIt is important to understand and to take in account the specific problems of the countries in the Central and South East Europe region. \r\n\r\nTo find out more please visit the [[http://www.livingbitsandthings.com/lbt13/2013/1|Living Bits and Things 2013 website]].", "recorded": "2013-11-12T09:00:00", "title": "Living Bits And Things: Discover the potential of Internet of Things (IoT) and Machine-to-Machine (M2M) technologies and Smart Living in Central and South East Europe, Bled 2013"}, {"url": "mlss05au_schraudolph_gmml", "desc": "Gradient methods locally optimize an unknown differentiable function, and thus provide the engines that drive much machine learning. Here we'll take a look under the hood, beginning with brief overview of classical gradient methods for unconstrained optimization: * Steepest descent, * Newton's method * Levenberg-Marquardt * BFGS * Conjugate gradient. To cope with the flood of data we find ourselves in today, stochastic approximation of the gradient from subsamples of data becomes a necessity. Unfortunately the noise this introduces into the gradient is not tolerated well by the classical gradient methods, with the exception of steepest descent, which however is very slow to converge. We'll see how local step size adaptation can be used to accelerate the convergence of stochastic gradient descent, culminating in the recent stochastic meta-descent (SMD) algorithm. SMD requires certain Hessian-vector products which can be computed efficiently via algorithmic (or automatic) differentiation (AD), a set of techniques that help automate the correct implementation of gradient methods in general. We'll discuss the basic concepts of AD, and learn simple ways to implement the forward mode of AD, and with it the fast Hessian-vector product.", "recorded": "2005-01-24T00:00:00", "title": "Gradient Methods for Machine Learning"}, {"url": "kdd2014_horvitz_people_society", "desc": "Deep societal benefits will spring from advances in data availability and in computational procedures for mining insights and inferences from large data sets. I will describe efforts to harness data for making predictions and guiding decisions, touching on work in transportation, healthcare, online services, and interactive systems. I will start with efforts to learn and field predictive models that forecast flows of traffic in greater city regions. Moving from the ground to the air, I will discuss fusing data from aircraft to make inferences about atmospheric conditions and using these results to enhance air transport. I will then focus on experiences with building and fielding predictive models in clinical medicine. I will show how inferences about outcomes and interventions can provide insights and guide decision making. Moving beyond data captured by hospitals, I will discuss the promise of transforming anonymized behavioral data drawn from web services into large-scale sensor networks for public health, including efforts to identify adverse effects of medications and to understand illness in populations. I will conclude by describing how we can use machine learning to leverage the complementarity of human and machine intellect to solve challenging problems in science and society.", "recorded": "2014-08-26T09:00:00", "title": "Data, Predictions, and Decisions in Support of People and Society"}, {"url": "rraa09_seattle", "desc": "Function approximation from noisy data is a central task in robot learning. Relevant problems include sensor modeling, manipulation, control, and many others. A large number of function approximation methods have been proposed from statistics, machine learning, and control system theory to address robotics-related issues such as online updates, active sampling, high dimensionality, non-homogeneous noise, and missing features.\r\n\r\nIn this workshop, we would like to develop a common understanding of the benefits and drawbacks of different function approximation approaches and to derive practical guidelines for selecting a suitable approach to a given problem.\r\n\r\nIn addition, we would like to discuss two key points of criticism in current robot learning research. First, data-driven machine learning methods do, in fact, not necessarily outperform models designed by human experts and we would like to explore what function approximation problems in robotics really have to be learned. Second, function approximation/regression methods are typically evaluated using different metrics and data sets, making standardized comparisons challenging.\r\n\r\nFor more information visit the [[http://robreg.org/|Workshop website]].\r\n", "recorded": "2009-06-28T09:00:00", "title": "International Workshop on Regression in Robotics - Approaches and Applications, Seattle 2009"}, {"url": "bootcamp2010_ralaivola_ikm", "desc": "In this talk, we are going to see the basics of kernels methods. After a brief\r\npresentation of a very simple kernel classifier, we'll give the definition of a\r\npostive definite kernel and explain Support vector machine learning. Then, a few\r\nkernels for structured data, namely sequences and graphs, will be described. The\r\nrepresenter theorem is presented, which explains the rationale for the usual\r\nkernel expansion encountered when working with kernel methods. Finally, a few\r\nelements from statistical learning theory are given.", "recorded": "2010-07-08T08:00:00", "title": "Introduction to Kernel Methods"}, {"url": "pmsb06_schietgat_hmctg", "desc": "Prediction of gene function is a so-called hierarchical multilabel classification (HMC) task: a single instance can be labelled with multiple classes rather than just one (i.e., a gene can have multiple functions), and these classes are organized in a hierarchy. Many machine learning methods focus on learning predictive models with a single target variable. One can then learn to predict all classes separately and combine the predictions afterwards.", "recorded": "2006-06-18T00:00:00", "title": "Hierarchical Multilabel Classification Trees for Gene Function Prediction"}, {"url": "yalephil176s07_kagan_lec19", "desc": "The lecture begins with further exploration of the question of whether it is desirable to live forever under the right circumstances, and then turns to consideration of some alternative theories of the nature of well-being. What makes a life worth living? One popular theory is hedonism, but the thought experiment of being on an \"experience machine\" suggests that this view may be inadequate.\r\n\r\n**Reading assignment:**\r\n\r\nBarnes, Julian. \"The Dream.\" In History of the World in 10 \u00bd Chapters. ", "recorded": "2007-04-03T09:00:00", "title": "Lecture 19 - Immortality Part II; The value of life, Part I"}, {"url": "sikdd2014_zdravkova_multiword_expressions", "desc": "One of the crucial challenges of statistical machine \r\ntranslation is the lexical consistency of manually translated \r\nwords and multiword expressions (MWEs) with multiple \r\noccurrences in the source language. In this paper, we present \r\nthe degree of translation inconsistency and we introduce the \r\nindex of translation completeness of fixed MWEs. The \r\nresearch was based on the recently developed system that \r\nintends to extract the entire candidate MWEs from Orwell\u2019s \r\n1984 parallel corpora and to predict their translations \r\nbetween English, Macedonian, and Slovene. ", "recorded": "2014-10-06T13:30:00", "title": "Consistency And Completeness Of Multiword Expressions During Translation"}, {"url": "nipsworkshops09_wackernagel_ggp", "desc": "Gaussian process methodology has inspired a number of stimulating new ideas in the area of machine learning. Kriging has been introduced as a statistical interpolation method for the design of computer experiments twenty years ago. However, some aspects of the geostatistical methodology originally developed for natural resource estimation have been ignored when switching to this new context. This talk reviews concepts of geostatistics and in particular the estimation of components of spatial variation in the context of multiple correlated outputs.", "recorded": "2009-12-12T07:45:00", "title": "Geostatistics for Gaussian Processes "}, {"url": "icml2015_sun_information_geometry", "desc": "We study parametric unsupervised mixture learning. We measure the loss of intrinsic information from the observations to complex mixture models, and then to simple mixture models. We present a geometric picture, where all these representations are regarded as free points in the space of probability distributions. Based on minimum description length, we derive a simple geometric principle to learn all these models together. We present a new learning machine with theories, algorithms, and simulations.\r\n", "recorded": "2015-07-09T14:46:53", "title": "Information Geometry and Minimum Description Length Networks"}, {"url": "pmsb06_pelckmans_mevtc", "desc": "This work studies a machine learning technique designed for exploring relations between microarray experiment data and the corpus of gene-related literature available via PubMed. The use of this task is found in that it provides better clusters of genes by fusing both information sources together, while it can also be used to guide the expert through the large corpus of gene-related literature based on insights into microarray experiments and vice versa.", "recorded": "2006-06-17T00:00:00", "title": "Mutual Spectral Clustering: Microarray Experiments Versus Text Corpus"}, {"url": "mitworld_mindell_bhm", "desc": "Today, the relationship between feedback, control and computing is associated with Norbert Wiener's 1948 formulation of cybernetics. But the theoretical and practical foundations for cybernetics, control engineering, and digital computing were laid earlier, between the two world wars. In his book, David A. Mindell shows how the modern sciences of systems emerged from disparate engineering cultures and their convergence during World War II.", "recorded": "2002-10-10T09:32:00", "title": "Between Human And Machine:Feedback, Control and Computing Before Cybernetics"}, {"url": "samt08_hunter_tpms", "desc": "This paper will describe the next generation of hybrid scalable classification systems that combine social tagging, machine-learning and traditional library classification approaches. It will also discuss approaches to the related challenge of aggregating light-weight community-generated tags with complex MPEG-7 descriptions and discipline-specific ontologies through common, extensible upper ontologies - to enhance discovery and re-use of multimedia content across disciplines and communities.", "recorded": "2008-12-04T09:30:00", "title": "Tracking the Progress of Multimedia Semantics - from MPEG-7 to Web 3.0"}, {"url": "ijcai09_witbrock_uapapk", "desc": "The ability to perform machine classification is a\r\ncritical component of an intelligent system. We\r\npropose to unite the logical, a priori approach\r\nto this problem with the empirical, a posteriori\r\napproach. We describe in particular how the a\r\npriori knowledge encoded in Cyc can be merged\r\nwith technology for probabilistic inference using\r\nMarkov logic networks. We describe two problem\r\ndomains \u2013 the Whodunit Problem and noun phrase\r\nunderstanding \u2013 and show that Cyc\u2019s commonsense\r\nknowledge can be fruitfully combined with probabilistic\r\nreasoning.", "recorded": "2009-07-11T15:45:00", "title": "Uniting \"a priori\" and \"a posteriori\" knowledge: A research framework"}, {"url": "colt2011_hand_world", "desc": "Quite properly, the early days of computational and machine learning assumed the data were well-behaved when developing learning algorithms and strategies. Unfortunately, such an assumption is unwise when one comes to apply the methods to real problems.  This talk illustrates some of the complications of real problems, showing how blind application of highly effective methods can lead one seriously astray.  Solutions require deep thought and the development of more sophisticated theory and algorithms. ", "recorded": "2011-07-09T10:45:00", "title": "Learning in the real world"}, {"url": "acl07_yngve_interview", "desc": "Prof. Victor Yngve is a key witness of the earliest days of computational linguistics, and of the original split, as one might call it, between Chomsky and computational linguistics. He was at MIT at that time and is in one sense the founder of computational linguistics \u2212 as distinct from machine translation. He broke with Chomsky over the key issue of syntax and processing. This video shows Prof. Yngve's reminiscences and is part of a series of interviews with the founders of computational linguistics.\r\n\r\n", "recorded": "2005-01-01T08:00:00", "title": "Prof. Victor Yngve Interviewed by Prof. Yorick Wilks"}, {"url": "icml2010_lowd_libra", "desc": "The Libra machine learning toolkit includes implementations of a variety of algorithms for learning and inference with Bayesian networks and arithmetic circuits:\r\n\r\nLearning algorithms -- Structure learning for BNs and ACs; Chow-Liu algorithm; AC weight learning\r\n\r\nInference algorithms - Mean field, belief propagation, Gibbs sampling, AC variable elimination, AC exact inference\r\n\r\nLibra's strength is exploiting context-specific independence (such as decision tree CPDs) to allow exact inference in models with high treewidth.", "recorded": "2010-06-25T14:02:29", "title": "Libra"}, {"url": "machine_fadili_splitting_method", "desc": "We describe efficient implementations of the proximity calculation for a useful class of functions; the implementations exploit the piece-wise linear nature of the dual problem. The second part of the paper applies the previous result to acceleration of convex minimization problems, and leads to an elegant quasi-Newton method. The optimization method compares favorably against state-of-the-art alternatives. The algorithm has extensive applications including signal processing, sparse regression and recovery, and machine learning and classification.", "recorded": "2012-12-04T15:30:00", "title": "A quasi-Newton proximal splitting method"}, {"url": "eswc2014_tresp_machine_learning", "desc": "Most successful applications of statistical machine learning focus on response learning or signal-reaction learning where an output is produced as a direct response to an input. An important feature is a quick response time, the basis for, e.g., real-time ad-placement on the Web, real-time address reading in postal automation, or a fast reaction to threats for a biological being. One might argue that knowledge about specific world entities and their relationships is necessary if the complexity of an agent's world increases, for example if an agent needs to function in a complex social community. As one is quite aware in the Semantic Web community, a natural representation of knowledge about entities and their relationships is a directed labeled graph where nodes represent entities and where a labeled link stands for a true fact. A number of successful graph-based knowledge representations, such as DBpedia, YAGO, or the Google Knowledge Graph, have recently been developed and are the basis of applications ranging from the support of search to the realization of question answering systems. Statistical machine learning can play an important role in knowledge graphs as well. By exploiting statistical relational patterns one can predict the likelihood of new facts, find entity clusters and determine if two entities refer to the same real world object. Furthermore, one can analyze new entities and map them to existing entities (recognition) and predict likely relations for the new entity. These learning tasks can elegantly be approached by first transforming the knowledge graph into a 3-way tensor where two of the modes represent the entities in the domain and the third mode represents the relation type. Generalization is achieved by tensor factorization using, e.g., the RESCAL approach. A particular feature of RESCAL is that it exhibits collective learning where information can propagate in the knowledge graph to support a learning task. In the presentation the RESCAL approach will be introduced and applications of RESCAL to different learning and decision tasks will be presented.", "recorded": "2014-05-29T09:00:00", "title": "Machine Learning with Knowledge Graphs"}, {"url": "smart", "desc": "Check the SMART web page \\\\\r\nhttp://www.smart-project.eu/node/1\r\n\r\n==== What is the SMART project?\r\n\r\nMore than half of the EU citizens are not able to hold a conversation in a language other than their mother tongue, let alone to conduct a negotiation, or interpret a law. In a time of wide availability of communication technologies, language barriers are a serious bottleneck to European integration and to economic and cultural exchanges in general. More effective tools to overcome such barriers, in the form of software for machine translation and other cross-lingual textual information access tasks, are in strong demand.\r\n\r\nStatistical methods are promising, in that they achieve performances equivalent or superior to those of rule-based systems, at a fraction of the development effort. There are, however, some identified shortcomings in these methods, preventing their broad diffusion. As an example, even though lexical choice is usually more accurate with Statistical Machine Translation (SMT) systems than with their rule-based counterparts, the text they produce tends to be less fluent. As a second example, SMT systems are trained in batch mode and do not adapt by taking user feedback into account. Finally, in Cross-Language Information Retrieval tasks, query words are most often translated independent of one another, thus giving up possibly relevant contextual clues.\r\n\r\n\r\n==== What willl be the key aspect of the project?\r\n\r\nSMART is an attempt to address these and other shortcomings by the methods of modern Statistical Learning. The scientific focus is on developing new and more effective statistical approaches while ensuring that existing know-how is duly taken into account. By bringing together leading research institutions in Statistical Learning, Machine Translation and Textual Information Access, the SMART consortium is well positioned to achieve this goal.\r\n\r\n\r\n==== Basic training\r\n\r\n;[[/stw07_grenoble/|SMART Tutorial Workshop]]\r\n: Grenoble, 2007\r\n\r\n", "recorded": "2009-07-01T10:33:36", "title": "SMART- Statistical Multilingual Analysis for Retrieval and Translation "}, {"url": "nipsworkshops2011_personalized_medicine", "desc": "The purpose of this cross-discipline workshop is to bring together machine learning and healthcare researchers interested in problems and applications of predictive models in the field of personalized medicine. The goal of the workshop will be to bridge the gap between the theory of predictive models and the applications and needs of the healthcare community. There will be exchange of ideas, identification of important and challenging applications and discovery of possible synergies. Ideally this will spur discussion and collaboration between the two disciplines and result in collaborative grant submissions. The emphasis will be on the mathematical and engineering aspects of predictive models and how it relates to practical medical problems.\r\n\r\nAlthough, predictive modeling for healthcare has been explored by biostatisticians for several decades, this workshop focuses on substantially different needs and problems that are better addressed by modern machine learning technologies. For example, how should we organize clinical trials to validate the clinical utility of predictive models for personalized therapy selection? This workshop does not focus on issues of basic science; rather, we focus on predictive models that combine all available patient data (including imaging, pathology, lab, genomics etc.) to impact point of care decision making.\r\n\r\nWorkshop homepage: http://agbs.kyb.tuebingen.mpg.de/wikis/bg/NIPSPM11", "recorded": "2011-12-16T07:30:00", "title": "From Statistical Genetics to Predictive Models in Personalized Medicine"}, {"url": "eswc06_hovy_tlsss", "desc": "Building on the successes of the past decade\u2019s work on statistical methods, there are signs that continued quality improvement for QA, summarization, information extraction, and possibly even machine translation require more-elaborate and possibly even (shallow) semantic representations of text meaning.\r \r But how can one define a large-scale shallow semantic representation system and contents adequate for NLP applications, and how can one create the corpus of shallow semantic representation structures that would be required to train machine learning algorithms? This talk addresses the components required (including a symbol definition ontology and a corpus of (shallow) meaning representations) and the resources and methods one needs to build them (including existing ontologies, human annotation procedures, and a verification methodology). To illustrate these aspects, several existing and recent projects and applicable resources are described, and a research programme for the near future is outlined. Should NLP be willing to face this challenge, we may in the not-too-distant future find ourselves working with a whole new order of knowledge, namely (shallow) and doing so in increasing collaboration (after a 40-years separation) with specialists from the Knowledge Representation and reasoning community.", "recorded": "2006-06-14T00:00:00", "title": "Toward Large-Scale Shallow Semantics for Higher-Quality NLP"}, {"url": "iswc07_pell_nlpsw", "desc": "The **Semantic Web** promises to revolutionize access to information by adding machine-readable semantic information to content which is normally interpretable only by people. In addition, it will also revolutionize access to services by adding semantic information to create machine-readable service descriptions. This ambitious vision has been slow to take off because of a chicken and egg problem. Markup is required before people will build applications, applications are required before it is worth the hard work of doing markup. **Natural language processing (NLP)** has advanced to the point where it can break the impasse and open up the possibilities of the Semantic Web. First, NLP systems can now automatically create annotations from unstructured text. This provides the data that semantic web applications require. Second, NLP systems are themselves consumers of semantic web information and thus provide economic motivation for people to create and maintain such information. For example, a new generation of natural language search systems, as illustrated by Powerset, can take advantage of semantic web markup and ontologies to augment their interpretation of underlying textual content. They can also expose semantic web services directly in response to natural language queries.", "recorded": "2007-11-14T09:31:02", "title": "POWERSET - Natural Language and the Semantic Web"}, {"url": "icml09_zhang_pvmlsssl", "desc": "Practical data analysis and mining rarely falls exactly into the supervised learning scenario. Rather, the growing amount of unlabelled \r\ndata from various scienti\ufb01c domains poses a big challenge to large-scale semi-supervised learning (SSL). We note that the computational \r\nintensiveness of graph-based SSL arises largely from the manifold or graph regularization, which may in turn lead to large models that \r\nare dif\ufb01cult to handle. To alleviate this, we proposed the prototype vector machine (PVM), a highly scalable, graph-based algorithm for \r\nlarge-scale SSL. Our key innovation is the use of \u201cprototypes vectors\u201d for ef\ufb01cient approximation on both the graph-based regularizer \r\nand the model representation. The choice of prototypes are grounded upon two important criterion: they not only perform effective low- \r\nrank approximation on the kernel matrix, but also span a model suffering the minimum information loss compared with the complete \r\nmodel. These criterion lead to consistent prototype selection scheme, allowing us to design a uni\ufb01ed algorithm (PVM) that demonstrates \r\nencouraging performance while at the same time possessing appealing scaling properties (empirically linear with sample size).", "recorded": "2009-06-15T13:35:00", "title": "Prototype Vector Machine for Large Scale Semi-Supervised Learning"}, {"url": "kdd2014_mullainathan_machine_learning", "desc": "Social scientists increasingly criticize the use of machine learning techniques to understand human behavior. Criticisms include: (1) They are atheoretical and hence of limited scientific value; (2) They do not address causality and are hence of limited policy value; and (3) They are uninterpretable and hence of limited generalizability value (outside contexts very narrowly similar to the training dataset). These criticisms, I argue, miss the enormous opportunity offered by ML techniques to fundamentally improve the practice of empirical social science. Yet each criticism does contain a grain of truth and overcoming them will require innovations to existing methodologies. Some of these innovations are being developed today and some are yet to be tackled. I will in this talk sketch (1) what these innovations look like or should look like; (2) why they are needed; and (3) the technical challenges they raise. I will illustrate my points using a set of applications that range from financial markets to social policy problems to computational models of basic psychological processes. This talk describes joint work with Jon Kleinberg and individual projects with Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, Anuj Shah, Chenhao Tan, Mike Yeomans and Tom Zimmerman.", "recorded": "2014-08-27T15:55:00", "title": "Bugbears or Legitimate Threats? (Social) Scientists' Criticisms of Machine Learning"}, {"url": "colt2011_kallweit_look", "desc": "Concept classes can canonically be represented by sign-matrices, i.e., by matrices with\r\nentries 1 and \u22121. The question whether a sign-matrix (concept class) A can be learned\r\nby a machine that performs large margin classification is closely related to the \u201cmargin\r\ncomplexity\u201d associated with A. We consider several variants of margin complexity, reveal\r\nhow they are related to each other, and we reveal how they are related to other notions of\r\nlearning-theoretic relevance like SQ-dimension, CSQ-dimension, and the Forster bound.", "recorded": "2011-07-09T16:00:00", "title": "A Close Look to Margin Complexity and Related Parameters"}, {"url": "um05_murgue_lppgi", "desc": "In this paper, we propose a WEB USAGE MINING pre-processing method to retrieve missing data from the server log files. Moreover, we propose two levels of evaluation: directly on reconstructed data, but also after a machine learning step by evaluating inferred grammatical models. We conducted some experiments and we showed that our algorithm improves the quality of user data. Keywords: log pre-processing, web usage mining, grammatical inference, evaluation", "recorded": "2005-07-25T00:00:00", "title": "Log pre-processing and grammatical inference for Web usage mining"}, {"url": "icml08_paiement_dmr", "desc": "Modeling long-term dependencies in time series has proved very difficult to achieve with traditional machine learning methods. This problem occurs when considering music data. In this paper, we introduce a model for rhythms based on the distributions of distances between subsequences. A specific implementation of the model when considering Hamming distances over a simple rhythm representation is described. The proposed model consistently outperforms a standard Hidden Markov Model in terms of conditional prediction accuracy on two different music databases.", "recorded": "2008-07-07T01:30:00", "title": "A Distance Model for Rhythms"}, {"url": "wapa2011_sanchez_mendoza_microbial", "desc": "The aim of this paper is to report the achievement of Ichnaea, a fully computer-based prediction system that is able to make fairly accurate predictions for Microbial Source Tracking studies. The system accepts examples showing different concentration levels, uses indicators (variables) with different environmental persistence, and can be applied at different geographical or climatic areas. We describe the inner workings of the system and report on the specific problems and challenges arisen from the machine learning point of view and how they have been addressed.", "recorded": "2011-10-21T11:15:00", "title": "A Software System for the Microbial Source Tracking Problem"}, {"url": "nipsworkshops2012_xu_theorem", "desc": "We consider two widely used notions in machine learning, namely: sparsity and stability. Both notions are deemed desirable, and are believed to lead to good generalization ability. We show that these two notions contradict each other: a sparse algorithm can not be stable and vice versa. Thus, one has to tradeoff sparsity and stability in designing a learning algorithm. This implies that, in contrast to \\ell_2 regularized regression, \\ell_1 regularized regression (Lasso) cannot be stable.", "recorded": "2012-12-07T16:45:00", "title": " Sparse Algorithms are Not Stable: A No-free-lunch Theorem"}, {"url": "sikdd2013_kazic_activity_recognition", "desc": "The focus of this work is to explore the possibilities of recognizing three common user activities (sitting,\r\nwalking and running) with accelerometer data from smartphones. Among five common machine learning\r\nalgorithms, Na\u00efve Bayes classifier proved to be the best choice. Classification accuracy of more than 90% was achieved when phone is carried in a pocket. It is shown that this method is appropriate and that the phone\u2019s orientation information is not needed. Finally, the classification of one day-long data set is presented.", "recorded": "2013-10-07T14:24:50", "title": "Sensor-based single-user activity recognition"}, {"url": "nipsworkshops2011_garcia_garcia_supervision", "desc": "Many machine learning problems can be interpreted as differing just in the level of supervision provided to the learning process. In this work we provide a unifying way of dealing with these different degrees of supervision. We show how the framework developed to accommodate this vision can deal with the continuum between classification and clustering, while also naturally accommodating less standard settings such as learning from label proportions, multiple instance learning,...All this emanates from a simple common principle: when in doubt, assume the simplest possible classification problem on the data.", "recorded": "2011-12-16T17:45:00", "title": "Degrees of Supervision"}, {"url": "oiml05_zoeter_cplsl", "desc": "Exact inference in probabilistic models is often infeasible due to (i) a complicated conditional independence structure, and/or (ii) troublesome local integrals. Most challenging inference problems found in physics, such as the computation of the partition function in an Ising model or Boltzmann machine are examples of problems that suffer from a complex structure. All variables are binary, but the cycles in the model prevent an efficient recursive formulation of an inference algorithm.", "recorded": "2005-01-21T00:00:00", "title": "Kikuchi free energies with weak consistency constraints: change point learning in switching linear dynamical systems"}, {"url": "ecmlpkdd08_do_afmft", "desc": "We propose a new algorithm for training a linear Support Vector Machine in the primal. The algorithm mixes ideas from non smooth optimization, subgradient methods, and cutting planes methods. This yields a fast algorithm that compares well to state of the art algorithms. It is proved to require $O(1/{lambdaepsilon})$ iterations to converge to a solution with accuracy $epsilon$. Additionally we provide an exact shrinking method in the primal that allows reducing the complexity of an iteration to much less than $O(N)$ where $N$ is the number of training samples.", "recorded": "2008-09-15T11:20:00", "title": "A Fast Method for Training Linear SVM in the Primal"}, {"url": "mlss06tw_lin_rigc", "desc": "We discuss the problem of ranking individuals from their group competition results. Many real-world problems are of this type. For example, ranking players from team games is important in some sports. In machine learning, this is closely related to multi-class classification and probability estimates. We propose new models for estimating individuals' abilities, and hence rankings of individuals. We develop easy and effective solution procedures. Experiments on real bridge records and multi-class classification demonstrate the viability of the proposed models.", "recorded": "2006-07-27T00:00:00", "title": "Ranking Individuals by Group Comparisons"}, {"url": "pmsb06_d_alche_cob", "desc": "Elucidating biological networks appears nowadays as one of the most important challenge in systems biology. Due to the availability of various sources of data, machine learning has to play a major role regarding this issue, given its large spectrum of tools ranging from generative models to concept learning methods. In this work the focus is narrowed on the completion of biological interactions networks for which some of the interactions between variables (usually genes or proteins) are already known.", "recorded": "2006-06-18T00:00:00", "title": "Completion of biological networks : the output kernel trees approach"}, {"url": "nips09_chakraborty_hcm", "desc": "Over the last couple of decades, the increasing focus on accessibility has resulted in the design and\r\ndevelopment of several assistive technologies to aid people with visual impairments in their daily\r\nactivities. Most of these devices have been centered on enhancing the interaction of a user who\r\nis blind or visually impaired with objects and environments, such as a computer monitor, personal\r\ndigital assistant, cellphone, road traffic, or a grocery store. Although these efforts are very essential\r\nfor the quality of life of these individuals, there is also a need (which has so far not been seriously\r\nconsidered) to enrich the interactions of individuals who are blind, with other individuals.\r\n\r\nNon-verbal cues (including prosody, elements of the physical environment, the appearance of communicators\r\nand physical movements) account for as much as 65% of the information communicated\r\nduring social interactions [1]. However, more than 1.1 million individuals in the US who are legally\r\nblind (and 37 million worldwide) have a limited experience of this fundamental privilege of social\r\ninteractions. These individuals continue to be faced with fundamental challenges in coping with\r\neveryday interactions in their social lives. The work described in this paper is based on the design\r\nand development of a Social Interaction Assistant that is intended to enrich the experience of social\r\ninteractions for individuals who are blind, by providing real-time access to information about\r\nindividuals and their surrounds. The realization of a Social Interaction Assistant device involves\r\nsolving several challenging problems in pattern analysis and machine intelligence such as person\r\nrecognition/tracking, head/body pose estimation, gesture recognition, expression recognition, etc on\r\na wearable real-time platform. A list of eight significant daily challenges faced by these individuals\r\nwas identified in our initial focus group studies conducted with 27 individuals who are blind or visually\r\nimpaired. Each of these problems raises unique machine learning challenges that need to\r\nbe addressed.", "recorded": "2009-12-10T15:45:00", "title": "Human-Centered Machine Learning in a Social Interaction Assistant for Individuals with Visual Impairments"}, {"url": "turing100_brooks_pilot_ace", "desc": "Alan Turing\u2019s contribution to the theory of computing is unmatched.  Here we look beyond that, at the architecture and implementation of his proposed computer, the National Physical Laboratory\u2019s Pilot ACE.\r\n\r\nArchitecturally, the ACE is the very epitome of an \u201cunclean\u201d machine.  This is understandable, given  the stated objectives of Turing and the team:  to build a running computer with a little equipment as possible.  Hence the architecture follows directly from the implementation and the technologies chosen.\r\n\r\nThe architecture represents a wide deviation from that proposed in Burks, Goldstine, and von Neumann\u2019s \u201cPreliminary Discussion of the Logical Design of an Electronic Computing Instrument\u201d (1946).  It introduces several important new concepts.  The most important is the use of chained instructions rather than sequential ones, to enable optimum placement of instructions and data in the serial main memory.  Each instruction contains the (highly abbreviated) address of the next instruction.  This technique became familiar via the popular IBM 650 (1954)  A second important innovation is the one-instruction transfer of a variable-sized block of words to/from the backing store.  The I/O provisions are exceptional for the date and substantially enhanced the usefulness of the machine.\r\n\r\nSome architectural decisions seem quite ill-considered, such as the delegation of multiplication to a subroutine.  Wilkes\u2019 deep experience with scientific computing on desk calculators gave him a better instinctive grasp of operation relative frequencies, so his EDSAC didn\u2019t embody this mistake.   The ACE also embodies the (universal) mistake of radically underestimating the need for main memory and the provision of adequate addressing for sizes quite beyond the initially affordable.\r\nNPL had various troubles realizing the machine, so it didn\u2019t operate until October, 1950, and couldn\u2019t be harnessed for use until May, 1951, almost two years after Kilburn\u2019s Manchester Baby (1948) and Wilkes\u2019s Cambridge EDSAC (1949).  It served as a well-used scientific computer for some years, and spawned descendents:  the English Electric ACE, the EE Deuce, the Bendix (later CDC) G-15 and G-20.", "recorded": "2012-06-23T15:45:00", "title": "Turing\u00b4s Pilot ACE: Why Not Important?"}, {"url": "mmdss07_shanahan_mre", "desc": "The Turn automatic targeting network provides advertisers a\r revolutionary option for online advertising campaigns (online advertising is a\r $16 billion industry in 2006 according to the Interactive Advertising Bureau).\r An advertiser simply inputs its ad into a self-serve console, and Turn does the\r rest. Unlike many ad networks operating today, Turn incorporates extensive\r industry expertise and innovative technology from the fields of machine\r learning, information science and statistics, to truly make online advertising\r risk-free, relevant, simple, effective, and most importantly, profitable. Unlike\r traditional ad networks where advertisers need teams of employees to manage\r manual targeting including selecting sites or selecting and optimizing hundreds\r of thousands of keywords, the Turn network automatically analyzes and targets\r ads. Turn\u2019s technology dynamically selects and blends hundreds of variables\r such as past performance, brand strength, user profiles, action type and site\r categories to determine the best targets for each ad, thus eliminating guesswork,\r time and complexity. The Turn network is based on statistical technology that\r intelligently targets both text and graphical ads. By dynamically and\r automatically selecting and blending targeting variables, Turn can determine\r the best ad or group of ads for any situation. Turn offers true pay-forperformance\r with its unique bidded CPA model. Because advertisers pay for\r actions that they define, Turn eliminates the risk of worthless or fraudulent\r clicks. Whether an advertiser is paying for product purchases, site visits, leads,\r or email signups, the advertiser is in control of what they pay for and when they\r pay for it. In the context of this problem setting (with billions of ad\r impressions), this poster will address some key issues in modeling rare events\r using machine learning and data mining such as uncertainty, the regression\r versus classification dilemma and feature engineering.", "recorded": "2007-09-18T17:15:00", "title": "Modeling rare events: online advertisement targeting using machine learning and data mining"}, {"url": "mitworld_kurzweil_gelernter_brooks_cmmm", "desc": "Two of the sharpest minds in the computing arena spar gamely, but neither scores a knockdown in one of the oldest debates around: whether machines may someday achieve consciousness. (NB: Viewers may wish to brush up on the work of computer pioneer Alan Turing and philosopher John Searle in preparation for this video.)\n\nRay Kurzweil confidently states that artificial intelligence will, in the not distant future, \u201cmaster human intelligence.\u201d He cites the \u201cexponential power of growth in technology\u201d that will enable both a minute, detailed understanding of the human brain, and the capacity for building a machine that can at least simulate original thought. The \u201cfrontier\u201d such a machine must cross is emotional intelligence\u2014\u201cbeing funny, expressing loving sentiment\u2026\u201d And when this occurs, says Kurzweil, it\u2019s not entirely clear that the entity will have achieved consciousness, since we have no \u201cconsciousness detector\u201d to determine if it is capable of subjective experiences.\n\nAcknowledging that his position will prove unpopular, David Gelernter launches his attack: \u201cWe won\u2019t even be able to build super-intelligent zombies unless we approach the problem right.\u201d This means admitting that a continuum of cognitive styles exists among humans. As for building a conscious machine, he sees no possibility of one emerging from even the most sophisticated software. \u201cConsciousness means the presence of mental states strictly private with no visible functions or consequences. A conscious entity can call on a thought or memory merely to feel happy, be inspired, soothed, feel anger\u2026\u201d Software programs, by definition, can be separated out, peeled away and run in a logically identical way on any computing platform. How could such a program spontaneously give rise to \u201ca new node of consciousness?\u201d\n\nKurzweil concedes the difficulty of defining consciousness, but does not want to wish away the concept, since it serves as the basis for our moral and ethical systems. He maintains his argument that reverse engineering of the human brain will enable machines that can act with a level of complexity, from which somehow consciousness will emerge.\n\nGelernter replies that believing this \u201cseems a completely arbitrary claim. Anything might be true, but I don\u2019t see what makes the claim plausible.\u201d Ultimately, he says, Kurzweil must explain objectively and scientifically what consciousness is -- \u201chow it\u2019s created and got there.\u201d Kurzweil stakes his claim on our future capacity to model digitally the actions of billions of neurons and neurotransmitters, which in humans somehow give rise to consciousness. Gelernter believes such a machine might simulate mental states, but not actually pass muster as a conscious entity. Ultimately, he questions the desirability of building such computers: \u201cWe might reach the state some day when we prefer the company of a robot from Walmart to our next-door neighbor or roommates.\u201d", "recorded": "2006-11-30T14:48:00", "title": "Creativity: The Mind, Machines, and Mathematics: Public Debate"}, {"url": "icml2015_hardt_reliable_leaderboard", "desc": "The organizer of a machine learning competition faces the problem of maintaining an accurate leaderboard that faithfully represents the quality of the best submission of each competing team. What makes this estimation problem particularly challenging is its sequential and adaptive nature. As participants are allowed to repeatedly evaluate their submissions on the leaderboard, they may begin to overfit to the holdout data that supports the leaderboard. Few theoretical results give actionable advice on how to design a reliable leaderboard. Existing approaches therefore often resort to poorly understood heuristics such as limiting the bit precision of answers and the rate of re-submission. In this work, we introduce a notion of leaderboard accuracy tailored to the format of a competition. We introduce a natural algorithm called the Ladder and demonstrate that it simultaneously supports strong theoretical guarantees in a fully adaptive model of estimation, withstands practical adversarial attacks, and achieves high utility on real submission files from a Kaggle competition. Notably, we are able to sidestep a powerful recent hardness result for adaptive risk estimation that rules out algorithms such as ours under a seemingly very similar notion of accuracy. On a practical note, we provide a completely parameter-free variant of our algorithm that can be deployed in a real competition with no tuning required whatsoever.", "recorded": "2015-07-07T11:19:33", "title": "The Ladder: A Reliable Leaderboard for Machine Learning Competitions"}, {"url": "mlss09us_hasegawa-johnson_mlasp", "desc": "This tutorial presents a framework for understanding and comparing applications of pattern recognition in acoustic signal processing. Representative applications will be delimited by two binary features: (1) regression vs. (2) classification (inferred variables are continuous vs. discrete), (A) instantaneous vs. (B) dynamic. (1. Regression) problems include imaging and sound source tracking using a device with unknown properties, and inverse problems, e.g., articulatory estimation from speech audio. (2. Classification) problems include, e.g., the detection of syllable onsets and offsets in a speech signal, and the classification of non-speech audio events. (A. Instantaneous) inference is performed using a universal approximator (neural network, Gaussian mixture, kernel regression), constrained or regularized, if necessary, to reduce generalization error (resulting in a support vector machine, shrunk net, pruned tree, or boosted classifier combination). (B. Dynamic) inference methods apply prior knowledge of state transition probabilities, either in the form of a regularization term (e.g., using Bayesian inference) or in the form of set constraints (e.g., using linear programming) or both; examples include speech-to-text transcription, acoustic-to-articulatory inversion using a switching Kalman filter, and computation of the query presence probability in an audio information retrieval task.", "recorded": "2009-06-11T15:30:00", "title": "Machine Learning in Acoustic Signal Processing"}, {"url": "solomon_lavrac_sdrer", "desc": "Traditionally, machine learning has focussed on induction of classification and prediction rules. More recently, non-predictive or descriptive induction is gaining substantial interest of machine learning researchers. Two major trends in descriptive induction are association rule learning and subgroup discovery. In this seminar we present our recent work in descriptive induction. \r\n \r\n We also argue that accuracy is not always an appropriate evaluation measure in the descriptive induction framework, and propose quality measures designed for subgroup evaluation in ROC space. After a brief presentation of the APRIORI-C and SD-algorithm, we give a detailed presentation of the CN2-SD algorithm, which includes a new -- weighted -- covering algorithm, a new search heuristic (weighted relative accuracy), probabilistic classification of instances, and a new measure for evaluating the results of subgroup discovery (area under ROC curve). \r\n \r\n The presented work was done in collaboration with V. Jovanoski (APRIORI-C), D. Gamberger (SD-algorithm), B. Kavsek and L. Todorovski (CN2-SD algorithm). \r\n \r\n Our research was supported by the Slovenian Ministry of Education, Science and Sport, the IST-1999-11495 project Data Mining and Decision Support for Business Competitiveness: A European Virtual Enterprise, and the British Council project Partnership in Science PSP-18.", "recorded": "2002-04-02T13:00:00", "title": "Subgroup discovery and rule evaluation in ROC space"}, {"url": "nipsworkshops2012_titov_semantic_representations", "desc": "Cross-lingual representations of linguistic units (e.g., words or phrases) can facilitate transfer of annotation from resource-rich to resource-poor languages and have many potential multilingual applications (e.g., machine translation and crosslingual information retrieval). In this talk, I will discuss our ongoing work which aims to induce cross-lingual representations relying primarily on monolingual unannotated texts readily available for many languages. From the learning standpoint, our approaches maximize the likelihood of monolingual unannotated texts but also use a form of regularization which favors agreement on a smaller collection of parallel data (i.e. sentences along with their translations). I will address the induction of different types of cross-lingual representations (clusters and distributed representations) for different types of units (words, phrases and predicateargument structures). We show that these models induce linguistically-plausible semantic representations and that cross-lingual induction both helps to induce better representations for individual languages and benefits various cross-lingual applications. Specifically, I will consider direct transfer of a classifier for a document classification task from one language to another, and show preliminary results in the context of low resource machine translation.", "recorded": "2012-12-07T16:20:46", "title": "Inducing Cross-Lingual Semantic Representations of Words, Phrases, Sentences and Events"}, {"url": "machine_nagata_prediction", "desc": "Residue-residue contact prediction is a fundamental problem in protein structure prediction. Hower, despite considerable research efforts, contact prediction methods are still largely unreliable. Here we introduce a novel deep machine-learning architecture which consists of a multidimensional stack of learning modules. For contact prediction, the idea is implemented as a three-dimensional stack of Neural Networks NN^k_{ij}, where i and j index the spatial coordinates of the contact map and k indexes ''time''. The temporal dimension is introduced to capture the fact that protein folding is not an instantaneous process, but rather a progressive refinement. Networks at level k in the stack can be trained in supervised fashion to refine the predictions produced by the previous level, hence addressing the problem of vanishing gradients, typical of deep architectures. Increased accuracy and generalization capabilities of this approach are established by rigorous comparison with other classical machine learning approaches for contact prediction. The deep approach leads to an accuracy for difficult long-range contacts of about 30%, roughly 10% above the state-of-the-art. Many variations in the architectures and the training algorithms are possible, leaving room for further improvements. Furthermore, the approach is applicable to other problems with strong underlying spatial and temporal components.", "recorded": "2012-12-06T10:26:00", "title": "Deep Spatio-Temporal Architectures and Learning for Protein Structure Prediction"}, {"url": "icml08_winn_pmui", "desc": "Getting a computer to understand an image is challenging due to the numerous sources of variability that influence the imaging process. The pixels of a typical photograph will depend on the scene type and geometry, the number, shape and appearance of objects present in the scene, their 3D positions and orientations, as well as effects such as occlusion, shading and shadows. The good news is that research into physics and computer graphics has given us a detailed understanding of how these variables affect the resulting image. This understanding can help us to build the right prior knowledge into our probabilistic models of images. In theory, building a model containing all of this knowledge would solve the image understanding problem. In practice, such a model would be intractable for current inference methods. The open challenge for machine learning and machine vision researchers is to create a model which captures the imaging process as accurately as possible, whilst remaining tractable for accurate inference. To illustrate this challenge, I will show how different aspects of the imaging process can be incorporated into models for object detection and segmentation, and discuss techniques for making inference tractable in such models.\r \r **//Disclaimer:// Videolectures.Net emphasises that the quality of this video can not be improved,\r because of low light quality conditions provided in the lecture auditorium.**", "recorded": "2008-07-06T08:45:32", "title": "Probabilistic models for understanding images"}, {"url": "cogsys2012_sandee_robots", "desc": "At its core, RoboEarth is a World Wide Web for robots: a giant network and database repository where robots can share information and learn from each other about their behavior and their environment. Bringing a new meaning to the phrase \u201cexperience is the best teacher\u201d, the goal of RoboEarth is to allow robotic systems to benefit from the experience of other robots, paving the way for rapid advances in machine cognition and behaviour, and ultimately, for more subtle and sophisticated human-machine interaction. \n\nRoboEarth offers a complete Cloud Robotics infrastructure, which includes everything needed to close the loop from robot to RoboEarth to robot. The RoboEarth World-Wide-Web style database is implemented on a server with Internet and Intranet functionality, making it attractive for both research and business applications. It stores information required for object recognition (e.g., images, object models), navigation (e.g., maps, world models), tasks (e.g., action recipes, manipulation strategies) and hosts intelligent services (e.g., image annotation, offline learning).\n\nTo close the loop, the RoboEarth Collaborators have implemented components for a ROS compatible, robot-unspecific, high-level operating system as well as components for robot-specific, low level controllers accessible via a Hardware Abstraction Layer.", "recorded": "2012-02-22T15:00:00", "title": "RoboEarth: Connecting robots world-wide"}, {"url": "sahd2014_bach_stochastic_gradient", "desc": "Many machine learning and signal processing problems are traditionally cast as convex\r\noptimization problems. A common difficulty in solving these problems is the size of the data, where\r\nthere are many observations (\"large n\") and each of these is large (\"large p\"). In this setting, online\r\nalgorithms such as stochastic gradient descent which pass over the data only once, are usually\r\npreferred over batch algorithms, which require multiple passes over the data. Given n\r\nobservations/iterations, the optimal convergence rates of these algorithms are O(1/\\sqrt{n}) for\r\ngeneral convex functions and reaches O(1/n) for strongly-convex functions. In this talk, I will show\r\nhow the smoothness of loss functions may be used to design novel algorithms with improved\r\nbehavior, both in theory and practice: in the ideal infinite-data setting, an efficient novel Newtonbased\r\nstochastic approximation algorithm leads to a convergence rate of O(1/n) without strong\r\nconvexity assumptions, while in the practical finite-data setting, an appropriate combination of\r\nbatch and online algorithms leads to unexpected behaviors, such as a linear convergence rate for\r\nstrongly convex problems, with an iteration cost similar to stochastic gradient descent. (joint work\r\nwith Nicolas Le Roux, Eric Moulines and Mark Schmidt)", "recorded": "2014-09-05T09:30:00", "title": "Beyond stochastic gradient descent for large-scale machine learning"}, {"url": "clsp_quirk_corpora", "desc": "As we scale statistical machine translation systems to general domain, we face many challenges. This talk outlines two approaches for building better broad-domain systems.\r\n\r\nFirst, progress in data-driven translation is limited by the availability of parallel data. A promising strategy for mitigating data scarcity is to mine parallel data from comparable corpora. Although comparable corpora seldom contain parallel sentences, they often contain parallel words or phrases. Recent fragment extraction approaches have shown that including parallel fragments in SMT training data can significantly improve translation quality. We describe efficient and effective generative models for extracting fragments, and demonstrate that these algorithms produce substantial improvements on out-of-domain test data without suffering in-domain degradation.\r\n\r\nSecond, many modern SMT systems are very heavily lexicalized. While such information excels on in-domain test data, quality falls off as the test data broadens. This next section of the talk describes robust generalized models that leverage lexicalization when available, and back off to linguistic generalizations otherwise. Such an approach results in large improvements over baseline phrasal systems when using broad domain test sets.", "recorded": "2008-11-11T16:22:14", "title": "Broadening statistical machine translation with comparable corpora and generalized models"}, {"url": "kdd2013_ramakrishnan_map_reduce", "desc": "The amount of data being collected is growing at a staggering pace. The default is to capture and store any and all data, in anticipation of potential future strategic value, and vast amounts of data are being generated by instrumenting key customer and systems touchpoints. Until recently, data was gathered for well-defined objectives such as auditing, forensics, reporting and line-of-business operations; now, exploratory and predictive analysis is becoming ubiquitous. These differences in data scale and usage are leading to a new generation of data management and analytic systems, where the emphasis is on supporting a wide range of data to be stored uniformly and analyzed seamlessly using whatever techniques are most appropriate, including traditional tools like SQL and BI and newer tools for graph analytics and machine learning. These new systems use scale-out architectures for both data storage and computation.\r\nHadoop has become a key building block in the new generation of scale-out systems. Early versions of analytic tools over Hadoop, such as Hive and Pig for SQL-like queries, were implemented by translation into Map-Reduce computations. This approach has inherent limitations, and the emergence of resource managers such as YARN and Mesos has opened the door for newer analytic tools to bypass the Map-Reduce layer. This trend is especially significant for iterative computations such as graph analytics and machine learning, for which Map-Reduce is widely recognized to be a poor fit. In this talk, I will examine this architectural trend, and argue that resource managers are a first step in re-factoring the early implementations of Map-Reduce, and that more work is needed if we wish to support a variety of analytic tools on a common scale-out computational fabric. I will then present REEF, which runs on top of resource managers like YARN and provides support for task monitoring and restart, data movement and communications, and distributed state management. Finally, I will illustrate the value of using REEF to implement iterative algorithms for graph analytics and machine learning.", "recorded": "2013-08-12T08:50:46", "title": "Scale-out Beyond MapReduce"}, {"url": "um05_kaski_umifp", "desc": "Our research consortium develops user modeling methods for proactive applications. In this project we use machine learning methods for predicting users\u2019 preferences from implicit relevance feedback. Our prototype application is information retrieval, where the feedback signal is measured from eye movements or user\u2019s behavior. Relevance of a read text is extracted from the feedback signal with models learned from a collected data set. Since it is hard to define relevance in general, we have constructed an experimental setting where relevance is known a priori.", "recorded": "2005-07-25T00:00:00", "title": "User models from implicit feedback for proactive information retrieval"}, {"url": "aop05_erice", "desc": "Automatic pattern analysis of data is a pillar of modern science, technology and business, with deep roots in statistics, machine learning, pattern recognition, theoretical computer science, and many other fields. A unified conceptual understanding of this strategic field is of utmost importance for researchers as well as for users of this technology.\n\nThis workshop-course emphasizes the common principles and roots of modern pattern analysis technology, developed independently by many different scientific communities over the past 30 years, and their impact on modern science and technology.", "recorded": "2005-10-28T00:00:00", "title": "The Analysis of Patterns, Erice 2005"}, {"url": "mlss06au_kambhampati_ltp", "desc": "In this lecture, I aim to provide an overview of the learning techniques that have found use in automated planning. Unlike most the clustering and classification tasks that have dominated the recent machine learning literature, learning in planning requires handling relational and first order representations, and foregrounds the need for knowledge-intensive learning techniques. I will start with a brief review of the planning models, and discuss the opportunities for learning in planning. I will then provide a survey of the explanation-based, case-based and inductive learning techniques that have been successfully used to tackle them.", "recorded": "2006-02-14T00:00:00", "title": "Learning techniques in Planning"}, {"url": "nipsworkshops09_coates_slcv", "desc": "Computer vision is a challenging application area of machine learning. Recent work has shown that large training sets may yield higher performance in vision tasks like object detection. We overview our work in object detection using a scalable, distributed training system capable of training on more than 100 million examples in just a few hours. We also briefly describe recent work with deep learning algorithms that may allow us to apply these architectures to large datasets as well. ", "recorded": "2009-12-11T09:38:00", "title": "Scalable Learning in Computer Vision"}, {"url": "nips2011_shamir_rounding", "desc": "Most online algorithms used in machine learning today are based on variants of mirror descent or follow-the-leader. In this paper, we present an online algorithm based on a completely different approach, which combines \"random playout\" and randomized rounding of loss subgradients. As an application of our approach, we provide the first computationally efficient online algorithm for collaborative filtering with trace-norm constrained matrices. As a second application, we solve an open question linking batch learning and transductive online learning.", "recorded": "2011-12-14T11:10:00", "title": "Efficient Online Learning via Randomized Rounding"}, {"url": "ecmlpkdd2011_scholz_rfid", "desc": "This paper focuses on resource-aware and cost-effective indoor-localization at room-level using RFID technology. In addition to the tracking information of people wearing active RFID tags, we also include information about their proximity contacts. We present an evaluation using real-world data collected during a conference: We complement state-of-the-art machine learning approaches with strategies utilizing the proximity data in order to improve a core localization technique further.", "recorded": "2011-09-06T14:40:00", "title": "Resource-Aware On-Line RFID Localization Using Proximity Data"}, {"url": "ecmlpkdd09_marchiori_gbddgcif", "desc": "Graph theory has been shown to provide a powerful tool for representing and\r\ntackling machine learning problems, such as clustering, semi-supervised learning, and feature ranking. This paper proposes a graph-based discrete differential operator for detecting and eliminating competence-critical instances and class label noise from a training set in order to improve classification performance. Results of extensive experiments on artificial and real-life classification problems substantiate the effectiveness of the proposed approach.", "recorded": "2009-09-10T16:25:00", "title": "Graph-Based Discrete Differential Geometry for Critical Instance Filtering"}, {"url": "stanfordcs106bw08_zelenski_lec04", "desc": "If it\u2019s in the clusters, that\u2019s actually been done for you, but if you\u2019re doing it on your own\r\nmachine there\u2019s just a surprisingly vast array of things that can go wrong along the way\r\nthat you don\u2019t want to be wrestling with at the last minute, so certainly try to get on that\r\nsoon and then getting in touch with us if you run into some snags so that we can help you\r\nget past that and move on to running the code that you need read this week. ...\r\n\r\nSee the whole transcript at [[http://see.stanford.edu/materials/icspacs106b/transcripts/ProgrammingAbstractions-Lecture04.pdf|Programming Abstractions - Lecture 04]]", "recorded": "2008-01-10T14:11:53", "title": "Lecture 4: C++ Console I/O"}, {"url": "nipsworkshops2013_viegas_new_service", "desc": "CodaLab is open source platform which lets communities explore Experiments together and create Competitions designed to advance the state of the art. The first two communities helping build CodaLab are the Machine Learning and the Medical Imaging communities.\r\n\r\nCodaLab Experiments enable collaborative research and computational research to be done in an efficient and reproducible manner. By providing modularity, live execution, and inline annotation of code with rich explanations, CodaLab enables you to quickly sketch ideas and collaborate with fellow community members\r\n\r\nCodaLab competitions provide an opportunity for researchers and developers to create solutions for problems across a wide range of domains, and advance the state of the art for their respective areas of interest; once the challenge is over, further work can be pursued by the broader community as Experiments on CodaLab.\r\n\r\nCodaLab is a community-driven effort led by Percy Liang from Stanford University who built the precursor of CodaLab, namely, MLComp.\r\nWe invite the NIPS community to participate in CodaLab by creating experiments as executable papers and by sharing them with the rest of the community at http://codalab.org\r\nThese \u201cexecutable papers\u201d can then be freely reproduced, appended, and otherwise modified to improve productivity and accelerate the pace of discovery and learning among machine learning research professionals.", "recorded": "2013-12-09T14:30:00", "title": "Codalab - A new service for data exchange, code execution, benchmarks and reproducible research"}, {"url": "nipsworkshops09_temporal_segmentation", "desc": "**Temporal Segmentation: Perspectives from statistics, machine learning, and signal processing**\r\n\r\nData with temporal (or sequential) structure arise in several applications, such as speaker diarization, human action segmentation, network intrusion detection, DNA copy number analysis, and neuron activity modelling, to name a few. A particularly recurrent temporal structure in real applications is the so-called change-point model, where the data may be temporally partitioned into a sequence of segments delimited by change-points, such that a single model holds within each segment whereas different models hold accross segments. Change-point problems may be tackled from two points of view, corresponding to the practical problem at hand: retrospective (or \"a posteriori\"), aka multiple change-point estimation, where the whole signal is taken at once and the goal is to estimate the change-point locations, and online (or sequential), aka quickest detection, where data are observed sequentially and the goal is to quickly detect change-points. The purpose of this workshop is to bring together experts from the statistics, machine learning, signal processing communities, to address a broad range of applications from robotics to neuroscience, to discuss and cross-fertilize ideas, and to define the current challenges in temporal segmentation.\r\n----\r\nThe Workshop homepage can be found at http://www.harchaoui.eu/zaid/workshops/nips09/index.html\r\n----", "recorded": "2009-12-12T07:30:00", "title": "Temporal Segmentation"}, {"url": "solomon_muggleton_mlbn", "desc": "In this talk we survey work being conducted at the Centre for\r\nIntegrative Systems Biology at Imperial College on the use of\r\nmachine learning to build models of biochemical pathways.\r\nWithin the area of Systems Biology these models provide\r\ngraph-based descriptions of bio-molecular interactions which\r\ndescribe cellular activities such as gene regulation, metabolism\r\nand transcription.  One of the key advantages of the approach taken,\r\nInductive Logic Programming, is the availability of background knowledge\r\non existing known biochemical networks from publicly available\r\nresources such as KEGG and Biocyc.  The topic has clear societal impact\r\nowing to its application in Biology and Medicine. Moreover, object\r\ndescriptions in this domain have an inherently relational structure in the\r\nform of spatial and temporal interactions of the molecules involved.\r\nThe relationships include biochemical reactions in which one set\r\nof metabolites is transformed to another mediated by the involvement\r\nof an enzyme.  Existing genomic information is very incomplete\r\nconcerning the functions and even the existence of genes and\r\nmetabolites, leading to the necessity of techniques such as\r\nlogical abduction to introduce novel functions and invent\r\nnew objects. Moreover, the development of active learning\r\nalgorithms has allowed automatic suggestion of new experiments\r\nto test novel hypotheses. The approach thus provides support\r\nfor the overall scientific cycle of hypothesis generation and\r\nexperimental testing.", "recorded": "2010-04-07T13:00:00", "title": "Machine Learning Biological Network Models"}, {"url": "nipsworkshops2010_computational_biology", "desc": "The field of computational biology has seen dramatic growth\r\nover the past few years, both in terms of new available data,\r\nnew scientific questions, and new challenges for learning and\r\ninference. In particular, biological data is often relationally\r\nstructured and highly diverse, well-suited to approaches that\r\ncombine multiple weak evidence from heterogeneous sources.\r\nThese data may include sequenced genomes of a variety of\r\norganisms, gene expression data from multiple technologies,\r\nprotein expression data, protein sequence and 3D structural\r\ndata, protein interactions, gene ontology and pathway databases,\r\ngenetic variation data (such as SNPs), and an enormous amount\r\nof textual data in the biological and medical literature. New types\r\nof scientific and clinical problems require the development of\r\nnovel supervised and unsupervised learning methods that can\r\nuse these growing resources. \r\n\r\nThe goal of this workshop is to\r\npresent emerging problems and machine learning techniques\r\nin computational biology. We invited several speakers from\r\nthe biology/bioinformatics community who presented current\r\nresearch problems in bioinformatics, and we invited contributed\r\ntalks on novel learning approaches in computational biology.\r\nWe encouraged contributions describing either progress on\r\nnew bioinformatics problems or work on established problems\r\nusing methods that are substantially different from standard\r\napproaches.\r\n\r\nWorkshop homepage: http://mlcb.org/", "recorded": "2010-12-10T07:30:00", "title": "Machine Learning in Computational Biology"}, {"url": "nipsworkshops2010_guestrin_kml", "desc": "Exponentially increasing dataset sizes have driven Machine Learning experts to explore using parallel and distributed computing for their research. Furthermore, cloud computing resources such as Amazon EC2 have become increasingly available, providing cheap and scalable platforms for large scale computation. However, due to the complexities involved in distributed design, it can be difficult for ML researchers to take full advantage of cloud resources. Existing high-level parallel abstractions like MapReduce are insufficiently expressive while low-level tools like MPI and Pthreads leave ML experts repeatedly solving the same design challenges.\r\n\r\nBy targeting common patterns in ML, we developed GraphLab, which compactly expresses asynchronous iterative algorithms with sparse computational dependencies common in ML, while ensuring data consistency and achieving a high degree of parallel performance. We demonstrate the expressiveness of the GraphLab framework by designing and implementing parallel versions for a variety of ML tasks, including learning graphical models with approximate inference, Gibbs sampling, tensor factorization, Co-EM, Lasso and Compressed Sensing. We show that using GraphLab we can achieve excellent parallel performance on large-scale real-world problems and demonstrate their scalability on Amazon EC2, using up to 256 processors.", "recorded": "2010-12-11T15:30:00", "title": "Machine Learning in the Cloud with GraphLab"}, {"url": "cogsys2012_auer_composing", "desc": "One of the aspirations of machine learning is to develop intelligent systems that can address a wide variety of control problems of many different types. However, although the community has developed successful technologies for many individual problems, these technologies have not previously been integrated into a unified framework. As a result, the technology used to specify, solve and analyse one control problem typically cannot be reused on a different problem. The community has fragmented into a diverse set of specialists with particular solutions to particular problems. The purpose of this project is to develop a unified toolkit for intelligent control in many different problem areas. This toolkit will incorporate many of the most successful approaches to a variety of important control problems within a single framework, including bandit problems, Markov Decision Processes (MDPs), Partially Observable MDPs (POMDPs), continuous stochastic control, and multi-agent systems. In addition, the toolkit will provide methods for the automatic construction of representations and capabilities, which can then be applied to any of these problem types. Finally, the toolkit will provide a generic interface to specifying problems and analysing performance, by mapping intuitive, human-understandable goals into machine-understandable objectives, and by mapping algorithm performance and regret back into human-understandable terms.", "recorded": "2012-02-22T10:40:00", "title": "Composing Learning for Artificial Cognitive Systems (CompLACS)"}, {"url": "coa08_janzing_wppsoop", "desc": "Machine learning has traditionally been focused on prediction. Given observations that have been generated\r\nby an unknown stochastic dependency, the goal is to infer a law that will be able to correctly predict future\r\nobservations generated by the same dependency. Statistics, in contrast, has traditionally focused on data\r\nmodeling, i.e., on the estimation of a probability law that has generated the data. During recent years, the\r\nboundaries between the two disciplines have become blurred and both communities have adopted methods\r\nfrom the other, however, it is probably fair to say that neither of them has yet fully embraced the \ufb01eld\r\nof causal modeling, i.e., the detection of causal structure underlying the data. Since the Eighties there has\r\nbeen a community of researchers, mostly from statistics and philosophy, who have developed methods aiming\r\nat inferring causal relationships from observational data. While this community has remained relatively\r\nsmall, it has recently been complemented by a number of researchers from machine learning. The goal of\r\nthis workshop is to discuss new approaches to causal discovery from empirical data, their applications and\r\nmethods to evaluate their success. Emphasis will be put on the de\ufb01nition of objectives to be reached and\r\nassessment methods to evaluate proposed solutions. The participants are encouraged to participate in a\r\ncompetition pot-luck in which datasets and problems will be exchanged and solutions proposed.", "recorded": "2008-12-12T07:30:00", "title": "Welcome and program presentation, short overview over the posters"}, {"url": "ecmlpkdd2011_athens", "desc": "This event builds upon a very successful series of 21 ECML and 14 PKDD conferences, which have been jointly organized for the past ten years.\r\n\r\nIt has become the major European scientific event in these fields and in 2011 it comprises presentations of contributed papers and invited speakers, a wide program of workshops and tutorials, a discovery challenge, a demo track and an industrial track.\r\n\r\nDetailed information about the conference can be found at [[http://www.ecmlpkdd2011.org/|ECML PKDD 2011]].", "recorded": "2011-09-05T09:00:00", "title": "European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD), Athens 2011"}, {"url": "iswc2011_scherp_data", "desc": "We present SchemEX, an approach and tool for web-scale, real-time\r\nindexing and schema extraction of Linked Open Data (LOD) at linear runtime\r\ncomplexity. As we cannot assume that a complete retrieval of the LOD cloud\r\non a local machine is feasible, we follow a stream-based approach that makes\r\nno assumption about how the RDF triples are retrieved from the web by a data\r\ncrawler. We show the applicability of our approach by applying SchemEX to the\r\nBillion Triple Challenge Dataset 2011 and a smaller dataset with 11M triples.", "recorded": "2011-10-26T15:20:00", "title": "SchemEX -- Web-Scale Indexed Schema Extraction of Linked Open Data"}, {"url": "ecmlpkdd2011_kertesz_farkas_ptm", "desc": "This paper focuses on resource-aware and cost-effective indoor-localization at room-level using RFID technology. In addition to the tracking information of people wearing active RFID tags, we also include information about their proximity contacts. We present an evaluation using real-world data collected during a conference: We complement state-of-the-art machine learning approaches with strategies utilizing the proximity data in order to improve a core localization technique further.", "recorded": "2011-09-06T15:00:00", "title": "PTMSearch: a Greedy Tree Traversal Algorithm for f inding Protein Post-Translational Modifications in Tandem Mass Spectra"}, {"url": "mla09_pelckmans_lap", "desc": "This survey considers the design of methods for learning mathematical models from data. The contemporary toolbox of machine learning consists of a wide set of techniques, essentially reducing to a few formal arguments. The aim of this presentation is then to give insight in some of them, and to argue how they could be implemented successfully. When doing so, we will touch topics as risk-based modeling, probabilistic inference, convex optimization and kernel-based  learning amongst others. I will exemplify two application areas, namely (A) identification of dynamic systems, and (B) modeling and prediction of reliability data.", "recorded": "2009-07-03T11:30:00", "title": "Learning and Prediction - A Survey"}, {"url": "akbcwekex2012_wick_human_machine", "desc": "Knowledge bases (KB) provide support for\r\nreal-world decision making by exposing data\r\nin a structured format. However, constructing\r\nknowledge bases requires gathering data from\r\nmany heterogeneous sources. Manual efforts\r\nfor this task are accurate, but lack scalability,\r\nand automated approaches provide good\r\ncoverage, but are not reliable enough for real world\r\ndecision makers to trust. These two\r\napproaches to KB construction have complementary\r\nstrengths: in this paper we propose\r\na novel framework for supporting human proposed\r\nedits to knowledge bases.", "recorded": "2012-06-07T15:20:00", "title": "Human-Machine Cooperation: User Corrections for AKBC"}, {"url": "ssspr2010_cesme", "desc": "The joint workshops aim at promoting interaction and collaboration among researchers working in areas covered by TC1 and TC2. We are also keen to attract participants working in fields that make use of statistical, structural or syntactic pattern recognition techniques (e.g. image processing, computer vision, bioinformatics, chemo-informatics, machine learning, document analysis, etc.).\r\n\r\nMore about the workshops at [[http://www.rvg.ua.es/ssspr2010/|SSSPR 2010]]", "recorded": "2010-08-18T09:00:00", "title": "Joint IAPR International Workshops on Structural and Syntactic Pattern Recognition (SSPR) and Statistical Techniques in Pattern Recognition (SPR), Cesme 2010"}, {"url": "etvc08_paris", "desc": "This year, **the international colloquium of LIX (Ecole Polytechnique) - organized by Frank Nielsen** - focuses on the emerging trends and challenges of the foundations of the cross-disciplinary area of visual computing. Visual computing encompasses computational geometry, computer graphics, machine vision and learning (just to name a few), and relies at its very heart on information geometry. Visual computing is underpinning major industrial applications as attested recently by the emerging fields of computational photography, 3D cinematography and advanced biomedical imaging.\n\n----\nLink to the official website: http://www.lix.polytechnique.fr/etvc08/\n----", "recorded": "2008-11-18T13:00:00", "title": "Emerging Trends in Visual Computing"}, {"url": "nips2012_le_roux_gradient_method", "desc": "We propose a new stochastic gradient method for optimizing\r\nthe sum of a finite set of smooth functions, where the sum is\r\nstrongly convex. While standard stochastic gradient methods\r\nconverge at sublinear rates for this problem, the proposed\r\nmethod incorporates a memory of previous gradient values\r\nin order to achieve a linear convergence rate. In a machine\r\nlearning context, numerical experiments indicate that the new\r\nalgorithm can dramatically outperform standard algorithms,\r\nboth in terms of optimizing the training error and reducing the\r\ntest error quickly.", "recorded": "2012-12-04T14:50:00", "title": "A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets"}, {"url": "nipsworkshops2012_machart_methods", "desc": "In this paper, we investigate the trade-off between convergence rate and computational cost when minimizing a composite functional with proximal-gradient methods, which are popular optimisation tools in machine learning. We consider the case when the proximity operator is approximated via an iterative procedure, which yields algorithms with two nested loops. We show that the strategy minimizing the computational cost to reach a desired accuracy in finite time is to keep the number of inner iterations constant, which differs from the strategy indicated by a convergence rate analysis.", "recorded": "2012-12-07T16:25:00", "title": "Optimal Computational Trade-Off of Inexact Proximal Methods"}, {"url": "internetofeducation2013_ghani_data", "desc": "This talk will discuss ways in which data, analytics, machine learning, and all other related buzzwords can be used to solve problems with social impact. I'll give examples from a recent summer fellowship program organized at the University of Chicago on \"Data Science for Social Good\" where we worked with non-profits and government organizations to solve problems in education, healthcare, sustainability, public transit, and public safety. I will focus on specific areas in education that are ripe for using data-driven approaches and how that has the potential to improve the current state of education.", "recorded": "2013-11-11T13:00:00", "title": "Using Data for Social Good"}, {"url": "nips09_arbelaitz_dmb", "desc": "This position paper tackles the problem of automatic web personalization using machine learning\r\ntechniques to model the users' behavior. The target population is people with physical, sensory or\r\ncognitive restrictions. In this paper we present our plans to study the possibility of creating user\r\nmodels using the information extracted from web navigation logs by means of data mining methods.\r\nWe discuss the expected advantages of adopting data mining to generate information about the user,\r\nin comparison with traditional methods.", "recorded": "2009-12-10T16:30:00", "title": "Data Mining Based User Modeling Systems for Web Personalization Applied to People with Disabilities"}, {"url": "nipsworkshops09_bilmes_lsgb", "desc": "We consider the issue of scalability of graph-based semi-supervised learning (SSL) algorithms. In this context, we propose a fast graph node ordering algorithm that improves parallel spatial locality by being cache cognizant. This approach allows for a linear speedup on a shared-memory parallel machine to be achievable, and thus means that graph-based SSL can scale to very large data sets. We use the above algorithm an a multi-threaded implementation to solve a SSL problem on a 120 million node graph in a reasonable amount of time. ", "recorded": "2009-12-11T17:35:00", "title": "Large-Scale Graph-based Transductive Inference"}, {"url": "mlss2011_green_bayesian", "desc": "Inference is the process of discovering from data about mechanisms that may have caused or generated that data, or at least explain it. The goals are varied - perhaps simply predicting future data, or more ambitiously drawing conclusions about scientific or societal truths. In the language of applied mathematics, these are inverse problems. Bayesian inference is about using probability to do all this. One of its strengths is that all sources of uncertainty in a problem can be simultaneously and coherently considered. It is model-based (in the language of machine learning, these are generative models), and we can use Bayesian methods to choose and criticize the models we use. ", "recorded": "2011-09-08T09:00:00", "title": "Bayesian Inference"}, {"url": "mlss05us_schapire_b", "desc": "Boosting is a general method for producing a very accurate classification rule by combining rough and moderately inaccurate \"rules of thumb.\" While rooted in a theoretical framework of machine learning, boosting has been found to perform quite well empirically. This tutorial will introduce the boosting algorithm AdaBoost?, and explain the underlying theory of boosting, including explanations that have been given as to why boosting often does not suffer from overfitting, as well as some of the myriad other theoretical points of view that have been taken on this algorithm. Some recent applications and extensions of boosting will also be described.", "recorded": "2005-05-25T00:00:00", "title": "Boosting"}, {"url": "ijcai09_delp_wiig", "desc": "WiiGesture is a gesture recognition program for actions that use accelerometer data. It uses artificial intelligence to classify gestures using a wiimote from a few examples of each gesture. This was a project for a Machine Learning class. Many algorithms were tried, like LCSS, Bagged Trees, SVMs, and Fast Fourier Transforms and the video highlights the one we found most useful (Cross Correlation). This was a really fun project that has real world applications in the video game industry, and I hope it encourages students to consider studying Artificial Intelligence.", "recorded": "2009-05-27T12:00:00", "title": "29. WiiGesture"}, {"url": "bbci2014_berlin", "desc": "Brain-Computer Interfaces (BCIs) and alternative applications of Neurotechnology based on sophisticated data analysis methods have become an active and flourishing field of research. Its attractivity and also its complexity is based on the fact that this area requires concerted expertise and effort of a number of different fields including neurophysiology, machine learning, electrical engineering and psychology.\r\n\r\nWith the aim of fostering interdisciplinary training and exchange, we organize the [[http://bbci.de/winterschool-2014|BBCI Winter School 2014]] on selected topics in BCI and Neurotechnology from February 24th to 28th.", "recorded": "2014-02-24T14:30:00", "title": "BBCI Winter School on Neurotechnology, Berlin 2014"}, {"url": "w3cworkshop2011_pajntar_corpora", "desc": "With the constant growth of web based content large collections of textual become available. Many if not most professional non-English web sites offer translated webpages to English and other languages of their clients and partners. This are usually professional translation and are abundant. We call this Hidden Web. We intend to present possibilities, problems and best practices for harnessing such aligned textual corpora. Such data can then be efficiently used as a translation memory for example as help for a human translators or as training data for machine translation algorithms.", "recorded": "2011-04-05T09:30:00", "title": "Collecting aligned textual corpora from the Hidden Web"}, {"url": "w3cworkshop2011_vasiljevs_languages", "desc": "Small markets, limited language resources, tiny research communities \u2013 these are some of the obstacles in development of technologies for smaller languages. In this presentation we will share experience and best practices from EU collaborative projects with a particular focus on acquiring resources and developing machine translation technologies for smaller languages. Novel methods help to collect more training data for statistical MT, involve users in data sharing and MT customization, collect multilingual terminology and adapt MT to terminology and stylistic requirements of particular applications.", "recorded": "2011-04-05T09:30:00", "title": "Bridging technological gap between smaller and larger languages"}, {"url": "colt2015_paris", "desc": "Learning Theory is a research field devoted to studying the design and analysis of machine learning algorithms. In particular, such algorithms aim at making accurate predictions or representations based on observations.\r\n\r\nThe emphasis in COLT is on rigorous mathematical analysis using techniques from various connected fields such as probability, statistics, optimization, information theory and geometry. While theoretically rooted, learning theory puts a strong emphasis on efficient computation as well.\r\n\r\nFor more information visit the [[http://www.learningtheory.org/colt2015/|COLT 2015 website]].", "recorded": "2015-07-03T00:00:00", "title": "28th Annual Conference on Learning Theory (COLT), Paris 2015"}, {"url": "nips2010_zhu_hlu", "desc": "When the distribution of unlabeled data in feature space lies along a manifold, the information it provides may be used by a learner to assist classification in a semi-supervised setting. While manifold learning is well-known in machine learning, the use of manifolds in human learning is largely unstudied. We perform a set of experiments which test a human's ability to use a manifold in a semi-supervised learning task, under varying conditions. We show that humans may be encouraged into using the manifold, overcoming the strong preference for a simple, axis-parallel linear boundary.", "recorded": "2010-12-09T11:40:00", "title": "Humans Learn Using Manifolds, Reluctantly"}, {"url": "active09_getoor_lm", "desc": "Statistical machine learning is in the midst of a \"relational revolution\".  After many decades of focusing on independent and identically-distributed (iid) examples, researchers are now studying problems in which the examples are linked together into complex networks. These networks can be a simple as sequences and 2-D meshes (such as those arising in part-of-speech tagging and remote sensing) or as complex as the collaboration structures produced by knowledge workers performing a variety of tasks in different contexts within an enterprise.  In this talk, I will give an overview of this newly emerging research, focusing specifically on link mining tasks and algorithms.", "recorded": "2009-09-04T13:45:00", "title": "Link Mining"}, {"url": "mlss2011_cesa_bianchi_learningtheory", "desc": "The theoretical foundations of machine learning have a double nature: statistical and game-theoretic. In this course we take advantage of both paradigms to introduce and investigate a number of basic topics, including mistake bounds and risk bounds, empirical risk minimization, online linear optimization, compression bounds, overfitting and regularization. \r\nThe goal of the course is to provide a sound mathematical framework within which one can investigate basic questions in learning theory, such as the dependence of the predictive performance of a model on the complexity of the model class and on the amount of training information.", "recorded": "2011-09-14T17:15:00", "title": "Learning Theory: statistical and game-theoretic approaches"}, {"url": "eml07_sonnenburg_lme", "desc": "Over the years many different classification methods have been proposed in machine learning. However it is currently very difficult to judge which method is the most efficient with respect to training time and memory requirements and classification performance, which are the practically relevant criteria. A possible explanation for this difficulty is that methods are (often) evaluated under different conditions: For instance different datasets, evaluation criteria, model parameters and stopping conditions are used. We would therefore like to organize a competition, that is designed to be fair and enables a direct comparison of current large scale classifiers. To this end we plan to provide a generic evaluation framework tailored to the specifics of the competing methods, for example for Support Vector Machine classifiers, one would in addition to test-error record the objective value of the primal problem. Providing a wide range of datasets, each of which having specific properties, like extremely sparse, dense, high or low dimensional, we propose to evaluate the methods based on the following figures: training time vs. test error, dataset size vs. test error and dataset size vs. training time. We seek help from the community to gather relevant large-scale real-world data sets and to critically review and discuss fair evaluation criteria and finally invite researchers to co-organize and to participate in this challenge.", "recorded": "2007-12-08T07:30:00", "title": "Learning with Millions of Examples and Dimensions - Competition proposal"}, {"url": "reasecs_pell_nlsw", "desc": "The Semantic Web promises to revolutionize access to information by adding machine-readable semantic information to content which is normally interpretable only by people. In addition, it will also revolutionize access to services by adding semantic information to create machine-readable service descriptions.\nThis ambitious vision has been slow to take off because of a chickenand egg problem. Markup is required before people will build applications, applications are required before it is worth the hard work of doing markup.\nNatural language processing (NLP) has advanced to the point where it can break the impasse and open up the possibilities of the Semantic Web.\nFirst, NLP systems can now automatically create annotations from unstructured text. This provides the data that semantic web applications require. Second, NLP systems are themselves consumers of semantic web information and thus provide economic motivation for people to create and maintain such information. For example, a new generation of natural language search systems, as illustrated by Powerset, can take advantage of semantic web markup and ontologies to augment their interpretation of underlying textual content. They can also expose semantic web services directly in response to natural language queries.\n\nThis presentation was given as a keynote speech as the 6th International Semantic Web Conference (ISWC) 2007. The video of the presentation is provided by courtesy of videolectures.net", "recorded": "2007-12-04T00:00:00", "title": "Natural Language and the Semantic Web"}, {"url": "kdd2010_lee_imla", "desc": "Stroke is the third leading cause of death and the principal\r\ncause of serious long-term disability in the United States.\r\nAccurate prediction of stroke is highly valuable for early intervention\r\nand treatment. In this study, we compare the\r\nCox proportional hazards model with a machine learning\r\napproach for stroke prediction on the Cardiovascular Health\r\nStudy (CHS) dataset. Specifically, we consider the common\r\nproblems of data imputation, feature selection, and prediction\r\nin medical datasets. We propose a novel automatic feature\r\nselection algorithm that selects robust features based\r\non our proposed heuristic: conservative mean. Combined\r\nwith Support Vector Machines (SVMs), our proposed feature\r\nselection algorithm achieves a greater area under the\r\nROC curve (AUC) as compared to the Cox proportional hazards\r\nmodel and L1 regularized Cox model. Furthermore, we\r\npresent a margin-based censored regression algorithm that\r\ncombines the concept of margin-based classifiers with censored\r\nregression to achieve a better concordance index than\r\nthe Cox model. Overall, our approach outperforms the current\r\nstate-of-the-art in both metrics of AUC and concordance\r\nindex. In addition, our work has also identified potential\r\nrisk factors that have not been discovered by traditional\r\napproaches. Our method can be applied to clinical prediction\r\nof other diseases, where missing data are common and\r\nrisk factors are not well understood.", "recorded": "2010-07-28T10:30:00", "title": "An Integrated Machine Learning Approach to Stroke Prediction"}, {"url": "mitworld_coen_gleitman_hsll", "desc": "This workshop, explains Michael Coen, is an effort to engender temperate, collaborative discussion of a matter that inspires hot dispute: whether machine learning helps explain how humans acquire language. In particular, says Coen, machine learning advocates believe they have evidence against Noam Chomsky\u2019s \u201cpoverty of stimulus argument,\u201d which in essence states that language is built into us, that \u201cchildren don\u2019t receive enough linguistic inputs to explain linguistic outputs.\u201d\n\nCoen, who doesn\u2019t think much of such claims, worries about a deeper problem, that scientists have \u201cbegun to discuss engineering at the expense of science.\u201d He describes 13-year-old Bobby Fischer\u2019s astonishing match with a world chessmaster, where Fischer managed to look 16 moves ahead -- eliminating about 10 to the 30th board positions. We had no way to represent his thinking process then, and we don\u2019t today, although scientists have built a machine, Deep Blue, that can topple any human chess champion. It seems there\u2019s nothing left to say about chess, yet we know absolutely nothing about how humans play chess, says Coen. \u201cIf you\u2019re an engineer, this may be fine, but if you\u2019re a scientist, that\u2019s deeply troubling.\u201d\n\nOne problem with machine models, says Lila Gleitman, is that \u201cthey don\u2019t try to learn what the human already knows,\u201d and we really aren\u2019t sure \u201chow big a piece of the pie that is in the first place.\u201d Gleitman distinguishes between acquiring language, and acquiring *a* language, like French or German. In her years of researching how children learn language, and specifically children who have been deprived of linguistic input entirely, Gleitman does not find a blank slate: \u201cChildren don\u2019t just sit there; they start to make gestures.\u201d Gleitman reviews various studies that describe a basic sequence in language acquisition that holds true regardless of specific \u2018inputs.\u2019 If researchers make models that are to be \u201cof any interest, they ought to take into account the fact that you may not have to learn some of this.\u201d\n\nGleitman has conducted simulations with adults, giving them incomplete scenes on video or paper (dropping words or substituting Lewis Carroll type doggerel) to see how we acquire the meaning of common nouns and verbs through contextual clues and inference. The more sources of evidence people get in these tests, the better they do. But such language acquisition \u201cdoesn\u2019t scale up\u201d to higher level categories of words,\u201d such as \u201cthink.\u201d Says Gleitman, \u201cIt\u2019s crazy\u2026to suppose there\u2019s no biological given in a language learning situation. There\u2019s plenty. Some of it is maybe the substance of language and some of that is about the sophisticated learning procedures themselves.\u201d So any kind of \u201cinformative statistical modeling requires a matrix of conspiring cues, intrinsically ordered in time of appearance\u2026Realistic models of incremental learning will incorporate what the learner brings to the task.\u201d", "recorded": "2007-10-19T14:51:28", "title": "Human Simulations of Language Learning"}, {"url": "reasecs_olmedilla_rbprr", "desc": "The Semantic Web aims at enabling sophisticated and autonomic machine to machine interactions without human intervention, by providing machines not only with data but also with its meaning (semantics). In this setting, traditional security mechanisms are not suitable anymore. For example, identity-based access control assumes that parties are known in advance. Then, a machine first determines the identity of the requester in order to either grant or deny access, depending on its associated information (e.g., by looking up its set of permissions). In the Semantic Web, any two strangers can interact with each other automatically and therefore this assumption does not hold. Hence, a semantically enriched process is required in order to regulate an automatic access to sensitive formation. Policy-based access control provides sophisticated means in order to support protecting sensitive resources and information disclosure.\r\n\r\nHowever, the term policy is often overloaded. A general definition might be \"a statement that defines the behaviour of a system''. However, such a general definition encompasses different notions, including security policies, trust management policies, business rules and quality of service specifications, just to name a few. Researchers have mainly focussed on one or more of such notions separately but not on a comprehensive view. Policies are pervasive in web applications and play crucial roles in enhancing security, privacy, and service usability as well. Interoperability and self-describing semantics become key requirements and here is where Semantic Web comes into play. There has been extensive research on policies, also in the Semantic Web community, but there still exist some issues that prevent policy frameworks from being widely adopted by users and real world applications.\r\n\r\nThis document aims at providing an overall view of the state of the art (requirements for a policy framework, some existing policy frameworks/languages, policy negotiation, context awareness, etc.) as well as open research issues in the area (policy understanding in a broad sense, integration of trust management, increase in system cooperation, user awareness, etc.) required to develop a successful Semantic Policy Framework.\r\n\r\nDocuments:\r\n;[[Representation&Reasoning.pdf]]", "recorded": "2007-10-15T00:00:00", "title": "Rule Based Policy Representation & Reasoning for the Semantic Web"}, {"url": "aistats2011_yu_modeling", "desc": "Information technology has enabled collection of massive amounts of data in science, engineering, social science, finance and beyond.  Extracting useful information from massive and high-dimensional data is the focus of today's statistical research and practice.  After broad success of statistical machine learning on prediction through regularization, interpretability is gaining attention and sparsity is being used as its proxy.  With the virtues of both regularization and sparsity, sparse modeling methods (e.g., Lasso) has attracted much attention for theoretial research and for data modeling. ", "recorded": "2011-04-11T08:00:00", "title": "Sparse modeling: some unifying theory and \u201ctopic-imaging\u201d"}, {"url": "icml08_mnih_ebs", "desc": "Sampling is a popular way of scaling up machine learning algorithms to large datasets. The question often is how many samples are needed. Adaptive stopping algorithms monitor the performance in an online fashion and make it possible to stop early, sparing valuable computation time. We concentrate on the setting where probabilistic guarantees are desired and demonstrate how recently-introduced empirical Bernstein bounds can be used to design stopping rules that are efficient. We provide upper bounds on the sample complexity of the new rules as well as empirical results on model selection and boosting in the filtering setting.", "recorded": "2008-07-08T10:40:00", "title": "Empirical Bernstein Stopping"}, {"url": "icml08_roy_spp", "desc": "We use Church, a Turing-universal language\r for stochastic generative processes and the probability\r distributions they induce, to study and extend\r several objects in nonparametric Bayesian statistics.\r We connect exchangeability and de Finetti measures\r with notions of purity and closures from functional\r programming. We exploit delayed evaluation to provide\r finite, machine-executable representations for various\r nonparametric Bayesian objects. We relate common\r uses of the Dirichlet process to a stochastic generalization\r of memoization, and use this abstraction to\r compactly describe and extend several nonparametric\r models. Finally, we briefly discuss issues of computability\r and inference.", "recorded": "2008-07-09T14:30:00", "title": "A stochastic programming perspective on nonparametric Bayes"}, {"url": "icml2015_fetaya_mahalanobis_distances", "desc": "For many tasks and data types, there are natural transformations to which the data should be invariant or insensitive. For instance, in visual recognition, natural images should be insensitive to rotation and translation. This requirement and its implications have been important in many machine learning applications, and tolerance for image transformations was primarily achieved by using robust feature vectors. In this paper we propose a novel and computationally efficient way to learn a local Mahalanobis metric per datum, and show how we can learn a local invariant metric to any transformation in order to improve performance.", "recorded": "2015-07-08T14:46:53", "title": "Learning Local Invariant Mahalanobis Distances"}, {"url": "mlg07_getoor_gi", "desc": "Within the machine learning community, there has been a growing interest in learning structured models from input data that is itself structured. Graph identification refers to methods that transform an observed input graph into an inferred output graph. Examples include inferring organizational hierarchies from social network data and identifying gene regulatory networks from protein-protein interactions. The key processes in graph identification are entity resolution, link prediction, and collective classification. I will overview algorithms for these tasks and discuss the need for integrating the results to solve the overall problem collectively. ", "recorded": "2007-08-02T14:30:00", "title": "Graph Identification"}, {"url": "ecmlpkdd2010_barcelona", "desc": "This event builds upon a very successful series of 20 ECML and 13 PKDD conferences, the last nine editions of which have been organized jointly.\r\n\r\nECML PKDD has become the major European scientific event in these fields and in 2010 it comprises presentations of contributed papers and invited speakers, a wide program of workshops and tutorials, a discovery challenge, a demo track and an industrial track.\r\n\r\nDetailed information about the conference can be found at [[http://www.ecmlpkdd2010.org/|ECML PKDD 2010]].", "recorded": "2010-09-20T09:00:00", "title": "European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD), Barcelona 2010"}, {"url": "mcvc08_cannes", "desc": "The Conference is open to the scientific community at large, including the whole Muscle Community, the European Commission, as well as all EU funded projects.\r This event will be an opportunity for MUSCLE NoE to highlight and demonstrate to all, the major results achieved by the members of its community in the field of Multimedia Understanding through Semantics, Computation and Machine Learning.\r The MUSCLE scientific conference will be co-chaired by Nozha Boujemaa and Eric Pauwels\r In addition to the keynote talks, this event includes MUSCLE e-Team presentations and showcase demonstrations as well as presentations from other EU-funded projects.", "recorded": "2008-02-11T10:00:49", "title": "MUSCLE Conference joint with VITALAS Conference"}, {"url": "mlss09us_schapire_tab", "desc": "Boosting is a general method for producing a very accurate classification rule by combining rough and moderately inaccurate \"rules of thumb\". While rooted in a theoretical framework of machine learning, boosting has been found to perform quite well empirically. This tutorial will introduce the boosting algorithm AdaBoost, and explain the underlying theory of boosting, including explanations that have been given as to why boosting often does not suffer from overfitting, as well as some of the myriad other theoretical points of view that have been taken on this algorithm. Some practical applications and extensions of boosting will also be described. ", "recorded": "2009-06-05T08:30:00", "title": "Theory and Applications of Boosting"}, {"url": "www2010_perez_aguera_bss", "desc": "Information Retrieval (IR) approaches for semantic web search engines have become very popular in the last years. Popularization of different IR libraries, like Lucene, that allows IR implementations almost out-of-the-box have make easier IR integration in Semantic Web search engines. However, one of the most important features of Semantic Web documents is the structure, since this structure allow us to represent semantic in a machine readable format. In this paper we analyze the specific problems of structured IR and how to adapt weighting schemas for semantic document retrieval.", "recorded": "2010-04-26T10:25:00", "title": "Using BM25F for Semantic Search"}, {"url": "icml2015_goernitz_anomaly_detection", "desc": "We introduce a new anomaly detection methodology for data with latent dependency structure. As a particular instantiation, we derive a hidden Markov anomaly detector that extends the regular one-class support vector machine. We optimize the approach, which is non-convex, via a DC (difference of convex functions) algorithm, and show that the parameter v can be conveniently used to control the number of outliers in the model. The empirical evaluation on artificial and real data from the domains of computational biology and computational sustainability shows that the approach can achieve significantly higher anomaly detection performance than the regular one-class SVM.", "recorded": "2015-07-08T14:46:53", "title": "Hidden Markov Anomaly Detection"}, {"url": "coinactivess2010_warren_ai", "desc": "ACTIVE is an EU Integrating Project which involves 12 partners in 7 countries. ACTIVE is coordinated by BT and managed by Eurescom. The project duration is from March 2008 to February 2011.\r\n\r\nACTITVE has 3 case studies:\r\n;- telecoms\r\n;- consultancy\r\n;-electronics design.\r\n\r\nACTIVE is using informal semantics and machine intelligence to:\r\n;- combat information overload\r\n;- support knowledge work processes\r\n;- aid knowledge sharing.\r\n\r\nIt is combining the low user-barriers of Web2.0 with the power of semantic technology. ACTIVE is being trialled in large-scale enterprise environments. Prototype available for trial from ACTIVE website:\r\nhttp://www.active-project.eu", "recorded": "2010-10-18T09:56:00", "title": "ACTIVE Introduction"}, {"url": "mit6046jf05_leiserson_lec15", "desc": "//\"So, the topic today is dynamic programming. The term programming in the name of this term doesn't refer to computer programming. OK, programming is an old word that means any tabular method for accomplishing something. So, you'll hear about linear programming and dynamic programming. Either of those, even though we now incorporate those algorithms in computer programs, originally computer programming, you were given a datasheet and you put one line per line of code as a tabular method for giving the machine instructions as to what to do...\"//", "recorded": "2005-11-07T09:00:00", "title": "Lecture 15: Dynamic Programming, Longest Common Subsequence"}, {"url": "sikdd2011_tomasev_oceanographic", "desc": "In this paper we examine how the high dimensionality of oceanographic sensor data impacts the potential use of nearest-neighbor machine learning methods. We focus on one particular consequence of the curse of dimensionality \u2013 hubness. We examine the hubness of oceanographic data and show how it can be used to visualize and detect both prototypical sensors/locations, as well as ambiguous and\r\npotentially erroneous ones. We proceed to define an easy classification problem on the data, showing that the recently developed hubness-aware classification methods may help to overcome some of the hubness-related issues in sensor data.", "recorded": "2011-10-10T15:20:00", "title": "Exploring the hubness-related properties of oceanographic sensor data"}, {"url": "icml08_sonnenburg_ipsvm", "desc": "Support vector machine training can be represented as a large quadratic program.\r We present an efficient and numerically stable algorithm for this problem using primal-dual interior point methods. Reformulating the problem to exploit separability of the Hessian eliminates the main source of computational complexity, resulting in an algorithm which requires only O(n) operations per iteration. Extensive use of L3 BLAS functions enables good parallel efficiency on shared-memory processors. As the algorithm works in primal and dual spaces simultaneously, our approach has the advantage of obtaining the hyperplane weights and bias directly from the solver.", "recorded": "2008-07-09T16:00:00", "title": "Interior Point SVM"}, {"url": "coa08_whistler", "desc": "Machine learning has traditionally been focused on prediction. Given observations that have been generated by an unknown stochastic dependency, the goal is to infer a law that will be able to correctly predict future observations generated by the same dependency. Statistics, in contrast, has traditionally focused on data modeling, i.e., on the estimation of a probability law that has generated the data. During recent years, the boundaries between the two disciplines have become blurred and both communities have adopted methods from the other, however, it is probably fair to say that neither of them has yet fully embraced the field of causal modeling, i.e., the detection of causal structure underlying the data. Since the Eighties there has been a community of researchers, mostly from statistics and philosophy, who have developed methods aiming at inferring causal relationships from observational data. While this community has remained relatively small, it has recently been complemented by a number of researchers from machine learning.\n\nThe goal of this workshop is to discuss new approaches to causal discovery from empirical data, their applications and methods to evaluate their success. Emphasis will be put on the definition of objectives to be reached and assessment methods to evaluate proposed solutions. The participants are encouraged to participate in a \"\"competition pot-luck\"\" in which datasets and problems will be exchanged and solutions proposed.\n\nMore information about the workshop can be found [[http://clopinet.com/isabelle/Projects/NIPS2008|here]].", "recorded": "2008-12-12T07:30:00", "title": "NIPS Workshop on Causality: Objectives and Assessment, Whistler 2008"}, {"url": "bbci2012_schalk_neuroscience", "desc": "The intersection of signal processing/machine learning, computer science, material engineering and neuroscience is beginning to open up exciting opportunities for important advances in systems and cognitive neuroscience and in translational neuroengineering.  Our work over the past 15 years has focused on taking advantages of these opportunities. Our neuroscience research investigates the neural basis of motor, language, and cognitive function by applying computational techniques to recordings from the surface of the brain (electrocorticography (ECoG)) in humans.  For example, we study how local field potentials in different cortical areas prepare for and execute hand or finger movements.  Our neuroengineering research is taking advantage of the resulting neuroscientific understanding and aims to address particular clinical problems.  This work includes statistical signal processing, machine learning, and real-time system design and implementation.  For example, we have been developing a new real-time imaging technique for invasive brain surgery. In this talk, I will describe the types of signals that can be detected in ECoG and the emerging understanding of their physiological origin.  I will then demonstrate that ECoG encodes detailed aspects of function, such as actual or imagined speech.  Finally, I will show demonstrations of ECoG-based communication and control, and of our real-time passive functional mapping technique.  Overall, this talk aims to communicate the substantial research and emerging commercial opportunities that arise from integration of neuroscience and neuroengineering, and hopes to inspire the neurotechnology community to participate in them.", "recorded": "2012-09-19T09:00:00", "title": "ECoG-Based Neuroscience and Neuroengineering"}, {"url": "nips05_whistler", "desc": "**Kernel Methods and Structured Domains**\r\n\r\nSubstantial recent work in machine learning has focused on the problem of dealing with inputs and outputs on more complex domains than are provided for in the classical regression/classification setting. Structured representations can give a more informative view of input domains, which is crucial for the development of successful learning algorithms: application areas include determining protein structure and protein-protein interaction; part-of-speech tagging; the organization of web documents into hierarchies; and image segmentation. Likewise, a major research direction is in the use of structured output representations, which have been applied in a broad range of areas including several of the foregoing examples (for instance, the output required of the learning algorithm may be a probabilistic model, a graph, or a ranking). \r\n\r\n**Large Scale Kernel Machines**\r\n\r\nDatasets with millions of observations can be gathered by crawling the web, mining business databases, or connecting a cheap video tuner to a laptop. Vastly more ambitious learning systems are theoretically possible. The literature shows no shortage of ideas for sophisticated statistical models. The computational cost of learning algorithms is now the bottleneck. During the last decade, dataset size has outgrown processor speed. Meanwhile, machine learning algorithms became more principled, and also more computationally expensive.", "recorded": "2005-12-09T00:00:00", "title": "NIPS Workshop on Kernel Methods and Structured Domains / NIPS Workshop on Large Scale Kernel Machines, Whistler 2005"}, {"url": "ecmlpkdd2011_bauckhage_kersting_matrices", "desc": "Low-rank approximations of data matrices have become an important tool in machine learning and data mining. They allow for embedding high dimensional data in lower dimensional spaces and can therefore mitigate effects due to noise, uncover latent relations, or facilitate further processing. These properties have been proven successful in many applications areas such as bio-informatics, computer vision, text process ing, recommender systems, social network analysis, among others. Present day technologies are characterized by exponentially growing amounts of data. Recent advances in sensor technology, Internet applications, and communication networks call for methods that scale to very large and/or growing data matrices. In this tutorial, we discuss basic characteristics of matrix factorization and introduce several recent approaches that scale to modern massive data analysis problems.\r\n\r\nThe tutorial aims at a wide audience as it reviews both machine learning and data mining techniques. It is intended for PhD students, practitioners, and researchers who are interested in large scale machine learning and data analysis.\r\n\r\nThe tutorial is divided into three parts:\r\n\r\n    * Part I: Matrix Factorization \u2014 Traditional Optimization Approaches and Statistical Foundations: In this block, we will discuss foundations and multi-linear extensions of traditional methods such as SVD, PCA, K-Means, and Vector Quantization.\r\n    * Part II: Constraint Matrix Factorization Many real-world applications of matrix factorization impose constraints on the factorization problem. For instance, matrix factors need to be non-negative, convex combinations of existing data, or compact binary codes. Among others, we discuss techniques such as Spectral Hashing, NMF, Archetypal Analysis, CNMF, and CH-NMF.\r\n    * Part III: Data-driven Matrix Factorization Techniques: The first and second part of the tutorial consider norm minimization problems to obtain suitable matrix factors. Recent approaches that extend matrix factorization towards massive data assume a different point of view: they attempt to maximize the volume of a selection of rows and columns of a given data matrix. In this final part of the tutorial, we present and review approaches such as FastMap, CUR, CMD, and SiVM.\r\n\r\nIn each of the parts, we present practical applications from fields such as image processing, computer vision, robotics, web mining, and social media analysis.", "recorded": "2011-09-05T14:00:00", "title": "Factorizing Gigantic Matrices"}, {"url": "ecmlpkdd2012_bristol", "desc": "This event builds upon a very successful series of 22 ECML and 15 PKDD conferences, which have been jointly organized for the past 11 years.\r\n\r\nECML-PKDD is the prime European scientific event in these fields. It will feature presentations of contributed papers and invited speakers, a wide program of workshops and tutorials on the first and last days, a discovery challenge, and a DINe track with demo, industry, and \u2018nectar\u2019 talks.\r\n\r\nDetailed information about the conference can be found at [[http://www.ecmlpkdd2012.net/|ECML PKDD 2012]].", "recorded": "2012-09-24T09:00:00", "title": "European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD), Bristol 2012"}, {"url": "sahd2014_london", "desc": "[[http://www.ee.ucl.ac.uk/sahd2014/index.html|The first UCL-Duke University Workshop on Sensing and Analysis of High-Dimensional Data]], which acts as the European counterpart of the Biannual Duke University SAHD Workshop, aims to bring together leading researchers in the general fields of mathematics, statistics, computer science and engineering that work at the intersection of computational statistics, machine learning, signal processing, information and learning theory, and computer science, with the goal to advance the field of sensing, analysis and processing of high-dimensional data.", "recorded": "2014-09-04T09:00:00", "title": "1st UCL-Duke University Workshop on Sensing and Analysis of High-Dimensional Data (SAHD), London 2014"}, {"url": "eswc2011_tori_consumer", "desc": "Do you like content farms? You are helping them succeed. Every improvement in open structured data, machine learning and NLP gives rouge web players an ammo in the battle against search engines and users. On the other hand many owners of the curated and high quality data aren't (yet) reaping benefits from those technologies. At time of this writing Demand Media has the same market cap as New York Times. So let's look at the good, the bad and the ugly on the web. Who are real beneficiaries of semantic web and related tech? Where are they hiding?", "recorded": "2011-06-01T14:00:00", "title": "Quantity vs. quality on the consumer web"}, {"url": "eml07_lecun_int", "desc": "His lab has projects in computer vision, object detection, object recognition, mobile robotics, bio-informatics, biological image analysis, medical signal processing, signal processing, and financial prediction,...The Videolectures.Net team talked to him at NIPS 2007, we asked him stuff like: *What is your current topic of research? *How can you comment on your humoristic approach in giving lectures? *Humor and content? *What happend in your research between ML Summer school 2006 in Chicago and today? *How can you explain your work to a 4 year old child? *Machine Learning dream come true.... *What is your philosophy?", "recorded": "2007-12-08T12:00:00", "title": "Interview with Yann LeCun"}, {"url": "iccc2014_elgammal_metric_learning", "desc": "We approach the challenging problem of discovering influences between painters based on their \r\nfine-art paintings. In this work, we focus on comparing paintings of two painters in terms of visual similarity.  This  comparison  is  fully  automatic  and  based  on  computer  vision  approaches  and \r\nmachine learning. We investigated different visual features and similarity measurements based on \r\ntwo  different metric  learning  algorithms  to  find  the most  appropriate  ones  that  follow  artistic \r\nmotifs. We  evaluated  our  approach  by  comparing  its  result with  ground  truth  annotation  for  a \r\nlarge collection of fine-art paintings. ", "recorded": "2014-06-11T17:30:00", "title": "Knowledge Discovery of Artistic Influences: A Metric Learning Approach"}, {"url": "mcslw04_vert_locs", "desc": "In this talk, I will present an analysis of the asymptotic behaviour of the One-Class support vector machine (SVM), a popular algorithm for outlier detection. I will show that One-Class SVM asymptotically estimates a truncated version of the density of the distribution generating the data, in the case where the Gaussian kernel is used with a well-calibrated decreasing bandwidth parameter, and the regularization parameter involved in the algorithm is held fixed as the training sample size goes to infinity.A long version of this work can be found at www.lri.fr/~vert/Publi/regularizeGaussianKernel.ps , in which extensions to the 2-class case and to more general convex loss functions are considered.", "recorded": "2005-10-03T16:30:00", "title": "The Limit of One-Class SVM"}, {"url": "sikdd2014_kovacic_government_transparency", "desc": "This paper presents usage of Supervizor, an online \r\napplication that provides information on financial \r\ntransactions of the public sector bodies. Supervizor \r\ncontains by now 50 mio. transactions from both \r\ngovernment and local agencies to government \r\ncontractors from 2003 to 2014 and matches such \r\ntransactions to company records from the Business \r\nRegister including director lists and corporate \r\nleadership. The application, which has been designed \r\nand developed by the Commission for the Prevention of \r\nCorruption, won the UN Public Service Award in 2013, \r\nan important recognition of excellence in public service. \r\nThe data on transactions from Supervizor are also \r\nprovided in machine readable form.", "recorded": "2014-10-06T14:30:00", "title": "Government Transparency Through Technology"}, {"url": "colt2014_karnin_learning", "desc": "Numerous machine learning problems require an exploration basis - a mechanism to explore the action space. We define a novel geometric notion of exploration basis with low variance called volumetric spanners, and give efficient algorithms to construct such bases. We show how efficient volumetric spanners give rise to an efficient and near-optimal regret algorithm for bandit linear optimization over general convex sets. Previously such results were known only for specific convex sets, or under special conditions such as the existence of an efficient self-concordant barrier for the underlying set.", "recorded": "2014-06-15T12:40:00", "title": "Volumetric Ellipsoids: An exploration basis for learning"}, {"url": "kdd2014_li_stochastic_optimization", "desc": "Stochastic gradient descent (SGD) is a popular technique for large-scale optimization problems in machine learning. In order to parallelize SGD, minibatch training needs to be employed to reduce the communication cost. However, an increase in minibatch size typically decreases the rate of convergence. This paper introduces a technique based on approximate optimization of a conservatively regularized objective function within each minibatch. We prove that the convergence rate does not decrease with increasing minibatch size. Experiments demonstrate that with suitable implementations of approximate optimization, the resulting algorithm can outperform standard SGD in many scenarios.", "recorded": "2014-08-26T15:30:00", "title": "Efficient Mini-batch Training for Stochastic Optimization"}, {"url": "mlss2011_teh_nonparametrics", "desc": "Machine learning researchers often have to contend with issues of model selection and model fitting in the context of large complicated models and sparse data. The idea which I am pushing for in this project is that these can be nicely handled using Bayesian techniques.\r\n\r\nModel selection is selecting, among a class of models each of which has finite capacity, the model of the right capacity. Nonparametric Bayesian modelling sidesteps model selection by simply using models of potentially unbounded (or infinite) capacity. Overfitting is avoided simply by the usual Bayesian approach of integrating out all parameters (perhaps using MCMC or variational methods).", "recorded": "2011-09-12T09:00:00", "title": "Bayesian Nonparametrics"}, {"url": "ecmlpkdd2010_agarwal_gvc", "desc": "In Bayesian machine learning, conjugate priors are popular, mostly due to mathematical convenience. In this paper, we show that there are deeper reasons for choosing a conjugate prior. Specifically, we formulate the conjugate prior in the form of Bregman divergence and show that it is the inherent geometry of conjugate priors that makes them appropriate and intuitive. This geometric interpretation allows one to view the hyperparameters of conjugate priors as the effective  sample points, thus providing additional intuition. We use this geometric understanding of conjugate priors to derive the hyperparameters and expression of the prior used to couple the generative and discriminative components of a hybrid model for semi-supervised learning. ", "recorded": "2010-09-22T11:11:00", "title": "A Geometric View of Conjugate Priors"}, {"url": "nc04_geer_svmlp", "desc": "We consider an i.i.d. sample from (X,Y), where X is a feature and Y a binary label, say with values +1 or -1. We use a high-dimensional linear approximation of the regression of Y on X and support vector machine loss with l1 penalty on the regression coefficients. This procedure does not depend on the (unknown) noise level or on the (unknown) sparseness of approximations of Bayes rule, but nevertheless its prediction error is smaller for smaller noise levels and/or sparser approximations. Thus, it adapts to unknown properties of the underlying distribution. In an example, we show that up to terms logarithmic in the sample size, the procedure yields minimax rates for the excess risk.", "recorded": "2004-10-07T00:00:00", "title": "Support vector machines loss with l1 penalty"}, {"url": "aispds08_kappen_easop", "desc": "Stochastic optimal control theory is a principled approach to compute optimal actions with delayed rewards. The use of this approach in AI and machine learning has been limited due to the computational intractabilities. In this talk, I introduce a class of control problems where the intractabilities appear as the computation of a partition sum, as in a statistical mechanical system. This opens the possibility to study phase transitions and to apply exisiting approximation methods such as BP and the variational method to optimal control theory. The talk gives a gentle introduction into control theory and illustrates these new phenomena with a number of examples.", "recorded": "2008-05-29T09:30:00", "title": "An efficient approach to stochastic optimal control"}, {"url": "mlss05au_aberdeen_rl", "desc": "Reinforcement learning is about learning good control policies given only weak performance feedback: occasional scalar rewards that might be delayed from the events that led to good performance. Reinforcement learning inherently deals with feedback systems rather than (data, class) data samples, providing a more flexible control-like framework than many standard machine algorithms. These lectures will summarise reinforcement learning along 3 axes: # Learning with or without knowledge of the system dynamics. # Using state values as an intermediate solution, or learning a policy directly. # Learning with or without fully observable system states.", "recorded": "2005-02-02T00:00:00", "title": "Reinforcement Learning"}, {"url": "mlcued08_azran_mcl", "desc": "**Machine Learning Tutorial Lecture** Spectral clustering is a technique for finding group structure in data. It is based on viewing the data points as nodes of a connected graph and clusters are found by partitioning this graph, based on its spectral decomposition, into subgraphs that posses some desirable properties. My plan for this talk is to give a review of the main spectral clustering algorithms, demonstrate their abilities and limitations and offer some insight into when the method can be expected to be successful. No previous knowledge is assumed, and anyone who is interested in clustering (or fun applications of linear algebra) might find this talk interesting.", "recorded": "2007-10-25T16:00:00", "title": "Spectral Clustering"}, {"url": "nipsworkshops2010_taylor_mkt", "desc": "We apply methods of multiple kernel learning to the problem of system identification\r\nfor multi-dimensional temporal data. Rather than building a full probabilistic\r\nmodel, we take a computationally simple approach that uses out of the\r\nbox machine learning methods. We attempt to learn the covariance function of a\r\nstochastic process via multiple kernel learning. We achieve promising preliminary\r\nresults and the work suggests an abundance of future theoretical work. We hope\r\nto draw on the theory of SVM methods to give a principled learning theory style\r\ndescription of system identification in stochastic processes.", "recorded": "2010-12-11T16:30:00", "title": "Multiple Kernel Testing for SVM-based System Identification"}, {"url": "mla09_bontempi_otuosl", "desc": "A crucial issue in the design of complex systems is the evaluation of a large number of potential alternative designs. A too expensive evaluation procedure can consequently slow down the search for good configurations mainly in the case of high dimensional parameter spaces. The talk will discuss the use of machine learning techniques for speeding up the evaluation and the exploration of large design spaces. In particular, two supervised learning techniques, feedforward neural networks and lazy learning, are assessed and compared in the task of accelerating the design of a heat-pipe, a cooling \r\ndevice commonly used in aeronautics and electronics.", "recorded": "2009-07-04T11:00:00", "title": "On the Use of Supervised Learning Techniques to Speed up the Design of Aeronautics Components"}, {"url": "icml08_caron_sbnr", "desc": "One of the most common problems in machine learning and statistics consists of estimating the mean response X.beta from a vector of observations y assuming y=X.beta+epsilon where X is known, beta is a vector of parameters of interest and epsilon a vector of stochastic errors. We are particularly interested here in the case where the dimension K of beta is much higher than the dimension of y. We propose some flexible Bayesian models which can yield sparse estimates of beta. We show that as K tends to infinity, these models are closely related to a class of Levy processes. Simulations demonstrate that our models outperform significantly a range of popular alternatives.", "recorded": "2008-07-07T13:55:00", "title": "Sparse Bayesian Nonparametric Regression"}, {"url": "russir09_agichtein_mubi", "desc": "Hundreds of millions of users search the web daily, clicking on the results, submitting and refining queries and otherwise interacting with the search engines. The vast amount of information generated as a by-product of these interactions can be mined to dramatically improve the effectiveness of web search, and information access in general.\r\nThis course will survey the research in modeling user behavior in web search, and how this information can improve web search effectiveness. The emphasis will be on learning and analyzing the appropriate data mining and machine learning techniques for the user behavior and interaction data, and on the integration of the behavioral models into the search engine operation. ", "recorded": "2009-09-11T14:45:00", "title": "Modeling Web Searcher Behavior and Interactions "}, {"url": "wsnsme2010_smolnikar_vsn", "desc": "Versatile Sensor Node (VSN) is a high performance sensor network platform with modular structure, long-life autonomy and flexible radio. Wireless interface spans over several industrial, scientific and medical frequency bands and supports multiple communication technologies, including ZigBee, 6LoWPAN, Bluetooth and WiFi. Various sensors and actuators can be connected via digital and analog peripherals, which makes VSN adaptable to diverse application requirements. By supporting semantic technologies and intelligent machine learning algorithms it provides a transparent infrastructure in which sensors are offered as a service.\r\n\r\n\r\n", "recorded": "2010-05-20T11:57:00", "title": "Versatile Sensor Node - A Platform for the Sensor as a Service Concept"}, {"url": "mlss06tw_langford_mlr", "desc": "There are several different classification problems commonly encountered in real world applications such as 'importance weighted classification', 'cost sensitive classification', 'reinforcement learning', 'regression' and others. Many of these problems can be related to each other by simple machines (reductions) that transform problems of one type into problems of another type. Finding a reduction from your problem to a more common problem allows the reuse of simple learning algorithms to solve relatively complex problems. It also induces an organization on learning problems \u2014 problems that can be easily reduced to each other are 'nearby' and problems which can not be so reduced are not close.", "recorded": "2006-07-24T00:00:00", "title": "Machine Learning Reductions"}, {"url": "eccv2012_parikh_attributes", "desc": "Traditional active learning allows a (machine) learner to query the (human) teacher for labels on examples it finds confusing. The teacher then provides a label for only that instance. This is quite\r\nrestrictive. In this paper, we propose a learning paradigm in which the learner communicates its belief (i.e. predicted label) about the actively chosen example to the teacher. The teacher then confirms or\r\nrejects the predicted label. More importantly, if rejected, the teacher communicates an explanation for why the learner's belief was wrong. This explanation allows the learner to propagate the feedback\r\nprovided by the teacher to many unlabeled images. This allows a classifier to better learn from its mistakes, leading to accelerated discriminative learning of visual concepts even with few labeled images. In order for such communication to be feasible, it is crucial to have a language that both the human supervisor and the machine learner understand. Attributes provide precisely this channel. They\r\nare human-interpretable mid-level visual concepts shareable across categories e.g. furry , spacious , etc. We advocate the use of attributes for a supervisor to provide feedback to a classifier and directly communicate his knowledge of the world. We employ a straightforward approach to incorporate this feedback in the classifier, and demonstrate its power on a variety of visual recognition scenarios such as image classification and annotation. This application of attributes for providing classifiers feedback is very powerful, and has not been explored in the community. It introduces a new mode of supervision, and opens up several avenues for future\r\nresearch.", "recorded": "2012-10-09T12:20:00", "title": "Attributes for classifier feedback"}, {"url": "cyberstat2012_granada", "desc": "The topic of the present workshop is stochastic optimal control theory and its relations to machine learning and robotics, statistical mechanics, quantum theory and the theory of large deviations.\r\n\r\nFor many years, the deterministic control theory has dominated control applications in robotics and autonomous systems, mainly because of computational restrictions. Relatively recently, there have been several approaches to restate the stochastic optimal control computation as an inference problem and to obtain efficient solutions using approximate inference. In these control theories, concepts from classical mechanics and control theory (variational calculus and Hamilton-Jacobi equations) and stochastic processes and large deviations theory (Feynman-Kac formula) are intimately related. This approach provides novel insights for the design of efficient algorithms to efficiently compute optimal stochastic control solutions in robotics.\r\n\r\nThe Hamilton Jacobi equation also plays a crucial role in the computation of non-equilibrium large deviations. In recent developments, this large deviation theory has provided a rather general framework in which the macrobehavior of non-equilibrium systems can be studied. In addition, there is a connection between these control formulations and Nelsons stochastic mechanics, which aims to provide a particle interpretation of quantum mechanics.\r\n\r\nThis workshop brings together researchers from control theory, machine learning, physics and mathematics to explore these connections.\r\n\r\nTo find out more please visit the [[http://www.snn.ru.nl/cyberstat_granada/index.html|workshop\u00b4s homepage]].", "recorded": "2012-09-12T09:30:00", "title": "Workshop on Statistical Physics of Inference and Control Theory, Granada 2012"}, {"url": "nips2010_parkes_iml", "desc": "In the economic theory of mechanism design, the goal is to elicit private information from each of multiple agents in order to select a desirable system wide outcome, and despite agent self-interest in promoting individually beneficial outcomes. Auctions provide a canonical example, with information elicited in the form of bids, and an allocation of resources and payments defining an outcome. Indeed, one aspect of the emerging interplay between machine learning (ML) and mechanism design (MD) arises by interpreting auctions as a method for learning agent valuation functions. In addition to seeking sufficient accuracy to support optimal resource allocation, we require for incentive compatibility that prices are insensitive to the inputs of any individual agent and find an interesting connection with regularization in statistical ML. More broadly, ML can be used for de novo design, in learning payment rules with suitable incentive properties. Ideas from MD are also flowing into ML. One example considers the use of mechanisms to elicit private state, reward and transition models, in enabling coordinated exploration and exploitation in multi-agent systems despite self-interest. Another application is to supervised learning, where labeled training data is elicited from self-interested agents, each with its own objective criterion on the hypothesis learned by the mechanism. Looking ahead, a tantalizing challenge problem is to adopt incentive mechanisms for the design of robust agent architectures, for example in assigning internal rewards that promote modular intelligent systems. ", "recorded": "2010-12-08T08:30:00", "title": "The Interplay of Machine Learning and Mechanism Design"}, {"url": "mitworld_technology_day08", "desc": "This year's Technology Day taps into the fundamental human fascination with the universe. In \"Out of This World,\" faculty will explore the cosmos, human efforts to probe deeper into space, and human-machine interactions here on Earth. Physics professor Max Tegmark discussed his work on precision cosmology; aeronautics and astronautics professor Dava Newman commented on the challenges of sending humans to Mars; and Media Lab professor Cynthia Breazeal explored the world of humanoid robots.\n\n**About the Host** - **[[http://alum.mit.edu/|MIT\nAlumni Association]]**\n\n**This MIT World Series** is available at http://mitworld.mit.edu/series/view/133", "recorded": "2008-06-07T09:00:00", "title": "Technology Day 2008 - Out of this World"}, {"url": "mlsb2010_szaboova_pdna", "desc": "We use logic-based machine learning to distinguish DNAbinding\r\nproteins from non-binding proteins. We combine previously suggested\r\ncoarse-grained features (such as the dipole moment) with automatically\r\nconstructed structural (spatial) features. Prediction based only\r\non structural features already improves on the state-of-the-art predictive\r\naccuracies achieved in previous work with coarse-grained features.\r\nAccuracies are further improved when the combination of both feature\r\ncategories is used. An important factor contributing to accurate prediction\r\nis that structural features are not Boolean but rather interpreted by\r\ncounting the number of their occurences in a learning example.", "recorded": "2010-10-16T10:20:00", "title": "Prediction of DNA-binding proteins from structural features"}, {"url": "nipsworkshops2012_villa_methods", "desc": "Proximal gradient methods are popular first order algorithms currently used to solve several machine learning and inverse problems. We consider the case where the proximity operator is not available in closed form and is thus approximated via an iterative procedure leading to a nested algorithm. For the first time, we show that relying on an appropriate notion of approximations, which gives an explicit\r\nstopping rule for the inner loop, convergence rates for the two-loops algorithm can be proved for accelerated procedures for a large class of approximation algorithms. An experimental comparison with a benchmark primal-dual algorithm is reported and confirms a good empirical performance.", "recorded": "2012-12-08T17:55:00", "title": " Convergence rates of nested accelerated inexact proximal methods"}, {"url": "ecmlpkdd09_marinescu_mepsv", "desc": "The long term goal of this work is to develop models of operatic\r\nsingers and use them to generate expressive performances similar in\r\nvoice quality and style with what original performances by those singers\r\nwould sound like. This paper focuses on learning timing models of expressive\r\nperformance by using high-level descriptors extracted from existing\r\naudio recordings. Our approach is based on applying machine learning\r\nto discover singer-specific timing patterns of expressive singing based\r\non existing performances. The experimental results show a significant\r\ncorrelation between the note durations of real performances and those\r\npredicted by our model.", "recorded": "2009-09-07T15:31:00", "title": "Modeling Expressive Performances of the Singing Voice"}, {"url": "icml09_gieseke_femm", "desc": "The maximum margin clustering approach is a recently proposed extension of the concept of support vector machines to the clustering problem. Briefly stated, it aims at finding an optimal partition of the data into two classes such that the margin induced by a subsequent application of a support vector machine is maximal. We propose a method based on stochastic search to address this hard optimization problem. While a direct implementation would be infeasible for large data sets, we present an efficient computational shortcut for assessing the ``quality'' of intermediate solutions. Experimental results show that our approach outperforms existing methods in terms of clustering accuracy.", "recorded": "2009-06-16T11:45:00", "title": "Fast Evolutionary Maximum Margin Clustering"}, {"url": "mlss05us_langford_mlr", "desc": "There are several different classification problems commonly encountered in real world applications such as 'importance weighted classification', 'cost sensitive classification', 'reinforcement learning', 'regression' and others. Many of these problems can be related to each other by simple machines (reductions) that transform problems of one type into problems of another type. \r \r Finding a reduction from your problem to a more common problem allows the reuse of simple learning algorithms to solve relatively complex problems. It also induces an organization on learning problems \u2014 problems that can be easily reduced to each other are 'nearby' and problems which can not be so reduced are not close.", "recorded": "2005-05-17T00:00:00", "title": "Tutorial on Machine Learning Reductions"}, {"url": "colt2015_spielman_laplacian_matrices", "desc": "The Laplacian matrices of graphs arise in many fields including Machine Learning, Computer Vision, Optimization, Computational Science, and of course Network Analysis.  We will explain what these matrices are and why they arise in so many applications.  We then will survey recent progress on the design of algorithms that allow us to solve such systems of linear equations in nearly linear time.\r\nIn particular, we will show how fast algorithms for graph sparsification directly lead to fast Laplacian system solvers.  As an application, we will explain how Laplacian system solvers can be used to quickly solve linear programs arising from natural graph problems.", "recorded": "2015-07-04T11:15:00", "title": "Laplacian Matrices of Graphs: Algorithms and Applications"}, {"url": "kdd2010_jahrer_cpar", "desc": "We analyze the application of ensemble learning to recommender systems on the Netflix Prize dataset. For our analysis we use a set of diverse state-of-the-art collaborative filtering (CF) algorithms, which include: SVD, Neighborhood Based Approaches, Restricted Boltzmann Machine, Asymmetric Factor Model and Global Effects. We show that linearly combining (blending) a set of CF algorithms increases the accuracy and outperforms any single CF algorithm. Furthermore, we show how to use ensemble methods for blending predictors in order to outperform a single blending algorithm. The dataset and the source code for the ensemble blending are available online.\r\n", "recorded": "2010-07-27T10:30:00", "title": "Combining Predictions for Accurate Recommender Systems"}, {"url": "solomon_numao_cauia", "desc": "We propose a method to locate relations and constraints between a music score and its impressions, by which we show that machine learning techniques may provide a powerful tool for composing music and analyzing human feelings. \r \r We examine its generality by modifying some arrangements to provide the subjects with a specified impression. \r \r This paper introduces some user interfaces, which are capable of predicting feelings and creating new objects based on seed structures, such as spectrums and their transition for sounds that have been extracted and are perceived as favorable by the test subject. \r \r We would like to discuss a role of such interfaces in the Active Mining project.", "recorded": "2002-11-28T13:00:00", "title": "Constructive Adaptive User Interfaces and Active Mining"}, {"url": "ecml07_landwehr_rtt", "desc": "The ability to recognize human activities from sensory information is essential for developing the next generation of smart devices. Many human activity recognition tasks are from a machine learning perspective quite similar to tagging tasks in natural language processing. Motivated by this similarity, we develop a relational transformation-based tagging system based on inductive logic programming principles, which is able to cope with expressive relational representations as well as a background theory. The approach is experimentally evaluated on two activity recognition tasks and compared to Hidden Markov Models, one of the most popular and successful approaches for tagging.", "recorded": "2007-09-17T16:00:00", "title": "Relational Transformation-based Tagging for Human Activity Recognition"}, {"url": "mmdss07_grobelnik_oml", "desc": "We address the problem of constructing light-weight ontology from social network data. As an example we use social network of a mid size research institution obtained based on e-mail communication. The main contribution is an architecture consisting from five major steps that enable transformation of the data from a given e-mail transactions recordings to an ontology estimating the structure of the organization. Once having a set of sparse vectors, we apply an approach to semi-automated ontology construction as implemented in the OntoGen tool. The experiments and illustrative evaluation show that our approach is useful and applicable in real life situations where the goal is to model social structures based on communication records.", "recorded": "2007-09-13T11:15:11", "title": "Ontologies and Machine Learning"}, {"url": "nips2010_hein_gls", "desc": "The commute distance between two vertices in a graph is the expected time it takes a random walk to travel from the first to the second vertex and back. We study the behavior of the commute distance as the size of the underlying graph increases. We prove that the commute distance converges to an expression that does not take into account the structure of the graph at all and that is completely meaningless as a distance function on the graph. Consequently, the use of the raw commute distance for machine learning purposes is strongly discouraged for large graphs and in high dimensions. As an alternative we introduce the amplified commute distance that corrects for the undesired large sample effects.", "recorded": "2010-12-06T19:26:00", "title": "Getting lost in space: Large sample analysis of the resistance distance"}, {"url": "coinactivess2010_djordjevic_acs", "desc": "A number of enterprise tasks, such as Proposal Writing, Marketing, Selling/Estimation, Risk management and other are performed by the consulting companies every day.\r\n\r\nAccenture ACTIVE goals include:\r\n;- motivation, influence, and validation of the research in ACTIVE\r\n;- improvement of the collaboration among knowledge workers for Accenture and its clients.\r\n\r\nProcess and context sensitive search & browsing and Collaborative proposal development support are the Accenture use cases in ACTIVE.\r\n\r\nThe videolecture also discusses the privacy preserving for protecting enterprise data and an applied example: process\u2013specific and context sensitive knowledge work.", "recorded": "2010-10-21T14:00:00", "title": "Accenture Case Study: Enterprise Collaboration & Knowledge Management   through Machine Learning and Semantic Technologies"}, {"url": "mlcb08_whistler", "desc": "The field of computational biology has seen dramatic growth over the past few years, both in terms of new available data, new scientific questions, and new challenges for learning and inference. In particular, biological data is often relationally structured and highly diverse, well-suited to approaches that combine multiple weak evidence from heterogeneous sources. These data may include sequenced genomes of a variety of organisms, gene expression data from multiple technologies, protein expression data, protein sequence and 3D structural data, protein interactions, gene ontology and pathway databases, genetic variation data (such as SNPs), and an enormous amount of textual data in the biological and medical literature. New types of scientific and clinical problems require the development of novel supervised and unsupervised learning methods that can use these growing resources.\n\nThe goal of this workshop is to present emerging problems and machine learning techniques in computational biology. We invited several speakers from the biology/bioinformatics community who will present current research problems in bioinformatics, and we invite contributed talks on novel learning approaches in computational biology. We encourage contributions describing either progress on new bioinformatics problems or work on established problems using methods that are substantially different from standard approaches. Kernel methods, graphical models, feature selection and other techniques applied to relevant bioinformatics problems would all be appropriate for the workshop.\n\nMore information about the workshop can be found [[http://www.mlcb.org|here]].", "recorded": "2008-12-12T07:30:00", "title": "NIPS Workshop on Machine Learning in Computational Biology, Whistler 2008"}, {"url": "nipsworkshops2010_willsky_pjf", "desc": "This talk outlines a meandering line of research, started in 1988, that began with some signal processors and control theorists trying to make statistical sense of the emerging field of wavelet analysis and, to the speaker's surprise, moved into areas that certainly take advantage of his S&S/CT background but evolved into topics in a variety of fields involving efficient numerical methods for large-scale data assimilation and mapping and, eventually, a rapprochement with graphical models and machine learning. \r\n\r\nThe talk will touch on our early work on multiresolution tree models (motivated by wavelets but only indirectly relevant to them), the way control theorists think of inference and approximate modeling / stochastic realization, with at least one application that rings of the way a machine learning researcher might build a model \u2013 but not a mathematical physicist! I\u2019ll provide (finally) a real rapprochement with wavelets and then turn to approximate inference on loopy graphs. \r\n\r\nThe first approach builds on an idea used in the solution of PDEs, namely nested dissection, but with some machine learning twists, and some control-theoretic stability issues (showing how control might have a few things to provide to inference algorithm designers). I will then turn to a topic again related to the ways in which numerical linear algebraists think about solving sparse systems of equations, but in the context of graphical models, this leads to the idea of walk-sum analysis, a surprisingly useful (and at least I think intuitive) idea. Walk-sum analysis then allows us to say some fairly strong things about a variety of iterative algorithms (generally known as either Richardson iterations, Jacobi iterations, or Gauss-Seidel iterations), including adaptive algorithms to guide iterations for fast convergence. Walk-sum analysis is also key in another approach with linear algebraic interpretations, involving so-called feedback vertex sets. I will also touch on an alarmingly accurate method for computing variances in graphical models that involves using a low-rank approximation to the identity matrix(!) and then return to multiresolution models but now looking at models motivated by two quite different classes of numerical algorithms: multigrid algorithms and multipole algorithms. These algorithms motivate two quite different classes of models, the latter of which requires the introduction of what we refer to as conjugate graphs. If I have any time and energy left, I will comment on some prospective topics.", "recorded": "2010-12-11T08:00:00", "title": "A Personal Journey: From Signals and Systems to Graphical Models"}, {"url": "mlmi04ch_martigny", "desc": "**AMI (Augmented Multiparty Interaction, http://www.amiproject.org)** is a newly launched (January 2004) European Integrated Project (IP) funded under Framework FP6 as part of its IST program. AMI targets computer enhanced multi-modal interaction in the context of meetings. The project aims at substantially advancing the state-of-the-art, within important underpinning technologies (such as human-human communication modeling, speech recognition, computer vision, multimedia indexing and retrieval). It will also produce tools for off-line and on-line browsing of multi-modal meeting data, including meeting structure analysis and summarizing functions. The project also makes recorded and annotated multimodal meeting data widely available for the European research community, thereby contributing to the research infrastructure in the field.\n\n**PASCAL (Pattern Analysis, Statistical Modelling and Computational Learning, http://www.pascal-network.org)** is a newly lauched (December 2003) European Network of Excellence (NoE) as part of its IST program. The NoE brings together experts from basic research areas such as Statistics, Optimisation and Computational Learning and from a number of application areas, with the objective of integrating research agendas and improving the state of the art in all concerned fields.\n\n**IM2 (Interactive Multimodal Information Management, http://www.im2.ch)** is a Swiss National Center of Competence in Research (NCCR) aiming at the advancement of research, and the development of prototypes, in the field of man-machine interaction. IM2 is particularly concerned with technologies coordinating natural input modes (such as speech, image, pen, touch, hand gestures, head and/or body movements, and even physiological sensors) with multimedia system outputs, such as speech, sounds, images, 3D graphics and animation. Among other applications, IM2 is also targeting research and development in the context of smart meeting rooms.\n\n**M4 (Multi-Modal Meeting Manager, http://www.m4project.org)** is an EU IST project launched in March 2002 concerned with the construction of a demonstration system to enable structuring, browsing and querying of an archive of automatically analysed meetings. The archived meetings will have taken place in a room equipped with multimodal sensors.\n\nGiven the multiple links between **AMI**, **PASCAL**, **IM2** and **M4**, it was decided to organize a join workshop in order to bring together researchers from the different communities around the common theme of advanced machine learning algorithms for processing and structuring multimodal human interaction in meetings.", "recorded": "2004-06-21T00:00:00", "title": "Joint AMI/PASCAL/IM2/M4 Workshop on Multimodal Interaction and Related Machine Learning Algorithms, Martigny 2004"}, {"url": "w3cworkshop2013_von_freyberg_web", "desc": "Standardisation efforts are particularly attractive if they promise to drive business applications forward. Although still in development, the ITS 2.0 standard (developed by the W3C's MultilingualWeb-LT project) is already proving that it can fulfil this promise. This talk shows how the German Industrial Machine Builders' Association (VDMA) and Cocomore, as its service provider, benefit from the development of the ITS 2.0 standard. It demonstrates how the systems created during the standardisation effort support developing client relationships and business opportunities. As a further aspect, it is shown how the results of the standard development process have impacted VDMA's ability to conserve valuable resources.", "recorded": "2013-03-12T14:20:00", "title": "Standardization for the Multilingual Web: A Driver of Business Opportunities"}, {"url": "mitworld_thompson_sm", "desc": "In this history of aural culture in early-twentieth-century America, Emily Thompson charts dramatic transformations in what people heard and how they listened. What they heard was a new kind of sound that was the product of modern technology, and the way they listened was as newly critical consumers of aural commodities. By examining the technologies that produced this sound, as well as the culture that enthusiastically consumed it, Thompson recovers a lost dimension of the Machine Age and deepens our understanding of the experience of change that characterized the era. The Soundscape of Modernity is published by The MIT Press, 2002, and is available from the Press at http://mitpress.mit.edu/0262201380.", "recorded": "2002-09-26T12:18:00", "title": "The Soundscape of Modernity: Architectural Acoustics and the Culture of Listening in America, 1900-1933"}, {"url": "nipsworkshops2012_shalev_shwartz_minimization", "desc": "Stochastic Gradient Descent (SGD) has become popular for solving large scale supervised machine learning optimization problems such as SVM, due to their strong theoretical guarantees. While the closely related Dual Coordinate Ascent (DCA) method has been implemented in various software packages, it has so far lacked good convergence analysis. We present a new analysis of Stochastic Dual Coordinate Ascent (SDCA) showing that this class of methods enjoy strong theoretical guarantees that are comparable or better than SGD. This analysis justifies the effectiveness of SDCA for practical applications.", "recorded": "2012-12-08T10:00:00", "title": "Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization"}, {"url": "mlws04_platt_rbvsh", "desc": "Many multimedia applications reduce to the problem of searching a database of high-dimensional regions to see whether any overlap a query point. There is a large literature of indexing techniques based on trees, all of which break down given high enough dimension of stored regions. We have created a new data structure, called redundant bit vectors (RBVs), that can effectively index high-dimensional regions.Using RBVs, we can search a database of 240K 64-dimensional hyperspheres, each with a different radius, up to 56 times faster than an optimized learning scan. RBVs are general-purpose, and may be useful for machine learning applications.", "recorded": "2004-10-08T15:30:00", "title": "Redundant Bit Vectors for Searching High-Dimensional Regions"}, {"url": "stanfordee364bs08_boyd_lec14", "desc": "So this is what we\u2019ll do. The problem was scaled in such a way that we could pull off a\r\nCholesky factorization. I think the Cholesky factor had something like 30 million\r\nnonzeros. So it\u2019ll take some time to do both the Cholesky factorization and also to do the\r\nbackward and forward substitution. So but direct is possible. All we have to do with this\r\nproblem is scale it by a factor of ten and direct becomes kind of out of the question, so\r\nthen at least on a little standard machine. ...\r\n\r\nSee the whole transcript at [[http://see.stanford.edu/materials/lsocoee364b/transcripts/ConvexOptimizationII-Lecture14.pdf|Convex Optimization II  - Lecture 14]]", "recorded": "2008-04-20T12:51:23", "title": "Lecture 14: Methods (Truncated Newton Method)"}, {"url": "ida07_ljubljana", "desc": "Our aim for the 7th IDA conference is to bring together a wide variety of researchers - academic, industrial, and otherwise who are concerned with extracting knowledge from data, including researchers from statistics, machine learning, neural networks, computer science, pattern recognition, database management, and other areas. The strategies adopted by people working in these areas are often different, and a synergy results if this is recognised. IDA-2007 is intended to stimulate interaction between these different areas, so that more powerful techniques and tools emerge for extracting knowledge from data and a better understanding is developed for the process of intelligent data analysis.", "recorded": "2007-09-06T09:00:00", "title": "7th International Symposium on Intelligent Data Analysis, Ljubljana 2007"}, {"url": "colt2014_williamson_geometry", "desc": "Loss functions are central to machine learning because they are the means by which the quality of a prediction is evaluated. Any loss that is not proper, or can not be transformed to be proper via a link function is inadmissible. All admissible losses for n-class problems can be obtained in terms of a convex body in Rn. We show this explicitly and show how some existing results simplify when viewed from this perspective. This allows the development of a rich algebra of losses induced by binary operations on convex bodies (that return a convex body). Furthermore it allows us to define an \u201cinverse loss\u201d which provides a universal \u201csubstitution function\u201d for the Aggregating Algorithm. In doing so we show a formal connection between proper losses and norms.", "recorded": "2014-06-15T15:40:00", "title": "The Geometry of Losses"}, {"url": "fgconference2015_ljubljana", "desc": "[[http://www.fg2015.org/|The 11th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2015)]] is part of the IEEE conference series on Automatic Face and Gesture Recognition - the premier international forum for research in image and video-based face, gesture, and body movement recognition. Its broad scope includes: advances in fundamental computer vision, pattern recognition and computer graphics; machine learning techniques relevant to face, gesture, and body motion; new algorithms and applications. The FG conference plays an important role in shaping related scientific, academic, and higher-education programs.", "recorded": "2015-05-04T09:00:00", "title": "11th IEEE International Conference on Automatic Face and Gesture Recognition (FG), Ljubljana 2015"}, {"url": "classconference2012_morin_clouds", "desc": "Cloud computing has rapidly gained momentum in industry and research. The Silicon Valley is a unique ecosystem in the world with large companies like Google, Apple and a myriad of start-ups shaping the future of cloud computing together with leading research labs such as the AmpLab laboratory at UC Berkeley and the Lawrence Berkeley National Laboratory (LBNL). While AmpLab research activities are at the intersection of machine learning, cloud computing, and crowd sourcing, researchers at LBNL investigate cloud computing and data analytics for Science. In my talk, I will present some of the on-going research and development projects on cloud computing carried out in the San Francisco Bay area.", "recorded": "2012-10-24T10:30:00", "title": "Clouds in San Francisco Bay Area"}, {"url": "eswc2013_minervini_class", "desc": "The increasing availability of structured machine-processable knowledge in the context of the Semantic Web, allows for inductive methods to back and complement purely deductive reasoning in tasks where the latter may fall short. This work proposes a new method for similarity-based class-membership prediction in this context. The underlying idea is the propagation of class-membership information among similar individuals. The resulting method is essentially non-parametric and it is characterized by interesting complexity properties, that make it a candidate for the application of transductive inference to large-scale contexts. We also show an empirical evaluation of the method with respect to other approaches based on inductive inference in the related literature.", "recorded": "2013-05-30T16:00:00", "title": "Transductive Inference for Class Membership Propagation in Web Ontologies"}, {"url": "reasecs_baldoni_pswi", "desc": "Personalization is a process by which it is possible to give the user optimal support in accessing, retrieving, and storing information, where solutions are built so as to fit the preferences, the characteristics and the taste of the individual. This result can be achieved only by exploiting machine-interpretable semantic information, e.g. about the possible resources, about the user him/herself, about the context, about the goal of the interaction. Personalization is realized by an inferencing process applied to the semantic information, which can be carried out in many different ways depending on the specific task. The objective of this paper is to provide a coherent introduction into issues and methods for realizing personalization in the Semantic Web.\r\n\r\n\r\nDocuments: \r\n;[[Personalization.pdf]]", "recorded": "2005-06-22T00:00:00", "title": "Personalization for the Semantic Web -Part II-"}, {"url": "ijcai09_dietterich_mleis", "desc": "Ecosystem Informatics brings together mathematical and computational tools to address scientific and policy challenges in the ecosystem sciences. These challenges include novel sensors for collecting data, algorithms for automated data cleaning, learning methods for building statistical models from data and for fitting mechanistic models to data, and algorithms for designing optimal policies for biosphere management. This talk will describe recent work on the first two of these---new devices for automated arthropod population counting and linear Gaussian DBNs for automated cleaning of sensor network data. It will also give examples of open problems along the whole spectrum from sensors to policies.", "recorded": "2009-07-16T14:00:00", "title": "Machine Learning in Ecosystem Informatics and Sustainability"}, {"url": "roks2013_zhou_learning", "desc": "Information theoretical learning is inspired by introducing information theory\r\nideas into a machine learning paradigm. Minimum error entropy is a principle of information\r\ntheoretical learning and provides a family of supervised learning algorithms. It is a substitution of the classical least squares method when the noise is non-Gaussian. Its idea is to extract\r\nfrom data as much information as possible about the data generating systems by minimizing\r\nerror entropies. In this talk we will discuss some minimum error entropy algorithms in a regression setting by minimizing empirical Renyi's entropy of order 2. Consistency results and\r\nlearning rates are presented. In particular, some error estimates dealing with heavy-tailed\r\nnoise will be given.", "recorded": "2013-07-10T14:30:00", "title": "Minimum Error Entropy Principle for Learning"}, {"url": "bbci09_muller_haynes_moller_bfnb", "desc": "The Bernstein Focus: Neurotechnology Berlin (BFNT-B) posits that neuroscientific results can be exploited for developing robust \u2018real-world\u2019 applications that have a major potential for (also non-medical) industry. Similar to the new paradigm of medical research \u2018from bench to bedside and back\u2019, the center brings together a multidisciplinary faculty with the aim of directly applying insights from basic neuroscience to relevant applications (\u2018from bench to desktop and back\u2019). The major aim of the BFNT-B is to foster novel noninvasive \u2018brain reading\u2019 techniques to enhance man-machine interactions. Their contributions will be evaluated, e.g. in the future-oriented field of usability studies for telecommunications systems and services, or driver-assisted measures for vehicle safety.", "recorded": "2009-07-10T16:11:51", "title": "Bernstein Focus: Neurotechnology Berlin"}, {"url": "mmdss07_tanev_les", "desc": "Automatic Event Extraction from texts emerges as an im-\r portant and complex text mining task. Its goal is to detect description\r of events of a speci\u00afc type described in the text. For each event the\r Event Extraction system is expected to \u00afnd the time, the location, the\r participants in this event and their roles, as well as other related circum-\r stances. In this talk we present a Machine Learning approach for learning\r of information extraction patterns, a method for semi-automatic lexical\r acquisition, and an information aggregation strategy implemented in a\r working prototype nexus which detects automatically security related\r events in clusters of news articles.", "recorded": "2007-09-20T16:15:00", "title": "Learning to Extract Security-related Event Information from Large News Collections"}, {"url": "russir2010_filippova_nlp", "desc": "Google's mission is to \"organize the world's information and make it universally accessible and useful\". In the first place this implies understanding and processing the vast amounts of natural language data available on the web -- news, (video-) blogs, books, forums -- all kinds of text and speech in many languages. This talk consists of two parts: The first part will be an overview of a variety of NLP problems solved at Google on a daily basis, such as machine translation, speech recognition, information extraction. In the second part I will consider the task of text summarization in more detail and will present a graph-based method of multi-document news summarization and a way of summarizing video content by looking at users' comments.", "recorded": "2010-09-13T11:00:00", "title": "NLP at Google"}, {"url": "nipsworkshops2010_singh_dmapi", "desc": "In this work, we distribute the MCMC-based MAP inference using the Map-Reduce framework. The variables are assigned randomly to machines, which leads to some factors that neighbor vari- ables on separate machines. Parallel MCMC-chains are initiated using proposal distributions that only suggest local changes such that factors that lie across machines are not examined. After a fixed number of samples on each machine, we redistribute the variables amongst the machines to enable proposals across variables that were on different machines. To demonstrate the distribution strategy on a real-world information extraction application, we model the task of cross-document coreference.", "recorded": "2010-12-11T16:30:00", "title": "Distributed MAP Inference for Undirected Graphical Models"}, {"url": "icml2015_richman_dynamic_sensing", "desc": "In many machine learning applications the quality of the data is limited by resource constraints (may it be power, bandwidth, memory, ...). In such cases, the constraints are on the average resources allocated, therefore there is some control over each sample\u2019s quality. In most cases this option remains unused and the data\u2019s quality is uniform over the samples. In this paper we propose to actively allocate resources to each sample such that resources are used optimally overall. We propose a method to compute the optimal resource allocation. We further derive generalization bounds for the case where the problem\u2019s model is unknown. We demonstrate the potential benefit of this approach on both simulated and real-life problems.", "recorded": "2015-07-09T14:46:53", "title": "Dynamic Sensing: Better Classification under Acquisition Constraints"}, {"url": "aaai2013_mooney_language_learning", "desc": "Most approaches to semantics in computational linguistics represent meaning in terms of words or abstract symbols. Grounded-language research bases the meaning of natural language on perception and/or action in the (real or virtual) world. Machine learning has become the most effective approach to constructing natural-language systems; however, current methods require a great deal of laboriously annotated training data. Ideally, a computer would be able to acquire language like a child, by being exposed to language in the context of a relevant but ambiguous environment, thereby grounding its learning in perception and action. We will review recent research in grounded language learning and discuss future directions.", "recorded": "2013-07-16T09:00:00", "title": "Grounded Language Learning"}, {"url": "icml09_dekel_glfet", "desc": "We consider a supervised machine learning scenario where labels are provided by a heterogeneous set of teachers, some of which are mediocre, incompetent, or perhaps even malicious. We present an algorithm, built on the SVM framework, that explicitly attempts to cope with low-quality and malicious teachers by decreasing their influence on the learning process. Our algorithm does not receive any prior information on the teachers, nor does it resort to repeated labeling (where each example is labeled by multiple teachers). We provide a theoretical analysis of our algorithm and demonstrate its merits empirically. Finally, we present a second algorithm with promising empirical results but without a formal analysis.", "recorded": "2009-06-16T15:40:00", "title": "Good Learners for Evil Teachers"}, {"url": "bbci2014_biessmann_machine_learning", "desc": "The combination of multiple neuroimaging modalities has become an important field of research. While the technical challenges associated with multimodal neuroimaging have been mastered more than a decade ago, analysis techniques for multimodal neuroimaging data are still being developed. This tutorial will cover data driven analysis techniques for multimodal neuroimaging, including recent advances in multimodal brain-computer-interfaces and in integration of neural bandpower signals with hemodynamic signals. A special focus will be placed on simple and efficient subspace methods that are useful in all stages of multimodal neuroimaging analyses, starting from basic preprocessing and artifact removal to integration of multiple modalities with complex spatiotemporal coupling dynamics. ", "recorded": "2014-02-28T09:00:00", "title": "Machine Learning for Multimodal Neuroimaging"}, {"url": "prib2010_niranjan_elrd", "desc": "We investigate if interactions of longer range than typically considered in local protein secondary structure prediction methods can be captured in a simple machine learning framework to improve the prediction of \u03b2  sheets. We use support vector machines and recursive feature elimination to show that the small signals available in long range interactions can indeed be exploited. The improvement is small but statistically significant on the benchmark datasets we used. We also show that feature selection within a long window and over amino acids at specific positions typically selects amino acids that are shown to be more relevant in the initiation and termination of \u03b2-sheet formation. ", "recorded": "2010-09-22T15:00:00", "title": "Exploiting Long-range Dependencies in Protein \u03b2-sheet secondary structure prediction"}, {"url": "ktsymposium2013_ljubljana", "desc": "**Foundations of Rule Learning**\\\\\r\nPresentation of the scientific monograph \"Foundations of Rule Learning\" recently published by Springer. The lectures were given by the three book authors: Johannes F\u00fcrnkranz, Nada Lavra\u010d and Dragan Gamberger.\r\n\r\n**Computational Creativity**\\\\\r\nPresentation of a new scientific area of computational creativity, introduced in Slovenia through four newly founded European projects MUSE, PROSECCO, WHIM and ConCreTe. The coordinators of PROSECCO (Tony Veale) and WHIM (Simon Colton) presented the field and the planned research advances while Hannu Toivonen and Tanja Urban\u010di\u010d presented two specific computational creativity approaches.", "recorded": "2013-07-04T09:30:00", "title": "Knowledge Technologies Symposium on Machine Learning and Computational Creativity, Ljubljana 2013"}, {"url": "machine_jun_monte_carlo", "desc": "We propose a novel method for scalable parallelization of SMC algorithms, Entangled Monte Carlo simulation (EMC). EMC avoids the transmission of particles between nodes, and instead reconstructs them from the particle genealogy. In particular, we show that we can reduce the communication to the particle weights for each machine while efficiently maintaining implicit global coherence of the parallel simulation. We explain methods to efficiently maintain a genealogy of particles from which any particle can be reconstructed. We demonstrate using examples from Bayesian phylogenetic that the computational gain from parallelization using EMC significantly outweighs the cost of particle reconstruction. The timing experiments show that reconstruction of particles is indeed much more efficient as compared to transmission of particles.", "recorded": "2012-12-04T11:48:00", "title": "Entangled Monte Carlo"}, {"url": "machine_defazio_scale_free_networks", "desc": "A key problem in statistics and machine learning is the determination of network structure from data. We consider the case where the structure of the graph to be reconstructed is known to be scale-free. We show that in such cases it is natural to formulate structured sparsity inducing priors using submodular functions, and we use their Lovasz extension to obtain a convex relaxation. For tractable classes such as Gaussian graphical models, this leads to a convex optimization problem that can be efficiently solved. We show that our method results in an improvement in the accuracy of reconstructed networks for synthetic data. We also show how our prior encourages scale-free reconstructions on a bioinfomatics dataset.", "recorded": "2012-12-04T11:40:00", "title": "A Convex Formulation for Learning Scale-Free Networks via Submodular Relaxation"}, {"url": "icml09_mahoney_itslima", "desc": "Given an m x n matrix A and a rank parameter k, define the leverage of the i-th row of A to be the i-th diagonal element of the projection matrix onto the span of the top k left singular vectors of A. In this case, \"high leverage\" rows have a disproportionately large amount of the \"mass\" in the top singular vectors. Historically, this statistical concept (and generalizations of it) has found extensive applications in diagnostic regression analysis. Recently, this concept has also been central in the development of improved randomized algorithms for several fundamental matrix problems that have broad applications in machine learning and data analysis. Two examples of the use of statistical leverage for improved worst-case analysis of matrix algorithms will be described. The first problem is the least squares approximation problem, in which there are n constraints and d variables. Classical algorithms, dating back to Gauss and Legendre, use O(nd2) time. We describe a randomized algorithm that uses only O(n d log d) time to compute a relative-error, i.e., 1+/-epsilon, approximation. The second problem is the problem of selecting a \"good\" set of exactly k columns from an m x n matrix, and the algorithm of Gu and Eisenstat provides the best previously existing result. We describe a two-stage algorithm that improves on their result. Recent applications of statistical leverage ideas in modern large-scale machine learning and data analysis will also be briefly described. This concept has proven to be particularly fruitful in large data applications where modeling decisions regarding what computations to perform are made for computational reasons, as opposed to having any realistic hope that the statistical assumptions implicit in those computations are satisfied by the data.", "recorded": "2009-06-18T09:30:00", "title": "Statistical Leverage and Improved Matrix Algorithms"}, {"url": "ecmlpkdd09_ben_david_ttu", "desc": "Machine learning enjoys a deep and powerful theory that has led to a wide variety of highly successful practical tools. However, most of this theory is developed under some simplifying assumptions that clearly fail in the real world. In particular, a fundamental assumption of the theory is that the data available for training and the data of the target application come from the same source. When this assumption fails, the learner is faced with a \u201cdomain adaptation\u201d challenge. In the past few years, the range of machine learning applications have been expanded to include various tasks requiring domain adaptation. Such application have been addressed by several heuristic paradigms. However, the common theoretical models fall short of providing useful analysis of these techniques. The key to domain adaptation is the similarity between the training and target domains. In this talk I will discuss several parameters along which task similarity can be defined and measured and discuss to what extent can they be utilized to direct learning algorithms and guarantee their success. Recent work can provide theoretical justification to some existing practical heuristics, as well as guide the development of novel algorithms for handling some types of data discrepancies. However, our current understanding leaves much to be desired. I shall devote the last part of the talk to describing some of the challenges and open questions that will have to be addressed before one can claim satisfactory understanding of learning in the presence of training-test discrepancies. The talk is based on joint works with John Blitzer, Koby Crammer and Fernando Pereira and with my students, David Pal, Teresa Luu and Tyler Lu.\r\n", "recorded": "2009-09-07T09:20:00", "title": "Invited Talk: Towards Theoretical Understanding of Domain Adaptation Learning"}, {"url": "nipsworkshops2012_optimization_for_ml", "desc": "Optimization lies at the heart of ML algorithms. Sometimes, classical textbook algorithms suffice, but the majority problems require tailored methods that are based on a deeper understanding of the ML requirements. ML applications and researchers are driving some of the most cutting-edge developments in optimization today. The intimate relation of optimization with ML is the key motivation for our workshop, which aims to foster discussion, discovery, and dissemination of the state-of-the-art in optimization as relevant to machine learning.\r\n\r\nMuch interest has focused recently on stochastic methods, which can be used in an online setting and in settings where data sets are extremely large and high accuracy is not required. Many aspects of stochastic gradient remain to be explored, for example, different algorithmic variants, customizing to the data set structure, convergence analysis, sampling techniques, software, choice of regularization and tradeoff parameters, distributed and parallel computation. The need for an up-to-date analysis of algorithms for nonconvex problems remains an important practical issue, whose importance becomes even more pronounced as ML tackles more and more complex mathematical models.\r\n\r\nFinally, we do not wish to ignore the not particularly large scale setting, where one does have time to wield substantial computational resources. In this setting, high-accuracy solutions and deep understanding of the lessons contained in the data are needed. Examples valuable to MLers may be exploration of genetic and environmental data to identify risk factors for disease; or problems dealing with setups where the amount of observed data is not huge, but the mathematical model is complex.\r\n\r\nWorkshop homepage: http://opt.kyb.tuebingen.mpg.de/", "recorded": "2012-12-08T07:30:00", "title": "Optimization for Machine Learning"}, {"url": "aop09_cagliari", "desc": "Modern society is increasingly reliant on our capability to automatically detect patterns in vast masses of data. This is affecting not only the way we do business and run our industries, but also is changing the very nature of the scientific method. Every science now has an e-version (computational biology, computational chemistry, etc) and in many cases this involves automatisation of both the production and the analysis of experimental data. The use of computer simulations increases our reliance on automatic analysis of data even further. This process is accelerating.\r\n\r\nThe distinct scientific communities that are working on various aspects of automatic analysis of data include Combinatorial Pattern Matching, Data Mining, Computational Statistics, Network Analysis, Text Mining, Image Processing, Syntactical Pattern Recognition, Machine Learning, Statistical Pattern Recognition, Computer Vision, and many others.\r\n\r\nA unified understanding of the challenges and opportunities ahead is essential for further progress, and is the purpose of this series of workshops / summer-schools: to promote a unified understanding of all the technical and conceptual issues relating to the automatic discovery and exploitation of patterns in data.\r\n\r\nThe previous 2 editions of this event took place in Erice, 2005 and Bertinoro, 2007. The videos of all lectures are available online.\r\n\r\nINTENDED AUDIENCE: The school is intended for PhD students, postdocs, and researchers (both academic and industrial), working in any of the disciplines involved in \"the analysis of patterns\" and hence including: bioinformatics, data mining, text analysis, machine learning, statistics, optimization, computer vision, stringology, network analysis, etc.\r\n\r\n----\r\nThe event homepage can be found at http://www.analysis-of-patterns.net/\r\n----", "recorded": "2009-09-28T09:00:00", "title": "The Analysis of Patterns, Cagliari 2009"}, {"url": "cidu2011_giannakis_time_series", "desc": "Many processes in atmosphere-ocean science develop multiscale temporal and spatial\r\npatterns, with complex underlying dynamics and time-dependent external forcings. Because of the possible advances in our understanding and prediction of climate phenomena, extracting that variability empirically from incomplete observations is a problem of wide contemporary interest. Here, we present a technique for analyzing climatic time series that exploits the geometrical relationships between the observed data points to recover features characteristic of strongly nonlinear dynamics (such as intermittency), which are not accessible to classical Singular Spectrum Analysis (SSA). The method utilizes Laplacian eigenmaps, evaluated after suitable time-lagged embedding, to produce a reduced representation of the observed samples, where standard tools of matrix algebra can be used to perform truncated Singular Value Decomposition despite the nonlinear manifold structure of the data set. As an application, we study the variability of the upper-ocean temperature in the North Pacific sector of a 700-year equilibrated integration of the CCSM3 model. Imposing no a priori assumptions (such as periodicity in the statistics), our machine-learning technique recovers three distinct types of temporal processes: (1) periodic processes, including annual and semiannual cycles; (2) decadal-scale variability with spatial patterns resembling the Pacific Decadal Oscillation; (3) intermittent processes associated with the Kuroshio extension and variations in the strength of the subtropical and subpolar gyres. The latter carry little variance (and are therefore not captured by SSA), yet their dynamical role is expected to be significant.", "recorded": "2011-10-21T09:35:00", "title": "Time Series Reconstruction via Machine Learning: Revealing Decadal Variability and Intermittency in the North Pacific Sector of a Coupled Climate Model"}, {"url": "nipsworkshops09_structured_data", "desc": "**Transfer Learning for Structured Data**\r\n\r\nRecently, transfer learning (TL) has gained much popularity as an approach to reduce the training-data calibration effort as well as improve generalization performance of learning tasks. Unlike traditional learning, transfer learning methods make the best use of data from one or more source tasks in order to learn a target task. Many previous works on transfer learning have focused on transferring the knowledge across domains where the data are assumed to be i.i.d. In many real-world applications, such as identifying entities in social networks or classifying Web pages, data are often intrinsically non i.i.d., which present a major challenge to transfer learning. In this workshop, we call for papers on the topic of transfer learning for structured data. Structured data are those that have certain intrinsic structures such as network topology, and present several challenges to knowledge transfer. A first challenge is how to judge the relatedness between tasks and avoid negative transfer. Since data are non i.i.d., standard methods for measuring the distance between data distributions, such as KL divergence, Maximum Mean Discrepancy (MMD) and A-distance, may not be applicable. A second challenge is that the target and source data may be heterogeneous. For example, a source domain is a bioinformatics network, while a target domain may be a network of webpage. In this case, deep transfer or heterogeneous transfer approaches are required. Heterogeneous transfer learning for structured data is a new area of research, which concerns transferring knowledge between different tasks where the data are non-i.i.d. and may be even heterogeneous. This area has emerged as one of the most promising areas in machine learning. In this workshop, we wish to boost the research activities of knowledge transfer across structured data in the machine learning community. We welcome theoretical and applied disseminations that make efforts (1) to expose novel knowledge transfer methodology and frameworks for transfer mining across structured data. (2) to investigate effective (automated, human-machined-cooperated) principles and techniques for acquiring, representing, modeling and engaging transfer learning on structured data in real-world applications. This workshop on Transfer learning for structured data will bring active researchers in artificial intelligence, machine learning and data mining together toward developing methods or systems together, to explore methods for solving real-world problems associated with learning on structured data. The workshop invites researchers interested in transfer learning, statistical relational learning and structured data mining to contribute their recent works on the topic of interest.\r\n----\r\nThe Workshop homepage can be found at http://www.cse.ust.hk/~sinnopan/nips09tlsd/index.html.\r\n----", "recorded": "2009-12-12T07:30:00", "title": "Structured Data"}, {"url": "icml09_niv_tnorl", "desc": "Overview and goals:\r\n\r\nOne of the most influential contributions of machine learning to understanding the human brain is the (fairly recent) formulation of learning in real world tasks in terms of the computational framework of reinforcement learning. This confluence of ideas is not limited to abstract ideas about how trial and error learning should proceed, but rather, current views regarding the computational roles of extremely important brain substances (such as dopamine) and brain areas (such as the basal ganglia) draw heavily from reinforcement learning. The results of this growing line of research stand to contribute not only to neuroscience and psychology, but also to machine learning: human and animal brains are remarkably adept at learning new tasks in an uncertain, dynamic and extremely complex world. Understanding how the brain implements reinforcement learning efficiently may suggest similar solutions to engineering and artificial intelligent problems. This tutorial will present the current state of the study of neural reinforcement learning, with an emphasis on both what it teaches us about the brain, and what it teaches us about reinforcement learning. \r\n\r\nTarget Audience:\r\nThe target audience are researchers working in the field of reinforcement learning, who are interested in the current state-of-the-art of neuroscientific applications of this theoretical framework, as well as researchers working in related fields of machine learning such as engineering and robotics. Familiarity/basic knowledge of reinforcement learning (MDPs, dynamic programming, online temporal difference methods) will be assumed; basic knowledge in neuroscience or psychology will not. \r\n\r\nTutorial outline:\r\nIntroduction: A coarse-grain overview of the brain and what we currently know about how it works\r\nLearning and decision making in animals and humans: is this really a reinforcement learning problem?\r\nDopamine and prediction errors: what we know about dopamine, why we think it computes a temporal difference prediction error, and why should we care? Evidence for the prediction error hypothesis of dopamine\r\nActor/Critic architectures in the basal ganglia: a distribution of functions in a learning network\r\nSARSA versus Q-learning: can dopamine reveal what algorithm the brain actually uses?\r\nMultiple learning systems in the brain: what is the evidence for both model based and model free reinforcement learning systems in the brain, why have more than one system, and how to arbitrate between them\r\nBeyond phasic dopamine: average reward reinforcement learning, tonic dopamine and the control of response vigor\r\nRisk and reinforcement learning: can the brain tell us something about learning of the variance of rewards?\r\nOpen challenges and future directions: what more can reinforcement learning teach us about the brain, and where can we expect the brain to teach us about reinforcement learning?", "recorded": "2009-06-14T13:00:00", "title": "The Neuroscience of Reinforcement Learning"}, {"url": "icml07_wenye_lsrl", "desc": "The advances in kernel-based learning necessitate the study on solving a large-scale non-sparse positive definite linear system. To provide a deterministic approach, recent researches focus on designing fast matrixvector multiplication techniques coupled with a conjugate gradient method. Instead of using the conjugate gradient method, our paper proposes to use a domain decomposition approach in solving such a linear system. Its convergence property and speed can be understood within von Neumann's alternating pro jection framework. We will report significant and consistent improvements in convergence speed over the conjugate gradient method when the approach is applied to recent machine learning problems.", "recorded": "2007-06-22T11:30:00", "title": "Large-scale RLSC Learning Without Agony"}, {"url": "mlss05us_bickel_bscs", "desc": "Machine learning in computer science and prediction and classification in statistics are essentially equivalent fields. I will try to illustrate the relation between theory and practice in this huge area by a few examples and results. In particular I will try to address an apparent puzzle: Worst case analyses, using empirical process theory, seem to suggest that even for moderate data dimension and reasonable sample sizes good prediction (supervised learning) should be very difficult. On the other hand, practice seems to indicate that even when the number of dimensions is very much higher than the number of observations, we can often do very well. We also discuss a new method of dimension estimation and some features of cross validation.", "recorded": "2005-05-16T00:00:00", "title": "On the Borders of Statistics and Computer Science"}, {"url": "sep2010_larkc_promo", "desc": " The aim of the EU FP 7 Large-Scale Integrating Project LarKC is to develop the Large Knowledge Collider (LarKC, for short, pronounced \u201clark\u201d), a platform for massive distributed incomplete reasoning that will remove the scalability barriers of currently existing reasoning systems for the Semantic Web.\r\n\r\nThis will be achieved by:\r\n\r\n    * Enriching the current logic-based Semantic Web reasoning methods with methods from information retrieval, machine learning, information theory, databases, and probabilistic reasoning,\r\n    * Employing cognitively inspired approaches and techniques such as spreading activation, focus of attention, reinforcement, habituation, relevance reasoning, and bounded rationality.\r\n    * Building a distributed reasoning platform and realizing it both on a high-performance computing cluster and via \u201ccomputing at home\u201d.\r\n", "recorded": "2010-10-15T12:15:26", "title": "LarKC Promotional video"}, {"url": "mloss08_albanese_mlp", "desc": "We introduce mlpy, a high-performance Python package for predictive modeling. It makes extensive use of\r\nNumPy to provide fast N-dimensional array manipulation and easy integration of C code. Mlpy provides high\r\nlevel procedures that support, with few lines of code, the design of rich Data Analysis Protocols (DAPs) for\r\npredictive classi\ufb01cation and feature selection. Methods are available for feature weighting and ranking, data\r\nresampling, error evaluation and experiment landscaping. The package includes tools to measure stability\r\nin sets of ranked feature lists, of special interest in bioinformatics for functional genomics, for which large\r\nscale experiments with up to 106 classi\ufb01ers have been run on Linux clusters and on the Grid.", "recorded": "2008-12-12T09:30:00", "title": "Machine Learning Py (mlpy)"}, {"url": "ida07_ricci_lta", "desc": "We present a new machine learning approach to the inverse\r parametric sequence alignment problem: given as training examples a\r set of correct pairwise global alignments, find the parameter values that\r make these alignments optimal.We consider the distribution of the scores\r of all incorrect alignments, then we search for those parameters for which\r the score of the given alignments is as far as possible from this mean,\r measured in number of standard deviations. This normalized distance is\r called the \u2018Z-score\u2019 in statistics. We show that the Z-score is a function\r of the parameters and can be computed with efficient dynamic programs\r similar to the Needleman-Wunsch algorithm.We also show that maximizing\r the Z-score boils down to a simple quadratic program. Experimental\r results demonstrate the effectiveness of the proposed approach.", "recorded": "2007-09-06T14:30:00", "title": "Learning to align: a statistical approach"}, {"url": "mlss05au_hutter_hpbme", "desc": "Most passive Machine Learning tasks can be (re)stated as sequence prediction problems. This includes pattern recognition, classification, time-series forecasting, and others. Moreover, the understanding of passive intelligence also serves as a basis for active learning and decision making. In the recent past, rich theories for sequence prediction have been developed, and this is still an ongoing process. On the other hand, we are arriving at the stage where some important results are already termed classical. While much of the current Learning Theory is formulated under the assumption of independent and identically distributed (i.i.d.) observations, this lecture series focusses on situations without this prerequisite (e.g. weather or stock-market time-series).", "recorded": "2005-01-24T00:00:00", "title": "How to predict with Bayes, MDL, and Experts"}, {"url": "reasecs_lska", "desc": "In the tutorial, we introduce a newly emerging technology called \u201cTriple Space\u201d, a system which applies the paradigm of space-based computing to realize a coordination middleware managing information formalized using Semantic Web representation languages and coordinating the exchange of machine-readable information among many distributed entities on the Web.  Triple Space Computing is a future solution for high scale knowledge distribution and coordination scenarios. We present the concept and the existing implementation,  guided by a concrete demonstration based on a Digital Asset Marketplace  for a large telecommunications company. Attendees will download and use a Triple Space kernel and learn how to use it as communication and coordination middleware for their Semantic Web applications.", "recorded": "2009-04-27T00:00:00", "title": "TripCom: Large-Scale Knowledge Applications"}, {"url": "kdd07_sheng_pea", "desc": "It is often expensive to acquire data in real-world data mining applications. Most previous data mining and machine learning research, however, assumes that a fixed set of training examples is given. In this paper, we propose an online cost-sensitive framework that allows a learner to dynamically acquire examples as it learns, and to decide the ideal number of examples needed to minimize the total cost. We also propose a new strategy for Partial Example Acquisition (PAS), in which the learner can acquire examples with a subset of attribute values to reduce the data acquisition cost. Experiments on UCI datasets show that the new PAS strategy is an effective method in reducing the total cost for data acquisition.", "recorded": "2007-08-15T12:51:13", "title": "Partial Example Acquisition in Cost-Sensitive Learning "}, {"url": "bark08_grasmere", "desc": "**Motivation**\\\\\r\nThe main aim of this workshop is to allow leading Bayesian researchers in machine learning to get together presenting their latest ideas and discussing future directions.\r\n\r\n**Themes**\\\\\r\n    * Incorporating Complex Prior Knowledge in Bayesian inference, for example mechanistic models (such as differential equations) or knowledge transfered from other related situations (e.g. hierarchical Dirichlet Processes).\r\n    * Model mismatch: the Bayesian lynch pin is that the model is correct, but it rarely is.\r\n    * Approximation techniques: how should we do Bayesian inference in practice. Sampling, variational, Laplace or something else?\r\n    * Your pet Bayesian issue here.\r\n\r\nVisit the Workshop website [[http://mlo.cs.man.ac.uk/events/bark08/|here]].", "recorded": "2008-09-06T16:42:17", "title": "Bayesian Research Kitchen Workshop (BARK), Grasmere 2008"}, {"url": "mbc07_rutkowski_tfs", "desc": "Brain responses to audio stimuli are analysed using data driven time-frequency analysis. This is achieved based on the electroencephalogram (EEG) recordings and with auditory chirps or music as the audio stimulus. The empirical mode decomposition (EMD) is applied to multichannel EEG recordings, and the insight into the brain responses is provided by the analysis of the dynamics of auditory steady-state responses (ASSR). The proposed approach is further illustrated on the analysis of EEG responses to classical music. A comprehensive synchrony analysis is provided based on the visualization of EMD and spectrogrammatching techniques. Simulation results illustrate the potential of the proposed approach in\r future brain computer/machine interfaces.", "recorded": "2007-12-07T17:50:00", "title": "Time-Frequency and Synchrony Analysis of Responses to Steady-State Auditory and Musical Stimuli from Multichannel EEG"}, {"url": "icml08_bergeron_mir", "desc": "This paper introduces a novel machine learning model called multiple instance ranking (MIRank) that enables ranking to be performed in a multiple instance learning setting. The motivation for MIRank stems from the hydrogen abstraction problem in computational chemistry, that of predicting the grouping of hydrogen atoms from which a hydrogen is abstracted (removed) during metabolism. The model predicts the preferred hydrogen grouping within a molecule by ranking the groups, with the ambiguity of not knowing which hydrogen within the preferred grouping is actually abstracted. This paper formulates MIRank in its general context and proposes an algorithm for solving MIRank problems using successive linear programming. The method outperforms multiple instance classification models on several real and synthetic datasets.", "recorded": "2008-07-08T14:25:00", "title": "Multiple Instance Ranking"}, {"url": "nipsworkshops2013_ben_david_adaptation_learning", "desc": "How can the learning of some target task benefit from training data generated by a different, yet related, task?\r\nIn the past few years, a range of machine learning applications led to the development of various heuristic paradigms that address these domain adaptation and transfer learning issues.\r\nSuch paradigms extend well beyond the scope of the currently available analysis. How should this a gap be addressed? I will survey some major algorithmic paradigms that have been proposed to address the transfer/adaptation learning and discuss the current theoretical understanding of these approaches. I also wish to touch upon what I view as useful vs not so insightful culture of research addressing this challenge.", "recorded": "2013-12-10T08:55:00", "title": "Understanding Domain Adaptation Learning - the good and the not so good"}, {"url": "emergingtrends2012_schwenk_translating", "desc": "This presentation will describe our research to perform high quality automatic  translations of scientific texts.  We have developed a complete operational system which is closely integrated into the national archive of scientific  papers in France.  The user can deposit an PDF document in English or French and the systems automatically provides a translation into the opposite language.  The user has the opportunity to correct the translation with an interactive interface.  These user interactions are recorded and reintegrated into the system to improve the translation quality. We will describe in detail the underlying techniques to adapt a generic statistical machine translation system to the domain of scientific papers. We plan to make the extracted corpora freely available to support research on the translation of scientific texts.", "recorded": "2012-11-07T12:02:00", "title": "Challenges when translating scientific documents"}, {"url": "dmss06_san_vincenzo", "desc": "The aim of the school is to promote advanced experiences and implementations and, more in general, to improve knowledge in these fields. The school will be open to researchers and practitioners who are interested in audio-visual and multimedia digital libraries and in advanced solutions and technologies for their implementation. It will also contribute to establish a common vision of the future ICT technologies for digital libraries, and ultimately will represent a significant opportunity to exchange experiences, to share common problems and to start new joint activities. No specific prerequisites are required, but it is recommended that participants have experience in using information management systems dealing with multimedia data.", "recorded": "2006-06-12T00:00:00", "title": "Join DELOS - MUSCLE Summer School on Multimedia digital libraries, Machine learning and cross-modal technologies for access and retrieval"}, {"url": "mlmi04ch_just_ricmm3", "desc": "In this paper, we address the problem of the recognition of isolated complex mono- and bi-manual hand gestures. In the proposed system, hand gestures are represented by the 3D trajectories of blobs. Blobs are obtained by tracking colored body parts in real-time using the EM algorithm. In most of the studies on hand gestures, only small vocabularies have been used. In this paper, we study the results obtained on a more complex database of mono- and bi-manual gestures. These results are obtain processing algorithm, namely Input/Output Hidden Markov Model (IOHMM), implemented within the framework of an open source machine learning library.", "recorded": "2004-06-21T00:00:00", "title": "Recognition of Isolated Complex Mono- and Bi-Manual 3D Hand Gestures using Discrete IOHMM"}, {"url": "xlike_kickoff2012_grobelnik_introduction", "desc": "The goal of the **[[http://www.xlike.org/|XLike project]]** is to develop technology to monitor and aggregate knowledge that is currently spread across mainstream and social media, and to enable cross-lingual services for publishers, media monitoring and business intelligence.\n\nThe aim is to combine scientific insights from several scientific areas to contribute in the area of cross-lingual text understanding. By combining modern computational linguistics, machine learning, text mining and semantic technologies we plan to deal with the following two key open research problems:\n\n    * to extract and integrate formal knowledge from multilingual texts with cross-lingual knowledge bases, and\n    * to adapt linguistic techniques and crowdsourcing to deal with irregularities in informal language used primarily in social media.", "recorded": "2012-01-18T09:00:00", "title": "Introduction to FP7 XLike project"}, {"url": "wims", "desc": "**[[http://wims.vestforsk.no/|WIMS]]** is a series of conferences concerned with intelligent approaches to transform the World Wide Web into a global reasoning and semantics-driven computing machine.\r\n\r\nThe purpose of the WIMS series is:\r\n\r\n* To provide a forum for established researchers and practitioners to present past and current research contributing to the state of the art of Web technology research and applications.\r\n\r\n* To give doctoral students an opportunity to present their research to a friendly and knowledgeable audience and receive valuable feedback.\r\n\r\n* To provide an informal social event where Web technology researchers and practitioners can meet.\r\n\r\nWIMS conference is aimed at the whole web community, from industry to research, or from developers to users and designers.", "recorded": "2014-10-09T13:47:19", "title": "International Conference Series on Web Intelligence, Mining and Semantics (WIMS)"}, {"url": "elex2011_round_table", "desc": "The field of Natural Language Processing continues to make progress in addressing major challenges such as machine translation, multilingual search, word-sense disambiguation, and text remediation. And for some time, dictionary-makers have been exploiting the fruits of NLP research. As this collaboration continues, opportunities arise for developing various applications and tools which, collectively, perform the same functions as \u2018the dictionary\u2019. The e-lexicography conference series is premised on the idea that print-based dictionaries will largely disappear, as content migrates to digital media. But what sort of future does the dictionary have in any medium \u2013 is the e-dictionary just another step on the way to extinction, or will older modes persist alongside new types of resource?", "recorded": "2011-11-11T17:51:00", "title": "Will there still be dictionaries in 2020?"}, {"url": "elex2011_paquot_table", "desc": "The field of Natural Language Processing continues to make progress in addressing major challenges such as machine translation, multilingual search, word-sense disambiguation, and text remediation. And for some time, dictionary-makers have been exploiting the fruits of NLP research. As this collaboration continues, opportunities arise for developing various applications and tools which, collectively, perform the same functions as \u2018the dictionary\u2019. The e-lexicography conference series is premised on the idea that print-based dictionaries will largely disappear, as content migrates to digital media. But what sort of future does the dictionary have in any medium \u2013 is the e-dictionary just another step on the way to extinction, or will older modes persist alongside new types of resource?", "recorded": "2011-11-11T17:41:00", "title": "Will there still be dictionaries in 2020?"}, {"url": "mmdss07_feldman_latm", "desc": "The information age has made it easy to store large amounts of\r data.The proliferation of documents available on the Web, on corporate\r intranets, on news wires, and elsewhere is overwhelming. However, while the\r amount of data available to us is constantly increasing, our ability to absorb and\r process this information remains constant. Search engines only exacerbate the\r problem by making more and more documents available in a matter of a few\r key strokes. Link Analysis is a new and exciting research area that tries to solve\r the information overload problem by using techniques from data mining,\r machine learning, Information Extraction, Text Categorization, Visualization\r and Knowledge Management.", "recorded": "2007-09-18T09:15:52", "title": "Link Analysis and Text Mining : Current State of the Art and Applications for Counter Terrorism"}, {"url": "iswc06_studer_sc", "desc": "The notion of the Semantic Web can be coined as a Web of data when bringing database content to the Web or as a Web of enriched human-readable content when encoding the semantics of web-resources in a machine-interpretable form.\r \\\\ \r It has been clear from the beginning that realizing the Semantic Web vision will require interdisciplinary research. At this the fifth ISWC, it is time to re-examine the extent to which interdisciplinary work has played and can play a role in Semantic Web research, and even how Semantic Web research can contribute to other disciplines. Core Semantic Web research has drawn from various disciplines, such as knowledge representation and formal ontologies, reusing and further developing their techniques in a new context.", "recorded": "2006-11-06T00:00:00", "title": "The Semantic Web: Suppliers and Customers"}, {"url": "dataforum2013_lewis_linked_data", "desc": "The outsourced language services market represents a large international industry with established commercial models for data reuse focussed of sentence and term translations. This paper presents emerging meta-data standards and linked data vocabularies that can now be combined to provide process and provenance annotation of such language data as linked data. These solutions serve both to reduce the current high level of interoperability overhead costs experienced across translation value chains as well as offering new opportunities for commercialising more fine-grained reuse of language data in data-driven language technologies such as machine translation and named entity recognition.", "recorded": "2013-04-10T15:00:00", "title": "Linked Data Reuse in the Language Services Industry"}, {"url": "pesb07_mjolsness_mrf", "desc": " \r \r Estimating parameters in biochemical network models is a central but often difficult problem. A general approach that may be worth developing further is first to seek simplified or \"reduced\" models with fewer dynamical degrees of freedom, estimate parameters for the reduced models, and then use that information to constrain the corresponding parameters in the full model. This approach can leverage appropriate human expertise and could in principle be applied recursively. The choice of variables to eliminate during model reduction could also be made by clustering or other machine learning methods. Some relevant model reductions already exist for quasi-equilibrium models of transcriptional regulation networks, which could provide a starting point for this strategy. ", "recorded": "2007-03-28T15:00:00", "title": "Model Reduction for Parameter Estimation"}, {"url": "elex2011_krek_table", "desc": "The field of Natural Language Processing continues to make progress in addressing major challenges such as machine translation, multilingual search, word-sense disambiguation, and text remediation. And for some time, dictionary-makers have been exploiting the fruits of NLP research. As this collaboration continues, opportunities arise for developing various applications and tools which, collectively, perform the same functions as \u2018the dictionary\u2019. The e-lexicography conference series is premised on the idea that print-based dictionaries will largely disappear, as content migrates to digital media. But what sort of future does the dictionary have in any medium \u2013 is the e-dictionary just another step on the way to extinction, or will older modes persist alongside new types of resource?", "recorded": "2011-11-11T16:45:00", "title": "Will there still be dictionaries in 2020?"}, {"url": "elex2011_rundell_table", "desc": "The field of Natural Language Processing continues to make progress in addressing major challenges such as machine translation, multilingual search, word-sense disambiguation, and text remediation. And for some time, dictionary-makers have been exploiting the fruits of NLP research. As this collaboration continues, opportunities arise for developing various applications and tools which, collectively, perform the same functions as \u2018the dictionary\u2019. The e-lexicography conference series is premised on the idea that print-based dictionaries will largely disappear, as content migrates to digital media. But what sort of future does the dictionary have in any medium \u2013 is the e-dictionary just another step on the way to extinction, or will older modes persist alongside new types of resource?", "recorded": "2011-11-11T16:52:00", "title": "Will there still be dictionaries in 2020?"}, {"url": "elex2011_mckean_table", "desc": "The field of Natural Language Processing continues to make progress in addressing major challenges such as machine translation, multilingual search, word-sense disambiguation, and text remediation. And for some time, dictionary-makers have been exploiting the fruits of NLP research. As this collaboration continues, opportunities arise for developing various applications and tools which, collectively, perform the same functions as \u2018the dictionary\u2019. The e-lexicography conference series is premised on the idea that print-based dictionaries will largely disappear, as content migrates to digital media. But what sort of future does the dictionary have in any medium \u2013 is the e-dictionary just another step on the way to extinction, or will older modes persist alongside new types of resource?", "recorded": "2011-11-11T17:05:00", "title": "Will there still be dictionaries in 2020?"}, {"url": "elex2011_tiberius_table", "desc": "The field of Natural Language Processing continues to make progress in addressing major challenges such as machine translation, multilingual search, word-sense disambiguation, and text remediation. And for some time, dictionary-makers have been exploiting the fruits of NLP research. As this collaboration continues, opportunities arise for developing various applications and tools which, collectively, perform the same functions as \u2018the dictionary\u2019. The e-lexicography conference series is premised on the idea that print-based dictionaries will largely disappear, as content migrates to digital media. But what sort of future does the dictionary have in any medium \u2013 is the e-dictionary just another step on the way to extinction, or will older modes persist alongside new types of resource?", "recorded": "2011-11-11T17:16:00", "title": "Will there still be dictionaries in 2020?"}, {"url": "elex2011_vossen_table", "desc": "The field of Natural Language Processing continues to make progress in addressing major challenges such as machine translation, multilingual search, word-sense disambiguation, and text remediation. And for some time, dictionary-makers have been exploiting the fruits of NLP research. As this collaboration continues, opportunities arise for developing various applications and tools which, collectively, perform the same functions as \u2018the dictionary\u2019. The e-lexicography conference series is premised on the idea that print-based dictionaries will largely disappear, as content migrates to digital media. But what sort of future does the dictionary have in any medium \u2013 is the e-dictionary just another step on the way to extinction, or will older modes persist alongside new types of resource?", "recorded": "2011-11-11T17:26:00", "title": "Will there still be dictionaries in 2020?"}, {"url": "elex2011_kilgarriff_table", "desc": "The field of Natural Language Processing continues to make progress in addressing major challenges such as machine translation, multilingual search, word-sense disambiguation, and text remediation. And for some time, dictionary-makers have been exploiting the fruits of NLP research. As this collaboration continues, opportunities arise for developing various applications and tools which, collectively, perform the same functions as \u2018the dictionary\u2019. The e-lexicography conference series is premised on the idea that print-based dictionaries will largely disappear, as content migrates to digital media. But what sort of future does the dictionary have in any medium \u2013 is the e-dictionary just another step on the way to extinction, or will older modes persist alongside new types of resource?", "recorded": "2011-11-11T17:37:00", "title": "Will there still be dictionaries in 2020?"}, {"url": "nipsworkshops2011_preference_learning", "desc": "    The workshop is motivated by the two following lines of research:\r\n\r\n    1. Large scale preference learning with sparse data: There has been a great interest and take-up of machine learning techniques for preference learning in learning to rank, information retrieval and recommender systems, as supported by the large proportion of preference learning based literature in the widely regarded conferences such as SIGIR, WSDM, WWW, and CIKM. Different paradigms of machine learning have been further developed and applied to these challenging problems, particularly when there is a large number of users and items but only a small set of user preferences are provided. \r\n\r\n    2. Personalization in social networks: recent wide acceptance of social networks has brought great opportunities for services in different domains, thanks to Facebook, LinkedIn, Douban, Twitter, etc. It is important for these service providers to offer personalized service (e.g., personalization of Twitter recommendations).  Social information can improve the inference for user preferences. However, it is still challenging to infer user preferences based on social relationship. \r\n\r\n    As such, we especially encourage submissions on theory, methods, and applications focusing on large-scale preference learning and choice models in social media. In order to avoid a dispersed research workshop, we solicit submissions (papers, demos and project descriptions) and participation that specifically tackle the research areas as below:\r\n\r\n        * Preference elicitation\r\n        * Ranking aggregation\r\n        * Choice models and inference\r\n        * Statistical relational learning for preferences\r\n        * Link prediction for preferences\r\n        * Learning Structured Preferences\r\n        * Multi-task preference learning \r\n        * (Social) collaborative filtering\r\n\r\nWorkshop homepage: https://sites.google.com/site/cmplnips11/", "recorded": "2011-12-17T07:30:00", "title": "Choice Models and Preference Learning"}, {"url": "ida07_chervonenkis_ep", "desc": "Many methods of Machine Learning are based on the idea of empirical risk minimisation. It is to find a decision rule or a model from some set which most perfectly fits the data presented in the training set. This idea is based on the large number law: empirical risk converges to real risk, if the training set is large enough. But if the class of decision rules or models is too large (in some sense) one meets the problem of oferfitting, the model perfectly corresponds to the data presented in the training set, but shows large errors on new data. It is due to the fact that only uniform convergence of empirical risk to the real risk guarantees closeness of the optimal model behaviour on the training set and on the new data. We introduce the notion of entropy of a decision rule class over a fixed sample sequence as log of the number of possible classifications of the sequence by the rules of the class. Maximum entropy over sequences of a fixed length l determines sufficient condition of the uniform convergence and corresponding estimates. But only average entropy H(l) behaviour determine necessary and sufficient condition of the uniform convergence. The condition is that H(l) / l (average entropy per symbol) should go to zero when the sequence length goes to infinity. If the condition does not hold then there exists a set of objects with non zero probability measure, such that almost all sequences of arbitrary finite length from this set may be divided in all possible ways by the rules of the class. One can easily see, that in this case overfitting is inevitable. Similar results are found for real dependencies instead of decision rules.", "recorded": "2007-09-07T09:00:00", "title": "Entropy Properties of a Decision Rule Class in Connection with machine learning abilities"}, {"url": "eswc2014_crete", "desc": "The ESWC is a major venue for discussing the latest scientific results and technology innovations around semantic technologies. Building on its past success, ESWC is seeking to broaden its focus to span other relevant research areas in which Web semantics plays an important role.\r\n\r\nThe goal of the Semantic Web is to create a Web of knowledge and services in which the semantics of content is made explicit and content is linked to both other content and services novel applications allowing to combine content from heterogeneous sites in unforeseen ways and support enhanced matching between users needs and content. This network of knowledge-based functionality will weave together a large network of human knowledge, and make this knowledge machine-processable to support intelligent behaviour by machines. Creating such an interlinked Web of knowledge which spans unstructured, RDF as well as multimedia content and services requires the collaboration of many disciplines, including but not limited to: Artificial Intelligence, Natural Language Processing, Database and Information Systems, Information Retrieval, Machine Learning Multimedia, Distributed Systems, Social Networks, Web Engineering, and Web Science. These complementarities are reflected in the outline of the technical program of the ESWC 2014; in addition to the research and in-use tracks, we feature two special tracks putting particular emphasis on inter-disciplinary research topics and areas that show the potential of exciting synergies for the future, eGovernment and Digital Libraries. ESWC 2014 presents the latest results in research, technologies and applications in its field.\r\n\r\nFor more information about the event please visit the [[http://2014.eswc-conferences.org/|ESWC 2014 website]].", "recorded": "2014-05-25T09:00:00", "title": "11th Extended Semantic Web Conference (ESWC), Crete 2014"}, {"url": "eswc2010_heraklion", "desc": "The mission of the Extended Semantic Web Conference (ESWC 2010) is to bring together researchers and practioners dealing with different aspects of semantics on the Web. ESWC2010 builds on the success of the former European Semantic Web Conference series, but seeks to extend its focus by engaging with other communities within and outside ICT, in which semantics can play an important role. At the same time, ESWC2010 is a truly international conference.\r\n\r\nSemantics of web content, enriched with domain theories (ontologies), data about web usage, natural language processing, etc. will enable a web that provides a qualitatively new level of functionality. It will weave together a large network of human knowledge and make this knowledge machine-processable. Various automated services, based on reasoning with metadata and ontologies, will help the users to achieve their goals by accessing and processing information in machine-understandable form. This network of knowledge systems will ultimately lead to truly intelligent systems, which will be employed for various complex decision-making tasks. Research about web semantics can benefit from ideas and cross-fertilization with many other areas: Artificial Intelligence, Natural Language Processing, Database and Information Systems, Information Retrieval, Multimedia, Distributed Systems, Social Networks, Web Engineering, and Web Science.\r\n\r\nESWC2010 will present the latest results in research and applications in its field. The research program will be organised in targeted tracks. In addition, the conference will feature a tutorial program, system descriptions and demos, a posters track, a Ph.D. symposium and a number of collocated workshops. The calls for these events are separate and can be found on the conference Web site (http://www.eswc2010.org/).", "recorded": "2010-05-30T09:00:00", "title": "7th Extended Semantic Web Conference (ESWC), Heraklion 2010"}, {"url": "nips2011_abernethy_prediction", "desc": "Machine Learning competitions such as the Netflix Prize have proven reasonably successful as a method of \u201ccrowdsourcing\u201d prediction tasks. But these competitions have a number of weaknesses, particularly in the incentive structure they create for the participants. We propose a new approach, called a Crowdsourced Learning Mechanism, in which participants collaboratively \u201clearn\u201d a hypothesis for a given prediction task. The approach draws heavily from the concept of a prediction market, where traders bet on the likelihood of a future event. In our framework, the mechanism continues to publish the current hypothesis, and participants can modify this hypothesis by wagering on an update. The critical incentive property is that a participant will profit an amount that scales according to how much her update improves performance on a released test set.", "recorded": "2011-12-14T16:00:00", "title": "A Collaborative Mechanism for Crowdsourcing Prediction Problems"}, {"url": "mlsb07_vert_srb", "desc": "The inference or reconstruction of various biological networks, including regulatory, signalling or metabolic pathways, from large-scale heterogeneous data is currently an active research subject with several important applications in systems biology. While several approaches proposed so far cast this problem as inferring a graph de novo from genomic data, I will argue in this talk that the network of interest is often partially known and that the reconstruction process should use this partial knowledge to guide the inference of the missing edges. I will then review how this paradigm leads naturally to various supervised machine learning algorithms for graph inference, and illustrate the relevance of the approach through several examples of successful prediction of missing enzymes in metabolic networks.", "recorded": "2007-09-24T09:15:00", "title": "Supervised reconstruction of biological networks"}, {"url": "icml08_szafranski_ckl", "desc": "The Support Vector Machine (SVM) is an acknowledged powerful tool for building classifiers, but it lacks flexibility, in the sense that the kernel is chosen prior to learning. Multiple Kernel Learning (MKL) enables to learn the kernel, from an ensemble of basis kernels, whose combination is optimized in the learning process. Here, we propose Composite Kernel Learning to address the situation where distinct components give rise to a group structure among kernels. Our formulation of the learning problem encompasses several setups, putting more or less emphasis on the group structure. We characterize the convexity of the learning problem, and provide a general wrapper algorithm for computing solutions. Finally, we illustrate the behavior of our method on multi-channel data where groups correspond to channels.", "recorded": "2008-07-07T10:25:00", "title": "Composite Kernel Learning"}, {"url": "colt2015_agarwal_property_elicitation", "desc": "Surrogate risk minimization is a popular framework for supervised learning; property elicitation is a widely studied area in probability forecasting, machine learning, statistics and economics. In this paper, we connect these two themes by showing that calibrated surrogate losses in  supervised learning can essentially be viewed as eliciting or estimating\r\ncertain properties of the underlying conditional label distribution that are sufficient to construct an optimal classifier under the target loss of interest. Our study helps to shed light on the design of convex calibrated surrogates. We also give a new framework for designing convex calibrated surrogates under low-noise conditions by eliciting properties that allow one to construct \u2018coarse\u2019 estimates of the underlying distribution.\r\n ", "recorded": "2015-07-05T10:30:00", "title": "On Consistent Surrogate Risk Minimization and Property Elicitation"}, {"url": "ida07_poel_asvma", "desc": "Part-of-Speech tagging, the assignment of Parts-of-Speech\r to the words in a given context of use, is a basic technique in many\r systems that handle natural languages. This paper describes a method\r for supervised training of a Part-of-Speech tagger using a committee of\r Support Vector Machines on a large corpus of annotated transcriptions\r of spoken Dutch. Special attention is paid to the decomposition of the\r large data set into parts for common, uncommon and unknown words.\r This does not only solve the space problems caused by the amount of\r data, it also improves the tagging time. The performance of the resulting\r tagger in terms of accuracy is 97.54 %, which is quite good, where the\r speed of the tagger is reasonably good.", "recorded": "2007-09-06T16:50:00", "title": "A Support Vector Machine Approach to Dutch Part-of-Speech Tagging"}, {"url": "icml2015_shen_yinyang_k_means", "desc": "This paper presents Yinyang K-means, a new algorithm for K-means clustering. By clustering the centers in the initial stage, and leveraging efficiently maintained lower and upper bounds between a point and centers, it more effectively avoids unnecessary distance calculations than prior algorithms. It significantly outperforms classic K-means and prior alternative K-means algorithms consistently across all experimented data sets, cluster numbers, and machine configurations. The consistent, superior performance\u2014plus its simplicity, user-control of overheads, and guarantee in producing the same clustering results as the standard K-means does\u2014makes Yinyang K-means a drop-in replacement of the classic K-means with an order of magnitude higher performance.", "recorded": "2015-07-09T14:46:53", "title": "Yinyang K-Means: A Drop-In Replacement of the Classic K-Means with Consistent Speedup"}, {"url": "nipsworkshops2010_ye_gbd", "desc": "Stochastic Gradient Boosted Decision Trees (GBDT) is one of the most widely used learning algorithms in machine learning today. It is adaptable, easy to interpret, and produces highly accurate models. However, most implementations today are computationally expensive and require all training data to be in main memory. As training data becomes ever larger, there is motivation for us to parallelize the GBDT algorithm. Parallelizing decision tree training is intuitive and various approaches have been explored in existing literature. Stochastic boosting on the other hand is inherently a sequential process and have not been applied to distributed decision trees. In this paper, we describe a distributed implementation of GBDT that utilizes MPI on the Hadoop grid environment as presented by us at CIKM in 2009.", "recorded": "2010-12-11T17:20:00", "title": "Gradient Boosted Decision Trees on Hadoop"}, {"url": "bootcamp07_higuera_bacm", "desc": "Between the many theoretical computer science issues that one should be\r\n aware of when working in Machine learning, we visit, in this series of\r\n lectures, two.\r\n \r\n The first corresponds to strings, and through the study of strings, the\r\n questions about more complex structures like trees and graphs. We describe\r\n the main algorithmic and combinatorial questions about substrings and\r\n subsequences, and concentrate our attention to the topological questions:\r\n ordering strings and computing distances and kernels.\r\n \r\n The second is complexity. Not only should we be aware (and have a\r\n reasonable control of the techniques involved) of the usual barriers, but\r\n we should know something about classes for randomized algorithms. We also\r\n show some examples concerning Las Vegas and Monte Carlo techniques.\r\n ", "recorded": "2007-07-02T10:30:00", "title": "Basics of algorithmics, computation models, formal languages"}, {"url": "deeplearning2015_montreal", "desc": "Deep neural networks that learn to represent data in multiple layers of increasing abstraction have dramatically improved the state-of-the-art for speech recognition, object recognition, object detection, predicting the activity of drug molecules, and many other tasks. Deep learning discovers intricate structure in large datasets by building distributed representations, either via supervised, unsupervised or reinforcement learning.\r\n\r\n[[https://sites.google.com/site/deeplearningsummerschool/|The Deep Learning Summer School 2015]] is aimed at graduate students and industrial engineers and researchers who already have some basic knowledge of machine learning (and possibly but not necessarily of deep learning) and wish to learn more about this rapidly growing field of research.", "recorded": "2015-08-03T00:00:00", "title": "Deep Learning Summer School, Montreal 2015"}, {"url": "w3cworkshop2013_nickel_lieske_contexts", "desc": "Textual content still dominates the Web. The linguistic quality of textual content\u2014correct spelling, terminology, grammar and style\u2014is of the utmost importance for various content-related processes. Search engines and Machine Translation systems, for example, become more accurate if they operate on high-quality content. Given the volume of content on the Web, automation is important for linguistic quality management. The presentation will address LanguageTool, an adaptable open-source tool that has implemented support for the currently drafted ITS 2.0 specification. It will focus on the experience of adapting LanguageTool in a real-world scenario (examples will be drawn from Russian and English) and using it for checking compliance with governmental regulations like the German BITV2.", "recorded": "2013-03-12T15:05:00", "title": "Tool-Supported Linguistic Quality in Web-Related Multilanguage Contexts"}, {"url": "ecmlpkdd2011_sra_dictionary", "desc": "We introduce Generalized Dictionary Learning (GDL), a simple but practical framework for learning dictionaries over the manifold of positive definite matrices. We illustrate GDL by applying it to Nearest Neighbor (NN) retrieval, a task of fundamental importance in disciplines such as machine learning and computer vision. GDL distinguishes itself from traditional dictionary learning approaches by explicitly taking into account the manifold structure of the data. In particular, GDL allows performing \"sparse coding\" of positive definite matrices, which enables better NN retrieval. Experiments on several covariance matrix datasets show that GDL achieves performance rivaling state-of-the-art techniques.", "recorded": "2011-09-06T16:50:00", "title": "Generalized Dictionary Learning for Symmetric Positive Definite Matrices with Application to Nearest Neighbor Retrieval"}, {"url": "wapa2011_garcia_saiz_performance", "desc": "Virtual teaching is constantly growing and, with it, the necessity of instructors to predict the performance of their students. In response to this necessity, different machine learning techniques can be used. Although there are so many benchmarks comparing their performance and accuracy, there are still very few experiments carried out on educational datasets which have very special features which make them different from other datasets. Therefore, in this work we compare the performance and interpretation level of the output of the different classification techniques applied on educational datasets and propose a meta-algorithm to preprocess the datasets and improve the accuracy of the model, which will be used by virtual instructors for their decision making through the ElWM tool. ", "recorded": "2011-10-19T16:45:00", "title": "Comparing classification methods for predicting distance\u000bstudents' performance"}, {"url": "mitworld_kaelbling_wrcl", "desc": "In recent years, machine learning methods have enjoyed great success in a variety of applications. Unfortunately, on-line learning in autonomous agents has not generally been one of them. Reinforcement-learning methods that were developed to address problems of learning agents have been most successful in off-line applications. This talk will briefly review the basic methods of reinforcement learning, point out some of their shortcomings, argue that we are expecting too much from such methods, and speculate about how to build complex, adaptive autonomous agents. These speculations are backed up by recent results demonstrating that a small amount of human-provided input can dramatically speed learning in a real mobile robot.", "recorded": "2001-11-19T17:47:44", "title": "Why Robbie Can't Learn: The Difficulty of Learning in Autonomous Agents"}, {"url": "nips09_koller_ugrntv", "desc": "A key biological question is to uncover the regulatory networks in a cellular system and to understand how this network varies across individuals, cell types, and environmental conditions. In this talk I will describe work that uses machine learning techniques to reconstruct gene regulatory networks from gene expression data. Specifically, we exploit novel forms of Bayesian regularized regression to enable transfer between multiple related learning problems, such as between different individuals or between different cell types. We demonstrate applications in two domains: understanding the effect of individual genetic variation on gene regulation and its effect on phenotypes including human disease; and understanding the regulatory mechanisms underling immune system cell differentiation.", "recorded": "2009-12-10T13:30:00", "title": "Understanding Gene Regulatory Networks and Their Variations"}, {"url": "different06_whistler", "desc": "Many machine learning algorithms assume that the training and the test data are drawn from the same distribution. Indeed many of the proofs of statistical consistency, etc., rely on this assumption. However, in practice we are very often faced with the situation where the training and the test data both follow the same conditional distribution, p(y|x), but the input distributions, p(x), differ. For example, principles of experimental design dictate that training data is acquired in a specific manner that bears little resemblance to the way the test inputs may later be generated. \r\n\r\nThe aim of this workshop is to try and shed light on the kind of situations where explicitly addressing the difference in the input distributions is beneficial, and on what the most sensible ways of doing this are. ", "recorded": "2006-12-09T08:00:00", "title": "NIPS Workshop on Learning when Test and Training Inputs Have Different Distributions, Whistler 2006"}, {"url": "icml2015_ene_decomposable_submodular_functions", "desc": "Submodular function minimization is a fundamental optimization problem that arises in several applications in machine learning and computer vision. The problem is known to be solvable in polynomial time, but general purpose algorithms have high running times and are unsuitable for large-scale problems. Recent work have used convex optimization techniques to obtain very practical algorithms for minimizing functions that are sums of \u201csimple\u201d functions. In this paper, we use random coordinate descent methods to obtain algorithms with faster linear convergence rates and cheaper iteration costs. Compared to alternating projection methods, our algorithms do not rely on full-dimensional vector operations and they converge in significantly fewer iterations.", "recorded": "2015-07-09T14:46:53", "title": "Random Coordinate Descent Methods for Minimizing Decomposable Submodular Functions"}, {"url": "w3cworkshop2011_lewis_semantic", "desc": "This talk will present a Semantic Model for end-to-end multilingual web content processing flows that encompass content generation, its localisation and its adaptive presentation to users. The Semantic Model is captured in the RDF language in order to both provide semantic annotation of web services and to explore the benefits of using federated triple stores, which form the Linked Open Data cloud that is powering a new range of real world applications. Key applications include the provenance-based Quality Assurance of content localisation and the harvesting and data cleaning of translated web content and terminology needed to train data-driven components such as statistical machine translation and text classifiers.", "recorded": "2011-04-05T09:00:00", "title": "Semantic Model for end-to-end multilingual web content processing"}, {"url": "icml2010_stodden_rric", "desc": "Scientific computation is emerging as absolutely central to the scientific method, but the prevalence of very relaxed practices is leading to a credibility crisis. Reproducible computational research, in which all details of computations\u2014code and data\u2014are made conveniently available to others, is a necessary response to this crisis. Results from a 2009 survey of the Machine Learning community (NIPS participants) designed to elucidate factors that affect data and code sharing will be presented. Intellectual property concerns create a significant barrier to sharing, and I will also present work on the \u201cReproducible Research Standard\u201d giving open licensing options designed to create an intellectual property framework for scientists consonant with longstanding scientific norms and facilitating reproducible research.\r\n", "recorded": "2010-06-25T16:25:00", "title": "Reproducible Research in Computational Science: Problems and Solutions For Data and Code Sharing\u2028"}, {"url": "mitworld_kaelbling_robbie", "desc": "In recent years, machine learning methods have enjoyed great success in a variety of applications. Unfortunately, on-line learning in autonomous agents has not generally been one of them. Reinforcement-learning methods that were developed to address problems of learning agents have been most successful in off-line applications. This talk will briefly review the basic methods of reinforcement learning, point out some of their shortcomings, argue that we are expecting too much from such methods, and speculate about how to build complex, adaptive autonomous agents. These speculations are backed up by recent results demonstrating that a small amount of human-provided input can dramatically speed learning in a real mobile robot.", "recorded": "2001-11-19T15:14:22", "title": "Why Robbie Can't Learn: The Difficulty of Learning in Autonomous Agents"}, {"url": "uai08_ganchev_mvl", "desc": "In many machine learning problems, labeled\r training data is limited but unlabeled data\r is ample. Some of these problems have instances that can be factored into multiple\r views, each of which is nearly sufficient in determining the correct labels. In this paper\r we present a new algorithm for probabilistic\r multi-view learning which uses the idea of\r stochastic agreement between views as regularization. Our algorithm works on structured and unstructured problems and easily generalizes to partial agreement scenarios. For the full agreement case, our algorithm minimizes the Bhattacharyya distance between the models of each view, and performs better than CoBoosting and two-view Perceptron on several flat and structured classification problems.", "recorded": "2008-07-11T09:00:00", "title": "Multi-View Learning over Structured and Non-Identical Outputs"}, {"url": "cmulls08_he_rcd", "desc": "Given an unlabeled unbalanced data set, the goal of rare category detection is to discover examples from the minority classes with a few label requests. Rare category detection is an open challenge in machine learning, and it has a lot of applications, such as financial fraud detection, network intrusion detection, astronomy, spam image detection, etc. In this talk, I will introduce two methods for rare category detection with spatial data. The first one essentially performs local density differential sampling, and it requires the prior information about the data set as input. The second one is based on specially designed exponential families, and it is prior-free. Experimental results demonstrate the effectiveness of these methods on different real data sets. ", "recorded": "2008-11-17T12:00:00", "title": "Rare Category Detection for Spatial Data"}, {"url": "mlsb2012_stojanova_ppi", "desc": "**Motivation:** Catalogs, such as Gene Ontology (GO) and MIPS-FUN, assume that functional classes\r\nare organized hierarchically (general functions include more specific functions). This has recently\r\nmotivated the development of several machine learning algorithms under the assumption that instances\r\nmay belong to multiple hierarchy organized classes. Besides relationships among classes,\r\nit is also possible to identify relationships among examples. Although such relationships have been\r\nidentified and extensively studied in the in the area of protein-to-protein interaction (PPI)\r\nnetworks, they have not received much attention in hierarchical protein function prediction. The\r\nuse of such relationships between genes introduces autocorrelation and violates the assumption\r\nthat instances are independently and identically distributed, which underlines most machine\r\nlearning algorithms. While this consideration introduces additional complexity to the learning\r\nprocess, we expect it would also carry substantial benefits.\\\\\r\n**Results:** This article demonstrates the benefits (in terms of predictive accuracy) of considering autocorrelation\r\nin multi-class gene function prediction. We develop a tree-based algorithm for considering\r\nnetwork autocorrelation in the setting of Hierarchical Multi-label Classification (HMC). The\r\nempirical evaluation of the proposed algorithm, called NHMC, on 24 yeast datasets using MIPSFUN\r\nand GO annotations and exploiting three different PPI networks, clearly shows that taking\r\nautocorrelation into account improves performance.\\\\\r\n**Conclusions:** Our results suggest that explicitly taking network autocorrelation into account increases\r\nthe predictive capability of the models, especially when the underlying PPI network is\r\ndense. Furthermore, NHMC can be used as a tool to assess network data and the information it\r\nprovides with respect to the gene function.", "recorded": "2012-09-08T16:30:00", "title": "Using PPI Networks in hierarchical multi-label classification trees for gene function prediction"}, {"url": "mlsb07_ratsch_dcs", "desc": "In order to characterize natural sequence variation in 20 strains of the model plant Arabidopsis thaliana, whole-genome resequencing with high-density oligonucleotide arrays was performed in collaboration with Perlegen Sciences Inc. Array data were analyzed with a combination of existing model-based (MB; Hinds et al., Science, 2005) and novel machine learning (ML) methods. For the identification of single nucleotide polymorphisms (SNPs) we developed an algorithm based on support vector machines. Training and evaluation was done on published alignments (Nordborg et al., PLoS Biology, 2005). At the same false discovery rates (FDR) as MB, the ML algorithm identifies significantly more true SNPs, especially in regions of high polymorphism density and/or low hybridization quality. The union of SNP predictions from both methods contains on average 143,572 SNPs per strain at a FDR of 2.8% (648,570 non-redundant SNPs). Furthermore, a machine learning algorithm was developed to detect polymorphic regions containing insertions, deletions and variational hotspots, where SNP detection algorithms typically fail to identify individual SNPs. It discovers the approximate location of a substantial additional proportion of polymorphisms (54% of deleted nucleotides and 33% of insertion sites). With a combination of all three methods 74% of SNPs can be directly called or are contained in a polymorphic region prediction (Zeller et al., in preparation). We examined the patterns of and forces shaping sequence variation in Arabidopsis (Clark et al., Science, 2007): e.g. significant differences were observed between gene families, and genes mediating interaction with the biotic environment harbor exceptional polymorphism levels.", "recorded": "2007-09-25T14:30:00", "title": "Discovering Common Sequence Variation in Arabidopsis thaliana"}, {"url": "fmri06_whistler", "desc": "In the past five years machine learning classifiers have met great interest in the field of cognitive neuroscience. The use of classifiers for decoding has emerged as a powerful technique that enables researchers to make predictions about the mental state of a subject directly from fMRI data. This work has received considerable attention because it is seen as a way to overcome limitations of more conventional fMRI analysis methods. Whereas conventional fMRI research is focused on spatially localising cognitive modules, decoding-based research allows for the first time the study of the neural encoding of specific mental contents in the human brain.\n\n The recent progress has also raised a number of fundamental questions, about the practice of decoding, the interpretation of results and their implications for theories of cognitive neuroscience. At a high level, one would like to know how decoding can help model-building in cognitive neuroscience and, ultimately, help develop theories of neural representation that explain the decoding-identified structure in the fMRI data. Conversely, we have the question of whether classifiers can be used as a confirmatory scientific tool for existing theories or hypotheses.\n\n We think that, given the number and type of specific open questions, this is more than just another application domain and thus there is space for machine learning researchers to come up with new methods or creative application and combination of existing ones. This workshop is designed to facilitate their entry into this field and put them in contact with cognitive neuroscientists receptive to their methods, as well as provide a venue for discussion of those questions as a possible agenda for the field.\n\nMore information about the workshop can be found [[http://www.cs.cmu.edu/~fmri/workshop/call.html|here]].", "recorded": "2006-12-08T08:00:00", "title": "NIPS Workshop on New Directions on Decoding Mental States from fMRI Data, Whistler 2006"}, {"url": "lmcv04_vetter_clafa2", "desc": "The variability of images of the human face challenges research in machine vision since its beginning. Sources of variability not only include individual appearance but also cover external parameters such as perspective and illumination that influence the image formation process heavily. Research on the analysis of face images currently splits into the directions of face detection and face recognition. Approaches and problem setting in this two areas are still quite different despite the common goal to compensate for the large variability of faces in images. In both areas machine learning strategies are used to learn from example faces a general model of the appearance of human faces. In the first part of our presentation we would like to review the current state of the art in face detection and in face recognition. In a second part we will compare the different strategies used and try to describe the requirements of a general image model that could serve as basis in detection as well as in recognition research. For face detection we will focus on methods based on the estimation of the probability distribution of large number of features computed on face examples. After selecting the subset of features which appear to be most relevant to the task, faces are detected by combining the outcome of suitably defined statistical tests. This method, which is based on positive examples only, seems to give very promising results compared to state of the art techniques based on both positive and negative examples. In face recognition we will concentrate on methods that use an analysis by synthesis framework such as morphable models, active appearance or shape models. Currently these approaches seem the most promising methods able to account for variations in perspective and illumination.", "recorded": "2004-05-04T00:00:00", "title": "Challenges in Learning the Appearance of Faces for Automated Image Analysis - Part 2"}, {"url": "russir08_moens_tmife", "desc": "communities (medical informatics, security, blog and news analysis, business information analysis, legal informatics, etc.). ?Still, today it is a somewhat fragmented subfield of human language technologies and information retrieval where the themes of (often forgotten) old-style pattern-based IE and more recent machine learning techniques, as applied in medical informatics, opinion mining and blog extraction, are scattered in various conferences and sessions (computational linguistics, artificial intelligence, machine learning, Web technologies, semantic computing).\r\nThe aim of this tutorial is to explain important technologies from handcrafted patterns to learning, and especially focus on how they blend together in order to suit the needs of current information systems that retrieve or mine information, or that make decisions and solve problems based on the extracted information. This unified perspective also entails valuable insights into the role of traditional pipelined system architectures and more recent probabilistic inference techniques.\r\nProbabilistic extraction, by which text is translated into a variety of semantic labels, pe\"../slides/rfectly integrates with probabilistic retrieval models that naturally combine surface text features and semantic labels in ranking computations, among which are the popular language retrieval models. Finally, information extraction alleviates the knowledge acquisition bottleneck in expert and question answering systems technology that operate in more restricted subject domains.\r\nWe conclude with some pointers to new challenges among which are the recognition of complex semantic concepts (e.g., narrative scripts, or issues such as medical malpractice or competitiveness) in texts.\r\nBecause of the reconciling aspects of the many techniques and application domains, the tutorial will attract students and researchers with different backgrounds. ", "recorded": "2008-09-01T09:00:00", "title": "Text Mining, Information and Fact Extraction (TMIFE)"}, {"url": "ecmlpkdd2011_frasca_cosnet", "desc": "The semi-supervised problem of learning node labels in graphs consists, given a partial graph labeling, in inferring the unknown labels of the unlabeled vertices. Several machine learning algorithms have been proposed for solving this problem, including Hopfield networks and label propagation methods; however, some issues have been only partially considered, e.g. the preservation of the prior knowledge and the unbalance between positive and negative labels. To address these items, we propose a Hopfield-based cost sensitive neural network algorithm (COSNet). The method factorizes the solution of the problem in two parts: 1) the subnetwork composed by the labelled vertices is considered, and the network parameters are estimated through a supervised algorithm; 2) the estimated parameters are extended to the subnetwork composed of the unlabeled vertices, and the attractor reached by the dynamics of this subnetwork allows to predict the labeling of the unlabeled vertices. The proposed method embeds in the neural algorithm the \"a priori\" knowledge coded in the labelled part of the graph, and separates node labels and neuron states, allowing to differentially weight positive and negative node labels. Moreover, COSNet introduces an efficient costsensitive strategy which allows to learn the near-optimal parameters of the network in order to take into account the unbalance between positive and negative node labels. Finally, the dynamics of the network is restricted to its unlabeled part, preserving the minimization of the overall objective function and significantly reducing the time complexity of the learning algorithm. COSNet has been applied to the genome-wide prediction of gene function in a model organism. The results, compared with those obtained by other semi-supervised label propagation algorithms and supervised machine learning methods, show the effectiveness of the proposed approach.", "recorded": "2011-09-08T10:30:00", "title": "COSNet: a Cost Sensitive Neural Network for Semi-supervised Learning in Graphs"}, {"url": "lmcv04_verri_clafa1", "desc": "The variability of images of the human face challenges research in machine vision since its beginning. Sources of variability not only include individual appearance but also cover external parameters such as perspective and illumination that influence the image formation process heavily. Research on the analysis of face images currently splits into the directions of face detection and face recognition. Approaches and problem setting in this two areas are still quite different despite the common goal to compensate for the large variability of faces in images. In both areas machine learning strategies are used to learn from example faces a general model of the appearance of human faces. In the first part of our presentation we would like to review the current state of the art in face detection and in face recognition. In a second part we will compare the different strategies used and try to describe the requirements of a general image model that could serve as basis in detection as well as in recognition research. For face detection we will focus on methods based on the estimation of the probability distribution of large number of features computed on face examples. After selecting the subset of features which appear to be most relevant to the task, faces are detected by combining the outcome of suitably defined statistical tests. This method, which is based on positive examples only, seems to give very promising results compared to state of the art techniques based on both positive and negative examples. In face recognition we will concentrate on methods that use an analysis by synthesis framework such as morphable models, active appearance or shape models. Currently these approaches seem the most promising methods able to account for variations in perspective and illumination.", "recorded": "2004-05-04T00:00:00", "title": "Challenges in Learning the Appearance of Faces for Automated Image Analysis - Part 1"}, {"url": "lsoldm2014_thanh_bandit_algorithms", "desc": "Bayesian learning is a widely used class of techniques in the online machine learning community, especially in large\r\nscale domains, due to its conceptional simplicity and philosophical intuitions [12, 10, 4]. In particular, it expresses\r\nthe expert knowledge and the current belief of the world through the choice of a prior distribution, which is then\r\nsubsequently updated and refined by (future) observations. This concept is theoretically backed up (in parametric\r\nmodels) by the celebrated Bernstein-von Mises Theorem, which guarantees that under some mild conditions, the\r\nprocess of belief updates (i.e., the posteriori) asymptotically converges (within the parameter space) towards the\r\nparameters of the true distribution, from which the observations are in fact generated [14, 25, 15]. This implies that\r\nif we have sufficiently many updates (that typically tends to infinity), the solution derived from a Bayesian learning\r\ntechnique also converges to the true solution of the underlying problem (also under some non-restrictive conditions).\r\nHowever, in many real-world applications, we do not have access to indefinitely large number of updates, due\r\nto various physical constraints. For example, it might be difficult to carry out technically and monetarily involved\r\nexperiments in order to observe new updates, such as in medical trials [3] or sparse event detections [6]. In such\r\ncases, there is no theoretical underpinning for the correctness of Bayesian learning in general. To date, while the\r\nbehaviour of Bayesian methods is well understood from the asymptotic point of view [14, 11, 21], there is only a\r\nlimited number of results in the machine learning literature about the finite-time analysis (i.e., performance analysis\r\nwith finite samples) of Bayesian learning algorithms. Furthermore, these results are typically restricted to special\r\nclasses of conjugate distributions1 [1, 13], and thus, they cannot be extended into the general framework 2. Given this,\r\nit is essential to provide generic mechanisms that can guarantee the (near) optimal performance of the Bayesian online\r\nlearning approach for finite sample sizes, which can be applied to a variety of machine learning domains (i.e., they are\r\nnot tailored to special cases).\r\nAgainst this background, this paper proposes generic theoretical tools that are suitable for finite-time analysis of\r\nBayesian online learning methods. In particular, we derive two concentration inequalities that can be described as\r\nfollows. The first provides an upper bound for the amount of mass that a posteriori can put outside a small ball,\r\ncentred at the true parameter of interest, after n (finite) number of updates. The second provides an upper bound for\r\nthe probability that the distance between the expected value of the posteriori and the true parameter of interest is larger\r\nthan a certain small value. These inequalities only require mild assumptions on the prior and likelihood functions that\r\nhold in many practical statistical models.", "recorded": "2014-09-11T12:00:00", "title": "New Concentration Bounds and their Application in Finite-Time Analysis of Bayesian Bandit Algorithms"}, {"url": "nipsworkshops2010_xiao_hco", "desc": "We consider multiclass classification problems in which the set of labels are organized\r\nhierarchically as a category tree, and the examples are classified recursively\r\nfrom the root to the leaves. We propose a hierarchical support-vector-machine that\r\nencourages the classifiers at each node of the tree to be different from the classifiers\r\nat its ancestors. More specifically, we introduce regularizations that force the\r\nnormal vector of the classifying hyperplane at each node of the tree to be orthogonal\r\nto those at its ancestors as much as possible. We establish sufficient conditions\r\nunder which such an objective is a convex function of the normal vectors. We also\r\npresent an efficient dual-averaging method for solving the resulting nonsmooth\r\nconvex optimization problem. We evaluate the method on a number of real-world\r\ntext categorization tasks and obtain state-of-the-art performance.", "recorded": "2010-12-10T15:30:00", "title": "Hierarchical Classification via Orthogonal Transfer"}, {"url": "icml2015_ciliberto_convex_learning", "desc": "Reducing the amount of human supervision is a key problem in machine learning and a natural approach is that of exploiting the relations (structure) among different tasks. This is the idea at the core of multi-task learning. In this context a fundamental question is how to incorporate the tasks structure in the learning problem.We tackle this question by studying a general computational framework that allows to encode a-priori knowledge of the tasks structure in the form of a convex penalty; in this setting a variety of previously proposed methods can be recovered as special cases, including linear and non-linear approaches. Within this framework, we show that tasks and their structure can be efficiently learned considering a convex optimization problem that can be approached by means of block coordinate methods such as alternating minimization and for which we prove convergence to the global minimum.", "recorded": "2015-07-08T13:44:45", "title": "Convex Learning of Multiple Tasks and their Structure"}, {"url": "nips2011_asuncion_networks", "desc": "The development of statistical models for continuous-time longitudinal network data is of increasing interest in machine learning and social science. Leveraging ideas from survival and event history analysis, we introduce a continuous-time regression modeling framework for network event data that can incorporate both time-dependent network statistics and time-varying regression coefficients. We also develop an efficient inference scheme that allows our approach to scale to large networks. On synthetic and real-world data, empirical results demonstrate that the proposed inference approach can accurately estimate the coefficients of the regression model, which is useful for interpreting the evolution of the network; furthermore, the learned model has systematically better predictive performance compared to standard baseline methods.", "recorded": "2011-12-15T09:30:00", "title": "Continuous-Time Regression Models for Longitudinal Networks"}, {"url": "mlws04_ghahramani_nbmml", "desc": "Bayesian methods make it possible to handle uncertainty in a principled manner, sidestep the problem of overfitting, and incorporate domain knowledge. However, most parametric models are too limited to adequately model complex real-world problems. Thus, interest has shifted to nonparametric models which can capture much richer and more complex probability distributions. This talk will review some of the core nonparametric tools for regression and classification (Gaussian processes; GPs) and density estimation (Dirichlet process mixtures). We will then focus on extensions of these basic tools (such as mixtures of GPs, warped GPs, and GPs for ordinal regression) and approximation methods which allow efficient inference in these models (such as expectation propagation; EP).", "recorded": "2004-09-07T14:00:00", "title": "Nonparametric Bayesian Models in Machine Learning"}, {"url": "is2012_pevec_individual_predictions", "desc": "We review some recent research topics by Laboratory for Cognitive Modeling (LKM) at Faculty of Computer and Information Science, University of Ljubljana, Slovenia. Classification and regression models, either automatically generated from data by machine learning algorithms, or manually encoded with the help of domain experts, are daily used to predict the labels of new instances. Each such individual prediction, in order to be accepted/trusted by users, should be accompanied by an explanation of the prediction as well as by an estimate of its reliability. In LKM we have recently developed a general methodology for explaining individual predictions as well as for estimating their reliability. Both, explanation and reliability estimation are general techniques, independent of the\r\nunderlying model and provide on-line (effective and efficient) support to the users of prediction models.", "recorded": "2012-10-11T17:15:15", "title": "Explanation and Reliability of Individual predictions: Recent Research by LKM"}, {"url": "wsdm2010_dong_trr", "desc": "In web search, recency ranking refers to ranking documents by relevance which takes freshness into account. In this paper, we propose a retrieval system which automatically detects and responds to recency sensitive queries. The system detects recency sensitive queries using a high precision classifier. The system responds to recency sensitive queries by using a machine learned ranking model trained for such queries. We use multiple recency features to provide temporal evidence which effectively represents document recency. Furthermore, we propose several training methodologies important for training recency sensitive rankers. Finally, we develop new evaluation metrics for recency sensitive queries. Our experiments demonstrate the efficacy of the proposed approaches.", "recorded": "2010-02-04T10:53:58", "title": "Towards Recency Ranking in Web Search"}, {"url": "nipsworkshops09_sriperumbudur_ccc", "desc": "The concave-convex procedure (CCCP) is a majorization-minimization algorithm\r\nthat solves d.c. (difference of convex functions) programs as a sequence of convex\r\nprograms. In machine learning, CCCP is extensively used in many learning algorithms\r\nlike sparse support vector machines (SVMs), transductive SVMs, sparse\r\nprincipal component analysis, etc. Though widely used in many applications, the\r\nconvergence behavior of CCCP has not gotten a lot of specific attention. In this\r\npaper, we provide a rigorous analysis of the convergence of CCCP by addressing\r\nthese questions:\r\n*(i) When does CCCP find a local minimum or a stationary point\r\nof the d.c. program under consideration?\r\n*(ii) When does the sequence generated\r\nby CCCP converge?\r\n\r\nWe also present an open problem on the issue of local\r\nconvergence of CCCP.", "recorded": "2009-12-12T16:40:00", "title": "On the Convergence of the Convex-Concave Procedure"}, {"url": "mlg08_zhang_ank", "desc": "Statistical machine learning techniques for\r data classification usually assume that all entities are i.i.d. (independent and identically distributed). However, real-world entities often interconnect with each other through explicit or implicit relationships to form a complex network. Although some graph-based classification methods have emerged in recent years, they are not really suitable for complex networks as they do not take the degree distribution of network into consideration. In this paper, we propose a new technique, Modularity Kernel, that can effectively exploit the latent community structure of networked entities for their classification. A number of experiments on hypertext datasets show that our proposed approach leads to excellent classification performance in comparison with the state-of-the-art methods.", "recorded": "2008-07-04T16:00:00", "title": "A New Kernel for Classification of Networked Entitiess"}, {"url": "mlss05us_dasgupta_ig", "desc": "This tutorial will focus on entropy, exponential families, and information projection. We'll start by seeing the sense in which entropy is the only reasonable definition of randomness. We will then use entropy to motivate exponential families of distributions \u2014 which include the ubiquitous Gaussian, Poisson, and Binomial distributions, but also very general graphical models. The task of fitting such a distribution to data is a convex optimization problem with a geometric interpretation as an \"information projection\": the projection of a prior distribution onto a linear subspace (defined by the data) so as to minimize a particular information-theoretic distance measure. This projection operation, which is more familiar in other guises, is a core optimization task in machine learning and statistics. We'll study the geometry of this problem and discuss two popular iterative algorithms for it.", "recorded": "2005-05-20T00:00:00", "title": "Information Geometry"}, {"url": "mlpmsummerschool2013_vert_computational_biology", "desc": "These lecture will introduce some general concepts and algorithms in statistical learning, illustrating them through applications in biology and personalized medicine. I will discuss linear methods in classification and regression, nonlinear extensions with positive definite kernels, and feature selection and structured sparsity. Application will include molecular diagnosis and prognosis in cancer, virtual screening in drug discovery, and biological network inference. \r\n\r\nOutline:\r\n\r\n*Introduction to pattern recognition and regression for biology and personalized medicine\r\n*Linear methods for regression and classification (OLS, RR, LDA, QDA, logistic regression, SVM...)\r\n*Nonlinear extensions with kernels\r\n*Feature selection and structured sparsity (lasso and variants)\r\n*Application: cancer prognosis from genomic data\r\n*Application: drug discovery\r\n*Application: gene networks inference", "recorded": "2014-04-16T10:16:31", "title": "Machine Learning in Computational Biology (the frequentist approach)"}, {"url": "estc08_jackson_perce", "desc": "We discuss some current thinking around the use of machine extraction and resolution to deliver what we call 'Intelligent Information'. Such information is tailored to a user's underlying task, and must be derived automatically from underlying content sources. We describe a platform called ClearForest Tags, which performs state of the art entity, fact and event extraction, and illustrate its utility with some commercial use cases.  We also describe an entity resolution toolkit called Dexter/Concord, which is capable of both mapping named entities to authority files and merging or updating database records.  We supply some technical details of these fully implemented and deployed systems, as well as describing plans to more tightly integrate them.", "recorded": "2008-09-25T13:01:36", "title": "Extraction and Resolution Capabilities for Entities, Events and Facts"}, {"url": "roks2013_kwok_data", "desc": "In many machine learning problems, the labels of the training examples are incomplete. These include, for example, (i) semi-supervised learning where labels are partially\r\nknown; (ii) multi-instance learning where labels are implicitly known; and (iii) clustering\r\nwhere labels are completely unknown. In this talk, focusing on the SVM as the learner, I will\r\ndescribe a label generation strategy that leads to a convex relaxation of the underlying mixed\r\ninteger programming problem. Computationally, it can be solved via a sequence of SVM subproblems that are much more scalable than other convex SDP relaxations. Empirical results\r\non the three weakly labeled learning tasks above also demonstrate improved performance.\r\n(joint work with Yu-Feng Li, Ivor W. Tsang, and Zhi-Hua Zhou)", "recorded": "2013-07-10T15:30:00", "title": "Learning from Weakly Labeled Data"}, {"url": "machine_ramaswamy_ranking_losses", "desc": "The design of convex, calibrated surrogate losses, whose minimization entails consistency with respect to a desired target loss, is an important concept to have emerged in the theory of machine learning in recent years. We give an explicit construction of a convex least-squares type surrogate loss that can be designed to be calibrated for any multiclass learning problem for which the target loss matrix has a low-rank structure; the surrogate loss operates on a surrogate target space of dimension at most the rank of the target loss. We use this result to design convex calibrated surrogates for a variety of subset ranking problems, with target losses including the precision@q, expected rank utility, mean average precision, and pairwise disagreement.", "recorded": "2014-01-20T11:55:00", "title": "Convex Calibrated Surrogates for Low-Rank Loss Matrices with Applications to Subset Ranking Losses"}, {"url": "icml09_mairal_odlsc", "desc": "Sparse coding\u2014that is, modelling data vectors as sparse linear combinations of basis elements\u2014is widely used in machine learning, \r\nneuroscience, signal processing, and statistics. This paper focuses on learning the basis set, also called dictionary, to adapt it to speci\ufb01c \r\ndata, an approach that has recently proven to be very effective for signal reconstruction and classi\ufb01cation in the audio and image processing \r\ndomains. This paper proposes a new online optimization algorithm for dictionary learning, based on stochastic approximations, which \r\nscales up gracefully to large datasets with millions of training samples. A proof of convergence is presented, along with experiments with \r\nnatural images demonstrating that it leads to faster performance and better dictionaries than classical batch algorithms for both small and \r\nlarge datasets.", "recorded": "2009-06-15T13:00:00", "title": "Online Dictionary Learning for Sparse Coding"}, {"url": "ecmlpkdd09_flaounas_ivn", "desc": "We develop a statistical methodology to validate the result of network inference algorithms, based on principles of statistical testing and machine learning. The comparison of results with reference networks, by means of similarity measures and null models, allows us to measure the significance of results, as well as their predictive power. The use of Generalised Linear Models allows us to explain the results in terms of available ground truth which we expect to be partially relevant. We present these methods for the case of inferring a network of News Outlets based on their preference of stories to cover. We compare three simple network inference methods and show how our technique can be used to choose between them. All the methods presented here can be directly applied to other domains where network inference is used.", "recorded": "2009-09-10T14:25:00", "title": "Inference and Validation of Networks"}, {"url": "icml2015_sharma_greedy_maximization", "desc": "Submodular function maximization is one of the key problems that arise in many machine learning tasks. Greedy selection algorithms are the proven choice to solve such problems, where prior theoretical work guarantees (1 - 1/e) approximation ratio. However, it has been empirically observed that greedy selection provides almost optimal solutions in practice. The main goal of this paper is to explore and answer why the greedy selection does significantly better than the theoretical guarantee of (1 - 1/e). Applications include, but are not limited to, sensor selection tasks which use both entropy and mutual information as a maximization criteria. We give a theoretical justification for the nearly optimal approximation ratio via detailed analysis of the curvature of these objective functions for Gaussian RBF kernels.", "recorded": "2015-07-09T14:46:53", "title": "On Greedy Maximization of Entropy"}, {"url": "pcw06_bos_wlihd", "desc": "We compare and combine two methods to approach the second textual entailment challenge (RTE-2): a shallow method based mainly on word-overlap and a method based on logical inference, using first-order theorem proving and model building techniques. We use a machine learning technique to combine features of both methods. We submitted two runs, one using only the shallow features, yielding an accuracy of 61.6%, and one using features of both methods, performing with an accuracy score of 60.6%. These figures suggest that logical inference didn\u00b4t help much. Closer inspection of the results revealed that only for some of the subtasks logical inference played a significant role in performance. We try to explain the reason for these results.", "recorded": "2006-04-10T00:00:00", "title": "When Logical Inference Helps Determining Textual Entailment (and When it Doesn\u00b4t)"}, {"url": "cidu2011_gomez_ontology", "desc": "An analysis of the role that the ontology plays in determining verb meaning and semantic roles is presented. The discussion will contrast the current approaches to semantic role labeling using supervised machine learning methods to semantic approaches that rely on the ontology and verb sub-categorization. The presentation will discuss the issues dealing with the construction of verb meaning and semantic roles for general domains, and for specific domains with their own clearly defined ontologies. It will be indicated that Wordnet can serve as the general ontology on which specific ontologies can be anchored. The semantic-based algorithms will search for concepts beginning with the domain-specific ontology, and then continue up to the WordNet ontology if concepts are not found within the domain dependent ontology. In this way, the domain-specific ontology may override the general ontology.", "recorded": "2011-10-19T13:30:00", "title": "Ontology, Verb Meaning and Semantic Roles"}, {"url": "kdd2014_rudin_machine_learning", "desc": "It is extremely important in many application domains to have transparency in predictive modeling. Domain experts do not tend to prefer \"black box\" predictive models. They would like to understand how predictions are made, and possibly, prefer models that emulate the way a human expert might make a decision, with a few important variables, and a clear convincing reason to make a particular prediction. I will discuss recent work on interpretable predictive modeling with decision lists and sparse integer linear models. I will describe several approaches, including an algorithm based on discrete optimization, and an algorithm based on Bayesian analysis. I will show examples of interpretable models for stroke prediction in medical patients and prediction of violent crime in young people raised in out-of-home care.\r\n", "recorded": "2014-08-26T14:30:00", "title": "Algorithms for Interpretable Machine Learning"}, {"url": "icml2015_heinrich_extensive_form_games", "desc": "Fictitious play is a popular game-theoretic model of learning in games. However, it has received little attention in practical applications to large problems. This paper introduces two variants of fictitious play that are implemented in behavioural strategies of an extensive-form game. The first variant is a full-width process that is realization equivalent to its normal-form counterpart and therefore inherits its convergence guarantees. However, its computational requirements are linear in time and space rather than exponential. The second variant, Fictitious Self-Play, is a machine learning framework that implements fictitious play in a sample-based fashion. Experiments in imperfect-information poker games compare our approaches and demonstrate their convergence to approximate Nash equilibria.", "recorded": "2015-07-09T14:46:53", "title": "Fictitious Self-Play in Extensive-Form Games"}, {"url": "icml2015_deisenroth_distributed_gaussian_processes", "desc": "To scale Gaussian processes (GPs) to large data sets we introduce the robust Bayesian Committee Machine (rBCM), a practical and scalable product-of-experts model for large-scale distributed GP regression. Unlike state-of-the-art sparse GP approximations, the rBCM is conceptually simple and does not rely on inducing or variational parameters. The key idea is to recursively distribute computations to independent computational units and, subsequently, recombine them to form an overall result. Efficient closed-form inference allows for straightforward parallelisation and distributed computations with a small memory footprint. The rBCM is independent of the computational graph and can be used on heterogeneous computing infrastructures, ranging from laptops to clusters. With sufficient computing resources our distributed GP model can handle arbitrarily large data sets.", "recorded": "2015-07-09T14:46:53", "title": "Distributed Gaussian Processes"}, {"url": "metaforum2012_pereira_semantic", "desc": "Advances in statistical and machine learning approaches to natural language processing have yielded a wealth of methods and applications in information retrieval, speech recognition, machine translation, and information extraction. Yet, even as we enjoy these advances, we recognize that our successes are to a large extent the result of clever exploitation of redundancy in language structure and use, allowing our algorithms to eke out a few useful bits that we can put to work in applications. By focusing on applications that extract a limited amount of information from the text, finer structures such as word order or syntactic structure could be largely ignored in information retrieval or speech recognition. However, by ignoring those finer details, our language-processing systems have been stuck in an \"idiot savant\" stage where they can find everything but cannot understand anything. The main language processing challenge of the coming decade is to create robust, accurate, efficient methods that learn to understand the main entities and concepts discussed in any text, and the main claims made. By advancing in that direction, our systems will provide more precise answers to questions, they will verify and update knowledge bases, and they will trace arguments for and against claims throughout the written record. I will argue with examples from our recent research that we need deeper levels of linguistic analysis to do this. But I will also argue that it is possible to do much that is useful even with our very partial understanding of linguistic and computational semantics, by taking (again) advantage of distributional regularities and redundancy in large text collections to learn effective analysis and understanding rules. Thus low-pass semantics: our scientific knowledge is very far from being able to map the full spectrum of meaning, but by combining signals from the whole Web, we are starting to hear some interesting tunes.", "recorded": "2012-06-21T16:00:00", "title": "Low-Pass Semantic \t"}, {"url": "akbcwekex2012_pereira_semantics", "desc": "Advances in statistical and machine learning approaches to natural language processing have yielded a wealth of methods and applications in information retrieval, speech recognition, machine translation, and information extraction. Yet, even as we enjoy these advances, we recognize that our successes are to a large extent the result of clever exploitation of redundancy in language structure and use, allowing our algorithms to eke out a few useful bits that we can put to work in applications. By focusing on applications that extract a limited amount of information from the text, finer structures such as word order or syntactic structure could be largely ignored in information retrieval or speech recognition. However, by ignoring those finer details, our language-processing systems have been stuck in an \"idiot savant\" stage where they can find everything but cannot understand anything. The main language processing challenge of the coming decade is to create robust, accurate, efficient methods that learn to understand the main entities and concepts discussed in any text, and the main claims made. By advancing in that direction, our systems will provide more precise answers to questions, they will verify and update knowledge bases, and they will trace arguments for and against claims throughout the written record. I will argue with examples from our recent research that we need deeper levels of linguistic analysis to do this. But I will also argue that it is possible to do much that is useful even with our very partial understanding of linguistic and computational semantics, by taking (again) advantage of distributional regularities and redundancy in large text collections to learn effective analysis and understanding rules. Thus low-pass semantics: our scientific knowledge is very far from being able to map the full spectrum of meaning, but by combining signals from the whole Web, we are starting to hear some interesting tunes.", "recorded": "2012-06-08T09:00:00", "title": "Low-Pass Semantics"}, {"url": "kdd2010_lee_lcd", "desc": "Much of research in data mining and machine learning has led to numerous practical applications. Spam filtering, fraud detection, and user query-intent analysis has relied heavily on machine learned classifiers, and resulted in improvements in robust classification accuracy. Combining multiple classifiers (a.k.a. Ensemble Learning) is a well studied and has been known to improve effectiveness of a classifier. To address two key challenges in Ensemble Learning-- (1) learning weights of individual classifiers and (2) the combination rule of their weighted responses, this paper proposes a novel Ensemble classifier, EnLR, that computes weights of responses from discriminative classifiers and combines their weighted responses to produce a single response for a test instance. The combination rule is based on aggregating weighted responses, where a weight of an individual classifier is inversely based on their respective variances around their responses. Here, variance quantifies the uncertainty of the discriminative classifiers' parameters, which in turn depends on the training samples. As opposed to other ensemble methods where the weight of each individual classifier is learned as a part of parameter learning and thus the same weight is applied to all testing instances, our model is actively adjusted as individual classifiers become confident at its decision for a test instance. Our empirical experiments on various data sets demonstrate that our combined classifier produces ``effective'' results when compared with a single classifier. Our novel classifier shows statistically significant better accuracy when compared to well known Ensemble methods -- Bagging and AdaBoost. In addition to robust accuracy, our model is extremely efficient dealing with high volumes of training samples due to the independent learning paradigm among its multiple classifiers. It is simple to implement in a distributed computing environment such as Hadoop.\r\n", "recorded": "2010-07-27T10:30:00", "title": "Learning to Combine Discriminative Classifiers"}, {"url": "bootcamp2010_linares_sp", "desc": "The lecture presents various aspects of automatic speech processing\r\n(SP), from spoken contents extraction to high level categorization of\r\nspeech signals. We show how machine learning provides solutions to the\r\nmain issues that the speech processing systems have to deal with.\r\nSpeech is one of the main way that humans communicate together. It is a\r\ncomplex process that involves, in a highly integrated way, perception\r\nabilities and cognitive processes. In spite of the efforts produced by\r\nthe scientific community for simulating these abilities, knowledge-based\r\napproaches failed in modeling of speech.\r\nNowadays, most of the SP methods relies on statistical modeling of\r\nspeech. The lecture presents this theorical framework in which the major\r\nissues in speech processing are formulated. Then, the main tasks of SP\r\nare overviewed, especially speaker identification and speech recognition\r\nand understanding.", "recorded": "2010-07-09T08:00:00", "title": "Speech Processing"}, {"url": "bootcamp2010_linares_unknown", "desc": "The lecture presents various aspects of automatic speech processing\r\n(SP), from spoken contents extraction to high level categorization of\r\nspeech signals. We show how machine learning provides solutions to the\r\nmain issues that the speech processing systems have to deal with.\r\nSpeech is one of the main way that humans communicate together. It is a\r\ncomplex process that involves, in a highly integrated way, perception\r\nabilities and cognitive processes. In spite of the efforts produced by\r\nthe scientific community for simulating these abilities, knowledge-based\r\napproaches failed in modeling of speech.\r\nNowadays, most of the SP methods relies on statistical modeling of\r\nspeech. The lecture presents this theorical framework in which the major\r\nissues in speech processing are formulated. Then, the main tasks of SP\r\nare overviewed, especially speaker identification and speech recognition\r\nand understanding.", "recorded": "2010-07-09T09:08:00", "title": "Georges Linar\u00e8s"}, {"url": "colt2014_levy_bounds", "desc": "The logistic loss function is often advocated in machine learning and statistics as a smooth and strictly convex surrogate for the 0-1 loss. In this paper we investigate the question of whether these smoothness and convexity properties make the logistic loss preferable to other widely considered options such as the hinge loss. We show that in contrast to known asymptotic bounds, as long as the number of prediction/optimization iterations is sub exponential, the logistic loss provides no improvement over a generic non-smooth loss function such as the hinge loss. In particular we show that the convergence rate of stochastic logistic optimization is bounded from below by a polynomial in the diameter of the decision set and the number of prediction iterations, and provide a matching tight upper bound. This resolves the COLT open problem of McMahan and Streeter (2012).", "recorded": "2014-06-14T15:50:00", "title": "Logistic Regression: Tight Bounds for Stochastic and Online Optimization"}, {"url": "sip08_bickel_krakd", "desc": "There has been a great deal of attention in recent times particularly in \r\nmachine learning to representation of multivariate data points x by K(x, \r\n\u00b7) \r\nwhere K is positive and symmetric and thus induces a reproducing kernel \r\nHilbert space.The idea is then to use the matrix \r\n||K(Xi , Xj )|| as a substitute for the empirical covariance matrix of a sample X1 , . . . , Xn for PCA \r\nand other inference.(Jordan and Fukumizu(2006) for instance. Nadler et. \r\nal(2006) connected this approach to one based on random walks and diffusion limits and indicated a connection to kernel density estimation.By \r\nmaking at least a formal connection to a multiplication operator on a function space we make further connection and show how clustering results of \r\nBeylkin ,Shih and Yu (2008) which apparently di\ufb00er from Nadler et al. can \r\nbe explained. \r\n\r\n", "recorded": "2008-12-05T14:00:00", "title": "Kernel Representations and Kernel Density Estimation"}, {"url": "icml2015_shah_crowdsourcing", "desc": "The growing need for labeled training data has made crowdsourcing an important part of machine learning. The quality of crowdsourced labels is, however, adversely affected by three factors: (1) the workers are not experts; (2) the incentives of the workers are not aligned with those of the requesters; and (3) the interface does not allow workers to convey their knowledge accurately, by forcing them to make a single choice among a set of options. In this paper, we address these issues by introducing approval voting to utilize the expertise of workers who have partial knowledge of the true answer, and coupling it with a (\u201cstrictly proper\u201d) incentive-compatible compensation mechanism. We show rigorous theoretical guarantees of optimality of our mechanism together with a simple axiomatic characterization. We also conduct preliminary empirical studies on Amazon Mechanical Turk which validate our approach.", "recorded": "2015-07-08T14:46:53", "title": "Approval Voting and Incentives in Crowdsourcing"}, {"url": "iswc2013_simperl_students_microtask_crowdsourcing", "desc": "Microtask crowdsourcing platforms, as one of the most popular instance of social computing technologies, are increasingly used to support massively collaborative projects on semantic content management. In this tutorial we will introduce the most popular approaches to microtask crowdsourcing for Semantic Web problems, as a mean to realize hybrid human-machine content management architectures. We will explain the core notions and technologies, including Amazon Mechanical Turk, CrowdFlower and specifically purposed tools building upon the functionality of these platforms. We will address questions related to quality assurance, resource management, and workflow design, and discuss a series of technical and socio-economical challenges and open issues related to the application of microtask crowdsourcing in given Semantic Web scenarios.\r\n\r\n**Tutorial webpage: https://sites.google.com/site/microtasktutorial/**", "recorded": "2013-10-22T09:00:01", "title": "Introduction of the tutorial topics and presenters  Microtask crowdsourcing fundamentals"}, {"url": "icml09_malioutov_itva", "desc": "In this talk we discuss a framework for computing accurate approximate variances in large scale Gaussian Markov Random Fields. We start by motivating the need to compute variances in GMRFs, and discuss related problems in machine learning. Our approach is based on constructing a certain low-rank aliasing matrix which takes advantage of the Markov graph of the model. We first construct such a matrix for models with short-range correlation, and then describe a wavelet-based construction for models with long-range correlation. The approach is based on fast solution of sparse linear systems, and we describe suitable preconditioners. We also describe how the approach can be used for problems with sparse plus low-rank structure, for example in approximate Kalman filtering with large state spaces.", "recorded": "2009-06-18T08:30:00", "title": "Variance Approximation in Large-Scale Gaussian Markov Random Fields"}, {"url": "mlss05us_ghahramani_bl", "desc": "Bayes Rule provides a simple and powerful framework for machine learning. This tutorial will be organised as follows:\r\n \r\n 1. I will give motivation for the Bayesian framework from the point of view of rational coherent inference, and highlight the important role of the marginal likelihood in Bayesian Occam's Razor.\r\n \r\n 2. I will discuss the question of how one should choose a sensible prior. When Bayesian methods fail it is often because no thought has gone into choosing a reasonable prior.\r\n \r\n 3. Bayesian inference usually involves solving high dimensional integrals and sums. I will give an overview of numerical approximation techniques (e.g. Laplace, BIC, variational bounds, MCMC, EP...).\r\n \r\n 4. I will talk about more recent work in non-parametric Bayesian inference such as Gaussian processes (i.e. Bayesian kernel \"machines\"), Dirichlet process mixtures, etc. ", "recorded": "2005-05-25T12:00:00", "title": "Bayesian Learning"}, {"url": "mbc07_dorard_csl", "desc": "In this paper a novel method for performing music in the style of famous pianists is presented. We use Kernel Canonical Correlation Analysis (KCCA), a method which looks for a common semantic representation between two views, to learn the correlation between a representation of a musical score and a representation of an artist\u2019s performance of that score. We use the performance representation based on the variations of beat level global loudness and tempo through time, as suggested by [3]. Therefore, the crux of the matter is the representation of the musical scores and by implication a similarity measure between relevant features that capture our prior knowledge of music. We therefore proceed to propose a novel kernel for musical scores, which is a Gaussian kernel adaptation to the distances between rhythm patterns, melodic contours and chords progressions.", "recorded": "2007-12-07T16:15:00", "title": "Can Style be Learned? A Machine Learning Approach Towards \u2018Performing\u2019 as Famous Pianists"}, {"url": "mlss05au_roweis_pgm", "desc": "My lectures will cover the basics of graphical models, also known as Bayes(ian) (Belief) Net(work)s. We will cover the basic motivations for using probabilities to represent and reason about uncertain knowledge in machine learning, and introduce graphical models as a qualitative and quantitative specification of large joint probability distributions. We will see how many common classification, regression and clustering models can be cast in this framework. We will cover the basic algorithm (called belief propagation) for inference in graphical model structures. We will also cover the major approaches to learning models from data (parameter estimation). The course will focus on directed models and the basic algorithms, but time and student desire permitting, I will also try to give some preliminary explanations of undirected models, approximate inference and learning, structure discovery and current applications.", "recorded": "2005-01-24T00:00:00", "title": "Probabilistic Graphical Models"}, {"url": "sicgt07_audibert_cotg", "desc": "Given a sample from a probability measure with support on a submanifold in Euclidean\r\n space one can construct a neighborhood graph which can be seen as an approximation\r\n of the submanifold. The graph Laplacian of such a graph is used in several\r\n machine learning methods like semi-supervised learning, dimensionality reduction and\r\n clustering. We will present the pointwise limit of three different graph Laplacians used\r\n in the literature as the sample size increases and the neighborhood size approaches zero.\r\n We show that for a uniform measure on the submanifold all graph Laplacians have the\r\n same limit up to constants. However in the case of a nonuniform measure on the submanifold\r\n only the so called random walk graph Laplacian converges to the weighted\r\n Laplace-Beltrami operator. We will give two applications of these theoretical results.", "recorded": "2007-09-07T18:51:15", "title": "Convergence of the graph Laplacian application to dimensionality estimation and image segmentation"}, {"url": "lce06_whistler", "desc": "The identification of an effective function to compare examples is essential to several machine learning problems. For instance, retrieval systems entirely depend on such a function to rank the documents with respect to their estimated similarity to the submitted query. Another example is kernel-based algorithms which heavily rely on the choice of an appropriate kernel function. In most cases, the choice of the comparison function (also called, depending on the context and its mathematical properties, distance metric, similarity measure, kernel function or matching measure) is done a-priori, relying on some knowledge/assumptions specific to the task. An alternative to this a-priori selection is to learn a suitable function relying on a set of examples and some of its desired properties. This workshop is aimed at bringing together researchers interested in such a task.", "recorded": "2006-12-08T08:00:00", "title": "NIPS Workshop on Learning to Compare Examples, Whistler 2006"}, {"url": "iccc2014_unemi_experimental_practice", "desc": "Evolutionary computing based on computational aesthetic measure as fitness criteria is one of the \r\npossible methods  to  let  the machine make  art.  The  author  developed  and  set  up  a  computer \r\nsystem  that produces  ten  short  animations  consisting  sequences of  abstract  images  and  sound \r\neffects everyday. The produced pieces are published on the internet using three methods, movie \r\nfiles,  HTML5  +  WebGL,  and  a  special  application  software.  The  latter  two  methods  provide \r\nviewers  experiences  of  a  high  resolution  lossless  animation.  Their  digest  versions  are  also \r\nuploaded  on  a  popular web  service  of movie  sharing.  It  started October  2011.  It  is  still  in  an \r\nexperimental level that we need to brush up, but it has not always but often succeeded to engage \r\nthe viewers.", "recorded": "2014-06-10T12:10:00", "title": "Automated Daily Production of Evolutionary Audio Visual Art - An Experimental Practice"}, {"url": "icml08_takeda_nsvm", "desc": "The nu-support vector classification (nu-SVC) algorithm was shown to work well and provide intuitive interpretations, e.g., the parameter nu roughly specifies the fraction of support vectors. Although nu corresponds to a fraction, it cannot take the entire range between 0 and 1 in its original form. This problem was settled by a non-convex extension of nu-SVC and the extended method was experimentally shown to generalize better than original nu-SVC. However, its good generalization performance and convergence properties of the optimization algorithm have not been studied yet. In this paper, we provide new theoretical insights into these issues and propose a novel nu-SVC algorithm that has guaranteed generalization performance and convergence properties", "recorded": "2008-07-06T11:02:44", "title": "Nu-Support Vector Machine as Conditional Value-at-Risk Minimization"}, {"url": "iswc2014_speck_learning", "desc": "A considerable portion of the information on the Web is still only available in unstructured form. Implementing the vision of the Semantic Web thus requires transforming this unstructured data into structured data. One key stepduringthisprocessistherecognitionofnamedentities.Previousworkssug- gest that ensemble learning can be used to improve the performance of named entity recognition tools. However, no comparison of the performance of existing supervised machine learning approaches on this task has been presented so far. Weaddressthisresearchgapbypresentingathoroughevaluationofnamedentity recognition based on ensemble learning. To this end, we combine four different state-of-the approaches by using 15 different algorithms for ensemble learning and evaluate their performace on \ufb01ve different datasets. Our results suggest that ensemblelearningcanreducetheerrorrateofstate-of-the-artnamedentityr ecognition systems by 40%, thereby leading to over 95% f-score in our best run.", "recorded": "2014-10-21T12:00:00", "title": "Ensemble Learning for Named Entity Recognition "}, {"url": "ijcai09_yang_fllshli", "desc": "Sensors provide computer systems with a window to the outside world. Activity recognition \"sees\" what is in the window to predict the locations, trajectories, actions, goals and plans of humans and objects. Building an activity recognition system requires a full range of interaction from statistical inference on lower level sensor data to symbolic AI at higher levels, where prediction results and acquired knowledge are passed up each level to form a knowledge food chain. In this talk, I will give an overview of activity recognition and explore its relation to other fields, including planning and knowledge acquisition, machine learning and Web search.  I will also describe its applications in assistive technologies, security monitoring and mobile commerce.\r\n\r\n", "recorded": "2009-07-17T09:00:00", "title": "  From Low-level Sensors to High-level Intelligence: Activity Recognition Links the Knowledge Food Chain"}, {"url": "kdd08_malik_fis", "desc": "There are billions of images on the Internet. Today, searching for a desired image is largely based on textual data such as filename or associated text on the web page; not much use is made of the image content. There are good reasons for this. The field of content-based image retrieval, which emerged during the 1990s, focused primarily on color and texture cues. These were easier to model than shape, but they turned out to be much less useful than originally hoped. I shall review some of the recent developments in the field of visual object recognition in the computer vision community that offer greater promise. Much better image features for characterizing shape, advances in machine learning techniques, and the availability of large amounts of training data lie at the heart of these approaches.", "recorded": "2008-08-26T00:09:00", "title": "The Future of Image Search"}, {"url": "nipsworkshops2010_chakraborty_mkl", "desc": "The Conformal Predictions framework is a recent development in machine learning\r\nto associate reliable measures of confidence with results in classification and\r\nregression. This framework is founded on the principles of algorithmic randomness\r\n(Kolmogorov complexity), transductive inference and hypothesis testing.\r\nWhile the formulation of the framework guarantees validity, the efficiency of the\r\nframework depends greatly on the choice of the classifier and appropriate kernel\r\nfunctions or parameters. While this framework has extensive potential to be useful\r\nin several applications, the lack of efficiency can limit its usability. In this paper,\r\nwe propose a novel Multiple Kernel Learning (MKL) methodology to maximize\r\nefficiency in the CP framework. This method is validated using the k-Nearest\r\nNeighbors classifier on a cardiac patient dataset, and our results show promise in\r\nusing MKL to obtain efficient conformal predictors that can be practically useful.", "recorded": "2010-12-11T16:50:00", "title": "Multiple Kernel Learning for Efficient Conformal Predictions"}, {"url": "icml09_nowozin_ssl", "desc": "We propose a new method to quantify the solution\r\nstability of a large class of combinatorial\r\noptimization problems arising in machine\r\nlearning. As practical example we apply the\r\nmethod to correlation clustering, clustering\r\naggregation, modularity clustering, and relative\r\nperformance signi\fcance clustering. Our\r\nmethod is extensively motivated by the idea\r\nof linear programming relaxations. We prove\r\nthat when a relaxation is used to solve the\r\noriginal clustering problem, then the solution\r\nstability calculated by our method is conservative,\r\nthat is, it never overestimates the solution\r\nstability of the true, unrelaxed problem.\r\nWe also demonstrate how our method\r\ncan be used to compute the entire path of\r\noptimal solutions as the optimization problem\r\nis increasingly perturbed. Experimentally,\r\nour method is shown to perform well\r\non a number of benchmark problems.", "recorded": "2009-06-15T10:20:00", "title": "Solution Stability in Linear Programming Relaxations: Graph Partitioning and Unsupervised Learning"}, {"url": "nips2011_balakrishnan_clustering", "desc": "Although spectral clustering has enjoyed considerable empirical success in machine learning, its theoretical properties are not yet fully developed. We analyze the performance of a spectral algorithm for hierarchical clustering and show that on a class of hierarchically structured similarity matrices, this algorithm can tolerate noise that grows with the number of data points while still perfectly recovering the hierarchical clusters with high probability. We additionally improve upon previous results for k-way spectral clustering to derive conditions under which spectral clustering makes no mistakes. Further, using minimax analysis, we derive tight upper and lower bounds for the clustering problem and compare the performance of spectral clustering to these information theoretic limits. We also present experiments on simulated and real world data illustrating our results.", "recorded": "2011-12-14T12:40:00", "title": "Noise Thresholds for Spectral Clustering"}, {"url": "colt2015_leike_universal_priors", "desc": "A big open question of algorithmic information theory is the choice of the universal Turing machine (UTM). For Kolmogorov complexity and Solomonoff induction we have invariance theorems: the choice of the UTM changes bounds only by a constant. For the universally intelligent agent AIXI (Hutter, 2005) no invariance theorem is known. Our results are entirely negative: we discuss cases in which unlucky or adversarial choice of the UTM causes AIXI to misbehave drastically. We show that Legg-Hutter intelligence and thus balanced Pareto optimality is entirely subjective, and that every policy is Pareto optimal in the class of all computable environments. This undermines all existing optimality properties for AIXI. While it may still serve as a gold standard for AI, our results imply that AIXI is a relative theory, dependent on the choice of the UTM.", "recorded": "2015-07-06T10:05:00", "title": "Bad Universal Priors and Notions of Optimality"}, {"url": "newtonschw03s08_data_analysis", "desc": "This closing workshop of the research programme will look both backwards and forwards, although mainly the latter. There will be a retrospective component that reviews some of the ideas and advances that have been generated or initiated during the preceding six months, but the emphasis will be on presentations that look to the future, in this vital area of the analysis of large-scale data-sets. The invited speakers will include leading researchers from statistics and machine learning, and will describe theoretical/methodological advances as well as issues associated with implementation in important applied fields, such as finance, climatology, analysis of sparse signals and genomics. \r\n\r\n**Workshop Homepage**\r\nhttp://www.newton.ac.uk/programmes/SCH/schw03.html\r\n\r\n**Programme**\r\nhttp://www.newton.ac.uk/programmes/SCH/schw03p.html", "recorded": "2008-06-23T10:00:00", "title": "SCHW03 Future Directions in High-Dimensional Data Analysis: New Methodologies: New Data Types and New Applications"}, {"url": "cidu2011_djorgovski_surveys", "desc": "We describe the development of a system for an automated, iterative, real\u2010time classification of transient events discovered in synoptic sky surveys. The system under development incorporates a number of Machine Learning techniques, mostly using Bayesian approaches, due to the sparse nature, heterogeneity, and variable incompleteness of the available data. The classifications are improved iteratively as the new measurements are obtained. One novel feature is the development of an automated follow\u2010up recommendation engine, that suggest those measurements that would be the most advantageous in terms of resolving classification ambiguities and/or characterization of the astrophysically most interesting objects, given a set of available follow\u2010up assets and their cost functions. This illustrates the symbiotic relationship of astronomy and applied computer science through the emerging discipline of AstroInformatics.", "recorded": "2011-10-20T13:30:00", "title": "Towards an Automated Classification of Transient Events in Synoptic Sky Surveys"}, {"url": "icml08_garcke_asvm", "desc": "The handling of large data sets by support vector machines (SVMs)(Vapnik, 1998) employing a nonlinear kernel suffers from the non-linear scaling of the numerical solution techniques for the underlying optimisation problem. This is in particular valid if the kernel matrix cannot be stored in the main memory anymore and therefore the evaluation of the kernel on given data points needs to be recomputed again and again. We investigate a simple approach to allow the processing of larger data sets: We separate the large data set into a number of smaller ones, each small enough to allow the caching of the kernel matrix, and learn a support vector machine for each of these data sets. For the evaluation on data points we then just simply average the results of the different SVMs.", "recorded": "2008-07-09T10:15:00", "title": "Averaging Support Vector Machines for Processing Large Data Sets"}, {"url": "icml08_lu_ump", "desc": "Tensorial data are frequently encountered in various machine learning tasks today and dimensionality reduction is one of their most important applications. This paper extends the classical principal component analysis (PCA) to its multilinear version by proposing a novel dimensionality reduction algorithm for tensorial data, named as uncorrelated multilinear PCA (UMPCA). UMPCA seeks a tensor-to-vector projection that captures most of the variation in the original tensorial input while producing uncorrelated features through successive variance maximization. We evaluate the proposed algorithm on a second-order tensorial problem, face recognition, and the experimental results show its superiority, especially in low-dimensional spaces, through the comparison with three other PCA-based algorithms.", "recorded": "2008-07-08T10:40:00", "title": "Uncorrelated Multilinear Principal Component Analysis through Successive Variance Maximization"}, {"url": "eswc2015_portoroz", "desc": "The ESWC is a major venue for discussing the latest scientific results and technology innovations around semantic technologies. Building on its past success, ESWC is seeking to broaden its focus to span other relevant research areas in which Web semantics plays an important role.\r\n\r\nThe goal of the Semantic Web is to create a Web of knowledge and services in which the semantics of content is made explicit and content is linked to both other content and services novel applications allowing to combine content from heterogeneous sites in unforeseen ways and support enhanced matching between users needs and content. This network of knowledge-based functionality will weave together a large network of human knowledge, and make this knowledge machine-processable to support intelligent behaviour by machines. Creating such an interlinked Web of knowledge which spans unstructured, RDF as well as multimedia content and services requires the collaboration of many disciplines, including but not limited to: Artificial Intelligence, Natural Language Processing, Database and Information Systems, Information Retrieval, Machine Learning Multimedia, Distributed Systems, Social Networks, Web Engineering, and Web Science. These complementarities are reflected in the outline of the technical program of the ESWC 2015; in addition to the research and in-use tracks, we feature two special tracks putting particular emphasis on inter-disciplinary research topics and areas that show the potential of exciting synergies for the future, eGovernment and Digital Libraries. ESWC 2015 presents the latest results in research, technologies and applications in its field. Besides the technical program organized over twelve tracks, the conference features a workshop and tutorial program, system descriptions and demos, a posters exhibition and a doctoral symposium.\r\n\r\nFor more information about the event please visit the [[http://2015.eswc-conferences.org/|ESWC 2015 website]].", "recorded": "2015-05-31T00:00:00", "title": "12th Extended Semantic Web Conference (ESWC), Portoro\u017e 2015"}, {"url": "mitworld_hawkins_neocortex", "desc": "Talk about intelligent designs: Jeff Hawkins says he\u2019s mapped out the way the human brain works, and has begun to fashion thinking machines to emulate the process. It comes down to Hierarchical Temporal Memory (HTM). Basically, he says, our brains take sensory inputs from the world and build a set of beliefs around the causes of those inputs. \u201cDiscovering causes is the pinnacle of what brains do,\u201d says Hawkins. But getting good at this kind of \u201cfancy pattern recognition\u201d is something developing humans seem to do effortlessly, and computers only with immense labor. Learning to differentiate a cat and a dog, for instance, doesn\u2019t come naturally to a computer. Hawkins layers his machine brains with nodes that make inferences about outside sensory data, and then pass these hunches on up a hierarchy of nodes until a consensus -- a belief -- evolves about the source of the data. The use of \u201cbelief propagation techniques\u201d, says Hawkins, enables an entire system to reach the best overall consensus swiftly. As the thinking machine develops common representations of objects or ideas, it can generalize about new data coming at it, and learn to attend only to information that matters.\r\n\r\nWhen Hawkins presented an HTM vision system with primitive line drawings of a helicopter and a mug, the system learned to identify them, even when their orientations changed dramatically, and when the lines were blurred. But the program also correctly rejected chopped-up versions of the same drawings as nonsense. \u201cStable beliefs at the top lead to changing predictions and behavior at the bottom,\u201d says Hawkins. Where does this lead? Possibly to \u201cmachines that are much smarter than humans,\u201d says Hawkins, computers whose abilities extend beyond sense biology and provide a means to expand such complex fields as weather, cosmology and genetics.", "recorded": "2005-09-28T20:52:00", "title": "Can a New Theory of the Neocortex Lead to Truly Intelligent Machines?"}, {"url": "ecmlpkdd2011_ciravegna_varga_networks", "desc": "Most research on information mining has focused on classic Information Extraction (IE) tasks, from structured and unstructured documents, like newspaper articles and web pages. In the last years however the staggering growth of social media as platform for sharing content has moved the focus towards a different type of extraction target. Social media pose a number of challenge to information extraction: contributions to social media sites like blogs, forums, Twitter, etc. are conversational in nature and thus tend to be brief and informal, containing imprecise, subjective and ambiguous information. The expanded context (who the author is, the social and geographical context, their social links, etc.) becomes relevant to disambiguate and interlink information.\r\n\r\nAim of this tutorial is to introduce and discuss issues, methodologies and technologies for extracting information from documents, with a particular focus on mining heterogeneous information networks (e.g. social websites) in order to mine complex entities.\r\n\r\nThe tutorial covers:\r\n\r\n* Introduction to information extraction from documents in general (20 minutes) and from information networks in particular (10 minutes)\r\n\r\n* Introduction to machine learning based methods for information extraction (75 minutes)\r\n\r\n   #representing documents and feature sets\r\n   #entity and terminology recognition\r\n   #learning gazetteers\r\n   #event and relation extraction\r\n   #extraction from multimedia documents\r\n\r\n* Annotation for training (15 minutes)\r\n\r\n   #feature selection\r\n   #annotation and error\r\n   #porting across domains\r\n\r\n* Information Extraction from information networks (45 minutes)\r\n\r\n   #using the Twitter and Facebook APIs\r\n   #entity recognition and resolution\r\n   #term association\r\n   #entity disambiguation over large scale\r\n\r\n* Conclusion and future work (15 minutes)\r\n\r\nThe focus is on Machine Learning based methods. We will cover - among others - methods using Rule Induction, SVM, CRF, HMM, Transfer Learning, Active Learning. We will Also discuss real world cases from the field of information and knowledge management.\r\n ", "recorded": "2011-09-09T09:00:00", "title": "Mining Complex Entities from Heterogeneous Information Networks"}, {"url": "eswc2013_montpellier", "desc": "The ESWC is a major venue for discussing the latest scientific results and technology innovations around semantic technologies. Building on its past success, ESWC is seeking to broaden its focus to span other relevant research areas in which Web semantics plays an important role.\r\n\r\nThe goal of the Semantic Web is to create a Web of knowledge and services in which the semantics of content is made explicit and content is linked to both other content and services novel applications allowing to combine content from heterogeneous sites in unforeseen ways and support enhanced matching between users needs and content. This network of knowledge-based functionality will weave together a large network of human knowledge, and make this knowledge machine-processable to support intelligent behaviour by machines. Creating such an interlinked Web of knowledge which spans unstructured, RDF as well as multimedia content and services requires the collaboration of many disciplines, including but not limited to: Artificial Intelligence, Natural Language Processing, Database and Information Systems, Information Retrieval, Machine Learning Multimedia, Distributed Systems, Social Networks, Web Engineering, and Web Science. These complementarities are reflected in the outline of the technical program of the ESWC 2013; in addition to the research and in-use tracks, we will feature two special tracks putting particular emphasis on inter-disciplinary research topics and areas that show the potential of exciting synergies for the future, eGovernment and Digital Libraries. ESWC 2013 will present the latest results in research, technologies and applications in its field. Besides the technical program organized over twelve tracks, the conference will feature a workshop and tutorial program, system descriptions and demos, a posters exhibition, a doctoral symposium, as well as the ESWC summer school, which will be held prior to the conference.\r\n\r\nDetailed information can be found at [[http://2013.eswc-conferences.org/|ESWC 2013 website]].", "recorded": "2013-05-28T09:00:00", "title": "10th Extended Semantic Web Conference (ESWC), Montpellier 2013"}, {"url": "cmulls08_xing_scml", "desc": "Recent advances in high-throughput technologies such as microarrays and genome-wide sequencing have led to an avalanche of new biological data that are dynamic, noisy, heterogeneous, and high-dimensional. They have raised unprecedented challenges in machine learning and high-dimensional statistical analysis; and their close relevance to human health and social welfare has often created unique demands on performance metric different from standard data mining or pattern recognition problems. In this talk, I will discuss two of such problems. First, I will present a new statistical formalism for modeling network evolution over time, and several new algorithms based on temporal extensions of the sparse graphical logistic regression, for parsimonious reverse-engineering the latent time varying networks. I will show some promising results on recovering the latent sequence of temporally rewiring gene networks over more than 4000 genes during the life cycle of Drosophila melanogaster from microarray time course, at a time resolution only limited by sample frequency. Second, I will present a family of sparse structured regression models in the context of uncovering true associations between linked genetic variations (inputs) in the genome and networks of human traits (outputs) in the phenome. If time allows, I will also present another class of new models known as the maximum entropy discrimination Markov networks, which address the same problem in the maximum margin paradigm, but using a entropic regularizer that lead to a distribution of structured prediction functions that are simultaneously primal and dual sparse (i.e., with few support vectors, and of low effective feature dimension).\r\n\r\nJoint work with Amr Ahmed, Seyoung Kim, Mladen Kolar, Le Song and Jun Zhu. ", "recorded": "2008-11-24T12:00:00", "title": "Some Challenging Machine Learning Problems in Computational Biology: Time-Varying Networks Inference and Sparse Structured Input-Out Learning"}, {"url": "turing100_ferrucci_beyond_jeopardy", "desc": "Computer systems that directly and accurately understand and answer people\u2019s questions over a broad domain of human knowledge have been envisioned by scientists and writers since the advent of computers themselves. Toy solutions are easy to create when the knowledge is narrowly bounded and the queries anticipated by the programmers. The real goal for Artificial Intelligence is for the machine to digest language as fluently and freely as humans, eliminating the need to manually and explicitly formalize the knowledge expressly for the machine. Being able to leverage knowledge as it is prolifically and naturally captured and communicated by humans would facilitate a new era in informed decision making, giving users efficient, context-aware and precise access to the enormous wealth of knowledge humans naturally create and enrich every day. Applications in business intelligence, healthcare, customer support, social computing, science and government could all benefit from computer systems capable of deeper language understanding. The DeepQA project at IBM is aimed at exploring how advancing and integrating Natural Language Processing (NLP), Information Retrieval (IR), Machine Learning (ML), Knowledge Representation and Reasoning (KR&R) and massively parallel computation can advance the science and application of automatic Question Answering and more general natural language understanding. An exciting proof-point in this challenge was developing a computer system that could successfully compete against top human players at the [[http://www.jeopardy.com/|Jeopardy! quiz show]].\r\n\r\nAttaining champion-level performance at Jeopardy! requires a computer to rapidly and accurately answer rich open-domain questions, and to predict its own performance on any given question. The system must deliver high degrees of precision and confidence over a very broad range of knowledge and natural language content with a 3-second response time. To do this, the DeepQA team advanced a broad array of NLP techniques to find, generate, evidence and analyze many competing hypotheses over large volumes of natural language content to build Watson (www.ibmwatson.com). An important contributor to Watson\u2019s success is its ability to automatically learn and combine accurate confidences across a wide array of algorithms and over different dimensions of evidence. Watson produced accurate confidences to know when to \u201cbuzz in\u201d against its competitors and how much to bet. High precision and accurate confidence computations are critical for real business settings where helping users focus on the right content sooner and with greater confidence can make all the difference. The need for speed and high precision demands a massively parallel computing platform capable of generating, evaluating and combing thousands of hypotheses and their associated evidence.\r\n\r\nIn this talk, I will introduce the audience to the Jeopardy! Challenge; explain how Watson was built to ultimately defeat the two most celebrated human champions of all time. I will discuss how Watson will advance beyond Jeopardy! to solve real problems in healthcare through natural language dialog, ultimately taking another step towards Turing\u2019s vision.", "recorded": "2012-06-23T13:30:00", "title": "Beyond Jeopardy! The Future of Watson"}, {"url": "licsb08_santos_pam", "desc": "In this paper we study the anti-cancer activity of - 4.000 unique compounds against a set of 60\r cell lines (e.g. Leukemia, Prostate, Breast). Small molecules play an important role in biology as\r they can be used as building blocks for more complex molecules and also interact with proteins\r inhibiting or promoting their action. In this case the consequence of adding such a compound to a\r cell can be far reaching as the protein may be involved in a very complex chain reaction. As such\r it is possible to design small molecules which can be useful drugs. Here we concentrate only in\r predicting a property of a given molecule: whether it will show anti-cancer activity (measured as\r causing at least 50% cell growing inhibition) against a given cancerous cell line. This computational\r prediction is important as there are a growing number of small molecules in databases worldwide\r and the capacity for proper lab testing is limited. For instance, the In Vitro Cell Line Screening\r Project at the National Cancer Institute (NCI) can currently evaluate (only) up to 3000 compounds\r per year for potential anti-cancer activity. From a machine learning perspective, biological\r problems are a good application because datasets are abundant, the data is real, the type of\r algorithms most suitable for a particular problem may vary substantial and it is not unusual for\r a problem to highlight research needs in machine learning. Finally, helping to solve biological\r problems may have a big impact in the wider scientific community. The molecule dataset we used\r is publicly available at the NCI site. We applied a range of data mining classification algorithms\r to this problem: Decision Trees, Inductive Logic Programming and Support Vector Machines\r (SVMs). As molecular features used for the learning we have used molecular weight, octanol\r water partition coefficient (logp) and fragment counts. A fragment is a set of connected atoms\r where each atom in a fragment is simply identified by its type. (e.g. carbon). If we look at the\r molecule as a graph, the fragment list consists of all connected components with diameter two.\r The experiments demonstrate that our results using support vector machines (with RBF kernel)\r are identical to previous published state of the art work yielding an average 73% predictive\r accuracy (having 54% as the baseline). We noticed however, to our surprise, that if instead of\r using fragment counts we use only atom counts the results are nearly identical (about 1% less\r accuracy, although the diference is statistical significant). An important point that must be made\r is that, although numerical black box algorithms like SVMs tend to be slightly more accurate than\r logic models (Decision Trees and ILPs in this dataset have an accuracy 3% to 4% below SVMs),\r it is arguable the relevance of this predictive accuracy for important practical applications like\r drug design. In a drug design setting what is useful is to have a set of rules that describe what\r a \"good\" compound should look like. That goal is much easily achieved with a human readable\r logic model like the ones we also describe in the paper.", "recorded": "2008-03-27T11:40:00", "title": "Predicting anti-cancer molecule activity using machine learning algorithms"}, {"url": "icml07_xiangyang_odm", "desc": "For large-scale classification problems, the training samples can be clustered beforehand as a downsampling pre-process, and then only the obtained clusters are used for training. Motivated by such assumption, we proposed a classification algorithm, Support Cluster Machine (SCM), within the learning framework introduced by Vapnik. For the SCM, a compatible kernel is adopted such that a similarity measure can be handled not only between clusters in the training phase but also between a cluster and a vector in the testing phase. We also proved that the SCM is a general extension of the SVM with the RBF kernel. The experimental results confirm that the SCM is very effective for largescale classification problems due to significantly reduced computational costs for both training and testing and comparable classification accuracies. As a by-product, it provides a promising approach to dealing with privacy-preserving data mining problems.", "recorded": "2007-06-23T11:35:00", "title": "Optimal Dimensionality of Metric Space for Classification"}, {"url": "ecmlpkdd08_nakamura_tmlal", "desc": "Polarizing discussions on political and social issues are common in mass and user-generated media. However, computer-based understanding of ideological discourse has been considered too difficult to undertake. In this paper we propose a statistical model for ideology discourse. By ideology we mean ``a set of general beliefs socially shared by a group of people.'' For example, Democratic and Republican are two major political ideologies in the United States. The proposed model captures lexical variations due to an ideological text's topic and due to an author or speaker's ideological perspective. To cope with the non-conjugacy of the logistic-normal prior we derived a variational inference algorithm for the model. We evaluate the proposed model on synthetic data as well as a written and a spoken political discourse. Experimental results strongly support that ideological perspectives are reflected in lexical variations.", "recorded": "2008-09-17T11:20:00", "title": "Towards Machine Learning of Grammars and Compilers of Programming Languages"}, {"url": "iswc2012_manukyan_amn_formalism", "desc": "An original approach to integrate information is considered. In the frame of this approach an extensible canonical model is created which is based on the algebraic model of an advanced XML data model. We consider the canonical model as an intermediate model for creating our mediator and this model has been formalized by means of the Abstract Machine Notation (AMN). For each source model we formalize it by means of the AMN and create a reversible mapping into an extension of the canonical model. After this B-technology is used to prove that the AMN semantics of the source model represents a refinement of the AMN semantics of the extended canonical model. Hereby the correctness of mapping and the ability to use extended canonical model for representation of schemas of the source model are proved. Finally, in order to illustrate our approach a mapping from relational data model to canonical model has been created.", "recorded": "2012-11-11T12:00:53", "title": "An approach to information integration based on the AMN formalism"}, {"url": "stanfordcs223aw08_khatib_lec13", "desc": "//\"Okay. So who\u2019s interested in juggling? Well, those who\r\nare interested in juggling could try it next quarter in Experimental Robotics. In fact, a lot\r\nof the projects in Experimental Robotics involve dynamic skills, throwing a ball into a\r\nbasket, playing ping-pong, or whatever. So juggling is quite challenging, actually. Well,\r\njuggling requires control and here we are. So this is a little bit of a concept that we are\r\ngoing to see over the discussions on control. And the concept is instead of really thinking\r\nabout the robot as a programmable machine where you need to find all the join motions\r\ncorresponding to your task. So you want to move to some location and you want to be\r\nable to reach that location with some orientation of your vector...\"//\r\n\r\nSee the whole transcript at [[http://see.stanford.edu/materials/aiircs223a/transcripts/IntroductionToRobotics-Lecture13.pdf|Introduction to robotics - Lecture 13]]", "recorded": "2008-01-23T14:04:15", "title": "Lecture 13: Control - Overview"}, {"url": "gbr07_aldea_ic", "desc": "We propose in this article an image classification technique based on kernel methods and graphs. Our work explores the possibility of applying marginalized kernels to image processing. In machine learning, performant algorithms have been developed for data organized as real valued arrays; these algorithms are used for various purposes like classification or regression. However, they are inappropriate for direct use on complex data sets. Our work consists of two distinct parts. In the first one we model the images by graphs to be able to represent their structural properties and inherent attributes. In the second one, we use kernel functions to project the graphs in a mathematical space that allows the use of performant classification algorithms. Experiments are performed on medical images acquired with various modalities and concerning di_erent parts of the body. ", "recorded": "2007-06-11T15:00:00", "title": "Image Classification Using Marginalized Kernels for Graphs"}, {"url": "nipsworkshops2011_joachims_learning", "desc": "Many systems, ranging from search engines to smart homes, aim to continually improve the utility they are providing to their users. While clearly a machine learning problem, it is less clear what the\r\ninterface between user and learning algorithm should look like. Focusing on learning problems that arise in recommendation and search, this talk explores how the interactions between the user and the system can be modeled as an online learning process. In particular, the talk investigates several techniques for eliciting implicit feedback, evaluates their reliability through user studies, and then proposes online learning models and methods that can make use of such feedback. A key finding is that implicit user feedback comes in the form of preferences, and that our online learning methods provide bounded regret for (approximately) rational users.", "recorded": "2011-12-17T07:45:10", "title": "Online Learning with Implicit User Preferences"}, {"url": "nipsworkshops2011_big_learning", "desc": "This workshop aims to:\r\n\r\n    * Bring together parallel system builders in industry and academia, machine learning algorithms experts, and end users to identify the key challenges, opportunities, and myths of Big Learning. What REALLY changes from the traditional learning setting when faced with terabytes or petabytes of data?\r\n    * Solicit practical case studies, demos, benchmarks and lessons-learned presentations, and position papers.\r\n    * Showcase recent and ongoing progress towards parallel ML algorithms\r\n    * Provide a forum for exchange regarding tools, software, and systems that address the Big Learning problem.\r\n    * Educate the researchers and practitioners across communities on state-of-the-art solutions and their limitations, particularly focusing on key criteria for selecting task- and domain-appropriate platforms and algorithms.\r\n\r\nWorkshop homepage: http://biglearn.org/index.php", "recorded": "2011-12-16T07:30:00", "title": "Big Learning: Algorithms, Systems, and Tools for Learning at Scale"}, {"url": "wehys08_whistler", "desc": "This workshop aims at collecting theoretical insights in the design of data-dependent learning strategies. Specifically we are interested in how far learned prediction rules may be characterized in terms of the observations themselves. This amounts to capturing how well data can be used to construct structured hypothesis spaces for risk minimization strategies - termed empirical hypothesis spaces. Classical analysis of learning algorithms requires the user to define a proper hypothesis space before seeing the data. In practice however, one often decides on the proper learning strategy or the form of the prediction rules of interest after inspection of the data. This theoretical gap constitutes exactly the scope of this workshop.\n\nMore information about the workshop can be found [[http://www.kuleuven.be/wehys/|here]].", "recorded": "2008-12-12T07:30:00", "title": "NIPS  Workshop on New Challenges in Theoretical Machine Learning: Learning with Data-dependent Concept Spaces, Whistler 2008"}, {"url": "nips2010_stobbe_emd", "desc": "Many combinatorial problems arising in machine learning can be reduced to the problem of minimizing a submodular function. Submodular functions are a natural discrete analog of convex functions, and can be minimized in strongly polynomial time. Unfortunately, state-of-the-art algorithms for general submodular minimization are intractable for practical problems. In this paper, we introduce a novel subclass of submodular minimization problems that we call decomposable. Decomposable submodular functions are those that can be represented as sums of concave functions applied to linear functions. We develop an algorithm, SLG, that can efficiently minimize decomposable submodular functions with tens of thousands of variables. Our algorithm exploits recent results in smoothed convex minimization. We apply SLG to synthetic benchmarks and a joint classification-and-segmentation task, and show that it outperforms the state-of-the-art general purpose submodular minimization algorithms by several orders of magnitude.", "recorded": "2010-12-07T17:00:00", "title": "Efficient Minimization of Decomposable Submodular Functions"}, {"url": "iswc2013_gangemi_event_extraction", "desc": "Events are elusive entities; as the authors of argue, even human annotators\r\ndo not agree on what is an event and what is its boundary in terms of the\r\nextension of its participants, temporal and geospatial extent, etc.\r\n\r\nMore aspects of events appear when trying to recognize or extract them\r\nfrom text: polarity of speaker's judgment on events, negation, modality, relations\r\n(temporal, causal, declarative, etc.) to other events, etc.\r\n\r\nFRED is a tool to automatically transform knowledge extracted from text\r\ninto RDF and OWL, i.e. it is a machine reader for the Semantic Web. It is event-\r\ncentric, therefore it natively supports event extraction. In a recent landscape analysis of\r\nknowledge extraction tools [3], FRED has got .73 precision, .93 recall, and .87 accuracy,\r\nlargely better than the other tools attempting event extraction.", "recorded": "2013-10-21T15:05:20", "title": "FRED as an Event Extraction Tool"}, {"url": "bbci2012_biessmann_neural_information", "desc": "The blood oxygen level dependent (BOLD) signal as measured by functional magnetic resonance imaging (fMRI) has become a standard marker of neural activity. The relationship between neural activity and its hemodynamic response is characterized by complex spatiotemporal dynamics. Most analyses rely on simple models of neurovascular coupling and thus underestimate the neural information in the BOLD signal. We use machine learning methods to obtain a model free estimate of the neural information in BOLD contrast data from simultaneous recordings of high resolution fMRI signals and intracranially measured neural bandpower signals. These estimates can help to guide parameter selection in fMRI studies for optimal decoding of stimulus information and might serve as a baseline to which studies without intracranial neural measurements can be compared.", "recorded": "2012-09-18T14:00:00", "title": "Towards an Estimate of the Neural Information of the BOLD Signal"}, {"url": "akbc2010_myaeng_aehak", "desc": "Knowledge on daily human activities in various domains is invaluable for many customized user services that can benefit from context-awareness or activity predictions. Past approaches to constructing a knowledge base of this kind have been domain-specific and not scalable. A recent attempt to extract activities of daily living (ADL) from Web resources deal with activities and objects involved in achieving them but not the sequence of actions in an activity. This paper describes an approach to automatically extracting human activity knowledge from Web articles that describe methods for performing tasks in a variety of domains. The target knowledge base is comprised of activity goals, actions, and ingredients, which are extracted with syntactic pattern-based and probabilistic machine learning based methods. The result is evaluated for accuracy and coverage against some baselines.", "recorded": "2010-05-19T12:00:00", "title": "Automatic Extraction of Human Activity Knowledge from Method-Describing Web Articles"}, {"url": "mlss09us_amit_ldm", "desc": "It is widely recognized that the fundamental building block in high level computer vision is the deformable template, which represents realizations of an object class in the image as noisy geometric instantiations of an underlying model. The instantiations typically come from a subset of some group centered at the identity which act on the model or template. Thus in contrast to some machine learning applications where one tries to discover some unspecified manifold structure, here it is entirely determined by the group action and the model. Given a choice of group action and family of template models a major challenge is to use a sample of images of the object to estimate the model and the distribution on the group. The primary obstacle is that the instantiations or group elements that produced each image are unobserved. I will describe a general formulation of this problem and then show some practical applications to object detection and recognition. ", "recorded": "2009-06-03T15:30:00", "title": "Learning Deformable Models"}, {"url": "kdd2014_chapelle_display_advertising", "desc": "In performance display advertising a key metric of a campaign effectiveness is its conversion rate -- the proportion of users who take a predefined action on the advertiser website, such as a purchase. Predicting this conversion rate is thus essential for estimating the value of an impression and can be achieved via machine learning. One difficulty however is that the conversions can take place long after the impression -- up to a month -- and this delayed feedback hinders the conversion modeling. We tackle this issue by introducing an additional model that captures the conversion delay. Intuitively, this probabilistic model helps determining whether a user that has not converted should be treated as a negative sample -- when the elapsed time is larger than the predicted delay -- or should be discarded from the training set -- when it is too early to tell. We provide experimental results on real traffic logs that demonstrate the effectiveness of the proposed model.", "recorded": "2014-08-27T11:30:00", "title": "Modeling Delayed Feedback in Display Advertising"}, {"url": "mloss08_zito_mdp", "desc": "Modular toolkit for Data Processing (MDP) is a Python data processing framework. From the user\u2019s per-\nspective, MDP is a collection of supervised and unsupervised learning algorithms and other data processing\nunits that can be combined into data processing sequences and more complex feed-forward network archi-\ntectures. From the scienti\ufb01c developer\u2019s perspective, MDP is a modular framework, which can easily be\nexpanded. \n\nThe implementation of new algorithms is easy and intuitive. The new implemented units are\nthen automatically integrated with the rest of the library. The base of available algorithms is steadily in-\ncreasing and includes, to name but the most common, Principal Component Analysis (PCA and NIPALS),\nseveral Independent Component Analysis algorithms (CuBICA, FastICA, TDSEP, and JADE), Slow Feature\nAnalysis, Gaussian Classi\ufb01ers, Restricted Boltzmann Machine, and Locally Linear Embedding.", "recorded": "2008-12-12T09:50:00", "title": "MDP \u2013 Modular toolkit for Data Processing"}, {"url": "kdd07_teo_asmcs", "desc": "A wide variety of machine learning problems can be described as minimizing a regularized risk functional, with different algorithms using different notions of risk and different regularizers. Examples include linear Support Vector Machines (SVMs), Logistic Regression, Conditional Random Fields (CRFs), and Lasso amongst others. This paper describes the theory and implementation of a highly scalable and modular convex solver which solves all these estimation problems. It can be parallelized on a cluster of workstations, allows for data-locality, and can deal with regularizers such as `1 and `2 penalties. At present, our solver implements 20 different estimation problems, can be easily extended, scales to millions of observations, and is up to 10 times faster than specialized solvers for many applications. The open source code is freely available as part of the ELEFANT toolbox.", "recorded": "2007-08-14T11:58:41", "title": "A Scalable Modular Convex Solver for Regularized Risk Minimization "}, {"url": "nipsworkshops09_graepel_pmlca", "desc": "In the past years online advertising has grown at least an order of magnitude faster than advertising on all other media. This talk focuses on advertising on search engines, where accurate predictions of the probability that a user clicks on an advertisement crucially benefit all three parties involved: the user, the advertiser, and the search engine. We present a Bayesian probabilistic classification model that has the ability to learn from terabytes of web usage data. The model explicitly represents uncertainty allowing for fully probabilistic predictions: 2 positives out of 10 instances or 200 out of 1000 both give an average of 20%, but in the first case the uncertainty about the prediction should be larger. We also present a scheme for approximate parallel inference that allows efficient training of the algorithm on a distributed data architecture. ", "recorded": "2009-12-11T08:30:00", "title": "Probabilistic Machine Learning in Computational Advertising"}, {"url": "slsfs05_navot_wonfl", "desc": "In this paper we discuss the problem of feature selection for supervised learning from the standpoint of statistical machine learning. We inquire what subset of features will lead to the best classification accuracy. It is clear that if the statistical model is known, or if there are an unlimited number of training samples, any additional feature can only improve the accuracy. However, we explicitly show that when the training set is finite, using all the features may be suboptimal, even if all the features are independent and carry information on the label. We analyze one setting analytically and show how feature selection can increase accuracy. We also find the optimal number of features as a function of the training set size for a few specific examples. This perspective on feature selection is different from the common approach that focuses on the probability that a specific algorithm will pick a completely irrelevant or redundant feature.", "recorded": "2005-02-24T00:00:00", "title": "What is the Optimal Number of Features? A learning theoretic perspective"}, {"url": "iswc06_gruber_wswms", "desc": "The Semantic Web is an ecosystem of interaction among computer systems. The social web is an ecosystem of conversation among people. Both are enabled by conventions for layered services and data exchange. Both are driven by human-generated content and made scalable by machine-readable data. Yet there is a popular misconception that the two worlds are alternative, opposing ideologies about how the web ought to be. Folksonomy vs. ontology. Practical vs. formalistic. Humans vs. machines.\r \r This is nonsense, and it is time to embrace a unified view. I subscribe to the vision of the Semantic Web as a substrate for collective intelligence. The best shot we have of collective intelligence in our lifetimes is large, distributed human-computer systems. The best way to get there is to harness the \"people power\" of the Web with the techniques of the Semantic Web. In this presentation I will show several ways that this can be, and is, happening.", "recorded": "2006-11-07T00:00:00", "title": "Where the Social Web Meets the Semantic Web"}, {"url": "colt2013_lattimore_sparse", "desc": "Online estimation and modelling of i.i.d. data for shortsequences over large or complex \u201calphabets\u201d is a ubiquitous (sub)problem in machine learning, information theory, data compression, statistical language processing, and document analysis. The Dirichlet-Multinomial distribution (also called Polya urn scheme) and extensions thereof are widely applied for online i.i.d. estimation. Good a-priori choices for the parameters in this regime are difficult to obtain though. I derive an optimal adaptive choice for the main parameter via tight, data-dependent redundancy bounds for a related model. The 1-line recommendation is to set the \u2019total mass\u2019 = \u2019precision\u2019 = \u2019concentration\u2019 parameter to m/[2lnn+1m], where n is the (past) sample size and m the number of different symbols observed (so far). The resulting estimator is simple, online, fast,and experimental performance is superb.", "recorded": "2013-06-12T16:30:00", "title": "Sparse Adaptive Dirichlet-Multinomial-like Processes"}, {"url": "ecmlpkdd2010_hachiya_fsr", "desc": "Model-free reinforcement learning (RL) is a machine learning approach to decision making in unknown environments. However, real-world RL tasks often involve high-dimensional state spaces, and then standard RL methods do not perform well. In this paper, we propose a new feature selection framework for coping with high dimensionality. Our proposed framework adopts conditional mutual information between return and state-feature sequences as a feature selection criterion, allowing the evaluation of  implicit state-reward dependency. The conditional mutual information is approximated by a least-squares method, which results in a computationally efficient feature selection procedure. The usefulness of the proposed method is demonstrated on grid-world navigation problems. ", "recorded": "2010-09-23T10:56:00", "title": "Feature Selection for Reinforcement Learning: Evaluating Implicit State-Reward Dependency via Conditional Mutual Information"}, {"url": "eccv2012_martinovic_parsing", "desc": "We propose a novel three-layered approach for semantic segmentation of building facades. In the first layer, starting from an oversegmentation of a facade, we employ the recently introduced machine learning technique Recursive Neural Networks (RNN) to obtain a probabilistic interpretation of each segment. In the second layer, initial labeling is augmented with the information coming from\r\nspecialized facade component detectors. The information is merged using a Markov Random Field. In the third layer, we introduce weak architectural knowledge, which enforces the final reconstruction to be architecturally plausible and consistent. Rigorous tests performed on two existing datasets of building facades demonstrate that we significantly outperform the current-state of the art, even when using outputs from earlier layers of the pipeline. Also, we show how the final output of the third layer can be used to create a procedural reconstruction.", "recorded": "2012-10-11T17:05:00", "title": "A Three-Layered Approach to Facade Parsing"}, {"url": "w3cworkshop2013_uszkoreit_internet", "desc": "As Europe has integration politically, languages are still one of the most pervasive barriers to interpersonal communication, cross-border commerce, and full participation in European democracy. Despite recent progress in machine translation, it is clear that the quality of today's Internet-based translation services is neither good enough for many tasks nor complete in terms of language coverage. Speakers of smaller languages thus find themselves largely excluded from vital discussions of European identity and policy. This talk will argue in favor of a concerted push in Europe for quality translation technology and address concrete preparatory actions performed by the EC-funded QT LaunchPad Project. It will discuss how these actions will directly influence the future of the Multilingual Web, both in Europe and around the world.", "recorded": "2013-03-12T16:50:00", "title": "Quality Translation: Addressing the Next Barrier to Multilingual Communication on the Internet"}, {"url": "mlss09us_yu_simhdd", "desc": " Extracting useful information from high-dimensional data is the focus of today's statistical research and practice. After broad success of statistical machine learning on prediction through regularization, interpretability is gaining attention and sparsity has been used as its proxy. With the virtues of both regularization and sparsity, Lasso (L1 penalized L2 minimization) has been very popular recently. In this talk, I would like to discuss the theory and pratcice of sparse modeling. First, I will give an overview of recent research on sparsity and explain what useful insights have been learned from theoretical analyses of Lasso. Second, I will present collaborative research with the Gallant Lab at Berkeley on building sparse models (linear, nonlinear, and graphical) that describe fMRI responses in primary visual cortex area V1 to natural images. ", "recorded": "2009-06-02T14:30:00", "title": "Seeking Interpretable Models for High Dimensional Data"}, {"url": "iswc2014_scheglmann_liteq", "desc": "The Semantic Web is intended as a web of machine readable data where every data source can be the data provider for different kinds of applications. However, due to a lack of support it is still cumbersome to work with RDF data in modern, object-oriented programming languages, in particular if the data source is only available through a SPARQL endpoint without further documentation or published schema information. In this setting, it is desirable to have an integrated tool-chain that helps to understand the data source during development and supports the developer in the creation of persistent data objects. To tackle these issues, we introduce LITEQ, a paradigm for integrating RDF data sources into programming languages and strongly typing the data. Additionally, we report on two use cases and show that compared to existing approaches LITEQ performs competitively according to the Halstead metric.", "recorded": "2014-10-22T15:40:00", "title": "Semantic Web Application Development with LITEQ"}, {"url": "icml08_laskov_scec", "desc": "We propose a new stopping condition for a Support Vector Machine (SVM) solver which precisely reflects the objective of the Leave-One-Out error computation. The stopping condition guarantees that the output on an intermediate SVM solution is identical to the output of the optimal SVM solution with one data point excluded from the training set. A simple augmentation of a general SVM training algorithm allows one to use a stopping criterion equivalent to the proposed sufficient condition. A comprehensive experimental evaluation of our method shows consistent speedup of the exact LOO computation by our method, up to the factor of 13 for the linear kernel. The new algorithm can be seen as an example of constructive guidance of an optimization algorithm towards achieving the best attainable expected risk at optimal computational cost.", "recorded": "2008-07-06T14:20:00", "title": "Stopping Conditions for Exact Computation of Leave-One-Out Error in Support Vector Machines"}, {"url": "aistats2010_shivaswamy_ebb", "desc": "Concentration inequalities that incorporate variance information (such as Bernstein's or Bennett's inequality) are often significantly tighter than counterparts (such as Hoeffding's inequality) that disregard variance. Nevertheless, many state of the art machine learning algorithms for classification problems like AdaBoost and support vector machines (SVMs) extensively use Hoeffding's inequalities to justify empirical risk minimization and its variants. This article proposes a novel boosting algorithm based on a recently introduced principle--sample variance penalization--which is motivated from an empirical version of Bernstein's inequality. This framework leads to an efficient algorithm that is as easy to implement as AdaBoost while producing a strict generalization. Experiments on a large number of datasets show significant performance gains over AdaBoost. This paper shows that sample variance penalization could be a viable alternative to empirical risk minimization. ", "recorded": "2010-05-13T17:50:00", "title": "Empirical Bernstein boosting"}, {"url": "icml2010_von_luxburg_ancr", "desc": "Non-geometric data is often represented in form of a graph where edges represent similarity or local relationships between instances. One elegant way to exploit the global structure of the graph is implemented by the commute distance (also known as resistance distance). Supposedly it has the property that vertices from the same cluster are \"close\" to each other whereas vertices from different clusters are \"far\" from each other. We study the behavior of the commute distance as the size of the underlying graph increases. We prove that the commute distance converges to an expression that does not take into account the structure of the graph and that is completely meaningless as a distance function on the graph. Consequently, the use of the raw commute distance for machine learning purposes is strongly discouraged for large graphs and in high dimensions. We suspect that a similar behavior holds for several other distances on graphs. ", "recorded": "2010-06-25T14:50:00", "title": "A note of caution regarding distances on graphs"}, {"url": "wsdm2010_hillard_iars", "desc": "We describe a machine learning approach for predicting sponsored search ad relevance. Our baseline model incorporates basic features of text overlap and we then extend the model to learn from past user clicks on advertisements. We present a novel approach using translation models to learn user click propensity from sparse click logs.\r\n\r\nOur relevance predictions are then applied to multiple sponsored search applications in both offline editorial evaluations and live online user tests. The predicted relevance score is used to improve the quality of the search page in three areas: filtering low quality ads, more accurate ranking for ads, and optimized page placement of ads to reduce prominent placement of low relevance ads. We show significant gains across all three tasks.", "recorded": "2010-02-06T11:50:49", "title": "Improving Ad Relevance in Sponsored Search"}, {"url": "sssc2011_berkeley", "desc": "Semantic Computing is a rapidly evolving research field that integrates representation, methods, and techniques from areas as diverse as multimedia, computational linguistics, semantic web, knowledge engineering, software engineering, with the goal of creating novel technologies and applications that connect intuitively formulated user intentions with the content and meaning of machine-represented data.\r\n\r\nAlready at its third edition, the International Summer School on Semantic Computing 2011 provides an introduction to this interdisciplinary field to industry and academic groups. A mix of young and well-established researchers and educators gave tutorials on cutting-edge results in the aforementioned areas, complemented by keynote talks by renowned experts and hands-on exercises that showcase the application of the most important technologies in real-world situations.\r\n\r\nDetailed information can be found at http://sssc2011.sti2.org/.", "recorded": "2011-08-08T09:00:00", "title": "International Summer School on Semantic Computing (SSSC), Berkeley 2011"}, {"url": "mlss09us_jordan_slffddd", "desc": "In 1951, David Blackwell published a seminal paper - widely cited in economics - in which a link was established between the risk based on 0-1 loss and a class of functionals known as f-divergences. The latter functionals have since come to play an important role in several areas of signal processing and information theory, including decentralized detection. Yet their role in these fields has largely been heuristic. We show that an extension of Blackwell\u00b4s programme provides a solid foundation for the use of f-divergences in decentralized detection, as well as in more general problems of experimental design. Our extension is based on a connection between f-divergences and the class of so-called surrogate loss funcions - computationally-inspired upper bounds on 0-1 loss that have become central in the machine learning literature on classification. (Joint work with XuanLong Nguyen and Martin Wainwright.)", "recorded": "2009-06-05T13:00:00", "title": "On Surrogate Loss Functions, f-Divergences and Decentralized Detection"}, {"url": "bbci2012_hansen_neurotechnology", "desc": "Brain imaging by PET, MR, EEG, and MEG has become a cornerstone in systems level neuroscience. Statistical analyses of neuroimage datasets face many interesting challenges including non-linearity and multi-scale spatial and temporal dynamics. The objectives of neuroimaging are dual, we are interested in the most accurate, i.e., predictive, statistical model, but equally important is model interpretation and visualization which often takes the form of \u201cbrain mapping\u201d. I will introduce some current machine learning strategies invoked for explorative and hypothesis driven neuroimage modeling, and present a general framework for model evaluation, interpretation, and visualization based on computer intensive data re-sampling schemes. Within the framework we obtain both an unbiased estimate of the predictive performance and of the reliability of the brain map visualization.", "recorded": "2012-09-21T11:00:00", "title": "Resampling Based Methods for Design and Evaluation of Neurotechnology"}, {"url": "icml09_ji_agmt", "desc": "We consider the minimization of a smooth loss function regularized by the trace norm of the matrix variable. Such formulation finds applications in many machine learning tasks including multi-task learning, matrix classification, and matrix completion. The standard semidefinite programming formulation for this problem is computationally expensive. In addition, due to the non-smoothness nature of the trace norm, the optimal first-order black-box method for solving such class of problems converges as O(1/sqrt(k)), where k is the iteration counter. In this paper, we exploit the special structure of the trace norm, based on which we propose an extended gradient algorithm that converges as O(1/k). We further propose an accelerated gradient algorithm, which achieves the optimal convergence rate of O(1/k^2) for smooth problems. Experiments on multi-task learning problems demonstrate the efficiency of the proposed algorithms.", "recorded": "2009-06-16T15:10:00", "title": "An Accelerated Gradient Method for Trace Norm Minimization"}, {"url": "nips09_torralba_uvs", "desc": "One remarkable aspect of human vision is the ability to understand the meaning of a novel image or event quickly and effortlessly. Within a single glance, we can comprehend the semantic category of a place, its spatial layout as well as identify many of the objects that compose the scene. Approaching human abilities at scene understanding is a current challenge for computer vision systems. The field of scene understanding is multidisciplinary, involving a growing collection of works in computer vision, machine learning, cognitive psychology, and neuroscience. In this tutorial, we will focus on recent work on computer vision attempting to solve the tasks of scene recognition and classification, visual context representation, object recognition in context, drawing parallelism with work in psychology and neuroscience. Devising systems that solve scene and object recognition in an integrated fashion will lead towards more efficient and robust artificial visual understanding systems.", "recorded": "2009-12-07T09:30:00", "title": "Understanding Visual Scenes"}, {"url": "icml08_donmez_oelr", "desc": "Learning to rank is becoming an increasingly popular research area in machine learning. The ranking problem aims to induce an ordering or preference relations among a set of instances in the input space. However, collecting labeled data is growing into a burden in many rank applications since labeling requires eliciting the relative ordering over the set of alternatives. In this paper, we propose a novel active learning framework for SVM-based and boosting-based rank learning. Our approach suggests sampling based on maximizing the estimated loss differential over unlabeled data. Experimental results on two benchmark corpora show that the proposed model substantially reduces the labeling effort, and achieves superior performance rapidly with as much as 30% relative improvement over the margin-based sampling baseline.", "recorded": "2008-07-07T11:15:00", "title": "Optimizing Estimated Loss Reduction for Active Sampling in Rank Learning"}, {"url": "icml09_bengio_cl", "desc": "Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them \"curriculum\r\nlearning\". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural\r\nnetworks), we explore curriculum learning in various set-ups. The experiments show that signi\fcant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an e\u000bffect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method\r\n(a general strategy for global optimization of non-convex functions).", "recorded": "2009-06-16T14:00:00", "title": "Curriculum Learning"}, {"url": "aaai2012_king_robot_scientists", "desc": "A robot scientist is a physically implemented robotic system that applies techniques from artificial intelligence to execute cycles of automated scientific experimentation. A Robot Scientist can automatically execute cycles of: hypothesis formation, selection of efficient experiments to discriminate between hypotheses, execution of experiments using laboratory automation equipment, and analysis of results. We developed the robot scientist Adam to investigate yeast functional genomics. Adam is the first time a machine has discovered novel scientific knowledge. This knowledge is described in a formal argument involving over 10,000 different research units that relates Adam's 6.6 million observations to its conclusions. Our new robot scientist Eve applies the same approach to drug design. Eve has efficiently found \"lead compounds\" for malaria and other neglected tropical diseases.", "recorded": "2012-07-25T13:25:49", "title": "Automating Biology Using Robot Scientists"}, {"url": "ilpmlgsrl09_leuven", "desc": "In 2009, three international conferences / workshops on learning from logical, relational, graph-based and probabilistic knowledge were co-located in Leuven.\r\n\r\n* ILP-2009, the 19th International Conference on Inductive Logic Programming. The ILP conference series has been the premier forum for work on logic-based approaches to learning for almost two decades. It has recently reached out to other forms of relational learning and to probabilistic approaches.\r\n* MLG-2009, the 7th International Workshop on Mining and Learning with Graphs. The MLG workshop series focuses on graph-based approaches to machine learning and data mining since its conception in 2003.\r\n*SRL-2009, the International Workshop on Statistical Relational Learning. The SRL workshop series focuses on statistical inference and learning with relational and first-order logical representations. The combination of probability theory with relational (or first-order logic) knowledge representations has been the subject of much recent research.\r\n\r\nWhile the three series clearly have their own identity, there is a significant overlap in the topics covered by each of them. The aim of this colocation was to increase interaction between the three communities. The format of the joint event stimulated such interaction by providing joint invited speakers and tutorials, joint sessions and poster sessions, as well as ample time and space for discussions in smaller groups, in addition to the regular programs of the three events.\r\n\r\nVideo recordings of the joint invited speakers and tutorials as well as on a panel are available through Videolectures.\r\nFurthermore, abstracts presented in Leuven can be found at http://www.cs.kuleuven.be/~dtai/ilp-mlg-srl/, a post-proceedings volume for\r\nILP 2009 will appear with Springer LNCS and a joint special issue of the Machine Learning Journal will be published.\r\n\r\n----\r\nMore about the workshop can be found at: http://www.cs.kuleuven.be/~dtai/ilp-mlg-srl/index.php\r\n----", "recorded": "2009-07-02T09:00:00", "title": "ILP/MLG/SRL collocated International conferences/workshops on learning from relational, graph-based and probabilistic knowledge, Leuven 2009"}, {"url": "lsoldm2014_varma_kernel_learning", "desc": "The time taken by an algorithm to make predictions is of critical importance\r\nas machine learning transitions to becoming a service available on the cloud.\r\nAlgorithms that are efficient at prediction can service more calls and utilize\r\nfewer cloud resources and thereby generate more revenue. They can also be used\r\nin real time applications where predictions need to be made in micro/milliseconds.\r\n\r\nNon-linear SVMs have defined the state-of-the-art on multiple benchmark tasks.\r\nUnfortunately, they are slow at prediction with costs that are linear in the number\r\nof training points. This reduces the attractiveness of non-linear SVMs trained on\r\nlarge amounts of data in cloud scenarios.\r\n\r\nIn this talk, we develop LDKL \u2013 an efficient non-linear SVM classifier with\r\nprediction costs that grow logarithmically with the number of training points.\r\nWe generalize Localized Multiple Kernel Learning so as to learn a deep primal\r\nfeature embedding which is high dimensional and sparse. Primal based classification\r\ndecouples prediction costs from the number of support vectors and\r\nour tree-structured features efficiently encode non-linearities while speeding\r\nup prediction exponentially over the state-of-the-art. We develop routines for\r\noptimizing over the space of tree-structured features and efficiently scale to\r\nproblems with millions of training points. Experiments on benchmark data\r\nsets reveal that LDKL can reduce prediction costs by more than three orders\r\nof magnitude over RBF-SVMs in some cases. Furthermore, LDKL leads to\r\nbetter classification accuracies as compared to leading methods for speeding up\r\nnon-linear SVM prediction.\r\n\r\nLDKL is available on AzureML, Microsoft\u2019s cloud machine learning platform,\r\nand I will briefly discuss how it can be used to develop a highly performant virus\r\nand malware classifier which needs to predict potential threats every time files are\r\nopened, saved or executed on hundreds of millions of machines on a daily basis.", "recorded": "2014-09-11T09:00:00", "title": "Local Deep Kernel Learning for Efficient Non-linear SVM Prediction"}, {"url": "eswc2012_heraklion", "desc": "The Extended Semantic Web Conference (ESWC) is a major venue for discussing the latest scientific results and technology innovations around semantic technologies. Building on its past success, ESWC is seeking to broaden its focus to span other relevant research areas in which Web semantics plays an important role.\r\n\r\nThe goal of the Semantic Web is to create a Web of knowledge and services in which the semantics of content is made explicit and content is linked to both other content and services novel applications allowing to combine content from heterogeneous sites in unforeseen ways and support enhanced matching between users needs and content. This network of knowledge-based functionality will weave together a large network of human knowledge, and make this knowledge machine-processable to support intelligent behaviour by machines. Creating such an interlinked Web of knowledge which spans unstructured, RDF as well as multimedia content and services requires the collaboration of many disciplines, including but not limited to: Artificial Intelligence, Natural Language Processing, Database and Information Systems, Information Retrieval, Machine Learning Multimedia, Distributed Systems, Social Networks, Web Engineering, and Web Science. These complementarities are reflected in the outline of the technical program of the ESWC 2012; in addition to the research and in-use tracks, we will feature two special tracks putting particular emphasis on inter-disciplinary research topics and areas that show the potential of exciting synergies for the future, eGovernment and Digital Libraries. ESWC 2012 will present the latest results in research, technologies and applications in its field. Besides the technical program organized over twelve tracks, the conference will feature a workshop and tutorial program, system descriptions and demos, a posters exhibition, a doctoral symposium, as well as the ESWC summer school, which will be held prior to the conference.\r\n\r\nDetailed information can be found at [[http://2012.eswc-conferences.org/|ESWC 2012 website]].", "recorded": "2012-05-21T09:00:00", "title": "9th Extended Semantic Web Conference (ESWC), Heraklion 2012"}, {"url": "kdd2010_feldman_qalbb", "desc": "As electronic communication, media and commerce increasingly permeate every aspect of modern life, real-time personalization of consumer experience through data-mining becomes practical. Effective classification, prediction and change modeling of consumer interests, behaviors and purchasing habits using machine learning and statistical methods drives efficiency, insights and consumer relevance that were never before possible. \r\n\r\n\r\nThe internet has brought on a rapid evolution in advertising. Everything about behavior on the internet can be quantified and responses to behavior can occur in real time. This dynamic interaction with the user has created opportunities to better understand the way in which individuals move from awareness of a product to considering a purchase, through to intent and ultimately a sale for the marketer. When a marketer can answer the question \u201edid those TV ads cause consumers to switch shampoo brands?\u201f they can model behavior change and adjust marketing strategies accordingly. \r\n\r\n\r\nUnderpinning this shift in how the world\u201fs trillion dollar marketing budget is spent is transactional data on an unprecedented scale, creating new challenges for software that must interpret this stream and make real time decisions tens, even hundreds of thousands of times every second. \r\n\r\n\r\nI will explore advances in modeling media consumption, advertising response and the real-time evaluation of media opportunities through reference to Quantcast, a business launched in September 2006 which today interprets in excess of 10 billion new digital media consumption records every day. We will examine the challenges of applying machine learning to non-search advertising and in doing so explore the creation of business environments \u2013 organization, infrastructure, tools, processes (and costs considerations) \u2013 in which scientists can quickly develop new petabyte scale algorithmic approaches, migrate them rapidly to real-time production and deliver fully customized experiences for marketers, publishers and consumers alike.\r\n", "recorded": "2010-07-28T09:00:00", "title": "The quantification of advertising and lessons from building a business based on large scale data mining"}, {"url": "mmdss07_laskov_mlit", "desc": "Intrusion detection is one of core technologies of computer security. The goal\r of intrusion detection goal is identi\fcation of malicious activity in a stream\r of monitored data which can be network tra\u000ec, operating system events or\r log entries. A majority of current intrusion detection systems (IDS) follows a\r signature-based approach in which, similar to virus scanners, events are detected\r that match speci\fc pre-de\fned patterns known as \\signatures\". The main\r limitation of signature-based IDS is their failure to identify novel attacks, and\r sometimes even minor variations of known patterns. Besides, a signi\fcant administrative\r overhead is incurred by the need to maintain signature databases.\r Machine learning o\u000bers a major opportunity to improve quality and to facilitate\r administration of IDS. Supervised learning can be used for automatic\r generation of detectors without a need to manually de\fne and update signatures.\r Anomaly detection and other unsupervised learning techniques can detect\r new kinds of attacks provided they exhibit unusual character in some feature\r space. In our contribution, kernel and distance based learning algorithms for\r network intrusion detection will be presented.\r The two essential parts of our approach are online learning algorithms and\r feature extraction. The major requirements on the algorithmic part are linear\r run-time, online learning and data type abstraction. Simple but e\u000bective anomaly\r detection algorithms will be presented that satisfy these requirements\r (1). Feature extraction algorithms can be reduced to computation of similarity\r measures between sequential objects. In order to access the feature from the\r application-layer network protocols, in which most of modern remote exploits\r operate, similarity measures are computed directly over byte streams of TCP\r connections. Algorithms and data structures will be presented that allow e\u000e-\r cient computation of similarity measures in linear time with very low run-time\r constants and memory consumption (2)", "recorded": "2007-09-13T18:10:52", "title": "Machine Learning for Intrusion Detection"}, {"url": "internetofeducation2013_ljubljana", "desc": "This year's Internet of Education conference takes place in Ljubljana, Slovenia and is organized by [[http://www.k4all.org/|Knowledge 4 All Foundation Ltd (K4A)]] and the [[http://ailab.ijs.si/|Artificial Intelligence Lab]] at the \u201cJo\u017eef Stefan\u201d Institute, covering countries in South-East Europe (SEE). \r\n\r\nThe goal of this event is to bring together researchers and policy makers from both university and academia to research into methods for improving the effectiveness of video based MOOC education but also promote discussion of the implications of MOOCs and new edutech trends for classical university education, especially in the South-East European region. We are keen to promote discussion and research into what new technology can contribute to effective online learning.\r\n\r\nWe would like to promote, clarify, identify and define how emerging technologies based on artificial intelligence (AI) and computer human interaction (CHI) tools, machine learning, machine translation, user analytics, automatic assessment, visualization, social collaboration and more can change and help create new trends in education.\r\n\r\nMore specifically how will these technologies change and influence today\u2019s traditions in Academic publishing, syllabus, validation and certification. Our aim is to form a technical and investigative think-tank of participants to debate and discuss these issues which in the future will affect all educational levels. The theme and goals of the Internet of Education 2013 are very well articulated with UNESCO's vision and actions therefore the conference has been granted UNESCO patronage.\r\n\r\n[[http://www.k4all.org/|Knowledge 4 All Foundation Ltd (K4A)]] and the [[http://ailab.ijs.si/|Artificial Intelligence Lab]] at the \"Jo\u017eef Stefan\" Institute are also the organizers of [[http://conference.ocwconsortium.org/2014/|OCWC 2014]] with its main theme \"Open Education for a Multicultural World\".\r\n\r\nTo find out more please visit the [[http://www.k4all.org/Internet_of_Education/|Internet of Education 2013 website]].", "recorded": "2013-11-11T09:00:00", "title": "1st Internet of Education Conference - The role of Computer Science in the Internet of Education, Ljubljana 2013"}, {"url": "mitworld_manning_mlld", "desc": "Christopher Manning thinks linguistics went astray in the 20th century when it searched \u201cfor homogeneity in language, under the misguided assumption that only homogeneous systems can be structured.\u201d In the face of human creativity with language, rigid categories of linguistic use just don\u2019t help explain how people actually talk and what they choose to say. For every hard and fast rule linguists find, other linguists can determine an exception. Categorical constraints rise, then come crashing down.\n\nManning argues for acceptance of variable systems of language, and for searching for structure in these systems using probabilistic methods. Manning applies quantitative techniques to sentence structure, digging for the frequency, probability and likelihood that people will use specific turns of phrase in certain real-world contexts. Looking at distributions in the ways people express ideas in a language \u201ccan give a much richer description of how language is used.\u201d Indeed, Manning finds that certain typical constraints on sentence structure in one language \u201cshow up as softer constraints and preferences in other languages.\u201d\n\nManning looks at raw data, like sentences from the Wall Street Journal, and gleans such information as typical word associations that begin to \u201ctell us about the dependencies of verbs and arguments.\u201d He looks for dependencies between words, the distance between them, and at a sentence\u2019s flow from left to right. Classes of words emerge, and clusters, yielding distributionally learned categories. Certain classes of syntax naturally fall together. Manning builds nested phrase structure trees, and branching structures, and derives simple probabilistic models that help explain \u201cgradual learning and robustness in acquisition, non-homogeneous grammars of individuals, and gradual language change over time.\u201d Manning says computational linguistics is also proving useful in such applied fields as information retrieval, machine translation, and text mining.", "recorded": "2007-10-19T14:48:49", "title": "Machine Learning of Language from Distributional Evidence"}, {"url": "icwsm2010_kearns_bes", "desc": "For four years now, we have been conducting\r\n\u201cmedium-scale\u201d experiments in how human\r\nsubjects behave in strategic and economic settings\r\nmediated by an underlying social network\r\nstructure. We have explored a wide range\r\nof networks inspired by generative models\r\nfrom the literature, and a diverse set of collective\r\nstrategic problems, including biased voting,\r\ngraph coloring, consensus, and networked\r\ntrading. These experiments have yielded a\r\nwealth of both specific findings and emerging\r\ngeneral themes about how populations of human\r\nsubjects interact in strategic networks.\r\nKearns will review these findings and themes,\r\nwith an emphasis on the many more questions\r\nthey raise than answer.\r\nMichael Kearns is a professor of computer\r\nand information science at the University of\r\nPennsylvania, where he holds the National Center\r\nChair in Resource Management and Technology.\r\nHe is the founding director of Penn Engineering\u2019s\r\nnew Market and Social Systems Engineering\r\n(MKSE) program. Kearns has\r\nsecondary appointments in the Statistics and\r\nOp erations and Information Management\r\n(OPIM) departments of the Wharton School,\r\nand is an affiliated faculty member of Penn\u2019s\r\nApplied Math and Computational Science graduate\r\nprogram. Kearns also serves as an advisor\r\nto Yodle, kaChing, Invite Media, and Kwedit.\r\nHis research interests include topics in machine\r\nlearning, algorithmic game theory, social networks,\r\ncomputational finance, and artificial intelligence.\r\nMost recently, he has been conducting\r\nhuman-subject experiments on strategic and\r\neconomic interaction in social networks. Kearns\r\nreceived his B.S in mathematics and computer\r\nscience from the University of California at\r\nBerkeley in 1985, and his Ph.D. in computer\r\nscience from Harvard University in 1989. He\r\nhas served as the program chair of NIPS, AAAI,\r\nCOLT, and ACM EC. He is a member of the\r\nNIPS Foundation and the steering committee\r\nfor the Snowbird Conference on Learning, and\r\nserves on the editorial board of The MIT Press\r\nseries on adaptive computation and machine\r\nlearning.", "recorded": "2010-05-26T09:00:00", "title": "Behavioral Experiments in Strategic Networks"}, {"url": "estc08_uszkoreit_gst", "desc": "As semantic technologies keep evolving and maturing, there is growing concern about the gigantic wealth of knowledge encoded in so-called unstructured data. Actually the bulk of human knowledge on the web (and in books) is represented in texts. Not even the most optimistic proponents of semantic representation standards expect that this information will be rewritten or extensively complemented by semantic meta-data through intellectual labour.  On the other hand, there is a discipline of science and technology called computational linguistics that has been concerned for several decades with the automatic analysis of human language. One of the original goals of this field was the automatic understanding of texts by translating them into a knowledge representation language that machines could use for reasoning. However, through sobering experience of the complexity of this task most applied computational linguists turned to easier challenges. There is now a wide variety of human language technologies, many of which have enabled new types of products. Among these applications are text classification, email response systems, text-to-speech software, grammar checking and statistical machine translation.\r\n\r\nIn this presentation, however, the state of the art and recent achievements in two strands of language technology will be explained and illustrated by examples. One of them is the automatic extraction of semantic relations, or more precisely of relation instances, from large volumes of texts. Such relation instances could be events, properties of objects, or opinions on products. Using results from our own research, I will demonstrate how machine learning techniques were combined with existing advanced language analysis methods for improving such an analysis beyond the best results achievable by either one of these approaches alone. I will also show how the semantic domain models can be utilized for improving the performance of the relation extraction.\r\n\r\nThe second strand of research to be presented is the deep syntactic and semantic analysis of human language. While most computational linguists had turned away from this fundamental challenge in favour of lower hanging fruit, a few groups continued the quest for text understanding. Because of the size of the problem and the desire to develop techniques that would work for more than language, several of them teamed up in international collaborations. I will briefly describe the two largest international collaborations in this area, the DELPH-IN initiative dedicated to deep language processing with HPSG and the PARGRAM initiative pursuing the same goal by LFG. HPSG and LFG are two advanced formal models of linguistic description developed in the seventies and eighties of last century. The results of the PARGRAM initiative were lead by PARC and are among the central assets of the search technology company Powerset which was recently acquired by Microsoft. The results of the DELPH-IN initiative are collected in growing a open-source repository of research resources.  I will explain the significance of recent advances by these two consortia and related research activities.\r\n\r\nIn the conclusion of the talk I will argue that a combination of the machine-learning approach to relation extraction with the advances of the deep linguistic processing research will open the way to an exploitation of large volumes of unstructured textual data by semantic technologies.", "recorded": "2008-09-24T14:08:47", "title": "Getting at the Semantics of Texts"}, {"url": "yalebeng100s08_saltzman_lec25", "desc": "In this final lecture, Professor Saltzman talks about artificial organs, with a stress on synthetic biomaterials. First, the body's responses (immunological and scar healing responses) to foreign materials are introduced. This leads to discussion of different types of polymer/plastic materials (i.e., Dacron and GORE-TEX) and their properties. Next, Professor Saltzman talks about the design and function of some artificial organs, such as lens implants, heart valves and vessels, hip, dialyzer, heart/lung bypass machine, and the artificial heart. Lastly, challenges and areas for improvement in the field are presented.\n\n**Reading assignment:**\n\nBiomedical Engineering: Bridging Medicine and Technology, in preparation by Mark Saltzman (forthcoming by Cambridge University Press); chapters 17 and 18 \n\n**Resources:**\n;[[http://oyc.yale.edu/sites/default/files/beng100chapter17_2.pdf|Summary and Key Concepts: Chapter 17[PDF]]]", "recorded": "2007-03-03T10:00:00", "title": "Lecture 25 - Biomedical Engineers and Artificial Organs"}, {"url": "icml2015_berlind_active_nearest_neighbors", "desc": "While classic machine learning paradigms assume training and test data are generated from the same process, domain adaptation addresses the more realistic setting in which the learner has large quantities of labeled data from some source task but limited or no labeled data from the target task it is attempting to learn. In this work, we give the first formal analysis showing that using active learning for domain adaptation yields a way to address the statistical challenges inherent in this setting. We propose a novel nonparametric algorithm, ANDA, that combines an active nearest neighbor querying strategy with nearest neighbor prediction. We provide analyses of its querying behavior and of finite sample convergence rates of the resulting classifier under covariate shift. Our experiments show that ANDA successfully corrects for dataset bias in multi-class image categorization.", "recorded": "2015-07-07T13:44:45", "title": "Active Nearest Neighbors in Changing Environments"}, {"url": "nipsworkshops2012_netrapalli_minimization", "desc": "Alternating minimization has emerged as a popular heuristic for large-scale machine learning problems involving low-rank matrices. However, there have been few (if any) theoretical guarantees on its performance. In this work, we investigate the natural alternating minimization algorithm for the popular matrix sensing problem first formulated in [RFP07]; this problem asks for the recovery of an unknown low-rank matrix from a small number of linear measurements thereof. We show that under suitable RIP conditions, alternating minimization linearly converges to the true matrix. Our result can be extended to matrix completion from randomly sampled entries. Our analysis uses only elementary linear algebra and\r\nexploits the fact that, under RIP, alternating minimization can be viewed as a noisy version of orthogonal iteration (which is used to compute the top singular vectors of a matrix).", "recorded": "2012-12-08T17:35:00", "title": "Provable Matrix Completion using Alternating Minimization"}, {"url": "simbad2011_gonen_carcinoma", "desc": "In kernel-based machine learning algorithms, we can learn a combination of different kernel functions in order to obtain a similarity measure that better matches the underlying problem instead of using a single fixed kernel function. This approach is called multiple kernel learning (MKL). In this paper, we formulate a nonlinear MKL variant and apply it for nuclei classification in tissue microarray images of renal cell carcinoma (RCC). The proposed variant is tested on several feature representations extracted from the automatically segmented nuclei. We compare our results with single-kernel support vector machines trained on each feature representation separately and three linear MKL algorithms from the literature. We demonstrate that our variant obtains more accurate classifiers than competing algorithms for RCC detection by combining information from different feature representations nonlinearly.", "recorded": "2011-09-28T16:00:00", "title": "Combining Data Sources Nonlinearly for Cell Nucleus Classification of Renal Cell Carcinoma"}, {"url": "mloss08_hill_bcpy", "desc": "BCPy2000\r\nJeremy Hill, Max-Planck-Institute for Biological Cybernetics, T\u00a8 ubingen, Germany\r\nBCPy2000 provides a platform for rapid, \ufb02exible development of experimental Brain-Computer Interface\r\nsystems based on the BCI2000.org project. From the developer\u2019s point of view, the implementation is\r\ncarried out in Python, taking advantage of various high-level packages: VisionEgg for stimulus presentation,\r\nNumPy and SciPy for signal processing and classi\ufb01cation, and IPython for interactive debugging. BCPy2000\r\nimplements a lot of infrastructure allowing you to get new experiments up and running quickly. It also\r\ncontains a set of optional tools, which are still a work in progress but which are rapidly turning into a\r\nkind of \u201cstandard library\u201d of object-oriented signal-processing and stimulus widgets. These features make\r\nit a \ufb02exible platform for developers of new NumPy/SciPy-based machine-learning algorithms in the \ufb01eld of\r\nrealtime biosignal analysis.", "recorded": "2008-12-12T17:05:00", "title": "BCPy2000"}, {"url": "snnsymposium2010_nijmegen", "desc": "Can machines think? This has been a conundrum for philosophers for years, but the answer to this question also has real social importance. Modern robots can assist us in our homes and have human-like qualities. The internet provides us with personalized tools that learn from our behavior. It is therefore of more than academic importance that we learn about the actual cognitive powers of computers, and what we can expect of them in the future.\r\n\r\nOn November 17, SNN organized a one-day symposium in Nijmegen, entitled \"Intelligent Machines\". Invited research leaders in the field of machine learning and robotics presented examples of intelligence in computers and discussed their views for the future. \r\n\r\nThe purpose of this meeting is to provide a platform for discussion and interaction between the academic and industrial research and development communities in the Netherlands. \r\n\r\nMore about the symposium at http://www.snn.ru.nl/symposium-2010/index.html.", "recorded": "2010-11-17T09:00:00", "title": "SNN Symposium: Intelligent Machines, Nijmegen 2010"}, {"url": "icml09_precup_ws", "desc": "In the last 25 years, reinforcement learning research has made great strides and has had a significant impact within several fields, including\r\n\r\n    * artificial intelligence\r\n    * optimal control\r\n    * neuroscience\r\n    * psychology\r\n    * economics\r\n    * operations research\r\n\r\nThese are diverse areas, with different goals and different evaluation criteria. It is striking that reinforcement learning ideas are playing new roles in all of them.  The purposes of this meeting are \r\n\r\n    * to recognize and assess this confluence of fields\r\n    * to celebrate the diversity of reinforcement learning research\r\n    * to exchange information among the fields\r\n\r\nMSRL consisted of a series of invited talks and an evening poster session. Participation in the poster session was based on extended abstracts.\r\n\r\nMSRL was co-located with a cluster of other meetings, including the International Conference on Machine Learning, the Conference on Uncertainty in Artificial Intelligence, and the Conference on Learning Theory.", "recorded": "2009-06-18T13:30:00", "title": "Welcome to the Multidisciplinary Symposium on Reinforcement Learning"}, {"url": "aistats2010_yan_mae", "desc": "Supervised learning from multiple labeling sources is an increasingly important problem in machine learning and data mining. This paper develops a probabilistic approach to this problem when annotators may be unreliable (labels are noisy), but also their expertise varies depending on the data they observe (annotators may have knowledge about different parts of the input space). That is, an annotator may not be consistently accurate (or inaccurate) across the task domain. The presented approach produces classification and annotator models that allow us to provide estimates of the true labels and annotator variable expertise. We provide an analysis of the proposed model under various scenarios and show experimentally that annotator expertise can indeed vary in real tasks and that the presented approach provides clear advantages over previously introduced multi-annotator methods, which only consider general annotator characteristics. ", "recorded": "2010-05-15T17:57:00", "title": "Modeling annotator expertise: Learning when everybody knows a bit of something"}, {"url": "icml09_taylor_fcr", "desc": "The Conditional Restricted Boltzmann Machine (CRBM) is a recently proposed model for time series that has a rich, distributed hidden state and permits simple, exact inference. We present a new model, based on the CRBM that preserves its most important computational properties and includes multiplicative three-way interactions that allow the effective interaction weight between two units to be modulated by the dynamic state of a third unit. We factorize the three-way weight tensor implied by the multiplicative model, reducing the number of parameters from O(N^3) to O(N^2). The result is an efficient, compact model whose effectiveness we demonstrate by modeling human motion. Like the CRBM, our model can capture diverse styles of motion with a single set of parameters, and the three-way interactions greatly improve the model's ability to blend motion styles or to transition smoothly between them.", "recorded": "2009-06-17T15:20:00", "title": "Factored Conditional Restricted Boltzmann Machines for Modeling Motion Style"}, {"url": "bbci2012_cohen_feature_space", "desc": "A wide variety of brain-derived signals presently are available to drive brain computer interface devices. These include the popular EEG recordings, magnetoencephalography, functional magnetic resonance imaging (fMRI), near-infrared spectroscopy and others. Each are known to be quantitatively altered by intentional mental activity and, with the power provided by statistical machine learning, each to varying degrees may be decoded to the end of controlling devices. I will speak to the question of the reverse challenge of understanding better the operations of the brain through analysis of the control signals and argue that careful selection of the features themselves might serve the dual purposes of improving the efficiency and accuracy of the brain-computer interface, and serve to improve our understanding of the underlying neurophysiology. The discussion will focus on the use of brain network features exposed through fMRI and on understanding the temporal dynamics of the individual features and their state transitions.", "recorded": "2012-09-19T16:00:00", "title": "Informative Brain-Mind Feature Space"}, {"url": "um05_edinburgh", "desc": "In building effective interfaces or computer human interaction devices, a main limitation of traditional presentation design is the inability to meet individual user expectation at run-time. On-line design of individualised presentations that surpass the limits of the \u201done size fits all\u201d approaches can be made possible by user modeling techniques. Building models of users can be done throughMachine Learning, but this requires techniques that are specific to the task.One particular issue is that the user models cannot remain static, in the sense that during the use of the intended interaction more knowledge can be gathered which should in turn be used to improve the user models. The knowledge from which the construction of the models can be made is many-fold: web logs, speech, images...\r\n\r\nFor more information visit the [[http://www-connex.lip6.fr/~artieres/UM2005/index.html|workshop website]].", "recorded": "2005-07-24T00:00:00", "title": "Workshop on Machine Learning for User Modeling: Challenges, Edinburgh 2005"}, {"url": "colt2014_barcelona", "desc": "The conference strongly supports a broad definition of learning theory, including, but not limited to:\r\n\r\n\u2022\tDesign and analysis of learning algorithms and their generalization ability\\\\\r\n\u2022\tComputational complexity of learning\\\\\r\n\u2022\tOptimization procedures for learning\\\\\r\n\u2022\tUnsupervised, semi-supervised learning, and clustering\\\\\r\n\u2022\tOnline learning\\\\\r\n\u2022\tInteractive learning\\\\\r\n\u2022\tKernel Methods\\\\\r\n\u2022\tHigh dimensional and non-parametric empirical inference, including sparsity methods\\\\\r\n\u2022\tPlanning and control, including reinforcement learning\\\\\r\n\u2022\tLearning with additional constraints: E.g. privacy, time or memory budget, communication\\\\\r\n\u2022\tLearning in other settings: E.g. social, economic, and game-theoretic\\\\\r\n\u2022\tAnalysis of learning in related fields: natural language processing, neuroscience, bioinformatics, privacy and security, machine vision, data mining, information retrieval.\r\n\r\nAdditional information can be found at [[http://orfe.princeton.edu/conferences/colt2014/|COLT 2014 home page]].", "recorded": "2014-06-13T09:00:00", "title": "27th Annual Conference on Learning Theory (COLT), Barcelona 2014"}, {"url": "mlss09us_koltchinskii_berml", "desc": "We will discuss a general approach to the problem of bounding the excess risk of learning algorithms based on empirical risk minimization (possibly penalized). This approach has been developed in the recent years by several authors (among others: Massart; Bartlett, Bousquet and Mendelson; Koltchinskii). It is based on powerful concentration inequalities due to Talagrand as well as on a variety of tools of empirical processes theory (comparison inequalities, entropy and generic chaining bounds on Gaussian, empirical and Rademacher processes, etc.). It provides a way to obtain sharp excess risk bounds in a number of problems such as regression, density estimation and classification and for many different classes of learning methods (kernel machines, ensemble methods, sparse recovery). It also provides a general way to construct sharp data dependent bounds on excess risk that can be used in model selection and adaptation problems.", "recorded": "2009-06-06T08:30:00", "title": "Bounding Excess Risk in Machine Learning"}, {"url": "colt2013_slivkins_problem", "desc": "Very recently crowdsourcing has become the de facto platform for distributing and collecting human computation for a wide range of tasks and applications such as information retrieval, natural language processing and machine learning. Current crowdsourcing platforms have some limitations in the area of quality control. Most of the effort to ensure good quality has to be done by the experimenter who has to manage the number of workers needed to reach good results.We propose a simple model for adaptive quality control in crowdsourced multiple-choice tasks which we call the \u201cbandit survey problem\u201d. This model is related to, but technically different from the well-known multi-armed bandit problem. We present several algorithms for this problem, and support them with analysis and simulations.Our approach is based in our experience conducting relevance evaluation for a large commercial search engine.", "recorded": "2013-06-14T17:10:00", "title": "Adaptive Crowdsourcing Algorithms for the Bandit Survey Problem "}, {"url": "eswc2011_backstrom_facebook", "desc": "Facebook has undergone tremendous growth in the last five years. Here we will start by looking at some basic statistics and trends that have accompanied this growth. We'll then dive into two different topics. First, we will look at a general trend to make data more structured at Facebook. Having more structured data makes it easier to manage, understand, and leverage it. I will briefly discuss the tools (Hive) that have been built to enable the massive-scale data analysis that goes on at Facebook on a daily basis. In the second part of the talk, I will dive into the details of one of the systems that has contributed to the growth of Facebook: People You May Know. This system generates a significant number of the friend connections on Facebook, and by using increasingly sophisticated machine learning techniques, we have been able to make large improvements to the ranking used by the system since its original launch.", "recorded": "2011-05-31T14:00:00", "title": "Dealing with structured and unstructured data at Facebook"}, {"url": "icml2015_sohl_dickstein_deep_unsupervised_learning", "desc": "A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.", "recorded": "2015-07-08T14:46:53", "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"}, {"url": "mit901f03_schneider_lec26", "desc": "1. What is the difference between habituation and adaptation?\n\n2. What allows you to sit in a chair for long periods of time without feeling constant pressure on the backs of your legs? (What system is responsible for this ability?)\n\n3. Describe examples of habituation in different sensory modalities: vision, audition, somatic sensation, olfaction. Do you think the process in these different modalities can be explained the same way?\n\n4. A novel stimulus, in any modality, causes a transient arousal in what system?\n\n5. What are some of the multiple components of the orienting response to novel stimulation?\n\n6. Give a specific example of an orienting response and why it is important.\n\n7. What does the lie detector measure? What system is the response caused by? Is such a machine infallible? In what sense? How is a lie similar to detection of stimulus novelty?\n\n8. What is \u201cstimulus generalization\u201d in studies of the phenomena of habituation?", "recorded": "2003-11-05T09:00:00", "title": "Lecture 26: Visual System 1: Anatomy, Ablations"}, {"url": "solomon_zenko_lpcr", "desc": "Predictive clustering is based on ideas from two machine learning subareas, \r predictive modeling and clustering. Methods for predictive clustering enable \r us to construct models for predicting multiple target variables, which are \r normally simpler and more comprehensible than the corresponding collection \r of models, each predicting a single variable. To this end, predictive clustering \r has been restricted to decision tree methods. Our goal is to extend this approach \r to methods for learning rules. We have developed a generalized version of \r the covering algorithm that enables learning of ordered or unordered rules, \r on single or multiple target classification or regression domains. Performance \r of the new method compares favorably to existing methods. Comparison of \r single target and multiple target prediction models shows that multiple target \r models offer comparable performance and drastically lower complexity than \r the corresponding collections of single target models.", "recorded": "2007-02-20T13:00:00", "title": "Learning predictive clustering rules"}, {"url": "clsp_hirschberg_deceptive", "desc": "This talk will discuss production and perception studies of deceptive speech and the acoustic/prosodic and lexical cues associated with deception. Experiments in which we collected a large corpus of deceptive and non-deceptive speech from naive subjects in the laboratory are described, together with perception experiments of this corpus. Features extracted from this corpus have been used in Machine Learning experiments to predict deception with classification accuracy from 64.0- 66.4%, depending upon feature-set and learning algorithm. This performance compares favorably with the performance of human judges on the same data and task, which averaged 58.2%. We also discuss current findings on the role of personality factors in deception detection, speaker-dependent models of deception, and future research. This work was done in collaboration with Frank Enos, Columbia University;Elizabeth Shriberg, Andreas Stolcke, and Martin Graciarena, SRI/ICSI; Stefan Benus, Brown University; and more.", "recorded": "2007-10-02T15:20:33", "title": "Detecting Deceptive Speech"}, {"url": "roks2013_bach_optimization", "desc": "Many machine learning and signal processing problems are traditionally cast as convex optimization problems. A common \u000edifficulty in solving these problems is the size of the data, where there are many observations (\"large n\") and each of these is large (\"large p\"). In this setting, online algorithms which pass over the data only once, are usually preferred over batch algorithms, which require multiple passes over the data. In this talk, I will present several recent results, showing that in the ideal in\ffinite-data setting, online learning algorithms based on stochastic approximation should be preferred, but that in the practical \ffinite-data setting, an appropriate combination of batch and online algorithms leads to unexpected behaviors, such as a linear convergence rate with an iteration cost similar to stochastic gradient descent. (joint work with Nicolas Le Roux, Eric Moulines and Mark Schmidt)", "recorded": "2013-07-09T00:09:00", "title": "Beyond Stochastic Gradient Descent"}, {"url": "nipsworkshops2010_milanfar_tmi", "desc": "Recent developments in computational imaging and restoration\r\nhave heralded the arrival and convergence of several powerful\r\nmethods for adaptive processing of multidimensional data.\r\nExamples include Moving Least Square (from Graphics), the\r\nBilateral Filter and Anisotropic Diffusion (from Vision), Boosting\r\nand Spectral Methods (from Machine Learning), Non-local\r\nMeans (from Signal Processing), Bregman Iterations (from\r\nApplied Math), Kernel Regression and Iterative Scaling (from\r\nStatistics). While these approaches found their inspirations in\r\ndiverse fields of nascence, they are deeply connected. In this talk,\r\nI will present a practical and unified framework for understanding\r\nsome common underpinnings of these methods. This leads to\r\nnew insights and a broad understanding of how these diverse\r\nmethods interrelate. I will also discuss several applications, and\r\nthe statistical performance of the resulting algorithms. Finally\r\nI briefly illustrate connections between these techniques and\r\nclassical Bayesian approaches.", "recorded": "2010-12-11T07:30:00", "title": "A Tour of Modern \"Image Processing\""}, {"url": "clsp_pereira_linear", "desc": "Over the last decade, linear models have become the standard machine learning approach for supervised classification, ranking, and structured prediction natural language processing. They can handle very high-dimensional problem representations, they are easy to set up and use, and they extend naturally to complex structured problems. But there is something unsatisfying in this work. The geometric intuitions behind linear models were developed with low-dimensional, continuous problems, while natural language problems involve very high dimension, discrete representations with long tailed distributions. Do the orignal intuitions carry over? In particular, do standard regularization methods make any sense for language problems? I will give recent experimental evidence that there is much to do in making linear model learning more suited to the statistics of language.", "recorded": "2008-11-25T16:26:19", "title": "Are Linear Models Right for Language?"}, {"url": "mitworld_brooks_rse", "desc": "As eager as he is to invent robots that can travel to a moon of Saturn or Jupiter, and function autonomously in these hostile environments, Rodney Brooks would love a shot to explore space himself. \u201cI made an offer to Jeff Bezos, Larry Page and Sergei Brin that if they would fund a one-way mission to Mars, I\u2019d go on it,\u201d says Brooks. But he knows that robots are cheaper to send than us, \u201cbig bags of skin with biological processes requiring replenishment of all sorts.\u201d Under the Bush Administration, NASA first laid out an ambitious program in robotic technology, involving sending machines to reconnoiter the moon and Mars and prepare habitation sites for humans. \u201cRobots would dig channels, then lower habitation modules into them, and when people come, they\u2019d live like moles underground,\u201d says Brooks. But why send people at all if these robots can accomplish so much? It turns out that there\u2019s a dangerously long lag time between sending a command to a robot and having the machine perform a function. Ultimately, human senses and timing will be needed on site.\n\nBut now NASA\u2019s grand robotic research plans are on hold, says Brooks, blocked by the difficulties and enormous expense of designing a new launch vehicle. The future of sophisticated robotic work seems earthbound, says Brooks. First, there are military innovations -- Congress has mandated that by 2015, 1/3rd of all US military missions should be unmanned. Also, the oil industry is pushing for machine-based solutions to such gritty problems as deep-ocean drilling and oil-well maintenance. And don\u2019t forget the new billionaire space cowboys, who dream of mining platinum fields on asteroids (for fuel cells on earth), or building space tourism businesses. But, Brooks reminds us, we have a way to go: After 40 years of research, \u201cthe generic object recognition that a two-year-old child could do, we can\u2019t do with our robots.\u201d", "recorded": "2006-01-10T11:30:00", "title": "Robotics in Space Exploration"}, {"url": "licsb2010_manolakos_mlm", "desc": "Two-dimensional gel electrophoresis (2DGE) remains the most widely used method for proteins identification and differential expression analysis, due to its lower cost and the existence of mature commercial software tools for 2DGE image analysis, despite the fact that non-gel based methods are gaining in popularity. Although there are several software packages that promise automation of the whole protein spot detection and quantification process, the hard reality remains today [1]  that as Fey and Larsen stated in 2001, \"There is no program that is remotely automatic when presented with complex 2-DE images\" ... \"most programs require often more than a day of user hands-on time to edit the image before it can be fully entered into the database\u201a\" [2]. \r\n\r\nTo address these limitations and develop an automated 2DGE image analysis workflow we have developed in previous works an effective image analysis methodology that first denoises the 2DGE  image  based on the Controurlet transform [3] and then  separates effectively the parts of the denoised image which include true protein spots (to be called Regions of Interest (ROIs) from  the background-only areas, by using Active Contours (AC) without edges [4]. In this work we complete the image analysis workflow by adding a well tuned pipeline of operations based on unsupervised machine learning methods for analyzing further each isolated ROI, in order to \"fish\" in it the centers and estimate the quantities of the individual  \"hidden\" spots.One-dimensional mixture modeling of the ROI pixel intensities histogram is applied first to identify and remove any remaining background pixels. Then the surviving ROI pixels are used as \"molecules generators\", in order to convert (by random sampling) the processed ROI image to an isomorphic dataset (through appropriate random sampling) representing the distribution of molecules of the underlying protein species (that are \"projected\" as spots on the gel image). This reverse engineering action rooted on machine learning constitutes a unique innovation of this work that, to the best of our knowledge, has not been applied before in 2DGE image analysis. The candidate protein spot centers are then located by applying hierarchical clustering. Finally the individual spot boundaries are delineated by fitting 2D Gaussian models to the data using generalized mixture modeling and the Minimum Message Length (MML) criterion to control the best model complexity. An extensive evaluation of this novel spot modeling methodology using both real and synthetic 2DGE images reveals that it is more precise and more specific than PDQuest in terms of spot detection while both methods achieve comparable high sensitivity. Furthermore, it can estimate more reliably the volumes of the extracted spots, even in the presence of substantial noise and in areas of the image where faint and overlapping (or saturated) spots are located close to each other. It should be noted that the end-to-end workflow that we have developed for 2DGE image analysis does not require any re-calibration of parameters every time a new gel image is presented for analysis. This desirable characteristic makes it a suitable candidate for the automatic processing of image stacks, as needed for highthroughput proteomics analysis to support systems biology projects.", "recorded": "2010-03-31T12:30:00", "title": "Machine learning methods for effective proteomics image analysis"}, {"url": "nipsworkshops09_language_learning", "desc": "**Grammar Induction, Representation of Language and Language Learning**\r\n\r\nNow is the time to revisit some of the fundamental grammar/language learning tasks such as grammar acquisition, language acquisition, language change, and the general problem of automatically inferring generic representations of language structure in a data driven manner. Though the underlying problems have been known to be computationally intractable for the standard representations of the Chomsky hierarchy, such as regular grammars and context free grammars, progress has been made by modifying or restricting these classes to make them more observable. Generalisations of distributional learning have shown promise in unsupervised learning of linguistic structure using tree based representations, or using non-parametric approaches to inference. More radically, significant advances in this domain have been made by switching to different representations such as the work in Clark, Eyrand & Habrard (2008) that addresses the issue of language acquisition, but has the potential to cross-fertilise a wide range of problems that require data driven representations of language. Such approaches are starting to make inroads into one of the fundamental problems of cognitive science: that of learning complex representations that encode meaning. This adds a further motivation for returning to this topic at this point. Grammar induction was the subject of an intense study in the early days of Computational Learning Theory, with the theory of query learning largely developing out of this research. More recently the study of new methods of representing language and grammars through complex kernels and probabilistic modelling together with algorithms such as structured output learning has enabled machine learning methods to be applied successfully to a range of language related tasks from simple topic classification through parts of speech tagging to statistical machine translation. These methods typically rely on more fluid structures than those derived from formal grammars and yet are able to compete favourably with classical grammatical approaches that require significant input from domain experts, often in the form of annotated data.\r\n----\r\nThe Workshop homepage can be found at http://www.cs.ucl.ac.uk/staff/rmartin/grll09/\r\n----", "recorded": "2009-12-11T07:30:00", "title": "Language Learning"}, {"url": "pmsb06_tuusula", "desc": "**Motivation**\r\n\r\nThe ever-ongoing growth in the amount of biological data, the development of genome-wide measurement technologies, and the shift from the study of individual genes to systems view all contribute to the need to develop computational techniques for learning models from data. At the same time, the increase in available computational resources has enabled new, more realistic modeling methods to be adopted.\r\n\r\nIn bioinformatics, most of the targets of interest deal with complex structured objects: sequences, 2D and 3D structures or interaction networks. In many cases these structures are naturally described by probabilistic graphical models, such as Hidden Markov Models, Conditional Random Fields or Bayesian Networks. Recently, approaches that combine Support Vector Machines and probabilistic models have been introduced (Fisher kernels, Max-margin Markov Networks, Structured SVM). These techniques benefit from efficient convex optimization approaches and thus are potentially well-scalable to large problems in bioinformatics.\r\n\r\nThe increasing amount of high-throughput experimental data begins to enable the use of these advanced modelling methods in bioinformatics and systems biology. At the same time new computational challenges emerge. Statistical methods are required to process the data so that underlying potentially complex statistical patterns can be discerned from spurious patterns created by random effects. At its simplest this problem calls for data normalization and statistical hypothesis testing, in the more general case, one is required to select a model (e.g. gene network) that best explains the data.\r\n\r\n**Objective**\r\n\r\nThe aim of this workshop is to provide a broad look at the state of the art in the probabilistic modeling and machine learning methods involving biological structures and systems, and to bring together method developers and experimentalists working with the problems.\r\n\r\nWe encourage submissions bringing forward methods for discovering complex structures (e.g. interaction networks, molecule/cellular structures) and methods supporting genome-wide data analysis. \r\n\r\nFind out more at the [[http://www.cs.helsinki.fi/group/bioinfo/events/pmsb06/|workshop website]].", "recorded": "2006-06-17T00:00:00", "title": "Workshop on Probabilistic Modeling and Machine Learning in Structural and Systems Biology (PMSB), Tuusula 2006"}, {"url": "ecmlpkdd09_do_fwumrbebosvm", "desc": "The Support Vector Machine error bound is a function of the margin and radius. Standard SVM algorithms maximize the margin within a given feature space, therefore the radius is fixed and thus ignored in the optimization. We propose an extension of the standard SVM optimization in which we also account for the radius in order to produce an even tighter error bound than what we get by controlling only for the margin. We use a second set of parameters, ${\\vect \\mu}$, that control the radius introducing like that an explicit feature weighting mechanism in the SVM algorithm. We impose an $l_1$ constraint on ${\\vect \\mu}$ which results in a sparse vector, thus performing feature selection. Our original formulation is not convex, we give a convex approximation and show how to solve it. We experiment with real world datasets and report very good predictive performance compared to standard SVM.", "recorded": "2009-09-09T13:45:00", "title": "Feature Weighting Using Margin and Radius Based Error Bound Optimization in SVMs"}, {"url": "colt2011_rudin_prediction", "desc": "We consider a supervised learning problem in which data are revealed sequentially and the\r\ngoal is to determine what will next be revealed. In the context of this problem, algorithms\r\nbased on association rules have a distinct advantage over classical statistical and machine\r\nlearning methods; however, there has not previously been a theoretical foundation estab-\r\nlished for using association rules in supervised learning. We present two simple algorithms\r\nthat incorporate association rules, and provide generalization guarantees on these algo-\r\nrithms based on algorithmic stability analysis from statistical learning theory. We include\r\na discussion of the strict minimum support threshold often used in association rule mining,\r\nand introduce an \\adjusted con\fdence\" measure that provides a weaker minimum support\r\ncondition that has advantages over the strict minimum support. The paper brings together\r\nideas from statistical learning theory, association rule mining and Bayesian analysis.", "recorded": "2011-07-09T09:50:00", "title": "Sequential Event Prediction with Association Rules"}, {"url": "mlss09us_nowak_castro_tmaal", "desc": "Traditional approaches to machine learning and statistical inference are passive, in the sense that all data are collected prior to analysis in a non-adaptive fashion. One can envision, however more active strategies in which information gleaned from previously collected data is used to guide the selection of new data. This talk discusses the emerging theory of such \"active learning\" methods. I will show that feedback between data analysis and data collection can be crucial for effective learning and inference. The talk will describe two active learning problems. First, I will consider binary-valued prediction (classification) problems, for which the prediction errors of passive learning methods can be exponentially larger than those of active learning. Second, I will discuss the role of active learning in the recovery of sparse vectors in noise. I will show that certain weak, sparse patterns are imperceptible from passive measurements, but can be recovered perfectly using selective sensing.", "recorded": "2009-06-04T08:30:00", "title": "Theory, Methods and Applications of Active Learning"}, {"url": "icml08_ye_tsvm", "desc": "Similarity matrices generated from many applications may not be positive semidefinite, and hence can't fit into the kernel machine framework. In this paper, we study the problem of training support vector machines with an indefinite kernel. We consider a regularized SVM formulation, in which the indefinite kernel matrix is treated as a noisy observation of some unknown positive semidefinite one (proxy kernel) and the support vectors and the proxy kernel can be computed simultaneously. We propose a semi-infinite quadratically constrained linear program formulation for the optimization, which can be solved iteratively to find a global optimum solution. We further propose to employ an additional pruning strategy, which significantly improves the efficiency of the algorithm, while retaining the convergence property of the algorithm. In addition, we show the close relationship between the proposed formulation and multiple kernel learning. Experiments on a collection of benchmark data sets demonstrate the efficiency and effectiveness of the proposed algorithm.", "recorded": "2008-07-07T10:50:00", "title": "Training SVM with Indefinite Kernels"}, {"url": "abi07_whistler", "desc": "Deterministic (variational) techniques are used all over Machine Learning to approximate Bayesian inference for continuous- and hybrid-variable problems. In contrast to discrete variable approximations, surprisingly little is known about convergence, quality of approximation, numerical stability, specific biases, and differential strengths and weaknesses of known methods. \n\nIn this workshop, we aim to highlight important problems and to gather ideas of how to address them. The target audience are practitioners, providing insight into and analysis of problems with certain methods or comparative studies of several methods, as well as theoreticians interested in characterizing the hardness of continuous distributions or proving relevant properties of an established method. We especially welcome contributions from Statistics (Markov Chain Monte Carlo), Information Geometry, Optimal Filtering, or other related fields if they make an effort of bridging the gap towards variational techniques.", "recorded": "2007-12-07T07:30:00", "title": "NIPS Workshop on Approximate Bayesian Inference in Continuous/Hybrid Models, Whistler 2007"}, {"url": "nc04_eindhoven", "desc": "The theoretical analysis of systems that learn from data has been an important topic of study in statistics, machine learning, and information theory. In all these paradigms, distinct methods have been developed to deal with inference when the models under consideration can be arbitrarily large. Recently, there has been a fruitful cross-fertilization of ideas and proof techniques. To give but one example, very recently, minimax optimal convergence rates of the information-theoretic MDL method were proved using ideas from the - computational - PAC-Bayesian paradigm and - statistical - empirical process techniques. The goal of this workshop is to bring together leading theoreticians to allow them to debate, compare and cross-fertilise ideas from these distinct inductive principles. At the workshop, we will establish a PASCAL special interest group for `merging computational and information-theoretic learning with statistics'.", "recorded": "2004-10-07T00:00:00", "title": "Notions of Complexity: Information-theoretic, Computational and Statistical Approaches Workshop, Eindhoven 2004"}, {"url": "wims2011_auer_interlinked", "desc": "Over the past 4 years, the semantic web activity has gained momentum with the widespread publishing of structured data as RDF. The Linked Data paradigm has therefore evolved from a practical research idea into a very promising candidate for addressing one of the biggest challenges in the area of the Semantic Web vision: the exploitation of the Web as a platform for data and information integration.\r\nTo translate this initial success into a world-scale reality, a number of research challenges need to be addressed: the performance gap between relational and RDF data management has to be closed, coherence and quality of data published on the Web have to be improved, provenance and trust on the Linked Data Web must be established and generally the entrance barrier for data publishers and users has to be lowered. This talk will discuss approaches for tackling these challenges and their integration into a mutual refinement cycle - the \"linked data washing machine\".", "recorded": "2011-05-26T10:30:00", "title": "Creating Knowledge Out of Interlinked Data"}, {"url": "acml2013_lin_large_linear_classification", "desc": "Linear classification is a useful tool in machine learning and data mining. For some data in a rich dimensional space, the prediction performance of linear classifiers has shown to be close to that of nonlinear classifiers such as kernel methods, but training and testing speed is much faster. Recently, many research works have proposed efficient optimization methods to construct linear classifiers. We briefly discuss some of them that were considered in our development of the software LIBLINEAR. We then move to discuss some extensions of linear classification. In particular, linear classifiers can be useful to either directly or indirectly approximate kernel classifiers. I will show some real-word examples for which we try to achieve fast training/testing speed, while maintain competitive accuracy. Finally, future challenges of this research topic, in particular, aspects on big-data linear classification, will be discussed.", "recorded": "2013-11-14T14:00:00", "title": "Recent Advances in Large Linear Classification"}, {"url": "eswc2013_lorey_data", "desc": "Publicly available Linked Data repositories provide a multitude of information. By utilizing Sparql, Web sites and services can consume this data and present it in a user-friendly form, e.g., in mash-ups. To gather RDF triples for this task, machine agents typically issue similarly structured queries with recurring patterns against the Sparql endpoint. These queries usually differ only in a small number of individual triple pattern parts, such as resource labels or literals in objects. We present an approach to detect such recurring patterns in queries and introduce the notion of query templates, which represent clusters of similar queries exhibiting these recurrences. We describe a matching algorithm to extract query templates and illustrate the benefits of prefetching data by utilizing these templates. Finally, we comment on the applicability of our approach using results from real-world Sparql query logs.", "recorded": "2013-05-28T14:30:00", "title": "Detecting SPARQL Query Templates for Data Prefetching"}, {"url": "prib2010_orsenigo_tsge", "desc": "Machine learning methods have been successfully applied to the phenotype classification of many diseases based on static gene  expression measurements. More recently microarray data have been collected over time, making available datasets composed by time series of expression gene profiles. In this paper we propose a new method for time series classification, based on a temporal extension of L  1-norm support vector machines, that uses dynamic time warping distance for measuring time series similarity. This results in a mixed-integer optimization model which is solved by a sequential approximation algorithm. Computational tests performed on two benchmark datasets indicate the effectiveness of the proposed method compared to other techniques, and the general usefulness of the approaches based on dynamic time warping for labeling time series gene expression data. ", "recorded": "2010-09-22T16:30:00", "title": "Time Series Gene Expression Data Classification via L1-norm Temporal SVM"}, {"url": "iswc2014_lecue_semantic_traffic_diagnosis", "desc": "IBM STAR-CITY is a system supporting Semantic road Traffic Analytics and Reasoning for CITY. The system has ben designed (i) to provide insight on historical and real-time traffic conditions, and (ii) to support efficient urban planning by integrating (human and machine-based) sensor data using variety of formats, velocities and volumes. Initially deployed and experimented in Dublin City (Ireland), the system and its architecture have been strongly limited\r\nby its flexibility and scalability to other cities. This paper describes its limitations and presents the \u201cany-city\u201d architecture of STAR-CITY together with its semantic configuration for flexible and scalable deployment in any city. This paper also strongly focuses on lessons learnt from its deployment and experimentation in Dublin (Ireland), Bologna (Italy), Miami (USA) and Rio (Brazil).\r\n", "recorded": "2014-10-21T15:00:00", "title": "Semantic Traffic Diagnosis with STAR-CITY: Architecture and Lessons Learned from Deployment in Dublin, Bologna, Miami and Rio"}, {"url": "mlss06au_tresp_dpnbm", "desc": "Bayesian modeling is a principled approach to updating the degree of belief in a hypothesis given prior knowledge and given available evidence. Both prior knowledge and evidence are combined using Bayes' rule to obtain the a posterior hypothesis. In most cases of interest to machine learning, the prior knowledge is formulated as a prior distribution over parameters and the evidence corresponds to the observed data. By applying Bayes' formula we can perform inference about new data. Having observed sufficient data, the a posteriori parameter distribution is increasingly concentrated and the influence of the prior distribution diminishes. Under some assumptions (in particular that the likelihood model is correct and that the true parameters have positive a priori probability), the a posteriori distribution converges to a point distribution located at the true parameters. The challenges in Bayesian modeling are, first, to find suitable application specific statistical models and, second, to (approximately) solve the resulting inference equations.", "recorded": "2006-02-16T00:00:00", "title": "Dirichlet Processes and Nonparametric Bayesian Modelling"}, {"url": "wsdm2010_moon_irir", "desc": "Ranking a set of retrieved documents according to their relevance to a given query has become a popular problem at the intersection of web search, machine learning, and information retrieval. Recent work on ranking focused on a number of different paradigms, namely, pointwise, pairwise, and list-wise approaches. Each of those paradigms focuses on a different aspect of the dataset while largely ignoring others. The current paper shows how a combination of them can lead to improved ranking performance and, moreover, how it can be implemented in log-linear time.\r\n\r\nThe basic idea of the algorithm is to use isotonic regression with adaptive bandwidth selection per relevance grade. This results in an implicitly-defined loss function which can be minimized efficiently by a subgradient descent procedure. Experimental results show that the resulting algorithm is competitive on both commercial search engine data and publicly available LETOR data sets.", "recorded": "2010-02-05T10:53:58", "title": "IntervalRank - Isotonic Regression with Listwise and Pairwise Constraints"}, {"url": "icwsm2012_pal_evolution", "desc": "Community Question Answering (CQA) services thrive as a result of a small number of highly active users, typically called experts, who provide a large number of high quality useful answers. Understanding the temporal dynamics and interactions between experts can present key insights into how community members evolve over time. In this paper, we present a temporal study of experts in CQA and analyze the changes in their behavioral patterns over time. Further, using unsupervised machine learning methods, we show the interesting evolution patterns that can help us distinguish experts from one another. Using supervised classification methods, we show that the models based on evolutionary data of users can be more effective at expert identification than the models that ignore evolution. We run our experiments on two large online CQA to show the generality of our proposed approach.", "recorded": "2012-06-05T15:30:00", "title": "Evolution of Experts in Question Answering Communities"}, {"url": "w3cworkshop2013_schmitz_web", "desc": "This presentation addresses the current status of ongoing projects under that responsibility of the Publications Office of the European Union (Publications Office) that contribute to the Semantic web: (a) CELLAR, a repository exposing metadata about official EU information as Linked Open Data that the Publications Office is preparing to open to the public with data loading ongoing since June 2012. (b) The Open Data Portal, (technical implementation directed by the Publications Office (a beta version of which has been available on the web since mid-December 2012), which provides metadata (information about datasets) and several data sets as Linked Open Data. c) Standardisation The Publications Office has contributed the definition of the European Legislation Identifier (ELI). ELI is based on machine-readable URI templates. The Publications Office provides multilingual controlled vocabularies for re-use.", "recorded": "2013-03-13T09:33:00", "title": "Public Linked Open Data - the Publications Office's Contribution to the Semantic Web"}, {"url": "ecmlpkdd09_wachman_kptsaa", "desc": "We present a method for applying machine learning algorithms to the automatic classification of astronomy star surveys using time series of star brightness. Currently such classification requires a large amount of domain expert time. We show that a combination of phase invariant similarity and explicit features extracted from the time series provide domain expert level classification. To facilitate this application, we investigate the cross-correlation as a general phase invariant similarity function for time series. We establish several theoretical properties of cross-correlation showing that it is intuitively appealing and algorithmically tractable, but not positive semidefinite, and therefore not generally applicable with kernel methods. As a solution we introduce a positive semidefinite similarity function with the same intuitive appeal as cross-correlation. An experimental evaluation in the astronomy domain as well as several other data sets demonstrates the performance of the kernel and related similarity functions.", "recorded": "2009-09-08T14:50:00", "title": "Kernels for Periodic Time Series Arising in Astronomy"}, {"url": "acai05_kononenko_ae", "desc": "One of crucial tasks in machine learning is the evaluation of the quality of attributes. For that purpose a number of measures have been developed that estimate the usefulness of the attribute for predicting the target variable. We will describe separately measures for classification (which are appropriate also for relational problems) and for regression. Most of the measures estimate the quality of one attribute independently of the context of other attributes. However, algorithm ReliefF and its regressional version RReliefF take into account also the context of other attributes and are therefore appropriate for problems with strong dependencies between attributes. The following measures will be described:\r\n - Measures for guiding the search in classification and relational problems are: information gain, Gain ratio, distance measure, minimum description length (MDL), J-measure, Gini-index and ReliefF.\r\n - The quality of attributes in regression can be evaluated using the following measures: expected change of variance, regressional ReliefF, and minimum description length principle (MDL).\r\n ", "recorded": "2005-07-04T09:00:00", "title": "Attribute estimation"}, {"url": "iswc2011_novacek_meaning", "desc": "Our outrageous idea concerns the very core of the Semantic\r\nWeb { the (lack of) semantics themselves. In Section 1 we claim that\r\nthe presently used conception of web semantics fails to capture the link\r\nbetween the machine-readable descriptions and their actual meaning (in\r\nthe sense of a formal grounding in reality). We also argue that this is\r\ninherently related to culprits of various difficulties the community has\r\nbeen coping with (e.g., knowledge acquisition bottleneck or data integration challenges). In Section 2 we propose a remedy of this unsatisfactory\r\nstate of a\u000bairs { a broadened notion of emergent web meaning that can\r\nbe derived in a bottom-up manner from the web data using principles\r\nof distributional semantics. Section 3 gives an overview of a preliminary\r\nsolution [1] we recently implemented with promising results. Finally, we\r\noutline directions of future research required to fully realise our vision.", "recorded": "2011-10-27T11:10:00", "title": "Why the Semantic Web Has Never Got Too Much of a Meaning and How to Put It There"}, {"url": "roks2013_pontil_learning", "desc": "A fundamental limitation of standard machine learning methods is the cost incurred by the preparation of the large training samples required for good generalization. A potential remedy is o\u000bffered by multi-task learning: in many cases, while individual sample sizes are rather small, there are samples to represent a large number of learning tasks (linear regression problems), which share some constraining or generative property. If this property is su\u000eficiently simple it should allow for better learning of the individual tasks despite their small individual sample sizes. In this talk I will review a wide class of multi-task learning methods which encourage low-dimensional representations of the regression vectors. I will describe techniques to solve the underlying optimization problems and present an analysis of the generalization performance of these learning methods which provides a proof of the superiority of multi-task learning under specific conditions.", "recorded": "2013-07-09T15:30:00", "title": "Multi-task Learning "}, {"url": "eccs07_wolpert_pog", "desc": "Optimization of many complex systems is often viewed as a black-box optimization problem. Such problems are often difficult to solve using conventional techniques, for a variety of reasons, such as the absence of derivatives, mixed data types, and so on. Techniques such as Genetic Algorithms, Estimation of Distribution Algorithms such as MIMIC and the CE method, and more recently, mathematically rigorous approaches such as Probability Collectives have been used for black-box optimization. It turns out that many of these techniques fall under the category of Monte Carlo Optimization. In this technique, we present a brief statistical analysis of Monte Carlo Optimization (MCO), and show that it is identical to Parametric Machine Learning (PL). Owing to this identity, we can use PL techniques to improve the performance of MCO. Then, we present a new version of the black-box optimization technique of Probability Collectives., and demonstrate the use of PL techniques to improve its optimization performance.", "recorded": "2007-10-03T15:00:00", "title": "Predicting the Outcome of a Game"}, {"url": "icwsm2010_irani_ssc", "desc": "Reaching hundreds of millions of users, major social networks have become important target media for spammers. Although practical techniques such as collaborative filters and behavioral analysis are able to reduce spam, they have an inherent lag (to collect sufficient data on the spammer) that also limits their effectiveness. Through an experimental study of over 1.9 million MySpace profiles, we make a case for analysis of static user profile content, possibly as soon as such profiles are created. We compare several machine learning algorithms in their ability to distinguish spam profiles from legitimate profiles. We found that a C4.5 decision tree algorithm achieves the highest accuracy (99.4%) of finding rogue profiles, while na\u00efve Bayes achieves a lower accuracy (92.6%). We also conducted a sensitivity analysis of the algorithms w.r.t. features which may be easily removed by spammers.", "recorded": "2010-05-26T12:30:00", "title": "Study of Static Classification of Social Spam Profiles in MySpace"}, {"url": "nips2011_salakhutdinov_hdmodels", "desc": "We introduce HD (or \"Hierarchical-Deep\") models, a new compositional learning architecture that integrates deep learning models with structured hierarchical Bayesian models. Specifically we show how we can learn a hierarchical Dirichlet process (HDP) prior over the activities of the top-level features in a Deep Boltzmann Machine (DBM). This compound HDP-DBM model learns to learn novel concepts from very few training examples, by learning low-level generic features, high-level features that capture correlations among low-level features, and a category hierarchy for sharing priors over the high-level features that are typical of different kinds of concepts. We present efficient learning and inference algorithms for the HDP-DBM model and show that it is able to learn new concepts from very few examples on CIFAR-100 object recognition, handwritten character recognition, and human motion capture datasets.", "recorded": "2011-12-14T10:40:00", "title": "Learning to Learn with Compound HD Models"}, {"url": "cidu2011_gariel_go_arounds", "desc": "This paper takes an empirical approach to identify operational factors at busy airports that may\r\npredate go-around maneuvers. Using four years of data from San Francisco International Airport, we begin our investigation with an analysis of sequence of landing aircraft that may increase the probability of go-around occurrence. Then we take a statistical approach to investigate which features of airborne, ground operations (e.g., number of inbound aircraft, number of aircraft taxiing from gate, etc.) or weather are most likely to fluctuate, relative to nominal operations, in the minutes immediately preceding a missed approach. We analyze these findings both in terms of their implication on current airport operations and discuss how the antecedent factors may affect NextGen. Finally, as a means to assist air traffic controllers, we draw upon techniques from the machine learning community to develop a preliminary alert system for go-around prediction.", "recorded": "2011-10-21T14:10:00", "title": "On the Statistics and Predictability of Go-Arounds"}, {"url": "gpip06_bletchley_park", "desc": "There has been a recent upsurge in interest in Gaussian processes for solving a variety of machine learning problems. Simultaneously there have been recent rapid developments in efficient approximation techniques for Gaussian processes and novel unifying theories of existing approximation techniques. This workshop will bring experts in Gaussian processes together with individuals who are using Gaussian processes at the forefront of research in their fields. We will start with a set of talks that will relate the latest developments in Gaussian processes (speakers include Chris Williams, Carl Rasmussen, David MacKay and Ed Snelson) while the second portion of talks will discuss application fields where Gaussian processes are being applied as state-of-the-art technologies: Robotics, Graphics and Vision (speakers include Brian Ferris, Aaron Hertzmann and Raquel Urtasun).\n\nDetailed information can be found at the [[http://ml.dcs.shef.ac.uk/gpip/|Workshop website]].", "recorded": "2006-06-12T00:00:00", "title": "Gaussian Processes in Practice Workshop, Bletchley Park 2006"}, {"url": "kdd07_ling_mlfss", "desc": "In this paper, we propose a new method called Prototype Ranking (PR) designed for the stock selection problem. PR takes into account the huge size of real-world stock data and applies a modified competitive learning technique to predict the ranks of stocks. The primary target of PR is to select the top performing stocks among many ordinary stocks. PR is designed to perform the learning and testing in a noisy stocks sample set where the top performing stocks are usually the minority. The performance of PR is evaluated by a trading simulation of the real stock data. Each week the stocks with the highest predicted ranks are chosen to construct a portfolio. In the period of 1978-2004, PR\u2019s portfolio earns a much higher average return as well as a higher risk-adjusted return than Cooper\u2019s method, which shows that the PR method leads to a clear profit improvement.", "recorded": "2007-08-14T15:45:00", "title": "Machine Learning for Stock Selection"}, {"url": "is2012_glavas_event_factuality", "desc": "There is a certain discrepancy between real-world events and their representations in text (linguistic events or event mentions). The event mentions often refer to future or hypothetical events that have not actually occurred or whose occurrence is uncertain. In this paper we address the problem of predicting event factuality in Croatian texts using supervised machine learning. For each event mention, we aim to predict its polarity (whether the denoted event has actually happened) and its certainty (the level of confidence that the denoted event has happened). We use only lexically-based features, in order to investigate how well this problem may be addressed for a resource-poor language such as Croatian. Our preliminary results suggest that while predicting event polarity using only lexically-based\r\nfeatures is feasible, predicting event certainty mandates the use of more sophisticated features.", "recorded": "2012-10-08T12:48:58", "title": "Are You for Real? Learning Event Factuality in Croatian Texts"}, {"url": "ocwc2014_grobelnik_x_like", "desc": "The goal of the XLike project is to develop technology to monitor and aggregate knowledge that is currently spread across mainstream and social media, and to enable cross-lingual services for publishers, media monitoring and business intelligence. The aim is to combine scientific insights from several scientific areas to contribute in the area of cross-lingual text understanding. By combining modern computational linguistics, machine learning, text mining and semantic technologies we plan to deal with the following two key open research problems:\r\n\r\nto extract and integrate formal knowledge from multilingual texts with cross-lingual knowledge bases, and\r\nto adapt linguistic techniques and crowdsourcing to deal with irregularities in informal language used primarily in social media.\r\nThe developed technology will be language-agnostic, while within the project we will specifically address English, German, Spanish, Chinese and Hindi as major world languages and Catalan and Slovenian as minority languages.", "recorded": "2014-04-25T10:30:00", "title": "x-like \u2013 Cross-lingual knowledge extraction"}, {"url": "solomon_gjoreski_kozina_rarefall", "desc": "The talk will present the RAReFall system, which is a real-time activity recognition and fall detection system. It is tuned for robustness and real-time performance by combining human-understandable rules and classifiers trained with machine learning algorithms. The system consists of two wearable accelerometers sewn into elastic sports-wear, placed on the abdomen and the right thigh. The recognition of the user's activities and detection of falls is performed on a laptop using the raw sensors' data acquired through Bluetooth. The system was evaluated at the EvAAL-2013 activity recognition competition and awarded the first place, achieving the score of 83.6%, which was for 14.2 percentage points better than the second-place system. The evaluation was performed in a living lab using several criteria: recognition performance, user-acceptance, recognition delay, system installation complexity and interoperability with other systems. ", "recorded": "2014-01-24T13:00:00", "title": "RAReFall activity recognition and fall detection system at the EvAAL competition"}, {"url": "smls09_castro_dsasf", "desc": "The study and use of sparse representations in data-rich applications has garnered signi\fcant\nattention in the signal processing, statistics, and machine learning communities. In the present\nwork we describe a novel sensing procedure called Distilled Sensing (DS), which is a sequential and\nadaptive approach for recovering sparse signals in noise.\nPassive sensing approaches, currently the most widespread data collection methods, involve non-\nadaptive data collection procedures that are completely speci\fed before any data is observed. In\ncontrast, DS collects data in a sequential and adaptive manner. Often such procedures are known\nas active sensing or sequential experimental design, and allow the use of data observed in earlier\nstages to guide the collection of future data. The added \nexibility of active sensing, together with a\nsparsity assumption, has the potential to enable extremely effcient and accurate inference.", "recorded": "2009-04-03T11:30:00", "title": "Distilled Sensing: Active sensing for sparse recovery"}, {"url": "coinactivess2010_grobelnik_bpm", "desc": "Part 1. Context Computing.\r\nContext is used as a term for packaging information for a particular need. A criterion for selecting or prioritization information from a broader pool of information could be called contextual model. Search can be contextual: http://searchpoint.ijs.si. The relevance of Context in computing seems to be growing. Many application areas see an opportunity in extending its value by introducing \"context sensitivity\". More details do to be found in ISWC2006 Tutorial on \"context sensitivity\": http://videolectures.net/iswc06_athens_ga/\r\n\r\nPart 2. Text Mining & Light Weight Semantics.\r\nVideolectures discusses the following topics:\r\n;- levels of text representations\r\n;- modeling the data (Support Vector Machine)\r\n;- classification into large taxonomies (DMoz)\r\n;- visual & contextual search (Search Point)\r\n;- multilingual search\r\n;- news bias, news visualization\r\n;- text enrichment (Enrycher)\r\n;- knowledge based summarization\r\n;- question answering (AnswerArt)\r\n;- Cyc knowledge base and reasoning", "recorded": "2010-10-18T14:00:00", "title": "Business Process Mining and Formalization"}, {"url": "eswc2011_sah_dadzie_stack", "desc": "The explosion in growth of the Web of Linked Data has provided, for the first time, a plethora of information in disparate locations, yet bound together by machine-readable, semantically typed relations. Utilisation of the Web of Data has been, until now, restricted to members of the community, eating their own dogfood, so to speak. To the regular web user browsing Facebook and watching Youtube, this utility is yet to be realised. The primary factor inhibiting such uptake is the usability of the Web of Data, which requires users to have prior knowledge of elements from the Semantic Web technology stack. We present a solution to this problem by hiding the stack, allowing end users to browse the Web of Data, explore and discover the information contained, and use Linked Data. Our solution employs a template-based visualisation approach where information attributed to a given resource is rendered according to its rdf:type.", "recorded": "2011-06-02T16:31:00", "title": "Hide the Stack: Toward Usable Linked Data"}, {"url": "icml08_sarkar_slg", "desc": "In this paper we investigate two aspects of ranking problems on large graphs. First, we augment the deterministic pruning algorithm in Sarkar and Moore (2007) with sampling techniques to compute approximately correct rankings with high probability under random walk based proximity measures at query time. Second, we prove some surprising locality properties of these proximity measures by examining the short term behavior of random walks. The proposed algorithm can answer queries on the fly without caching any information about the entire graph. We present empirical results on a 600,000 node author-word-citation graph from the Citeseer domain on a single CPU machine where the average query processing time is around 4 seconds. We present quantifiable link prediction tasks. On most of them our techniques outperform Personalized Pagerank, a well-known diffusion based proximity measure.", "recorded": "2008-07-06T11:30:00", "title": "Fast Incremental Proximity Search in Large Graphs"}, {"url": "icml08_bach_bmcle", "desc": "We consider the least-square linear regression problem with regularization by the l1-norm, a problem usually referred to as the Lasso. In this paper, we present a detailed asymptotic analysis of model consistency of the Lasso. For various decays of the regularization parameter, we compute asymptotic equivalents of the probability of correct model selection (i.e., variable selection). For a specific rate decay, we show that the Lasso selects all the variables that should enter the model with probability tending to one exponentially fast, while it selects all other variables with strictly positive probability. We show that this property implies that if we run the Lasso for several bootstrapped replications of a given sample, then intersecting the supports of the Lasso bootstrap estimates leads to consistent model selection. This novel variable selection algorithm, referred to as the Bolasso, is compared favorably to other linear regression methods on synthetic data and datasets from the UCI machine learning repository.", "recorded": "2008-07-07T14:20:00", "title": "Bolasso: Model Consistent Lasso Estimation through the Bootstrap"}, {"url": "semsearch09_akbik_wesr", "desc": "A great share of applications in modern information technology can bene\ft from large coverage, machine accessible knowledge bases. However, the bigger part of todays knowledge is provided in the form of unstructured data, mostly plain text. As an initial step to exploit such data, we present\r\nWanderlust, an algorithm that automatically extracts semantic relations from natural language text. The procedure uses deep linguistic patterns that are de\fned over the dependency grammar of sentences. Due to its linguistic nature, the method performs in an unsupervised fashion and is not\r\nrestricted to any speci\fc type of semantic relation. The applicability of the proposed approach is examined in a case study, in which it is put to the task of generating a semantic wiki from the English Wikipedia corpus. We present an exhaustive discussion about the insights obtained from this\r\nparticular case study including considerations about the generality of the approach.", "recorded": "2009-04-21T11:30:00", "title": "Wanderlust: Extracting Semantic Relations from Natural Language Text Using Dependency Grammar Patterns "}, {"url": "aaai2010_gomes_cai", "desc": "**//Disclaimer:// VideoLectures.NET emphasizes that the quality of this video was notably improved, because of poor sound quality conditions provided in the lecture auditorium.**\r\n\r\nComputational Sustainability is a new interdisciplinary research field with the overall goal of developing computational models, methods, and tools to help manage the balance between environmental, economic, and societal needs for a sustainable future. In this talk I will provide an overview of Computational Sustainability, with examples ranging from wildlife conservation and biodiversity, to poverty mitigation, to large-scale deployment and management of renewable energy sources. I will highlight overarching computational challenges for AI at the intersection of constraint reasoning, optimization, machine learning, and dynamical systems. Finally I will discuss the need for a new approach that views computational sustainability problems as \"natural\" phenomena, amenable to a scientific methodology, in which principled experimentation, to explore problem parameter spaces and hidden problem structure, plays as prominent a role as formal analysis.", "recorded": "2010-07-14T09:00:00", "title": "Challenges for AI in Computational Sustainability"}, {"url": "mlvj", "desc": "==== Aim\r\nThis new journal aims to give researchers a way of rapidly understanding the key message and main ideas behind a paper available\r\nelsewhere. One of the authors of each paper presents a short video summary of their paper enabling viewers to form an impression of the content and relevance of the paper to their research. We have also enabled a feature allowing viewers to add their own comments about papers. Each entry includes a link to the original paper together with any additional material or addenda that authors wish to include. \r\n\r\n====Background\r\nThis initiative arose mainly through the activities of the PASCAL (Pattern Analysis, Statistical Modelling and Computational Learning) Network of Excellence funded by the European Union over a period of ten years from 2003 to 2013 together with its legacy organisation Knowledge 4 All Foundation that aims among other initiatives to promote the availability of open access academic content.\r\n\r\n", "recorded": "2011-06-07T09:59:38", "title": "Machine Learning Video Journal"}, {"url": "machine_affandi_determinantal_processes", "desc": "Determinantal point processes (DPPs) are random point processes well-suited for modeling repulsion. In machine learning, the focus of DPP-based models has been on diverse subset selection from a discrete and finite base set. This discrete setting admits an efficient algorithm for sampling based on the eigendecomposition of the defining kernel matrix. Recently, there has been growing interest in using DPPs defined on continuous spaces. While the discrete-DPP sampler extends formally to the continuous case, computationally, the steps required cannot be directly extended except in a few restricted cases. In this paper, we present efficient approximate DPP sampling schemes based on Nystrom and random Fourier feature approximations that apply to a wide range of kernel functions. We demonstrate the utility of continuous DPPs in repulsive mixture modeling applications and synthesizing human poses spanning activity spaces.", "recorded": "2014-01-20T12:25:00", "title": "Approximate Inference in Continuous Determinantal Processes"}, {"url": "aaai2012_kautz_presidential", "desc": "For much of its history, the field of AI has been in retreat from the most ambitious goals of its founders. Rather than attempting to understand and replicate general intelligence, research concentrated on smaller, better-defined perceptual and reasoning tasks over limited domains. We are now at a point, however, where the subfields are coming together again, and the idea of\r\nachieving the old dream of AI is no longer fanciful. We are entering an age of computerized personal servants, self-driving vehicles, the universal natural language translator, and the solution to the mysteries of the brain. The merging of human and machine intelligence will drive progress on problems across science, industry, and education. Kautz will recap some of the transformative events we have witnessed over the past few years, and describe a vision of the near future that is both realistic and more wildly optimistic than most serious scientists would have dared imagine a decade ago.", "recorded": "2012-07-24T09:00:41", "title": "Revisiting the Dream"}, {"url": "turing100_penrose_mathematical_mind", "desc": "Following Alan Turing\u2019s ground-breaking 1937 paper, which introduced his notion of the Universal Turing machine, he suggested, in 1939, generalizations based on ordinal logic and oracle machines, these being apparently motivated by attempts to model the mathematical mind in a way that could evade the apparent limitations presented by G\u00f6del\u2019s incompleteness theorems. In this talk, I introduce the idea of a \u201ccautious oracle\u201d as a more human version of Turing\u2019s oracles. Nevertheless, I show that even this fails to capture the essence of the full capabilities of our understanding.\r\n\r\nI raise the issue of possible physical processes that would appear to be needed in order to circumvent these G\u00f6del-type restrictions. At the end of the talk, I report on some startling new experiments, which appear to point to new insights into the possible physical processes underlying conscious brain activity, and I speculate on how this might relate to the power of human understanding.", "recorded": "2012-06-24T20:00:00", "title": "The Problem of Modelling the Mathematical Mind"}, {"url": "ecmlpkdd2010_hullermeier_furnkranz_pl", "desc": "The topic of \"preferences\" has recently attracted considerable attention in artificial intelligence in general and machine learning in particular, where the topic of preference learning has emerged as a new, interdisciplinary research field with close connections to related areas such as operations research, social choice and decision theory. Roughly speaking, preference learning is about methods for learning preference models from explicit or implicit preference information, typically used for predicting the preferences of an individual or a group of individuals. Approaches relevant to this area range from learning special types of preference models, such as lexicographic orders, over \"learning to rank\" for information retrieval to collaborative filtering techniques for recommender systems. The primary goal of this tutorial is to survey the field of preference learning in its current stage of development. The presentation will focus on a systematic overview of different types of preference learning problems, methods and algorithms to tackle these problems, and metrics for evaluating the performance of preference models induced from data.\r\n", "recorded": "2010-09-24T10:28:00", "title": "Preference Learning"}, {"url": "mbc07_hardoon_int", "desc": "Until recently, the issue of musical representation had focused primarily on symbolic notation of musical information and structure, and on the representation of musical performance. Research on how we represent musical experience in the brain is emerging as a rich area of investigation thanks to ongoing advances in brain-scanning technology such as EEG and fMRI.\r \r This day of the workshop addresses the problem of representation of musical experience in the brain from a computational modelling approach chiefly based on machine learning and signal processing.\r \r The overarching question addressed by this workshop is whether we can devise efficient methods to study and model the representation of musical experience in the brain by combining traditional forms of musical representation (musical notation, audio, performance, etc.) with brain scanning technology.\r \r This problem is of particular relevance for the successful modelling of cognitive music behaviour, design of interactive music systems, informing techniques in contemporary composition, providing methods to enhance music performance, and even helping music analysis in suggesting ways of listening to music.\r ", "recorded": "2007-12-07T07:30:00", "title": "Introduction to the Workshop"}, {"url": "ecmlpkdd2011_ding_graph", "desc": "We present a new stochastic process, called as Social Diffusion Process (SDP), to address the graph modeling. Based on this model, we derive a graph evolution algorithm and a series of graph-based approaches to solve machine learning problems, including clustering and semi-supervised learning. SDP can be viewed as a special case of Matthew effect, which is a general phenomenon in nature and societies. We use social event as a metaphor of the intrinsic stochastic process for broad range of data. We evaluate our approaches in a large number of frequently used datasets and compare our approaches to other state-of-the-art techniques. Results show that our algorithm outperforms the existing methods in most cases. We also applying our algorithm into the functionality analysis of microRNA and discover biologically interesting cliques. Due to the broad availability of graph-based data, our new model and algorithm potentially have applications in wide range.", "recorded": "2011-09-07T15:00:00", "title": "Graph Evolution via Social Diffusion Processes"}, {"url": "icwsm2013_magdy_news_articles", "desc": "A reader of a news article would often be interested in the comments of other readers on an article, because comments give insight into popular opinions or feelings toward a given piece of news. In recent years, social media platforms, such as Twitter, have become a social hub for users to communicate and express their thoughts. This includes sharing news articles and commenting on them. In this paper, we propose an approach for identifying \u201ccomment-tweets\u201d that comment on news articles. We discuss the nature of comment-tweets and compare them to subjective tweets. We utilize a machine learning-based classification approach for distinguishing between comment-tweets and others that only report the news. Our approach is evaluated on the TREC-2011 Microblog track data after applying additional annotations to tweets containing comments. Results show the effectiveness of our classification approach. Furthermore, we demonstrate the effectiveness of our approach on live news articles.", "recorded": "2013-07-09T12:25:00", "title": "Detecting Comments on News Articles in Microblogs"}, {"url": "kdd2014_thye_goh_imbalanced_data", "desc": "The vast majority of real world classification problems are imbalanced, meaning there are far fewer data from the class of interest (the positive class) than from other classes. We propose two machine learning algorithms to handle highly imbalanced classification problems. The classifiers are disjunctions of conjunctions, and are created as unions of parallel axis rectangles around the positive examples, and thus have the benefit of being interpretable. The first algorithm uses mixed integer programming to optimize a weighted balance between positive and negative class accuracies. Regularization is introduced to improve generalization performance. The second method uses an approximation in order to assist with scalability. Specifically, it follows a \\textit{characterize then discriminate} approach, where the positive class is characterized first by boxes, and then each box boundary becomes a separate discriminative classifier. This method has the computational advantages that it can be easily parallelized, and considers only the relevant regions of feature space.", "recorded": "2014-08-25T11:15:00", "title": "Box Drawings for Learning with Imbalanced Data"}, {"url": "ida07_saul_mufl1", "desc": "Multiplicative update rules have proven useful in many areas\r of machine learning. Simple to implement, guaranteed to converge,\r they account in part for the widespread popularity of algorithms such\r as nonnegative matrix factorization and Expectation-Maximization. In\r this paper, we show how to derive multiplicative updates for problems in\r L1-regularized linear and logistic regression. For L1\u2013regularized linear regression,\r the updates are derived by reformulating the required optimization\r as a problem in nonnegative quadratic programming (NQP). The\r dual of this problem, itself an instance of NQP, can also be solved using\r multiplicative updates; moreover, the observed duality gap can be used\r to bound the error of intermediate solutions. For L1\u2013regularized logistic\r regression, we derive similar updates using an iteratively reweighted least\r squares approach. We present illustrative experimental results and describe\r efficient implementations for large-scale problems of interest (e.g.,\r with tens of thousands of examples and over one million features).", "recorded": "2007-09-06T14:00:00", "title": "Multiplicative Updates for L1-Regularized Linear and Logistic Regression"}, {"url": "nipsworkshops2012_hsu_algorithms", "desc": "Mixture models are a staple in machine learning and applied statistics \r\nfor treating data taken from multiple sub-populations. For many classes of\r\nmixture models, parameter estimation is computationally and/or\r\ninformation-theoretically hard in general. However, much progress has been\r\nmade over the past decade or so to overcome these hardness barriers by\r\nfocusing on sub-classes that rule out the intractable cases.\r\n\r\nOne very powerful and general sub-class is the multi-view setting, where\r\none can take advantage of several non-redundant sources of information to\r\nhelp distinguish different sub-populations. In this talk, I'll describe a\r\ngeneral technique that is applicable even in semi-parametric settings,\r\nwhere one may not have a parametric model for individual mixture\r\ncomponents. This technique also yields a number of new unsupervised\r\nlearning results for well-studied problems, as well as very practical and\r\nscalable learning algorithms.", "recorded": "2012-12-07T14:00:00", "title": "Efficient algorithms for estimating multi-view mixture models"}, {"url": "aaai2011_remy_bipedal", "desc": "Bipedal robotic locomotion presents a significant challenge to\r\nthe controls designer. The equations of motion governing these\r\nsystems are generally hybrid or switched due to intermittent\r\nground contact and consist of numerous coupled non-linear\r\ndifferential equations even in the simplest case. These\r\nattributes make traditional control techniques difficult to\r\napply. In this paper, an alternative controller for a 5-link\r\nplanar biped robot is created through a combination of\r\nfeedforward neural networks, genetic algorithms and traditional\r\nPD control. The neural network uses certain state variables as\r\ninput and generates a desired target joint state based on the\r\ncurrent state in a manner qualitatively similar to HZD. A PD\r\ncontroller than attempts to force the robot into this\r\nconfiguration. In this way the neural network specifies a time\r\ninvariant trajectory as a function of some combination of state\r\nvariables. A modified genetic algorithm is used to evolve\r\nsuccessful neural controllers for the system.", "recorded": "2011-08-05T11:42:34", "title": "Machine Learning for Bipedal Walking"}, {"url": "nipsworkshops2013_almeida_feature_engineering", "desc": "This cause-effect pairs challenge was motivated by the contrast between the costs to per- forming controlled experiments in order to determine causality and the abundance of observational data. Our goal was to provide a value representing our confidence of causality determined by the observation data which would help identify the most promising variables for experimental verification of their causal relationship.\r\nA novel approach was created focusing on feature engineering that requires minimal human intervention. By applying standard machine learning algorithms to the pairs of points, almost 9000 features were created by computing the goodness of fit of these algo- rithms in various ways. This approach was successful enough to attain the highest score in the competition\u2019s private leaderboard. Additionally, alternatives and their explanations of why they weren\u2019t used as well as possible improvements which could greatly improve accuracy were outlined.", "recorded": "2013-12-09T14:00:00", "title": "Automated Feature Engineering Applied to Causality"}, {"url": "ecmlpkdd08_kok_esnft", "desc": "Extracting knowledge from text has long been a goal of AI. Initial approaches were purely logical and brittle. More recently, the availability of large quantities of text on the Web has led to the development of machine learning approaches. However, to date these have mainly extracted ground facts, as opposed to general knowledge. Other learning approaches can extract logical forms, but require supervision and do not scale. In this paper we present an unsupervised approach to extracting semantic networks from large volumes of text. We use the TextRunner system [1] to extract tuples from text, and then induce general concepts and relations from them by jointly clustering the objects and relational strings in the tuples. Our approach is defined in Markov logic using four simple rules. Experiments on a dataset of two million tuples show that it outperforms three other relational clustering approaches, and extracts meaningful semantic networks.", "recorded": "2008-09-17T16:30:00", "title": "Extracting Semantic Networks from Text via Relational Clustering"}, {"url": "nipsworkshops2011_kohli_optimization", "desc": "Many problems in computer vision and machine learning require inferring the most probable states of certain hidden or unobserved variables. This inference problem can be formulated in terms of minimizing a function of discrete variables. The scale and form of computer vision problems raise many challenges in this optimization task. For instance, functions encountered in vision may involve millions or sometimes even billions of variables. Furthermore, the functions may contain terms that encode very high-order interaction between variables. These properties ensure that the minimization of such functions using conventional algorithms is extremely computationally expensive. In this talk, I will discuss how many of these challenges can be overcome by exploiting the sparse and heterogeneous nature of discrete optimization problems encountered in real world computer vision problems. Such problem-aware approaches to optimization can lead to substantial improvements in running time and allow us to produce good solutions to many important problems.", "recorded": "2011-12-17T07:50:00", "title": " Exploiting Problem Structure for Efficient Discrete Optimization"}, {"url": "icml08_finley_tssvm", "desc": "While discriminative training (e.g., CRF, structural SVM) holds much promise for machine translation, image segmentation, and clustering, the complex inference these applications require make exact training intractable. This leads to a need for approximate training methods. Unfortunately, knowledge about how to perform efficient and effective approximate training is limited. Focusing on structural SVMs, we provide and explore algorithms for two different classes of approximate training algorithms, which we call undergenerating (e.g., greedy) and overgenerating (e.g., relaxations) algorithms. We provide a theoretical and empirical analysis of both types of approximate trained structural SVMs, focusing on fully connected pairwise Markov random fields. We find that models trained with overgenerating methods have theoretic advantages over undergenerating methods, are empirically robust relative to their undergenerating brethren, and relaxed trained models favor non-fractional predictions from relaxed predictor", "recorded": "2008-07-07T10:24:49", "title": "Training Structural SVMs when Exact Inference is Intractable"}, {"url": "acs07_eindhoven", "desc": "Algorithms play an important role in the analysis of complex systems. The practical implementation of any procedure almost always depends on fast, stable and accurate algorithms.\n\nThe workshop concentrates on algorithms in the areas of statistical analysis under (shape) constraints, Bayesian procedures, image analysis and machine learning and will cover the latest advances in linear and quadratic programming, constrained maximization, sparse systems, differential equations in image analysis and simulation techniques.\n\nThe workshop brings together people from different areas which face common problems but approach them in different ways and have different algorithms. In this respect, its aims resemble those of the PASCAL funded workshop on Image Analysis and Inverse Problems held at EURANDOM in December 2006. The workshop was a great success and succeeded in its aim of cross-fertilization of methods and ideas.\n\nVisit Workshop website [[http://www.eurandom.nl/events/workshops/2007/Algorithms/Algorithms_main.htm|here]].", "recorded": "2007-09-24T10:00:00", "title": "Workshop on Algorithms in Complex Systems, Eindhoven 2007"}, {"url": "nipsworkshops2010_golovin_asm", "desc": "Solving stochastic optimization problems under partial\r\nobservability, where one needs to adaptively make decisions\r\nwith uncertain outcomes, is a fundamental but notoriously\r\ndifficult challenge. In this talk, I will introduce a new concept\r\ncalled adaptive submodularity, which generalizes submodular\r\nset functions to adaptive policies. In many respects adaptive\r\nsubmodularity plays the same role for adaptive problems as\r\nsubmodularity plays for nonadaptive problems. Specifically, just\r\nas many nonadaptive problems with submodular objectives have\r\nefficient algorithms with good approximation guarantees, so too\r\ndo adaptive problems with adaptive submodular objectives. We\r\nuse this fact to recover and generalize several previous results\r\nin adaptive optimization, including results for active learning\r\nand adaptive variants of maximum coverage and set cover.\r\nApplications include machine diagnosis, observation selection\r\nand sensor placement problems, and adaptive viral marketing.\r\nJoint work with Andreas Krause.", "recorded": "2010-12-11T18:25:00", "title": "Adaptive Submodularity: A New Approach to Active Learning and Stochastic Optimization"}, {"url": "eswc2013_zhao_knowledge", "desc": "The Linked Open Data (LOD) cloud contains tremendous amounts of interlinked instances, from where we can retrieve abundant knowledge. However, because of the heterogeneous and big ontologies, it is time consuming to learn all the ontologies manually and it is difficult to observe which properties are important for describing instances of a specific class. In order to construct an ontology that can help users easily access to various data sets, we propose a semi-automatic ontology integration framework that can reduce the heterogeneity of ontologies and retrieve frequently used core properties for each class. The framework consists of three main components: graph-based ontology integration, machine-learning-based ontology schema extraction, and an ontology merger. By analyzing the instances of the linked data sets, this framework acquires ontological knowledge and constructs a high-quality integrated ontology, which is easily understandable and effective in knowledge acquisition from various data sets using simple SPARQL queries.\r\n", "recorded": "2013-05-28T15:30:00", "title": "Instance-based ontological knowledge acquisition"}, {"url": "cernacademictraining09_cranmer_stpp", "desc": "This series will consist of four 1-hour lectures on statistics for particle physics.  The goal will be to build up to techniques meant for dealing with problems of realistic complexity while maintaining a formal approach.  I will also try to incorporate usage of common tools like ROOT, RooFit, and the newly developed RooStats framework into the lectures.  The first lecture will begin with a review the basic principles of probability, some terminology, and the three main approaches towards statistical inference (Frequentist, Bayesian, and Likelihood-based).  I will then outline the statistical basis for multivariate analysis techniques (the Neyman-Pearson lemma) and the motivation for machine learning algorithms.  Later, I will extend simple hypothesis testing to the case in which the statistical model has one or many parameters (the Neyman Construction and the Feldman-Cousins technique).  From there I will outline techniques to incorporate background uncertainties.  If time allows, I will touch on the statistical challenges of searches for physics beyond the standard model and the look-elsewhere effect.", "recorded": "2009-02-02T14:35:00", "title": "Statistical Techniques for Particle Physics"}, {"url": "rldm2015_edmonton", "desc": "Over the last few decades, reinforcement learning and decision making have been the focus of an incredible wealth of research spanning a wide variety of fields including psychology, artificial intelligence, machine learning, operations research, control theory, animal and human neuroscience, economics and ethology. Key to many developments in the field has been interdisciplinary sharing of ideas and findings, yet there has been no single conference that brings all these communities together. The idea of RLDM is to become that conference.\r\n\r\nThe focus of the new meeting can be broadly construed as \u201cdecision making over time to achieve a goal\u201d. Our aim is to create a recurring meeting characterized by the multidisciplinarity of the presenters and attendees, with cross-disciplinary conversations and teaching and learning being central objectives along with the dissemination of novel theoretical and experimental results.\r\n\r\nFind out more at the [[http://rldm.org/|RLDM website]].", "recorded": "2015-06-07T00:00:00", "title": "2nd Multidisciplinary Conference on Reinforcement Learning and Decision Making (RLDM), Edmonton 2015"}, {"url": "aistats2010_ranzato_f3wr", "desc": "Deep belief nets have been successful in modeling handwritten characters, but it has proved more difficult to apply them to real images. The problem lies in the restricted Boltzmann machine (RBM) which is used as a module for learning deep belief nets one layer at a time. The Gaussian-Binary RBMs that have been used to model real-valued data are not a good way to model the covariance structure of natural images. We propose a factored 3-way RBM that uses the states of its hidden units to represent abnormalities in the local covariance structure of an image. This provides a probabilistic framework for the widely used simple/complex cell architecture. Our model learns binary features that work very well for object recognition on the \"tiny images\" data set. Even better features are obtained by then using standard binary RBM's to learn a deeper model. ", "recorded": "2010-05-14T10:04:00", "title": "Factored 3-way restricted Boltzmann machines for modeling natural images "}, {"url": "ecmlpkdd2010_zaslavskiy_mmgm", "desc": "Graphs provide an efficient tool for object representation in various machine learning  applications. Once graph-based representations are constructed, an important question is how to compare graphs. This problem is often formulated as a graph matching problem where one seeks a mapping between vertices of two graphs which optimally aligns their structure. In the classical formulation of graph matching, only one-to-one correspondences between vertices are considered. However, in many applications, graphs cannot be matched perfectly and it is more interesting to consider many-to-many correspondences where clusters of vertices in one graph are matched to clusters of vertices in the other graph. In this paper, we formulate the many-to-many graph matching problem as a discrete optimization problem and propose two approximate algorithms based on alternative continuous relaxations of the combinatorial problem. We compare new methods with other existing methods on several benchmark datasets. ", "recorded": "2010-09-21T15:29:00", "title": "Many-to-Many Graph Matching: a Continuous Relaxation Approach "}, {"url": "mlg07_gaertner_tcoldc", "desc": "Classifying vertices in digraphs is an important machine learning setting with many applications. We consider learning problems on digraphs with three characteristic properties: (i) The target concept corresponds to a directed cut; (ii) the total cost of finding the cut has to be bounded a priori; and (iii) the target concept may change due to a hidden context. For one motivating example consider classifying intermediate products in some process, e.g., for manufacturing cars or the control flow in software, as faulty or correct. The process can be represented by a digraph and the concept is monotone: Typical faults that appear in an intermediate product will also be present in later stages of the product. The concept may depend on a hidden variable as some pre-assembled parts may vary and the fault may occur only for some charges and not for others. In order to be able to trade off between the cost of having a faulty product and the costs needed to find the cause of the fault, tight performance guarantees for finding the bug are needed.", "recorded": "2007-08-01T11:30:00", "title": "The Cost of Learning Directed Cuts"}, {"url": "icml08_franc_ocp", "desc": "We have developed a new Linear Support Vector Machine (SVM) training algorithm called OCAS. Its computational effort scales linearly with the sample size. In an extensive empirical evaluation OCAS significantly outperforms current state of the art SVM solvers, like SVMLight, SVMPerf and BMRM, achieving speedups of over 1,000 on some datasets over SVMLight and 20 over SVMPerf, while obtaining the same precise Support Vector solution. OCAS even in the early optimization steps shows often faster convergence than the so far in this domain prevailing approximative methods SGD and Pegasos. Effectively parallelizing OCAS we were able to train on a dataset of size 15 million examples (itself about 32GB in size) in just 671 seconds --- a competing string kernel SVM required 97,484 seconds to train on 10 million examples sub-sampled from this dataset.", "recorded": "2008-07-08T14:25:00", "title": "Optimized Cutting Plane Algorithm for Support Vector Machines"}, {"url": "mmdss07_provost_ilwn", "desc": "In many applications we would like to draw inferences about entities that are\r interconnected in complex networks. For example, calls, emails, IM, and web\r pointers link people into huge social networks. However, traditional statistical and\r machine learning classification methods assume that entities are independent of each\r other. I start by discussing various applications of \"classification\" (scoring) in\r networked data, from fraud detection to counterterrorism to network-based marketing.\r I then discuss four characteristics of networked data that allow improvements--\r sometimes substantial--over traditional classification: (i) models can take into account\r \"guilt by association,\" (ii) inference can be performed \"collectively,\" whereby\r inferences on linked entities mutually reinforce each other, (iii) characteristics of\r linked entities can be incorporated in models, and (iv) models can incorporate specific\r identifiers, such as the identities of particular individuals, to improve inference. I\r present results demonstrating the effectiveness of these techniques. ", "recorded": "2007-09-13T09:15:06", "title": "Inference and Learning with Networked Data"}, {"url": "yaleecon252s08_shiller_lec19", "desc": "The exchanges in which stocks and other securities are traded serve an important function in finance. They bring together people interested in buying and selling securities in order to create a universal price. Brokers and dealers are also an important part of the system, their methods and standards are ultimately behind the success of the exchanges. Many information innovations have advanced the functioning of exchanges, going all the way back to the ticker machine, which was created to communicate the price of securities at a point in time to all interested parties. Electronic Communication Networks and automatic exchanges, more generally, have significantly impacted the exchange of securities and few exchanges still have physical trading floors.\n\n**Reading assignment:**\n\nFabozzi et al.Foundations of Financial Markets and Institutions, chapters 17, 18, 19, 20 and 21\n\n**Resources:**\n\n;[[http://oyc.yale.edu/sites/default/files/Lecture19_0.pdf|PowerPoint slides from screen - Lecture 19[PDF]]]", "recorded": "2008-03-03T09:00:00", "title": "Lecture 19 -  Brokerage, ECNs, etc."}, {"url": "ecmlpkdd08_freund_mymcp", "desc": "The advance of fluorescent tagging and of confocal microscopy is allowing biologists to image biochemical processes at a level of detail that was unimaginable just a few years ago. However, as the analysis of these images is done mostly by hand, there is a severe bottleneck in transforming these images into useful quantitative data that can be used to evaluate mathematical models.\r\nOne of the inherent challenges involved in automating this transformation is that image data is highly variable. This requires a recalibration of the image processing algorithms for each experiment. We use machine learning methods to enable the experimentalist to calibrate the image processing methods without having any knowledge of how these methods work. This, we believe, will allow the rapid integration of computer vision methods with confocal microscopy and open the way to the development of quantitative spatial models of cellular processes.\r\nFor more information, see the\r\n[[http://seed.ucsd.edu/~yfreund/NewHomePage/Applications/Biomedical_Imaging.html|  Bio-medical image analysis page]]", "recorded": "2008-09-19T09:00:00", "title": "From Microscopy Images to Models of Cellular Processes"}, {"url": "icml2015_sheth_generalized_gp_models", "desc": "Gaussian processes (GP) provide an attractive machine learning model due to their non-parametric form, their flexibility to capture many types of observation data, and their generic inference procedures. Sparse GP inference algorithms address the cubic complexity of GPs by focusing on a small set of pseudo-samples. To date, such approaches have focused on the simple case of Gaussian observation likelihoods. This paper develops a variational sparse solution for GPs under general likelihoods by providing a new characterization of the gradients required for inference in terms of individual observation likelihood terms. In addition, we propose a simple new approach for optimizing the sparse variational approximation using a fixed point computation. We demonstrate experimentally that the fixed point operator acts as a contraction in many cases and therefore leads to fast convergence. An experimental evaluation for count regression, classification, and ordinal regression illustrates the generality and advantages of the new approach.", "recorded": "2015-07-09T14:46:53", "title": "Sparse Variational Inference for Generalized GP Models"}, {"url": "icml08_wang_bii", "desc": "In real-world machine learning problems, it is very common that part of the input feature vector is incomplete: either not available, missing, or corrupted. In this paper, we present a boosting approach that integrates features with incomplete information and those with complete information to form a strong classifier. By introducing hidden variables to model missing information, we form loss functions that combine fully labeled data with partially labeled data to effectively learn normalized and unnormalized models. The primal problems of the proposed optimization problems with these loss functions are provided to show their close relationships and the motivations behind them. We use auxiliary functions to bound the change of the loss functions and derive explicit parameter update rules for the learning algorithms. We demonstrate encouraging results on two real-world problems - visual object recognition in computer vision and named entity recognition in natural language processing - to show the effectiveness of the proposed boosting approach.", "recorded": "2008-07-08T08:55:00", "title": "Boosting with Incomplete Information"}, {"url": "lmcv04_williams_ls", "desc": "A simple and efficient way to model much image and video data is to decompose it into a set of 2-dimensional objects in layers. Each object is characterized by its shape and appearance (as with a \"sprite\" in computer graphics). Following earlier work on layer decompositions in computer vision (e.g. Wang and Adelson, 1994), Frey and Jojic (1999) stated the sprite-learning problem in terms of transformation-invariant clustering using mixture models and EM. This was later extended (Jojic and Frey, 2001) to learning multiple sprites/objects from a video sequence. The approach of building in knowledge about allowable transformations into the clustering algorithm is an important way that a machine learning algorithm (clustering) needs to be tailored to the computer vision domain. Frey and Jojic's approach to learning multiple sprites uses variational inference simultaneously on all sprites; we also discuss recent work by Williams and Titsias (2004) who describe a greedy sequential algorithm for this task.", "recorded": "2004-05-04T00:00:00", "title": "Learning Sprites"}, {"url": "nipsworkshops2012_multitradeoffs", "desc": "One of the main practical goals of machine learning is to identify relevant trade-offs in different problems, formalize, and solve them. We have already achieved fairly good progress in addressing individual trade-offs, such as model order selection or exploration-exploitation. In this workshop we would like to focus on problems that involve more than one trade-off simultaneously. We are interested both in practical problems where \"multi-trade-offs\" arise and in theoretical approaches to their solution. Obviously, many problems in life cannot be reduced to a single trade-off and it is highly important to improve our ability to address multiple trade-offs simultaneously. Below we provide several examples of situations, where multiple trade-offs arise simultaneously. The goal of the examples is to provide a starting point for a discussion, but they are not limiting the scope and any other multi-trade-off problem is welcome to be discussed at the workshop.\r\n\r\nMulti-trade-offs arise naturally in interaction between multiple learning systems or when a learning system faces multiple tasks simultaneously; especially when the systems or tasks share common resources, such as CPU time, memory, sensors, robot body, and so on. For a concrete example, imagine a robot riding a bicycle and balancing a pole. Each task individually (cycling and pole balancing) can be modeled as a separate optimization problem, but their solutions have to be coordinated, since they share robot resources and robot body. More generally, each learning system or system component has its own internal trade-offs, which have to be balanced against trade-offs of other systems, whereas shared resources introduce external trade-offs that enforce cooperation. The complexity of interaction can vary from independent systems sharing common resources to systems with various degrees of relation between their inputs and tasks. In multi-agent systems communication between the agents introduces an additional trade-off.\r\n\r\nWe are also interested in multi-trade-offs that arise within individual systems. For example, model order selection and computational complexity [1], or model order selection and exploration-exploitation [2]. For a specific example of this type of problems, imagine a system for real-time prediction of the location of a ball in table tennis. This system has to balance between at least three objectives that interact in a non-trivial manner: (1) complexity of the model of flight trajectory, (2) statistical reliability of the model, (3) computational requirements. Complex models can potentially provide better predictions, but can also lead to overfitting (trade-off between (1) and (2)) and are computationally more demanding. At the same time, there is also a trade-off between having fast crude predictions or slower, but more precise estimations (trade-off between (3) and (1)+(2)). Despite the complex nature of multi-trade-offs, there is still hope that they can be formulated as convex problems, at least in some situations [3].\r\n\r\nReferences:\\\\\r\n[1] Shai Shalev-Shwartz and Nathan Srebro. \"SVM Optimization: Inverse Dependence on Training Set Size\", ICML, 2008.\\\\\r\n[2] Yevgeny Seldin, Peter Auer, Fran\u00e7ois Laviolette, John Shawe-Taylor, and Ronald Ortner. \"PAC-Bayesian Analysis of Contextual Bandits\", NIPS, 2011.\\\\\r\n[3] Andreas Argyriou, Theodoros Evgeniou and Massimiliano Pontil. Convex multi-task feature learning. Machine Learning, 2008, Volume 73, Number 3.\r\n\r\nWorkshop homepage: https://sites.google.com/site/multitradeoffs2012/", "recorded": "2012-12-07T07:30:00", "title": "Multi-Trade-offs in Machine Learning"}, {"url": "ecmlpkdd09_diethe_kpfp", "desc": "Polytope Faces Pursuit (PFP) is a greedy algorithm that approximates the sparse solutions recovered by 1 regularised least-squares (Lasso) [4, 10] in a similar vein to (Orthogonal) Matching Pursuit (OMP) [16]. The algorithm is based on the geometry of the polar polytope where at each step a basis function is chosen by finding the maximal vertex using a path-following method. The algorithmic complexity is of a similar order to OMP whilst being able to solve problems known to be hard for (O)MP. Matching Pursuit was extended to build kernel-based solutions to machine learning problems, resulting in the sparse regression algorithm, Kernel Matching Pursuit (KMP) [17]. We develop a new algorithm to build sparse kernel-based solutions using PFP, which we call Kernel Polytope Faces Pursuit (KPFP). We show the usefulness of this algorithm by providing a generalisation error bound [7] that takes into account a natural regression loss and experimental results on several benchmark datasets.", "recorded": "2009-09-09T15:00:00", "title": "Kernel Polytope Faces Pursuit"}, {"url": "smartdw09_wang_lmsp", "desc": "This paper presents a novel learning algorithm for structured classification, where the task is to predict multiple and interacting labels\r\n(multilabel) for an input object. The problem of finding a large-margin separation between correct multilabels and incorrect ones is formulated as a linear program. Instead of explicitly writing out the entire problem with an exponentially large constraint set, the linear program is solved iteratively via column generation. In this case, the process of generating most violated constraints is equivalent to searching for highest-scored\r\nmisclassified incorrect multilabels, which can be easily achieved by decoding the structure based on current estimations. In addition, we\r\nalso explore the integration of column generation and an extragradient method for linear programming to gain further efficiency. The proposed method has the advantages that it can handle arbitrary structures and\r\nlarger-scale problems. Experimental results on part-of-speech tagging and statistical machine translation tasks are reported, demonstrating the competitiveness of our approach.", "recorded": "2009-05-13T16:00:00", "title": "Large-Margin Structured Prediction via Linear Programming"}, {"url": "iswc2014_yamada_linkify", "desc": "We frequently encounter unfamiliar entity names (e.g., a person\u2019s\r\nname or a geographic location) while reading texts such as newspapers,\r\nmagazines, and web pages. When it occurs, we typically perform\r\na sequence of wearisome actions: select the entity name, submit it to a\r\nsearch engine, and typically obtain detailed information from web sites.\r\nIn this paper, we propose Linkify, a novel tool that enhances text reading\r\nby automatically converting entity names into links and displaying\r\na synopsis of the entity retrieved from Linked Open Data when a user\r\nselects the link. The tool enables users to retrieve the information of\r\nthe entity simply by selecting the link. Further, in order to create only\r\nlinks that are helpful for users, we also developed a method that evaluates\r\nthe helpfulness of entities using a machine-learning algorithm with\r\na broad set of features. We have implemented our proposed tool as an\r\nadd-on for several major web browsers and made it publicly available at\r\nhttp://swc14.linkify.mobi.", "recorded": "2014-10-22T14:20:00", "title": "Linkify: Enhanced Reading Experience by Augmenting Text Using Linked Open Data"}, {"url": "mit6172f2010_performance_engineering", "desc": "Modern computing platforms provide unprecedented amounts of raw computational power. But significant complexity comes along with this power, to the point that making useful computations exploit even a fraction of the potential of the computing platform is a substantial challenge. Indeed, obtaining good performance requires a comprehensive understanding of all layers of the underlying platform, deep insight into the computation at hand, and the ingenuity and creativity required to obtain an effective mapping of the computation onto the machine. The reward for mastering these sophisticated and challenging topics is the ability to make computations that can process large amount of data orders of magnitude more quickly and efficiently and to obtain results that are unavailable with standard practice.\r\n\r\nThis class is a hands-on, project-based introduction to building scalable and high-performance software systems. Topics include performance analysis, algorithmic techniques for high performance, instruction-level optimizations, cache and memory hierarchy optimization, parallel programming, and building scalable distributed systems.", "recorded": "2010-09-09T09:00:00", "title": "6.172 Performance Engineering of Software Systems, Fall 2010 "}, {"url": "ecmlpkdd09_moschitti_ssknlid", "desc": "A core problem in data mining is to retrieve data in a easy and human friendly way. Automatically translating natural language questions into SQL queries would allow for the design of effective and useful database systems from a user viewpoint. Interesting previous work has been focused on the use of machine learning algorithms for automatically mapping natural language (NL) questions to SQL queries. In this paper, we present many structural kernels and their combinations for inducing the relational semantics between pairs of NL questions and SQL queries. We measure the effectiveness of such kernels by using them in  Support Vector Machines to select the queries that correctly answer to NL questions. Experimental results on two different datasets show that our approach is viable and that syntactic information under the form of pairs of\r\nsyntactic tree fragments (from queries and questions) plays a major role in deriving the relational semantics between the two languages.\r\n\r\n\r\n", "recorded": "2009-09-08T14:25:00", "title": "Syntactic Structural Kernels for Natural Language Interfaces to Databases"}, {"url": "ilpmlgsrl09_yu_slmain", "desc": "With the ubiquity of information networks and their broad applications, there have been numerous studies on the construction, online analytical processing, and mining of information networks in multiple disciplines, including social network analysis, World-Wide Web, database systems, data mining, machine learning, and networked communication and information systems. Algorithms like PageRank and HITS have been developed in late 1990s to explore links among Web pages to discover authoritative pages and hubs. Links have also been popularly used in citation analysis and social network analysis. However, there is a lack of systematic treatment on how to fully explore the power of links in scalable data analysis. In this talk, the power of links are examined in details to improve the effectiveness and efficiency of typical data analysis tasks, including information integration, on-line analytic processing, and other interesting data mining tasks, especially in the multi-relational databases and/or the World-Wid e Web environments.", "recorded": "2009-07-02T09:00:00", "title": "Scalable Link Mining and Analysis on Information Networks"}, {"url": "mlmi04uk_chu_asrsa", "desc": "An important step to bring speech technologies into wide deployment as a functional component in man-machine interfaces is to free the users from close-talk or desktop microphones, and enable far-field operation in various natural communication environments. In this work, we consider far-field automatic speech recognition and speech activity detection in conference rooms. The experiments are conducted on the smart room platform provided by the CHIL project.&#160; **The first half** of the paper addresses the development of speech recognition systems for the seminar transcription task. In particular, we look into the effect of combining parallel recognizers in both single-channel and multi-channel settings.&nbsp; **In the second half** of the paper, we describe a novel algorithm for speech activity detection based on fusing phonetic likelihood scores and energy features. It is shown that the proposed technique is able to handle non-stationary noise events and achieves good performance on the CHIL seminar corpus.", "recorded": "2005-06-12T12:00:00", "title": "Automatic Speech Recognition and Speech Activity Detection in the CHIL Smart Room"}, {"url": "ecmlpkdd2010_verwer_tam", "desc": "In this paper, we investigate how to modify the naive Bayes classifier in order to perform classification that is restricted to be independent with respect to a given sensitive attribute. Such independency restrictions occur naturally when the decision process leading to the labels in the data-set was biased; e.g., due to gender or racial discrimination. This setting is motivated by many cases in which there exist laws that disallow a decision that is partly based on discrimination. Naive application of machine learning techniques would result in huge fines for companies. We present three approaches for making the naive Bayes classifier discrimination-free: (i) modifying the probability of the decision being positive, (ii) training one model for every sensitive attribute value and balancing them, and (iii) adding a latent variable to the Bayesian model that represents the unbiased label and optimizing the model parameters for likelihood using expectation maximization. We present experiments for the three approaches on both artificial and real-life data. ", "recorded": "2010-09-21T10:50:00", "title": "Three Approaches for Making the Naive Bayes Classifier Discrimination-Free"}, {"url": "colt2015_chen_information_maximization", "desc": "Optimal information gathering is a central challenge in machine learning and science in general. A common objective that quantifies the usefulness of observations is Shannon's mutual information, defined w.r.t. a probabilistic model. Greedily selecting observations that maximize the mutual information is the method of choice in numerous applications, ranging from Bayesian experimental design to automated diagnosis, to active learning in Bayesian models. Despite its importance and widespread use in applications, little is known about the theoretical properties of sequential information maximization, in particular under noisy observations. In this paper, we analyze the widely used greedy policy for this task, and identify problem instances where it provides provably near-maximal utility, even in the challenging setting of persistent noise. Our results depend on a natural separability condition associated with a channel injecting noise into the observations. We also show that this separability condition is necessary: If it does not hold, the greedy policy can fail to select informative observations.", "recorded": "2015-07-05T10:00:00", "title": "Sequential Information Maximization: When is Greedy Near-optimal?"}, {"url": "icwsm2013_boston", "desc": "The International AAAI Conference on Weblogs and Social Media (ICWSM) is a unique forum bringing together researchers working at the nexus of computer science and the social sciences, with work drawing upon network science, machine learning, computational linguistics, sociology and communication. The broad goal of ICWSM is to increase understanding of social media in all its incarnations. Submissions describing research that blends social science and computational approaches are especially encouraged.\r\n\r\nThis conference, now in its seventh year, has become one of the premier venues for social scientists and technologists to gather and discuss cutting-edge research in social media. This is largely due to a typical acceptance rate of 20% for full-length research papers published in our conference proceedings and support from the [[http://www.aaai.org/home.html|Association for the Advancement of Artificial Intelligence (AAAI)]].\r\n\r\nDetailed program can be found at the [[http://icwsm.org/2013/|ICWSM 2013 website]].", "recorded": "2013-07-08T09:00:00", "title": "7th International AAAI Conference on Weblogs and Social Media (ICWSM), Boston 2013"}, {"url": "aistats2011_larochelle_neural", "desc": "We describe a new approach for modeling the distribution of high-dimensional vectors of discrete variables. This model is inspired by the restricted Boltzmann machine (RBM), which has been shown to be a powerful model of such distributions. However, an RBM typically does not provide a tractable distribution estimator, since evaluating the probability it assigns to some given observation requires the computation of the so-called partition function, which itself is intractable for RBMs of even moderate size. Our model circumvents this difficulty by decomposing the joint distribution of observations into tractable conditional distributions and modeling each conditional using a non-linear function similar to a conditional of an RBM. Our model can also be interpreted as an autoencoder wired such that its output can be used to assign valid probabilities to observations. We show that this new model outperforms other multivariate binary distribution estimators on several datasets and performs similarly to a large (but intractable) RBM.", "recorded": "2011-04-13T17:00:00", "title": "The Neural Autoregressive Distribution Estimator, incl. discussion by Yoshua Bengio"}, {"url": "pmteew05_london", "desc": "Traditional off-line learning methods are often not appropriate for applications in user modelling and user interfaces since to be useful the system must learn about the user or context during the process of interaction 'on the fly'. This immediately raises the fundamental problem of trading off exploration and exploitation in that as information is learnt the system may be tempted to act in line with this insight rather than further exploring alternatives. Machine learning has developed a number of models that attempt to capture and analyse this trade-off, from the simplest bandit problem to the full Markov decision processes underlying reinforcement learning. \n\nThe workshop includes tutorials covering the bandit analysis as well as its relevance to user modelling. Reinforcement learning would also be included with particular emphasis on applications in user interfaces. It is also hoped to launch a challenge in this area. This workshop comes under the Thematic Programme 4: Online User Modelling and Reinforcement Learning and is a core meeting of the PASCAL Network.", "recorded": "2005-07-06T00:00:00", "title": "Workshop on Principled Methods of Trading Exploration and Exploitation London, 2005"}, {"url": "ecmlpkdd2011_obradovic_annotators", "desc": "Supervised learning from multiple annotators is an increasingly important problem in machine leaning and data mining. This paper develops a probabilistic approach to this problem when annotators are not only unreliable, but also have varying performance depending on the data. The proposed approach uses a Gaussian mixture model (GMM) and Bayesian information criterion (BIC) to find the fittest model to approximate the distribution of the instances. Then the maximum a posterior (MAP) estimation of the hidden true labels and the maximum-likelihood (ML) estimation of quality of multiple annotators are provided alternately. Experiments on emotional speech classification and CASP9 protein disorder prediction tasks show performance improvement of the proposed approach as compared to the majority voting baseline and a previous data-independent approach. Moreover, the approach also provides more accurate estimates of individual annotators performance for each Gaussian component, thus paving the way for understanding the behaviors of each annotator.", "recorded": "2011-09-07T18:12:00", "title": "Learning from Inconsistent and Unreliable Annotators by a Gaussian Mixture Model and Bayesian Information Criterion"}, {"url": "ecmlpkdd09_zhang_ssmtr", "desc": "Labeled data are needed for many machine learning applications but the amount available in some applications is scarce. Semi-supervised learning and multi-task learning are two of the approaches that have been proposed to alleviate this problem. In this paper, we seek to integrate these two approaches for regression applications. We first propose a new supervised multi-task regression method called SMTR, which is based on Gaussian processes (GP) with the assumption that the kernel parameters for all tasks share a common prior. We then incorporate unlabeled data into SMTR by changing the kernel function of the GP prior to a data-dependent kernel function, resulting in a semi-supervised extension of SMTR, called SSMTR. Moreover, we incorporate pairwise information into SSMTR to further boost the learning performance for applications in which such information is available. Experiments conducted on two commonly used data sets for multi-task regression demonstrate the effectiveness of our methods.", "recorded": "2009-09-09T11:50:00", "title": "Semi-Supervised Multi-Task Regression"}, {"url": "solomon_sure_stsw", "desc": "The availability of electronically stored information increased\r drastically through the development of the World Wide Web.\r Currently the WWW contains more than a billion documents, but support for accessing and precessing information is limited. Most information is only presentable but not understandable by computers. Tim Berners-Lee envisioned the Semantic Web that aims at providing automated access to information due to machine-processable semantics of data. Ontologies formalize a shared understanding of a domain and therefore play a crucial role for communication among human beings and software agents.\r \r \r \r We will present the underlying ideas of the Semantic Web and will shortly introduce ontologies as the backbone of the Semantic Web. Further we will show how much effort is necessary to setup the Semantic Web and how tools can support this process. Additionally Web and Data Mining techniques can be used to bootstrap the Semantic Web. The idea of Semantic Web Mining is to improve the results of Web Mining by exploiting the new semantic structures in the web.", "recorded": "2003-06-17T13:00:00", "title": "A short Tutorial on Semantic Web"}, {"url": "ecmlpkdd09_pottenger_lhodbftc", "desc": "Traditional machine learning methods only consider relationships between feature values within individual data instances while disregarding the dependencies that link features across instances. In this work, we develop a general approach to supervised learning by leveraging higher-order dependencies between features. We introduce a novel Bayesian framework for classification named Higher Order Naive Bayes (HONB). Unlike approaches that assume data instances are independent, HONB leverages co-occurrence relations between feature values across different instances. Additionally, we generalize our framework by developing a novel data-driven space transformation that allows any classifier operating in vector spaces to take advantage of these higher-order co-occurrence relations. Results obtained on several benchmark\r\ntext corpora demonstrate that higher-order approaches achieve significant improvements in classification accuracy over the baseline (first-order) methods.", "recorded": "2009-09-09T11:25:00", "title": "Leveraging Higher Order Dependencies Between Features for Text Classification"}, {"url": "icml2015_du_plessis_convex_formulation", "desc": "We discuss binary classification from only from positive and unlabeled data (PU classification), which is conceivable in various real-world machine learning problems. Since unlabeled data consists of both positive and negative data, simply separating positive and unlabeled data yields a biased solution. Recently, it was shown that the bias can be canceled by using a particular non-convex loss such as the ramp loss. However, classifier training with a non-convex loss is not straightforward in practice. In this paper, we discuss a convex formulation for PU classification that can still cancel the bias. The key idea is to use different loss functions for positive and unlabeled samples. However, in this setup, the hinge loss is not permissible. As an alternative, we propose the double hinge loss. Theoretically, we prove that the estimators converge to the optimal solutions at the optimal parametric rate. Experimentally, we demonstrate that PU classification with the double hinge loss performs as accurate as the non-convex method, with a much lower computational cost.", "recorded": "2015-07-08T14:46:53", "title": "Convex Formulation for Learning from Positive and Unlabeled Data"}, {"url": "oiml05_opper_aecai", "desc": "I will discuss two types of applications of an approximate inference technique (EC = expectation consistent) recently developed together with Ole Winther. The EC method is an extension of the TAP (Thouless, Anderson & Palmer) approach which originated in the field of disordered materials and which has been further developed to become applicable to a variety of scenarios in probabilistic modelling & machine laerning. My first application (joint work with Doerthe Malzahn) deals with an approximation to resampling methods (such as the bootstrap) which allows to estimate eg generalization errors in supervised learning. While the exact resampling approach requires the drawing of many samples from the training data and a costly repeated retraining of the model, the approximation attempts an analytic average which combines the replica trick and an inference method which can be performed much faster. In the second application (ongoing work) I discuss the scenario of many solutions to the EC framework and the possibility of averaging them using Parisi's hierarchical scheme.", "recorded": "2005-01-22T00:00:00", "title": "Application of expectation consistent approximate inference"}, {"url": "icwsm2013_chen_filtering_tweets", "desc": "Many consumer brands have customer relationship agents that directly engage opinionated consumers on social streams, such as Twitter. To help agents find opinionated consumers, social stream monitoring tools provide keyword-based filters, which are often too coarse-grained to be effective. In this work, we introduce CrowdE, a Twitter-based filtering system that helps agents find opinionated customers through brand-specific intelligent filters. To minimize per-brand effort in creating these brand-specific filters, the system used a common crowd-enabled process that creates the filters through machine learning over crowd-labeled tweets. We validated the quality of the crowd labels and the performance of the filter algorithms built from the labels. A user evaluation further showed that CrowdE's intelligent filters improved task performance and were generally preferred by users in comparison to keyword-based filters in current social stream monitoring tools.", "recorded": "2013-07-10T12:19:00", "title": "CrowdE: Filtering Tweets for Direct Customer Engagements"}, {"url": "ecmlpkdd2011_wartell_code", "desc": "Robust, static disassembly is an important part of achieving\r\nhigh coverage for many binary code analyses, such as reverse engineering,\r\nmalware analysis, reference monitor in-lining, and software fault isola-\r\ntion. However, one of the major di\u000eculties current disassemblers face\r\nis di\u000berentiating code from data when they are interleaved. This paper\r\npresents a machine learning-based disassembly algorithm that segments\r\nan x86 binary into subsequences of bytes and then classi\fes each subse-\r\nquence as code or data. The algorithm builds a language model from a\r\nset of pre-tagged binaries using a statistical data compression technique.\r\nIt sequentially scans a new binary executable and sets a breaking point\r\nat each potential code-to-code and code-to-data/data-to-code transition.\r\nThe classi\fcation of each segment as code or data is based on the min-\r\nimum cross-entropy. Experimental results are presented to demonstrate\r\nthe e\u000bectiveness of the algorithm.", "recorded": "2011-09-06T07:00:00", "title": "Differentiating Code from Data in x86 Binaries"}, {"url": "iswc08_swcbtc", "desc": "The central idea of the Semantic Web is to extend the current human-readable web by encoding some of the semantics of resources in a machine-processable form. Moving beyond syntax opens the door to more advanced applications and functionality on the Web. Computers will be better able to search, process, integrate and present the content of these resources in a meaningful, intelligent manner.\r\n\r\nThe core technological building blocks are now in place and widely available: ontology languages, flexible storage and querying facilities, reasoning engines, etc. Standards and guidelines for best practice are being formulated and disseminated by the W3C.\r\n\r\nThe Semantic Web Challenge offers participants the chance to show the best of the Semantic Web. The Challenge thus serves several purposes:\r\n\r\n    * Helps us illustrate to society what the Semantic Web can provide\r\n    * Gives researchers an opportunity to showcase their work and compare it to others\r\n    * Stimulates current research to a higher final goal by showing the state-of-the-art every year\r\n", "recorded": "2008-10-29T16:00:00", "title": "Semantic Web Challenge & Billion Triple Challenge "}, {"url": "mitworld_dyson_aoe", "desc": "Thank goodness some inventors specialize in ways to make our lives a little easier, especially around the house. Borrowing a page from Buckminster Fuller, one of his heroes, James Dyson \u2018sees what needs to be done and just does it.\u2019\r\n\r\nIn a show-and-tell format, Dyson offers a compendium of his labor-saving, ingenious designs, with side bars on business and engineering details. While Dyson has come up with washing machines, wheelbarrows and boats, the lion\u2019s share of his presentation concerns his Dual Cyclone vacuum cleaner, which tidily eliminates several nasty aspects of ordinary vacuum cleaners: loss of suction, bag-changing and the emission of dirty air. \u201cAs a child, I remembered the screaming noise, smelling stale dust and picking things up in my hands the vacuum wouldn\u2019t suck up.\u201d\r\n\r\nIn the late 1970s, Dyson noticed a cyclone tower for removing particles at a saw mill and was inspired to mock up a vacuum cleaner based on the same principles. It took 5, 127 prototypes and almost five years to build. \u201cThe 5, 126th failure taught me so much. Making mistakes is the most important thing you can do,\u201d says Dyson. \u201cYou don\u2019t get things by sitting at a drawing board or lying in a bath.\u201d\r\n\r\nHis efforts to manufacture, produce and market his invention were nearly as inventive as the technology inside the vacuum. He provoked a mail order company into including the Dyson machine on its pages (\u201cI said the catalog was boring\u201d), and then proceeded to ignore the results of market research and retail opinion, including a transparent bin on the machine so users could watch the dirt collect. \u201cEngineers like dirt,\u201d says Dyson.\r\n\r\nDyson insists on getting things right, before and after product launch. 150 people in Malaysia run vacuums through obstacle courses, hitting them with hammers. \u201cIf it breaks, we can often make simple design changes,\u201d which is why, he says, \u201cwe have the lowest return rate of any vacuum cleaner in the U.S.\u201d A new version of the vacuum will be able to communicate directly to Dyson headquarters through a cell phone and let the company know if something goes wrong with it.", "recorded": "2006-04-26T21:54:05", "title": "The Art of Engineering"}, {"url": "nipsworkshops09_probabilistic_approaches", "desc": "**Probabilistic Approaches for Robotics and Control**\r\n\r\nDuring the last decade, many areas of Bayesian machine learning have reached a high level of maturity. This has resulted in a variety of theoretically sound and efficient algorithms for learning and inference in the presence of uncertainty. However, in the context of control, robotics, and reinforcement learning, uncertainty has not yet been treated with comparable rigor despite its central role in risk-sensitive control, sensori-motor control, robust control, and cautious control. A consistent treatment of uncertainty is also essential when dealing with stochastic policies, incomplete state information, and exploration strategies. A typical situation where uncertainty comes into play is when the exact state transition dynamics are unknown and only limited or no expert knowledge is available and/or affordable. One option is to learn a model from data. However, if the model is too far off, this approach can result in arbitrarily bad solutions. This model bias can be sidestepped by the use of flexible model-free methods. The disadvantage of model-free methods is that they do not generalize and often make less efficient use of data. Therefore, they often need more trials than feasible to solve a problem on a real-world system. A probabilistic model could be used for efficient use of data while alleviating model bias by explicitly representing and incorporating uncertainty. The use of probabilistic approaches requires (approximate) inference algorithms, where Bayesian machine learning can come into play. Although probabilistic modeling and inference conceptually fit into this context, they are not widespread in robotics, control, and reinforcement learning. Hence, this workshop aims to bring researchers together to discuss the need, the theoretical properties, and the practical implications of probabilistic methods in control, robotics, and reinforcement learning. One particular focus will be on probabilistic reinforcement learning approaches that profit recent developments in optimal control which show that the problem can be substantially simplified if certain structure is imposed. The simplifications include linearity of the (Hamilton-Jacobi) Bellman equation. The duality with Bayesian estimation allow for analytical computation of the optimal control laws and closed form expressions of the optimal value functions. \r\n\r\n----\r\nThe Workshop homepage can be found at http://mlg.eng.cam.ac.uk/marc/nipsWS09.\r\n----", "recorded": "2009-12-11T07:30:00", "title": "Probabilistic Approaches"}, {"url": "s3mr2011_hua_tagging", "desc": "Automatically converting visual content into textual description has long been a dream of a number of multimedia, computer vision and machine learning researchers. Image and video tagging, which has been studied heavily in recently years, can be regarded as a more realistic step to that ambitious goal. Especially, the explosion of media data and media users on the Internet, as well as the connections among users, among data and between users and data, bring us both challenges and opportunities. In this lecture, firstly we will review the evolution of multimedia tagging in the past decade, and then introduce state-of-the-art learning based tagging approaches, followed by summarizing manual tagging schemes on the Internet environment and presenting Internet-scale data-driven methods for scalable image and video tagging. Finally we will discuss the roles of models, data and users in multimedia tagging systems and study an sustainable ecosystem for multimedia tagging on the Internet environment. We will also discuss promising research and development directions in this area.", "recorded": "2011-06-27T09:00:00", "title": "Image and Video Tagging in the Internet Era"}, {"url": "eswc2015_kejriwal_boosted_classifiers", "desc": "Instance matching concerns identifying pairs of instances\r\nthat refer to the same underlying entity. Current state-of-the-art instance\r\nmatchers use machine learning methods. Supervised learning systems\r\nachieve good performance by training on significant amounts of manually\r\nlabeled samples. To alleviate the labeling effort, this paper presents a\r\nminimally supervised instance matching approach that is able to deliver\r\ncompetitive performance using only 2% training data and little parameter\r\ntuning. As a first step, the classifier is trained in an ensemble setting\r\nusing boosting. Iterative semi-supervised learning is used to improve the\r\nperformance of the boosted classifier even further, by re-training it on\r\nthe most confident samples labeled in the current iteration. Empirical\r\nevaluations on a suite of six publicly available benchmarks show that the\r\nproposed system outcompetes optimization-based minimally supervised\r\napproaches in 1\u20137 iterations. The system\u2019s average F-Measure is shown\r\nto be within 2.5% of that of recent supervised systems that require more\r\ntraining samples for effective performance.", "recorded": "2015-06-03T14:00:00", "title": "Semi-supervised Instance Matching Using Boosted Classifiers"}, {"url": "eswc2015_krishnamurthy_data_sources", "desc": "There is a huge demand to be able to find and integrate\r\nheterogeneous data sources, which requires mapping the attributes of a\r\nsource to the concepts and relationships defined in a domain ontology. In\r\nthis paper, we present a new approach to find these mappings, which we\r\ncall semantic labeling. Previous approaches map each data value individually,\r\ntypically by learning a model based on features extracted from the\r\ndata using supervised machine-learning techniques. Our approach differs\r\nfrom existing approaches in that we take a holistic view of the data values\r\ncorresponding to a semantic label and use techniques that treat this data\r\ncollectively, which makes it possible to capture characteristic properties\r\nof the values associated with a semantic label as a whole. Our approach\r\nsupports both textual and numeric data and proposes the top k semantic\r\nlabels along with their associated confidence scores. Our experiments\r\nshow that the approach has higher label prediction accuracy, has lower\r\ntime complexity, and is more scalable than existing systems.", "recorded": "2015-06-03T11:30:00", "title": "Assigning Semantic Labels to Data Sources"}, {"url": "eswc09_stuckenschmidt_iomu", "desc": "Despite serious research efforts, automatic ontology matching still suffers from severe problems with respect to the quality of matching results. Existing matching systems trade-off precision and recall and have their specific strengths and weaknesses. This leads to problems when the right matcher for a given task has to be selected. In this paper, we present a method for improving matching results by not choosing a specific matcher but applying machine learning techniques on an ensemble of matchers. Hereby we learn rules for the correctness of a correspondence based on the output of different matchers and additional information about the nature of the elements to be matched, thus leveraging the weaknesses of an individual matcher. We show that our method always performs significantly better than the median of the matchers used and in most cases outperforms the best matcher with an optimal threshold for a given pair of ontologies. As a side product of our experiments, we discovered that the majority vote is a simple but powerful heuristic for combining matchers that almost reaches the quality of our learning results.", "recorded": "2009-06-03T11:00:00", "title": "Improving Ontology Matching using Meta-level Learning"}, {"url": "nips2012_salakhutdinov_multimodal_learning", "desc": "We propose a Deep Boltzmann Machine for learning a\r\ngenerative model of multimodal data. We show how to use the\r\nmodel to extract a meaningful representation of multimodal\r\ndata. We find that the learned representation is useful for\r\nclassification and information retreival tasks, and hence\r\nconforms to some notion of semantic similarity. The model\r\ndefines a probability density over the space of multimodal\r\ninputs. By sampling from the conditional distributions over\r\neach data modality, it possible to create the representation\r\neven when some data modalities are missing. Our\r\nexperimental results on bi-modal data consisting of images\r\nand text show that the Multimodal DBM can learn a good\r\ngenerative model of the joint space of image and text inputs\r\nthat is useful for information retrieval from both unimodal and\r\nmultimodal queries. We further demonstrate that our model\r\ncan significantly outperform SVMs and LDA on discriminative\r\ntasks. Finally, we compare our model to other deep learning\r\nmethods, including autoencoders and deep belief networks,\r\nand show that it achieves significant gains.", "recorded": "2012-12-04T17:00:00", "title": "Multimodal Learning with Deep Boltzmann Machines"}, {"url": "icml08_salakhutdinov_qadb", "desc": "Deep Belief Networks (DBN's) are generative models that contain many layers of hidden variables. Efficient greedy algorithms for learning and approximate inference have allowed these models to be applied successfully in many application domains. The main building block of a DBN is a bipartite undirected graphical model called a restricted Boltzmann machine (RBM). Due to the presence of the partition function, model selection, complexity control, and exact maximum likelihood learning in RBM's are intractable. Annealed Importance Sampling (AIS), can be used to efficiently estimate the partition function of an RBM. We present a novel AIS scheme for comparing RBM's with different architectures. We further show how an AIS estimator, along with approximate inference, can be used to estimate a lower bound on the log-probability that a DBN model with multiple hidden layers assigns to the test data. This is, to our knowledge, the first step towards obtaining quantitative results that would allow us to directly assess the performance of Deep Belief Networks as generative models of data.", "recorded": "2008-07-08T09:25:19", "title": "On the Quantitative Analysis of Deep Belief Networks"}, {"url": "icml2015_gittens_spectral_clustering", "desc": "Spectral clustering is one of the most important algorithms in data mining and machine intelligence; however, its computational complexity limits its application to truly large scale data analysis. The computational bottleneck in spectral clustering is computing a few of the top eigenvectors of the (normalized) Laplacian matrix corresponding to the graph representing the data to be clustered. One way to speed up the computation of these eigenvectors is to use the \"power method\" from the numerical linear algebra literature. Although the power method has been empirically used to speed up spectral clustering, the theory behind this approach, to the best of our knowledge, remains unexplored. This paper provides the \\emph{first} such rigorous theoretical justification, arguing that a small number of power iterations suffices to obtain near-optimal partitionings using the approximate eigenvectors. Specifically, we prove that solving the k-means clustering problem on the approximate eigenvectors obtained via the power method gives an additive-error approximation to solving the k-means problem on the optimal eigenvectors.", "recorded": "2015-07-09T14:46:53", "title": "Spectral Clustering via the Power Method - Provably"}, {"url": "icml09_neumann_lcm", "desc": "Abstraction of complex, longer motor tasks into simpler elemental movements enables humans and animals to exhibit motor skills which have not yet been matched by robots. Humans intuitively decompose complex motions into smaller, simpler segments. For example when describing simple movements like drawing a triangle with a pen, we can easily name the basic steps of this movement.\r\n\r\nSurprisingly, such abstractions have rarely been used in artificial motor skill learning algorithms. These algorithms typically choose a new action (such as a torque or a force) at a very fast time-scale. As a result, both policy and temporal credit assignment problem become unnecessarily complex - often beyond the reach of current machine learning methods.\r\n\r\nWe introduce a new framework for temporal abstractions in reinforcement learning (RL), i.e. RL with motion templates. We present a new algorithm for this framework which can learn high-quality policies by making only few abstract decisions.", "recorded": "2009-06-17T14:40:00", "title": "Learning Complex Motions by Sequencing Simpler Motion Templates"}, {"url": "nipsworkshops2011_shamir_convex", "desc": "Stochastic gradient descent (SGD) is a simple and popular method to solve\r\nstochastic optimization problems which arise in machine learning. For strongly\r\nconvex problems, its convergence rate was known to be O(log(T)/T), by running\r\nSGD for T iterations and returning the average point. However, recent results\r\nshowed that using a different algorithm, one can get an optimal O(1/T)\r\nrate. This might lead one to believe that standard SGD is suboptimal, and maybe\r\nshould even be replaced as a method of choice. In this paper, we investigate the\r\noptimality of SGD in a stochastic setting. We show that for smooth problems, the\r\nalgorithm attains the optimal O(1/T) rate. However, for non-smooth problems,\r\nthe convergence rate with averaging might really be \r\n(log(T)/T), and this is not\r\njust an artifact of the analysis. On the flip side, we show that a simple modification\r\nof the averaging step suffices to recover the O(1/T) rate, and no other change of\r\nthe algorithm is necessary. We also present experimental results which support\r\nour findings, and point out open problems.", "recorded": "2011-12-16T17:50:00", "title": "Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization"}, {"url": "ijcai2011_t16_mining", "desc": "The Web continues to grow and evolve very fast, changing our daily lives. This activity represents the collaborative work of the millions of institutions and people that contribute content to the Web as well as the one billion people that use it. In this ocean of hyperlinked data there is explicit and implicit information and knowledge. Web Mining is the task of analyzing this data and extracting information and knowledge for many different purposes. The data comes in three main flavors: content (text, images, etc.), structure (hyperlinks) and usage (navigation, queries, etc.), implying different techniques such as text, graph or log mining. Each case reflects the wisdom of some group of people that can be used to make the Web better, for example, user generated tags in Web 2.0 sites. In this tutorial we will walk through the mining process and will show several applications, ranging from Web site design to search engines. The main goal is to introduce AI researchers to the myriad of challenges in Web mining, where other AI techniques, in addition to machine learning, might be applicable.", "recorded": "2011-07-17T00:00:00", "title": "Web Mining"}, {"url": "icml2010_balcan_lwsf", "desc": "Kernel functions have become an extremely popular tool in machine learning, with many applications and an attractive theory. This theory views a kernel as performing an implicit mapping of data points into a possibly very high dimensional space, and describes a kernel function as being good for a given learning problem if data is separable by a large margin in that implicit space. In this talk I will describe an alternative, more general, theory of learning with similarity functions (i.e., sufficient conditions for a similarity function to allow one to learn well) that does not require reference to implicit spaces, and does not require the function to be positive semi-definite (or even symmetric). \r\nIn particular, I will describe a notion of a good similarity function for a given learning problem that (a) is fairly natural and intuitive (it does not require an implicit space and allows for functions that are not positive semi-definite), (b) is a sufficient condition for learning well, and (c) strictly generalizes the notion of a large-margin kernel function in that any such kernel is also a good similarity function, though not necessarily vice-versa. ", "recorded": "2010-06-25T09:50:00", "title": "Learning with similarity functions"}, {"url": "mlsb2012_moreau_kernel", "desc": "Despite significant advances in omics techniques, the identification of genes causing rare \r\ngenetic diseases and the understanding of the molecular  networks  underlying  those \r\ndisorders remains difficult. Gene prioritization attempts to integrate multiple, \r\nheterogeneous data sources to identify candidate genes most likely to be associated with \r\nor causative for a disorder. Such strategies are useful both to support clinical genetic \r\ndiagnosis and to speed up biological discovery. Genomic data fusion algorithms are \r\nrapidly maturing statistical and machine learning techniques have emerged that \r\nintegrate complex, heterogeneous information (such as sequence similarity, interaction \r\nnetworks, expression data, annotation, or biomedical literature) towards prioritization, \r\nclustering, or prediction. In this talk, we will focus in particular on kernel methods and \r\nwill propose several strategies for prioritization and clustering in particular. We also go \r\nbeyond learning methods as such by addressing how such strategies can be embedded \r\ninto the daily practice of geneticists, mostly through collaborative knowledge bases that \r\nintegrate tightly with prioritization and network analysis methods.", "recorded": "2012-09-08T13:00:00", "title": "Kernel methods for genomic data fusion"}, {"url": "ecmlpkdd2011_ding_subspace", "desc": "This paper presents the multi-subspace discovery problem and provides a theoretical solution which is guaranteed to recover the number of subspaces, the dimensions of each subspace, and the members of data points of each subspace simultaneously. We further propose a data representation model to handle noisy real world data. We develop a novel optimization approach to learn the presented model which is guaranteed to converge to global optimizers. As applications of our models, we first apply our solutions as preprocessing in a series of machine learning problems, including clustering, classification, and semisupervised learning. We found that our method automatically obtains robust data presentation which preserves the affine subspace structures of high dimensional data and generate more accurate results in the learning tasks. We also establish a robust standalone classifier which directly utilizes our sparse and low rank representation model. Experimental results indicate our methods improve the quality of data by preprocessing and the standalone classifier outperforms some state-of-the-art learning approaches.", "recorded": "2011-09-06T17:40:00", "title": "Multi-Subspace Representation and Discovery"}, {"url": "mlss06tw_su_pslpb", "desc": "Prediction of subcellular localization of proteins is important for genome annotation, protein function prediction, and drug discovery. We present a prediction method for Gram-negative bacteria that uses ten one-versus-one support vector machine (SVM) classifiers, where compartment-specific biological features are selected as input to each SVM classifier. The final prediction of localization sites is determined by inte-grating the results from ten binary classifiers using a combination of majority votes and a probabilistic method. The overall accuracy reaches 91.4%, which is 1.6% better than the state-of-the-art system, in a ten-fold cross-validation evaluation on a bench-mark data set. We demonstrate that feature selection guided by biological knowledge and insights in one-versus-one SVM classifiers can lead to a significant improvement in the prediction performance. Our model is also used to produce highly accurate prediction of 92.8% overall accuracy for proteins of dual localizations.", "recorded": "2006-07-27T00:00:00", "title": "Protein Subcellular Localization Prediction Based on Compartment-Specific Biological Features"}, {"url": "eswc2013_abedjan_synonym", "desc": "Despite unified data models, such as the Resource Description Framework (Rdf) on structural level and the corresponding query language Sparql, the integration and usage of Linked Open Data faces major heterogeneity challenges on the semantic level. Incorrect use of ontology concepts and class properties impede the goal of machine readability and knowledge discovery. For example, users searching for movies with a certain artist cannot rely on a single given property artist, because some movies may be connected to that artist by the predicate starring. In addition, the information need of a data consumer may not always be clear and her interpretation of given schemata may differ from the intentions of the ontology engineer or data publisher. It is thus necessary to either support users during query formulation or to incorporate implicitly related facts through predicate expansion. To this end, we introduce a data-driven synonym discovery algorithm for predicate expansion. We applied our algorithm to various data sets as shown in a thorough evaluation of different strategies and rule-based techniques for this purpose.\r\n", "recorded": "2013-05-28T15:00:00", "title": "Synonym Analysis for Predicate Expansion"}, {"url": "icml08_meka_rmv", "desc": "Minimum rank problems arise frequently in machine learning applications and are notoriously difficult to solve due to the non-convex nature of the rank objective. In this paper, we present the first online learning approach for the problem of rank minimization of matrices over polyhedral sets. In particular, we present two online learning algorithms for rank minimization - our first algorithm is a multiplicative update method based on a generalized experts framework, while our second algorithm is a novel application of the online convex programming framework [Zinkevich, 2003]. In the latter, we flip the role of the decision maker by making the decision maker search over the constraint space instead of feasible points, as is usually the case in online convex programming. A salient feature of our online learning approach is that it allows us to give the first provable approximation guarantees for the rank minimization problem over polyhedral sets. We demonstrate the effectiveness of our methods on synthetic examples, and on the real-life application of low-rank kernel learning.", "recorded": "2008-07-08T10:40:00", "title": "Rank Minimization via Online Learning"}, {"url": "prib2010_deridder_kmi", "desc": "Integrative bioinformatics focuses on the construction of approximate models of biological phenomena, such as gene regulation, protein interaction and complex formation, or protein function. Such models are based on a wealth of prior knowledge (databases, literature) and high-throughput measurement data available. A major challenge is how to combine these various sources of information, which often differ in data type, bias, coverage etc. \r\n\r\nOver the last decade, kernel methods have been increasingly employed to tackle such problems. Kernels can be used in many algorithms, including classification and regression (the support vector machine), dimensionality reduction and statistics. A large number of kernels specifically tailored for certain types of (biological) data are now available, and various methods have been proposed to combine kernels. \r\n\r\nIn this tutorial, we will introduce kernel-based predictive algorithms, discuss a number of kernels relevant to biological modeling and methods to integrate various kernels for prediction. We will end by discussing some applications of kernel combination to biological problems.\r\n", "recorded": "2010-09-22T11:30:00", "title": "Kernel methods for integrating biological data"}, {"url": "solomon_ghani_rceir", "desc": "Information Retrieval is a major component of Knowledge Management systems in every business but most of the research that is being done in IR today focuses on the Web and not on the needs and challenges of businesses. This is primarily due to the availability of data on the Web for academic researchers as well as familiarity with the problems in Web IR since all of us are consumers and can relate to the domain. In contrast, for Enterprise Information Retrieval, the data is not available to most researchers and the challenges and needs are not obvious to people who are not everyday users of such systems.\r \r In this talk, I will point out some challenges in this domain, pose open research questions for the Information Retrieval, NLP, Machine Learning & Data Mining communities, and describe the experimental infrastructure that is being set up at Accenture Technology Labs to undertake those\r challenges. Our experimental test-bed for Enterprise Knowledge Management Research has access to potentially 150,000 users in Accenture and will allow us to collaborate with researchers and solve large-scale Enterprise Information Retrieval problems. \r ", "recorded": "2008-03-14T13:00:00", "title": "Research Challenges in Enterprise Information Retrieval"}, {"url": "iswc06_lecue_clm", "desc": "Automated composition of Web services or the process of forming\r new value added Web services is one of the most promising challenges in the\r semantic Web service research area. Semantics is one of the key elements for\r the automated composition of Web services because such a process requires rich\r machine-understandable descriptions of services that can be shared. Semantics\r enables Web service to describe their capabilities and processes, nevertheless\r there is still some work to be done. Indeed Web services described at functional\r level need a formal context to perform the automated composition of Web services.\r The suggested model (i.e., Causal link matrix) is a necessary starting point\r to apply problem-solving techniques such as regression-based search for Web\r service composition. The model supports a semantic context in order to find a\r correct, complete, consistent and optimal plan as a solution. In this paper an innovative\r and formal model for an AI planning-oriented composition is presented.", "recorded": "2006-11-07T16:00:00", "title": "Research 7: Causal link matrix and AI planning: A model for Web service composition"}, {"url": "colt2013_meka_matching", "desc": "We give the first polynomial-time algorithm for agnostically learning any function of a constant number of halfspaces with respect to any log-concave distribution (for any constant accuracy parameter). This result was not known even for the case of PAC learning the intersection of two halfspaces. We give two very different proofs of this result. The first develops a theory of polynomial approximation for log-concave measures and constructs a low-degree L1 polynomial approximator for sufficiently smooth functions. The second uses techniques related to the classical moment problem to obtain sandwiching polynomials. Both approaches deviate significantly from known Fourier-based methods, where essentially all previous work required the underlying distribution to have some product structure. Additionally, we show that in the smoothed-analysis setting, the above results hold with respect to distributions that have sub-exponential tails, a property satisfied by many natural and well-studied distributions in machine learning.", "recorded": "2013-06-12T11:25:00", "title": "Learning Halfspaces Under Log-Concave Densities: Polynomial Approximations and Moment Matching"}, {"url": "kdd2014_murphy_knowledge_vault", "desc": "Recent years have witnessed a proliferation of large-scale knowledge bases, including Wikipedia, Freebase, YAGO, Microsoft's Satori, and Google's Knowledge Graph. To increase the scale even further, we need to explore automatic methods for constructing knowledge bases. Previous approaches have primarily focused on text-based extraction, which can be very noisy. Here we introduce Knowledge Vault, a Web-scale probabilistic knowledge base that combines extractions from Web content (obtained via analysis of text, tabular data, page structure, and human annotations) with prior knowledge derived from existing knowledge repositories. We employ supervised machine learning methods for fusing these distinct information sources. The Knowledge Vault is substantially bigger than any previously published structured knowledge repository, and features a probabilistic inference system that computes calibrated probabilities of fact correctness. We report the results of multiple studies that explore the relative utility of the different information sources and extraction methods.", "recorded": "2014-08-25T12:20:00", "title": "Knowledge Vault: A Web-Scale Approach to Probabilistic Knowledge Fusion"}, {"url": "ecmlpkdd2011_lombardi_intrinsic", "desc": "Most of the machine learning techniques suffer the \"curse of dimensionality\" effect when applied to high dimensional data. To face this limitation, a common preprocessing step consists in employing a dimensionality reduction technique. In literature, a great deal of research work has been devoted to the development of algorithms performing this task. Often, these techniques require as parameter the number of dimensions to be retained; to this aim, they need to estimate the \"intrinsic dimensionality\" of the given dataset, which refers to the minimum number of degrees of freedom needed to capture all the information carried by the data. Although many estimation techniques have been proposed, most of them fail in case of noisy data or when the intrinsic dimensionality is too high. In this paper we present a family of estimators based on the probability density function of the normalized nearest neighbor distance. We evaluate the proposed techniques on both synthetic and real datasets comparing their performances with those obtained by state of the art algorithms; the achieved results prove that the proposed methods are promising.", "recorded": "2011-09-07T16:20:00", "title": "Minimum Neighbor Distance Estimators of Intrinsic Dimension"}, {"url": "mlsb07_moreau_cgp", "desc": "The overwhelming amount of biological data makes the assignment of candidate genes to diseases and biological pathways a formidable challenge. We present ENDEAVOUR, a generally applicable computational methodology to prioritize candidate genes based on their similarity to case-specific reference gene sets. \r Unlike previous methods, ENDEAVOUR is capable of flexibly utilizing multiple data sets from diverse sources. It allows the modular incorporation of de novo generated data sets and integrates distinct prioritizations into a global ranking by applying order statistics. We first validate the overallperformance in a statistical cross validation of 29 diseases and 3 biological pathways. We validate a novel candidate for DiGeorge syndrome in a zebrafish model and present several new candidates for congenital heart disease. \r We extend the basic ENDEAVOUR methodology using data from multiple species (human, mouse, rat, drosophila and C. elegans). We also present an alternative machine learning methodology for gene prioritization using kernel methods for novelty detection that outperforms our previous results.", "recorded": "2007-09-24T14:30:00", "title": "Candidate gene prioritization by genomic data fusion"}, {"url": "rease_mika_mws", "desc": "This is a one-hour video recording of the presentation of Peter Mika at the KnowledgeWeb summer school 2007. It comprises he video synchronized with the slides (requires Flash) or the video alone.\r\n\r\nTable of Contents: \r\nMaking the Web searchable, or the Future of Web Search\r\nAbout Yahoo!\r\nYahoo! by numbers (April, 2007)\r\nMaking the Web searchable, or the Future of Web Search\r\nOverview\r\nMotivation\r\nHard searches\r\nExample?\r\nThe Semantic Web (1996-?)\r\nProblem: difficulties in deployment\r\nWeb 2.0 (2003-)\r\nMicroformats\r\nExample: hCard\r\nWikipedia infoboxes\r\nProblem: lack of foundations\r\nThesis: making the Web searchable\r\nSemantic Web 2.0\r\nGRDDL: microformats to RDF\r\nExample: openacademia.org and RDFa\r\nRDFa\r\nExample: machine tags\r\nZoneTag project (Yahoo! Research Berkeley)\r\nExample: Freebase\r\nWeb IR 2.0\r\nExample: folksonomies\r\nThe more complete picture\r\nMining and modelling folksonomies\r\nVision: ontology-based search\r\nIdeal world\r\nTechnical challenges\r\nSocial challenges\r\nExample: Technorati and microformats\r\nConclusion\r\nWhat is there to gain?\r\nQuestions?", "recorded": "2007-11-23T00:00:00", "title": "Making the Web Searchable"}, {"url": "w3cworkshop2011_filip_multilingualweb", "desc": "MLW-LT, an FP7 funded coordination action, is going to set up a W3C Working Group (WG) for standardizing metadata exchange between Web CMS, Localization Tools and Language Technologies. This session will open the public discussion of the WG Charter and encourage participation in the WG from outside of the initial EC funded consortium. The WG aims to address three major interoperability gaps in the multilingual web content lifecycle, namely between Deep Web meta-data and localization (L10n); Surface Web meta-data and Real time Machine Translation; and Deep Web meta-data and meta-data driven MT training. Addressing these gaps will include alignment with other existing and ongoing LT and L10n standardization activities; prominently W3C ITS and OASIS XLIFF TC effort, as XLIFF will be used for prototyping MLW-LT metadata round-trips in the three main scenarios outlined above.", "recorded": "2011-10-21T10:30:00", "title": "MultilingualWeb-LT: Meta-data interoperability between Web CMS, Localization tools and Language Technologies at the W3C"}, {"url": "w3cworkshop2011_heyn_translation", "desc": "The translation editor has seen major technological advances over the last years. Compared to classic translation memory applications, current systems allow expert users to double, if not triple, the amount of words translated. Whereas the key technology advances are in the area of sub-segment reuse and statistical machine translation (SMT), the actual productivity gains relate to the ergonomics of how systems allow users to interact, control and automate the various data sources. This presentation will review key capabilities on the various document, segment and sub-segment levels like: Document level SMT, TrustScore, dynamic routing, dynamic preview; Match type differentiation, Auto-propagation, SMT integration and SMT configurations, segment-level SMT trust scores and feedback cycles (segment level); Auto-suggest dictionary and phrase completions (sub-segment level). The discussed capabilities will be brought into perspective of how the vast amount of multilingual online content are affected by such innovation.", "recorded": "2011-10-21T13:45:00", "title": "Efficient translation production for the Multilingual Web"}, {"url": "mitworld_breazeal_pr", "desc": "Cynthia Breazeal\u2019s eminently charming and huggable creatures appear to have stepped out of Santa\u2019s North Pole workshop. But Breazeal wants you to know that her robots are attempts to create socially intelligent machines \u201cwhose behaviors are governed not just by physics but by having a mind,\u201d and which might someday collaborate with humans in critical interactions.\n\nBreazeal wants to shift the concept of robots from machines that explore distant places like Mars, or vacuum floors, to devices that can function in society at large, dealing with people on a daily basis \u201cto enhance daily life, to help us as partners.\u201d\n\nBuilding sophisticated machines means delving into human social intelligence, our ability to develop a sense of self, communicate thoughts and feelings in words and gestures, and interact with others. Humans are wired to read the underlying mental states of our fellows. Can robots learn to \u201csense and perceive and interpret the same non-verbal cues to coordinate their \u2018mind\u2019 and behavior with people,\u201d wonders Breazeal. Indeed, could a robot \u201cpotentially leverage its interaction with people to help bootstrap its own cognitive development\u201d?\n\nShe demonstrates some remarkable milestones in the journey to develop such a machine. Leonardo, a Yoda-like creature, seems to have the cognitive savvy of a young child, with object permanence and a theory of other minds. He and a human confederate watch a Big Bird doll get hidden under a box. Then only Leonardo sees when a hooded man puts the doll beneath a basket. When his confederate enters the room, Leonardo can answer the question accurately, \u201cCan you find where I think Big Bird is?\u201d Leo points to the box (but like a child, gives the game away by looking at the basket). Leo has also absorbed social referents, reaching eagerly for Big Bird, who\u2019s been described in a cheerful voice as fun and jolly, and shrinking away from a Cookie Monster doll, which the human \u201cparent\u201d has described with a scary voice and gestures as bad. If robots are going to exist in our world, says Breazeal, they have to learn from us when things are safe to explore.\n\nBreazeal\u2019s next generation of mobile and personable creations may serve as helpmates, tutors, teammates, or even companions \u201caddressing the loneliness of old age.\u201d They will certainly bring us closer to the question of \u201cwhen might a machine be a person.\u201d", "recorded": "2008-06-07T13:34:29", "title": "Personal Robots"}, {"url": "wims2013_sheth_physical_cyber_social_computing", "desc": "The proper role of technology to improve human experience has been discussed by visionaries and scientists from the early days of computing and electronic communication. Technology now plays an increasingly important role in facilitating and improving personal and social activities and engagements, decision making, interaction with physical and social worlds, generating insights, and just about anything that an intelligent human seeks to do. I have used the term Computing for Human Experience (CHE) [1] to capture this essential role of technology in a human centric vision. CHE emphasizes the unobtrusive, supportive and assistive role of technology in improving human experience, so that technology \u201ctakes into account the human world and allows computers themselves to disappear in the background\u201d (Mark Weiser [2]).\r\n\r\nIn this talk, I will portray physical-cyber-social (PCS) computing that takes ideas from, and goes significantly beyond, the current progress in cyber-physical systems, socio-technical systems and cyber-social systems to support CHE [3]. I will exemplify future PCS application scenarios in healthcare and traffic management that are supported by (a) a deeper and richer semantic interdependence and interplay between sensors and devices at physical layers, (b) rich technology mediated social interactions, and (c) the gathering and application of collective intelligence characterized by massive and contextually relevant background knowledge and advanced reasoning in order to bridge machine and human perceptions. I will share an example of PCS computing using semantic perception [4], which converts low-level, heterogeneous, multimodal and contextually relevant data into high-level abstractions that can provide insights and assist humans in making complex decisions. The key proposition is to explain that PCS computing will need to move away from traditional data processing to multi-tier computation along data-information-knowledge-wisdom dimension that supports reasoning to convert data into abstractions that humans are adept at using.\r\n\r\nKeywords: Computing for Human Experience, Machine-Human-Social-Semantic Perception, Semantic Abstraction, Physical-Cyber-Social Systems, Physical-Cyber-Social Computing\r\n\r\n[1] A. Sheth, Computing for Human Experience\\\\\r\n[2] M. Weiser, The Computer for 21st Century\\\\\r\n[3] A. Sheth, Semantics empowered Cyber-Physical-Social Systems\\\\\r\n[4] C. Henson, A. Sheth, K. Thirunarayan, Semantic Perception: Converting Sensory Observations to Abstractions", "recorded": "2013-06-13T10:00:00", "title": "Physical Cyber Social Computing: An early 21st century approach to Computing for Human Experience"}, {"url": "mlss05au_kowalczyk_lvhdv", "desc": "Dedicated machine learning procedures have already become an integral part of modern genomics and proteomics. However, these very high dimensional and low learning sample tasks often stretch these procedures well beyond natural boundaries of their applicability. A few such challenges will be a subject of this series of lectures. We will start with a brief overview of classification of genomics (microarray) data. In particular we shall discuss, in some detail, examples of applications to cancer genomics and proteomics. Then we concentrate on a phenomenon of anti-learning, a case of supervised classification where standard supervised learning techniques systematically produce classifiers perfect on learning sample but with independent test error rates higher than that of the default (random) classification rule. The examples of natural and synthetic anti-learning data will be given and analysed from the stand point of implications to practical supervised and unsupervised classification. A series of practical tutorials will be organized in parallel. Participants will be exposed to classification of microarray data including first-hand experience with anti-learning.", "recorded": "2005-01-31T00:00:00", "title": "Bioinformatics Challenge: Learning in Very High Dimensions with Very Few Samples"}, {"url": "datamining2011_plant_based", "desc": "Alzheimer's disease is the most common form of age-related dementia. Early-stage diagnosis of Alzheimer is of major importance for the following reasons: Also easily curable conditions like depression, poor nutrition and drug side effects may cause symptoms like early-stage Alzheimer. Moreover, recently some medications have been developed which successfully attenuate the symptoms and delay the progression of Alzheimer, but to be effective, they need to be applied as soon as possible. However, early-stage diagnosis of Alzheimer is very difficult since the symptoms are very mild and can easily be confounded with effects of normal aging. In this paper, we introduce a bootstrapping-based feature extraction technique to identify early-stage Alzheimer's disease from resting-state functional resonance images. Our experiments demonstrate that subjects with early-stage Alzheimer's disease can be distinguished with an accuracy of 79% from age-matched healthy subjects using a support vector machine on the extracted features.", "recorded": "2011-08-21T13:30:00", "title": "Homogeneity-based Feature Extraction for Classification of Early-stage Alzheimer\u2019s disease from Functional Magnetic Resonance Images"}, {"url": "icml2015_yu_multivariate_spatiotemporal_streams", "desc": "Low-rank tensor learning has many applications in machine learning. A series of batch learning algorithms have achieved great successes. However, in many emerging applications, such as climate data analysis, we are confronted with largescale tensor streams, which pose significant challenges to existing solutions. In this paper, we propose an accelerated online low-rank tensor learning algorithm (ALTO) to solve the problem. At each iteration, we project the current tensor to a low-dimensional tensor, using the information of the previous low-rank tensor, in order to perform efficient tensor decomposition, and then recover the low-rank approximation of the current tensor. By randomly selecting additional subspaces, we successfully overcome the issue of local optima at an extremely low computational cost. We evaluate our method on two tasks in online multivariate spatio-temporal analysis: online forecasting and multi-model ensemble. Experiment results show that our method achieves comparable predictive accuracy with significant speed-up.", "recorded": "2015-07-08T14:46:53", "title": "Accelerated Online Low Rank Tensor Learning for Multivariate Spatiotemporal Streams"}, {"url": "nipsworkshops2011_optimization_for_ml", "desc": "This workshop builds on precedent established the previous OPT workshops:\r\n\r\n# (@NIPS*08): http://opt2008.kyb.tuebingen.mpg.de/\r\n# (@NIPS*09): http://opt.kyb.tuebingen.mpg.de/opt09/\r\n# (@NIPS*10): http://opt.kyb.tuebingen.mpg.de/opt10/\r\n\r\nBoth these workshops had packed (often overpacked) attendance almost throughout the day. This enthusiastic reception reflects the strong interest, relevance, and importance enjoyed by optimization in the greater ML community.\r\n\r\nOne could ask why does optimization attract such continued interest? The answer is simple but telling: optimization lies at the heart of almost every ML algorithm. For some algorithms textbook methods suffice, but the majority requires tailoring algorithmic tools from optimization; moreover, this tailoring depends on a deeper understanding of the ML requirements. In fact, ML applications and researchers are driving some of the most cutting-edge developments in optimization today. The intimate relation of optimization with ML is the key motivation for our workshop, which aims to foster discussion, discovery, and dissemination of the state-of-the-art in optimization, especially in the context of ML.\r\n\r\nWorkshop homepage: http://opt.kyb.tuebingen.mpg.de/index.html", "recorded": "2011-12-16T07:30:00", "title": "Optimization for Machine Learning"}, {"url": "nips09_fergus_sslg", "desc": "With the advent of the Internet it is now possible to collect hundreds of millions of images. These images come with varying degrees of label information. \"Clean labels'' can be manually obtained on a small fraction, \"noisy labels'' may be extracted automatically from surrounding text, while for most images there are no labels at all. Semi-supervised learning is a principled framework for combining these different label sources. However, it scales polynomially with the number of images, making it impractical for use on gigantic collections with hundreds of millions of images and thousands of classes. In this paper we show how to utilize recent results in machine learning to obtain highly efficient approximations for semi-supervised learning that are linear in the number of images.  Specifically, we use the convergence of the eigenvectors of the normalized graph Laplacian to eigenfunctions of weighted Laplace-Beltrami operators. We combine this with a label sharing framework obtained from Wordnet to propagate label information to classes lacking manual annotations. Our algorithm enables us to apply semi-supervised learning to a database of 80 million images with 74 thousand classes.", "recorded": "2009-12-08T11:00:00", "title": "Semi-Supervised Learning in Gigantic Image Collections"}, {"url": "kdd2010_mahoney_gtgm", "desc": "The tutorial will cover recent algorithmic and statistical work on identifying and exploiting \"geometric\" structure in large informatics graphs such as large social and information networks. Such tools (e.g., Principal Component Analysis and related non-linear dimensionality reduction methods) are popular in many areas of machine learning and data analysis due to their relatively-nice algorithmic properties and their connections with regularization and statistical inference. These tools are not, however, immediately-applicable in many large informatics graphs applications since graphs are more combinatorial objects; due to the noise and sparsity patterns of many real-world networks, etc. Recent theoretical and empirical work has begun to remedy this, and in doing so it has already elucidated several surprising and counterintuitive properties of very large networks. Topics include: underlying theoretical ideas; tips to bridge the theory-practice gap; empirical observations; and the usefulness of these tools for such diverse applications as community detection, routing, inference, and visualization.", "recorded": "2010-07-25T09:05:00", "title": "Geometric Tools for Graph Mining of Large Social and Information Networks"}, {"url": "eccv2014_ramakrishna_pose_machines", "desc": "State-of-the-art approaches for articulated human pose estimation are rooted in parts-based graphical models. These models are often restricted to tree-structured representations and simple parametric potentials in order to enable tractable inference. However, these simple dependencies fail to capture all the interactions between body parts. While models with more complex interactions can be defined, learning the parameters of these models remains challenging with intractable or approximate inference. In this paper, instead of performing inference on a learned graphical model, we build upon the inference machine framework and present a method for articulated human pose estimation. Our approach incorporates rich spatial interactions among multiple parts and information across parts of different scales. Additionally, the modular framework of our approach enables both ease of implementation without specialized optimization solvers, and efficient inference. We analyze our approach on two challenging datasets with large pose variation and outperform the state-of-the-art on these benchmarks.", "recorded": "2014-09-08T18:05:00", "title": "Pose Machines: Articulated Pose Estimation via Inference Machines"}, {"url": "wsdm2010_dupret_amtei", "desc": "We propose a new model to interpret the click through logs of a web search engine. This model is based on explicit assumptions on the user behavior. In particular, we draw conclusions on a document relevance by observing the user behavior after he examined the document and not based on whether a user clicks or not a document url. This results in a model based on intrinsic relevance, as opposed to perceived relevance. We use the model to predict document relevance and then use this as feature for a \u201cLearning to Rank\u201d machine learning algorithm. Comparing the ranking functions obtained by training the algorithm with and without the new feature we observe surprisingly good results. This is particularly notable given that the baseline we use is the heavily optimized ranking function of a leading commercial search engine. A deeper analysis shows that the new feature is particularly helpful for non navigational queries and queries with a large abandonment rate or a large average number of queries per session. This is important because these types of query is considered to be the most difficult to solve.", "recorded": "2010-02-05T12:00:55", "title": "A Model to Estimate Intrinsic Document Relevance from the Clickthrough Logs of a Web Search Engine"}, {"url": "fmri06_stephens_gmdrv", "desc": "Functional Magnetic Resonance Imaging (FMRI) provides an unprecedented window\r into the complex functioning of the human brain, typically detailing the activity\r of thousands of voxels for hundreds of time points. The interpretation of FMRI\r is complicated, however, because of the unknown connection between the hemodynamic\r response and neural activity, and the unknown spatiotemporal characteristics\r of the cognitive patterns themselves.\r Recent work has exploited techniques from machine learning to find patterns of\r voxel activity related to brain processes (see e.g., [1]). Many of these techniques\r involve decoding, inferring the value or category class of a stimulus !S given a pattern\r of voxel activations !V . Decoding can generally be split into two approaches, discriminative\r and generative [2]. With a discriminative model one learns the conditional\r distribution P(!S |!V ) directly by minimizing a loss such as minimum classification\r error. Alternatively, the generative approach obtains this conditional probability\r through Bayes rule; one posits and fits models for P(!S ) and P(!V |S) instead. Both\r approaches can reliably establish the existence of sufficient decoding information.", "recorded": "2006-12-08T00:00:00", "title": "Generative Models for Decoding Real-Valued Natural Experience in FMRI"}, {"url": "eswc2013_stolz_bmecat", "desc": "To date, the automatic exchange of product information between business partners in a value chain is typically done using Business-to Business (B2B) catalog standards such as EDIFACT, cXML, or BMEcat.\r\nAt the same time, the Web of Data, in particular the GoodRelations vocabulary, o\u000bthers the necessary means to publish highly-structured product data in a machine-readable format. The advantage of the publication of rich product descriptions can be manifold, including better integration and exchange of information between Web applications, high-quality data along the various stages of the value chain, or the opportunity to support more precise and more ef\u000bective searches. In this paper, we (1) stress the\r\nimportance of rich product master data for e-commerce on the Semantic Web, and (2) present a tool to convert BMEcat XML data sources into an RDF-based data model anchored in the GoodRelations vocabulary. The benefi\fts of our proposal are tested using product data collected from a set of 2500+ online retailers of varying sizes and domains. ", "recorded": "2013-05-30T15:06:03", "title": "Using BMEcat Catalogs as a Lever for Product Master Data on the Semantic Web"}, {"url": "cern_amaldi_acfhad", "desc": "Hadrontherapy was born in 1938, when neutron beams were used in cancer therapy, but it has become an accepted therapeutical modality only in the last fifteen years. Fast neutrons are still in use, even if their limitations are now apparent. Charged hadron beams are more favourable, since the largest specific energy deposition occurs at the end of their range in matter. The most used hadrons are at present protons and carbon ions, which allow a dose deposition which conforms to the tumour target. \r\n\r\nRadiobiological experiments and the results of the first clinical trials indicate that carbon ions have, besides this macroscopic property, a different way of interacting with cell at the microscopic level. There are thus solid hopes to use carbon beams of about 4500 MeV to control tumours which are radioresistant both to X-rays and to protons. \r\n\r\nAfter discussing these macroscopic and microscopic properties and presenting the work carried out at CERN in the framework of the Proton Ion Medical Machine Study (PIMMS), the hospital-based facilities in the world, running or under construction, will be reviewed.", "recorded": "2001-03-01T00:00:00", "title": "Accelerators for Hadrontherapy"}, {"url": "lmcv04_buhmann_liis", "desc": "Image segmentation is often defined as a partitioning of pixels or image blocks into homogeneous groups. These groups are characterized by a prototypical vector in feature space, e.g., the space of Gabor filter responses, by a prototypical histograms of features or by pairwise dissimilarities between image blocks. For all three data formats cost functions have been proposed to measure distortion and, thereby, to encode the quality of a partition. Learning in image segmentation can be defined as the inference of prototypical descriptors of segments like codebook vectors or average feature probability within a segment. Contrary to classification or regression, the empirical risk of image segmentation is often composed of sums of dependent random variables like in Normalized Cut, Pairwise Clustering or k-means clustering with smoothness constraints. One of the core challenges for machine learning is to discover what kind of information can be learned from these data sources assuming MRF cost functions as image models. The validation procedure for image segmentations strongly depends on this issue. I will demonstrate the learning and validation issue in the context of image analysis based on color and texture features.", "recorded": "2004-05-03T00:00:00", "title": "Learning issues in image segmentation"}, {"url": "eswc06_harmelen_wswnj", "desc": "Work on the Semantic Web is all to often phrased as a technological challenge: how to improve the precision of search engines, how to personalise web-sites, how to integrate weakly-structured data-sources, etc. This suggests that we will be able to realise the Semantic Web by merely applying (and at most refining) the results that are already available from many branches of Computer Science.\r \r I will argue in this talk that instead of (just) a technological challenge, the Semantic Web forces us to rethink the foundations of many subfields of Computer Science. This is certainly true for my own field (Knowledge Representation), where the challenge of the Semantic Web continues to break many often silently held and shared assumptions underlying decades of research. With some caution, I claim that this is also true for other fields, such as Machine Learning, Natural Language Processing, Databases, and others. For each of these fields, I will try to identify silently held assumptions which are no longer true on the Semantic Web, prompting a radical rethink of many past results from these fields. ", "recorded": "2006-06-13T00:00:00", "title": "Where Does It Break? Or: Why the Semantic Web is Not Just 'Research as Usual'"}, {"url": "ecmlpkdd2011_uguroglu_feature", "desc": "Common assumption in most machine learning algorithms is that, labeled (source) data and unlabeled (target) data are sampled from the same distribution. However, many real world tasks violate this assumption: in temporal domains, feature distributions may vary over time, clinical studies may have sampling bias, or sometimes sufficient labeled data for the domain of interest does not exist, and labeled data from a related domain must be utilized. In such settings, knowing in which dimensions source and target data vary is extremely important to reduce the distance between domains and accurately transfer knowledge. In this paper, we present a novel method to identify variant and invariant features between two datasets. Our contribution is two fold: First, we present a novel transfer learning approach for domain adaptation, and second, we formalize the problem of finding differently distributed features as a convex optimization problem. Experimental studies on synthetic and benchmark real world datasets show that our approach outperform other transfer learning approaches, and it aids the prediction accuracy significantly.", "recorded": "2011-09-08T18:00:00", "title": "Feature Selection for Transfer Learning"}, {"url": "newtonschw03s08_wolfe_nesm", "desc": "Spectral methods are of fundamental importance in statistics and machine learning, as they underlie algorithms from classical principal components analysis to more recent approaches that exploit manifold structure. In most cases, the core technical problem can be reduced to computing a low-rank approximation to a positive-definite kernel. Motivated by such applications, we present here two new algorithms for the approximation of positive semi-definite kernels, together with error bounds that improve upon known results. The first of these\u2014based on sampling\u2014leads to a randomized algorithm whereupon the kernel induces a probability distribution on its set of partitions, whereas the latter approach\u2014based on sorting\u2014provides for the selection of a partition in a deterministic way. After detailing their numerical implementation and verifying performance via simulation results for representative problems in statistical data analysis, we conclude with an extension of these results to the sparse representation of linear operators and the efficient approximation of matrix products.\r\n\r\n**Homepage Link**\r\n* http://www.newton.ac.uk/programmes/SCH/seminars/062611301.html", "recorded": "2008-06-26T11:30:00", "title": "The Nystr\u00f6m extension and spectral methods in learning: low-rank approximation of quadratic forms and products"}, {"url": "mitworld_syntax", "desc": "The impetus for this workshop, borrowing from a recent review by Yang in TICS (2004), is that \"Recent demonstrations of statistical learning in infants have reinvigorated the innateness versus learning debate in language acquisition,\" particularly regarding syntax. We aim to reexamine this issue in a single forum from the computational, cognitive, and formal linguistics perspectives. Our intent is to examine recent applications of statistical learning theory to language acquisition. That machine learning has something to offer in understanding language acquisition is not in doubt. However, we would like to examine the basic premise that computational approaches should be linguistically informed. The hypothesis put forth is that statistical approaches should work within the framework of classical linguistics rather than supplant it.\r\n\r\nThe goal of this workshop is to examine this hypothesis critically, be it wrong or right, and for each speaker to present evidence as they see fit. \r\n\r\n\r\n**About the Host** - **[[http://mitworld.mit.edu/host/view/157|MIT Laboratory for Information and Decision Systems]]**\r\n\r\n**This MIT World Series** is available at http://mitworld.mit.edu/series/view/119", "recorded": "2007-10-19T09:00:00", "title": "MIT World Series: Where Does Syntax Come From? Have We All Been Wrong?"}, {"url": "icml2015_luo_support_matrix_machines", "desc": "In many classification problems such as electroencephalogram (EEG) classification and image classification, the input features are naturally represented as matrices rather than vectors or scalars. In general, the structure information of the original feature matrix is useful and informative for data analysis tasks such as classification. One typical structure information is the correlation between columns or rows in the feature matrix. To leverage this kind of structure information, we propose a new classification method that we call support matrix machine (SMM). Specifically, SMM is defined as a hinge loss plus a so-called spectral elastic net penalty which is a spectral extension of the conventional elastic net over a matrix. The spectral elastic net enjoys a property of grouping effect, i.e., strongly correlated columns or rows tend to be selected altogether or not. Since the optimization problem for SMM is convex, this encourages us to devise an alternating direction method of multipliers algorithm for solving the problem. Experimental results on EEG and face image classification data show that our model is more robust and efficient than the state-of-the-art methods.", "recorded": "2015-07-07T11:30:21", "title": "Support Matrix Machines"}, {"url": "nipsworkshops2013_song_variable_models", "desc": "\r\nThe key idea of kernel embedding of distributions is to map distributions into the RKHS associated with a kernel function, such that subsequent manipulations of distributions can be achieved via RKHS distances, linear transformations, and spectral decompositions. This framework has led to simple and effective nonparametric algorithms in many machine learning problems, such as two-sample test, covariate shift correction, time-series modeling and nonparametric belief propagation. In this talk, I will focus on my recent works on kernel embedding of latent variable models. The presence of latent variables in a distribution induces sophisticated low rank structures in its kernel embedding, which is exploited for designing nonparametric algorithms for recovering latent variable model structures, learning latent parameters and improving density estimation. These novel algorithms connect kernel embedding of distributions to tensors and higher order tensors, and exploit spectral decomposition of these objects for efficient learning. I will discuss both theoretical guarantees and empirical results for these new approaches.", "recorded": "2013-12-10T08:30:00", "title": "Nonparametric Latent Variable Models via Kernel Embedding of Distributions"}, {"url": "ecmlpkdd2011_fu_building", "desc": "We propose a direct approach to learning sparse Support Vector Machine (SVM) prediction models for Multi-Instance (MI) classification. The proposed sparse SVM is based on a \"label-mean\" formulation of MI classification which takes the average of predictions of individual instances for bag-level prediction. This leads to a convex optimization problem, which is essential for the tractability of the optimization problem arising from the sparse SVM formulation we derived subsequently, as well as the validity of the optimization strategy we employed to solve it. Based on the \"label-mean\" formulation, we can build sparse SVM models for MI classification and explicitly control their sparsities by enforcing the maximum number of expansions allowed in the prediction function. An effective optimization strategy is adopted to solve the formulated sparse learning problem which involves the learning of both the classifier and the expansion vectors. Experimental results on benchmark data sets have demonstrated that the proposed approach is effective in building very sparse SVM models while achieving comparable performance to the state-of-the-art MI classifiers.", "recorded": "2011-09-08T10:00:00", "title": "Building Sparse Support Vector Machines for Multi-Instance Classification"}, {"url": "bbci09_egert_bfnf", "desc": "In spite of considerable progress towards prosthetic devices controlled by neuronal signals, brain-machine interfaces and other neurotechnological devices, however, user-friendly, neurotechnical devices for everyday use remain a vision of the future, with numerous fundamental biological, technical, computational, clinical, and ethical problems still to be solved. The aim of the BCNT-FT consortium is the development of bidirectional hybrid neurotechnical devices for human usage. This will be implemented in three research clusters, each consisting of projects organized around a common goal: to understand the principles, to advance technology, and to explore and extend clinical applications. Each project addresses issues central to neurotechnology, from basic questions on decoding neuronal signals, interfacing biological neuronal networks to technical devices, actuators and real-time feedback systems, via the stable recording and interpretation of neuronal signals, to the clinical testing and application of new technologies. Research in the BCNT-FT will be supported by a matching, interdisciplinary training program for neurotechnology. In collaboration with industrial, applied and clinical partners neuroprosthetical devices for biomedical application will be developed.", "recorded": "2009-07-10T16:50:00", "title": "Bernstein Focus: Neurotechnology Freiburg"}, {"url": "eswc09_ramollari_lsws", "desc": "Recent years have seen the utilisation of Semantic Web Service descriptions for automating a wide range of service-related activities, with a primary focus on service discovery, composition, execution and mediation. An important area which so far has received less attention is service validation, whereby advertised services are proven to conform to required behavioural specifications.  This paper proposes a method for validation of service-oriented systems through automated functional testing. The method leverages ontology-based and rule-based descriptions of service inputs, outputs, preconditions and effects (IOPE) for constructing a stateful EFSM specification. The specification is subsequently utilised for functional testing and validation using the proven Stream X-machine (SXM) testing methodology. Complete functional test sets are generated automatically at an abstract level and are then applied to concrete Web services, using test drivers created from the Web service descriptions.  The testing method comes with completeness guarantees and provides a strong method for validating the behaviour of Web services.", "recorded": "2009-06-04T12:30:00", "title": "Leveraging Semantic Web Service Descriptions for Validation by Automated Functional Testing"}, {"url": "machine_bach_convergence_rate", "desc": "We consider the stochastic approximation problem where a convex function has to be minimized, given only the knowledge of unbiased estimates of its gradients at certain points, a framework which includes machine learning methods based on the minimization of the empirical risk. We focus on problems without strong convexity, for which all previously known algorithms achieve a convergence rate for function values of O(1/n\u221a). We consider and analyze two algorithms that achieve a rate of O(1/n) for classical supervised learning problems. For least-squares regression, we show that averaged stochastic gradient descent with constant step-size achieves the desired rate. For logistic regression, this is achieved by a simple novel stochastic gradient algorithm that (a) constructs successive local quadratic approximations of the loss functions, while (b) preserving the same running time complexity as stochastic gradient descent. For these algorithms, we provide a non-asymptotic analysis of the generalization error (in expectation, and also in high probability for least-squares), and run extensive experiments showing that they often outperform existing approaches.", "recorded": "2014-01-20T11:35:00", "title": "Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n)"}, {"url": "complexnetworks2012_london", "desc": "The workshop was funded by the Royal Society under the the Research Fellows International Scientific Seminars scheme, and the PASCAL2 network provided funding to cover the filming.  The aim of the meeting was to bring together researchers from complex networks, and those working in machine learning and graph theory. The goal was  to identify current challenges in complex networks analysis and identify  possible methodologies for addressing them. The meeting was composed of four sessions:\n\n# methods for measuring and characterising complex network structure.\n# dynamic processes on complex networks.\n# complex network function prediction.\n# future directions, collaboration and networking.\n\nThe presentations were not intended to be lectures focused on specific research results. Instead they were  expected to summarize state-of-the-art or accepted wisdom, challenge it and pose a provocative agenda for the discussions.\n\nWe thank Sir Peter Knight and the staff at the Royal Society Kavli Centre for providing a conducive  and enjoyable environment for the meeting.\n{{http://carbon.videolectures.net/v00c/95/svp4ggdaqh4x5ln52berh25hgge24bsc.jpg}}", "recorded": "2012-05-28T09:00:00", "title": "Workshop on Function Prediction in Complex Networks, Kavli Royal Society Centre, Chicheley Hall 2012"}, {"url": "wims2011_grobelnik_processing", "desc": "Why people process text with computers? It all started many years ago, with the main goal in minds of researchers, to understand the text. In the meantime, the area of text processing developed in many different directions whereby the original goals were often forgotten. Funny enough, it seems, in several decades of computerized processing of textual data, the solution to the 'text understanding' problem didn't evolve much compared to some other, easier and often more profitable problems to deal with (such as information retrieval/search, machine translation or information extraction). In this paper we touch various aspects of text processing along several dimensions: (a) how we represent the textual data, (b) what kind of algorithms and techniques we use, and (c) what kind of problems we solve on the top of text. Finally, it is interesting to observe various research communities dealing with textual data in different ways. Most of them are still rather fragmented and don't learn enough from each other \u2010 many of the ideas developed within one community don't cross borders of that community for too long.", "recorded": "2011-05-27T10:30:00", "title": "Many Faces of Text Processing"}, {"url": "ptdm2012_adriaans_datamining", "desc": "The breakthrough of data mining in the mid-nineties can be explained as the culmination of several independent developments: the convergence of machine learning research, the maturity of data base technology, the workstation and client server-revolutions, the rapid growth of decentralized data collection, the stabilization of administrative systems in large organizations, the apparent failure of traditional marketing techniques. I will look back at these developments from my perspective as R&D manager/CEO at Syllogic and later Perot Systems. We started with ambitious AI projects in data base environments (OBIS, CAPTAINS) but we soon expanded our interest to the analysis of dynamic systems (ASM, ICT, JSF, Robosail). In the new Millennium, research turned its attention to e-Science, (VL-e) large RDF data bases and Big Data (Commit). My current research interest lies with complexity measures for large data sets and methodology for e-Science in the 21st century (Atlas of Complexity). Using examples from my R&D practice, I will give an overview of this period of nearly 30 years of research and sketch some perspectives for the future.", "recorded": "2012-12-10T16:00:25", "title": "Datamining \"Looking backward, looking forward\""}, {"url": "colt2015_rebeschini_point_processes", "desc": "We investigate the systematic mechanisms for designing fast mixing Markov chains Monte Carlo algorithms to sample from discrete point processes. Such processes are defined as probability distributions $\\mu(S)\\propto \\exp(f(S))$ over all subsets $S\\subseteq 2^V$ of a finite set $V$ through a bounded set function $f:2^V\\rightarrow \\mathbb{R}$. In particular, a subclass of discrete point processes characterized by submodular functions (which include determinantal point processes, log-submodular distributions, and submodular point processes) has recently gained a lot of interest in machine learning and shown to be effective for modeling diversity and coverage. We show that if the set function (not necessarily submodular) displays a natural notion of decay of correlation, then it is possible to design fast mixing Markov Chain Monte Carlo methods that provide size-free error bounds on marginal approximations. The conditions that we introduce involve a control on the second order (discrete) derivatives of set functions. We then provide sufficient conditions for fast mixing when the set function is submodular, and specialize our results for canonical examples.", "recorded": "2015-07-06T10:15:00", "title": "Fast Mixing for Discrete Point Processes"}, {"url": "iswc08_witbrock_fsc", "desc": "OpenCyc will be more accessible and Semantic Web interoperability will be enhanced if users are able to access just the parts of OpenCyc they need. The tutorial will describe how Semantic Web researchers and practitioners can benefit from integrating their representations with the extensive upper and middle level ontological content of the free and unrestricted OpenCyc knowledge base, and other integrative vocabularies like Okkam. The syntax of OpenCyc will be described both in raw form, and as mapped onto Semantic Web standard languages, and the content of the knowledge base will be described in overview. Based on that, we\u2019ll show how to extend the OpenCyc KB for user applications, and how to make use of it in a web-services environment to support knowledge integration, and simple machine learning applications. Finally, we\u2019ll demonstrate the use of the OpenCyc vocabulary to support a broad-applicability knowledge capture application, illustrative of the transition from Web2.0 to Web3.0. Hands on exercises will be used to illustrate knowledge use and construction, use of OpenCyc with inference, and use for semantic search over text in a web services environment.", "recorded": "2008-10-27T14:00:00", "title": "Free Semantic Content: Using OpenCyc in Semantic Web Applications"}, {"url": "nipsworkshops09_bagnell_ilpppnm", "desc": "Programming robot behavior remains a challenging task. While it is often easy to abstractly define or even demonstrate a desired behavior, designing a controller that embodies the same behavior is difficult, time consuming, and ultimately expensive. The machine learning paradigm offers the promise of enabling \"programming by demonstration\" for developing high-performance robotic systems. Unfortunately, many \"behavioral cloning\" approaches that utilize the classical tools of supervised learning (e.g. decision trees, neural networks, or support vector machines) do not fit the needs of modern robotic systems. Classical statistics and supervised machine learning exist in a vacuum: predictions made by these algorithms are explicitly assumed to not affect the world in which they operate. \r\nIn practice, robotic systems are often built atop sophisticated planning algorithms that efficiently reason far into the future; consequently, ignoring these planning algorithms in lieu of a supervised learning approach often leads to myopic and poor-quality robot performance. While planning algorithms have shown success in many real-world applications ranging from legged locomotion to outdoor unstructured navigation, such algorithms rely on fully specified cost functions that map sensor readings and environment models to quantifiable costs. Such cost functions are usually manually designed and programmed. Recently, our group has developed a set of techniques that learn these functions from human demonstration. These algorithms apply an Inverse Optimal Control (IOC) approach to find a cost function for which planned behavior mimics an expert's demonstration. \r\nI'll discuss these methodologies, both probabilistic and otherwise, for imitation learning. I'll focus on the Principle of Causal Maximum Entropy that generalizes the classical Maximum Entropy Principle, widely used in many fields including physics, statistics, and computer vision, to problems of decision making and control. This generalization enables MaxEnt to apply to a new class of problems including Inverse Optimal Control and activity forecasting. This approach further elucidates the intimate connections between probabilistic inference and optimal control. \r\nI'll consider case studies in activity forecasting of drivers and pedestrians as well as the imitation learning of robotic locomotion and rough-terrain navigation. These case-studies highlight key challenges in applying the algorithms in practical settings that utilize state-of-the-art planners and are constrained by efficiency requirements and imperfect expert demonstration.", "recorded": "2009-12-11T08:20:00", "title": "Imitation Learning and Purposeful Prediction: Probabilistic and Non-probabilistic Methods "}, {"url": "akbc2010_grenoble", "desc": "Good decision-making is dependent on comprehensive, accurate knowledge. But the information relevant to many important decisions in areas such as business, government, medicine and scientific research is massive, and growing at an accelerating pace. Relevant raw data is widely available on the web and other data sources, but usually in order to be useful it must be gathered, extracted, organized, and normalized into a knowledge base.\r\n\r\nHand-built knowledge bases such as Wikipedia have made us all better decision-makers. However more than human editing will be necessary to create a wide variety of domain-specific, deeply comprehensive, more highly structured knowledge bases.\r\n\r\nA variety of automated methods have begun to reach levels of accuracy and scalability that make them applicable to automatically constructing useful knowledge bases from text and other sources. These capabilities have been enabled by research in areas including natural language processing, information extraction, information integration, databases, search and machine learning. There are substantial scientific and engineering challenges in advancing and integrating such relevant methodologies.\r\n\r\nThis workshop gathered researchers in a variety of fields that contribute to the automated construction of knowledge bases.\r\n\r\nThere has recently been a tremendous amount of new work in this area, some of it in traditionally disconnected communities. In this workshop the organizers aim to bring these communities together.\r\n\r\nTopics of interest include:\r\n\r\n- information extraction; open information extraction, named entity extraction; entity resolution, relation extraction.\\\\\r\n- information integration; schema alignment; ontology alignment; ontology constrution.\\\\\r\n- monolingual alignment, alignment between knowlege bases and text.\\\\\r\n- joint inference between text interpretation and knowledge base\\\\\r\n- pattern analysis, semantic analysis of natural language, reading the web, learning by reading.\\\\\r\n- databases; distributed information systems; probabilistic databases.\\\\\r\n- scalable computation; distributed computation.\\\\\r\n- information retrieval; search on mixtures of structured and unstructured data; querying under uncertainty.\\\\\r\n- machine learning; unsupervised, lightly-supervised and distantly-supervised learning; learning from naturally-available data.\\\\\r\n- human-computer collaboration in knowledge base construction; automated population of wikis.\\\\\r\n- dynamic data, online/on-the-fly adaptation of knowledge.\\\\\r\n- inference; scalable approximate inference.\\\\\r\n- languages, toolkits and systems for automated knowledge base construction.\\\\\r\n- demonstrations of existing automatically-built knowledge bases.\\\\\r\n\r\nMore about the event [[http://akbc.xrce.xerox.com/|here]].", "recorded": "2010-05-17T09:00:00", "title": "1st Workshop on Automated Knowledge Based Construction (AKBC), Grenoble 2010"}, {"url": "ecmlpkdd2011_lavrac_vavpetic_mining", "desc": "The term semantic data mining denotes a data mining approach where domain ontologies are used as background knowledge. Such approach is motivated by large amounts of data that are increasingly becoming openly available and described using real-life ontologies represented in Semantic Web languages, arguably most extensively in the domain of biology. This recently opened up the possibility for interesting large-scale and real-world semantic applications.\r\n\r\nThe availability of semantically annotated data poses requirements for new kinds of approaches for data mining that would be able to deal with the complexity, and expressivity of the semantic representation languages, leverage on availability of ontologies and explicit semantics of the described resources, and account for novel assumptions (e.g., open world) that underlie reasoning services exploiting ontologies.\r\n\r\nThe tutorial addresses the above issues, focusing on the problems of how machine learning techniques can work directly on the richly structured Semantic Web data, exploit ontologies, and the Semantic Web technologies, what is the value added of machine learning methods exploiting ontologies, and what are the challenges for developers of semantic data mining methods. It also contains demonstrations of tools supporting semantic data mining.\r\n\r\nThe tutorial presents the topic of semantic data mining from three complementary perspectives.\r\n\r\nFirstly, it presents a general framework for semantic data mining, following the work [NVTL09]. The first part of the tutorial also discusses a new method for semantic subgroup discovery: g-SEGS. It is accompanied with a presentation of the developed tool, a part of Orange4WS environment.\r\n\r\nThe second part of tutorial covers the topic of learning from description logics (DL-learning), motivated by the fact that the standard Web ontology language, OWL, is theoretically based on description logics. This includes a demo of a tool supporting DL-learning (a plugin to the Rapid Miner system).\r\n\r\nFinally, the third part of the tutorial covers the topic of semantic meta-mining. This approach has three features that distinguish it from its predecessors. First, more than in previous work, it adopts a process-oriented approach where meta-learning is applied to support design choices at different stages of the complete data mining process or workflow. Second, it complements dataset descriptions with an in-depth analysis and characterization of algorithms\u2014their underlying assumptions, optimization goals and strategies, the models and patterns they generate. Finally, it relies on a data mining ontology which distills extensive background knowledge concerning knowledge discovery itself.", "recorded": "2011-09-09T14:00:00", "title": "Semantic Data Mining"}, {"url": "icml2015_han_log_determinant_computation", "desc": "Logarithms of determinants of large positive definite matrices appear ubiquitously in machine learning applications including Gaussian graphical and Gaussian process models, partition functions of discrete graphical models, minimum-volume ellipsoids and metric and kernel learning. Log-determinant computation involves the Cholesky decomposition at the cost cubic in the number of variables (i.e., the matrix dimension), which makes it prohibitive for large-scale applications. We propose a linear-time randomized algorithm to approximate log-determinants for very large-scale positive definite and general non-singular matrices using a stochastic trace approximation, called the Hutchinson method, coupled with Chebyshev polynomial expansions that both rely on efficient matrix-vector multiplications. We establish rigorous additive and multiplicative approximation error bounds depending on the condition number of the input matrix. In our experiments, the proposed algorithm can provide very high accuracy solutions at orders of magnitude faster time than the Cholesky decomposition and Shur completion, and enables us to compute log-determinants of matrices involving tens of millions of variables.", "recorded": "2015-07-08T14:46:53", "title": "Large-scale log-determinant computation through stochastic Chebyshev expansions"}, {"url": "bmvc2013_sharma_novel_approach", "desc": "The kernel trick \u2013 commonly used in machine learning and computer vision \u2013 enables\r\nlearning of non-linear decision functions without having to explicitly map the original\r\ndata to a high dimensional space. However, at test time, it requires evaluating the kernel\r\nwith each one of the support vectors, which is time consuming. In this paper, we propose\r\na novel approach for learning non-linear SVM corresponding to the histogram intersection\r\nkernel without using the kernel trick. We formulate the exact non-linear problem in\r\nthe original space and show how to perform classification directly in this space. The\r\nlearnt classifier incorporates non-linearity while maintaining O(d) testing complexity\r\n(for d-dimensional input space), compared to O(d \u0002Nsv) when using the kernel trick.\r\nWe show that the SVM problem with histogram intersection kernel is quasi-convex in\r\ninput space and outline an iterative algorithm to solve it. The proposed approach has\r\nbeen validated in experiments where it is compared with other linear SVM-based methods,\r\nshowing that the proposed method achieves similar or better performance at lower\r\ncomputational and memory costs.", "recorded": "2013-09-10T12:40:00", "title": "A Novel Approach for Efficient SVM Classification with Histogram Intersection Kernel"}, {"url": "kdd2014_xue_identifying_tourists", "desc": "Tourism industry has become a key economic driver for Singapore. Understanding the behaviors of tourists is very important for the government and private sectors, e.g., restaurants, hotels and advertising companies, to improve their existing services or create new business opportunities. In this joint work with Singapore's Land Transport Authority (LTA), we innovatively apply machine learning techniques to identity the tourists among public commuters using the public transportation data provided by LTA. On successful identification, the travelling patterns of tourists are then revealed and thus allow further analyses to be carried out such as on their favorite destinations, region of stay, etc. Technically, we model the tourists identification as a classification problem, and design an iterative learning algorithm to perform inference with limited prior knowledge and labeled data. We show the superiority of our algorithm with performance evaluation and comparison with other state-of-the-art learning algorithms. Further, we build an interactive web-based system for answering queries regarding the moving patterns of the tourists, which can be used by stakeholders to gain insight into tourists' travelling behaviors in Singapore.", "recorded": "2014-08-27T14:30:00", "title": "Identifying Tourists from Public Transport Commuters"}, {"url": "kdd2014_kannan_text_snippets", "desc": "Images are often used to convey many different concepts or illustrate many different stories. We propose an algorithm to mine multiple diverse, relevant, and interesting text snippets for images on the web. Our algorithm scales to all images on the web. For each image, all webpages that contain it are considered. The top-K text snippet selection problem is posed as combinatorial subset selection with the goal of choosing an optimal set of snippets that maximizes a combination of relevancy, interestingness, and diversity. The relevancy and interestingness are scored by machine learned models. Our algorithm is run at scale on the entire image index of a major search engine resulting in the construction of a database of images with their corresponding text snippets. We validate the quality of the database through a large-scale comparative study. We showcase the utility of the database through two web-scale applications: (a) augmentation of images on the web as webpages are browsed and (b)~an image browsing experience (similar in spirit to web browsing) that is enabled by interconnecting semantically related images (which may not be visually related) through shared concepts in their corresponding text snippets.", "recorded": "2014-08-27T10:45:00", "title": "Mining Text Snippets for Images on the Web"}, {"url": "nipsworkshops2012_bach_analysis", "desc": "We consider supervised learning problems within the positive-de\ufb01nite kernel framework, such as kernel ridge regression, kernel logistic regression or the support vector machine. With kernels leading to in\ufb01nite-dimensional feature spaces, a common practical limiting dif\ufb01culty is the necessity of computing the kernel matrix, which most frequently leads to algorithms with running time at least quadratic in the number of observations n, i.e., O(n^2). Low-rank approximations of the kernel matrix are often considered as they allow the reduction of running time complexities to O(p^2 n), where p is the rank of the approximation. The practicality of such methods thus depends on the required rank p. In this talk, I will show that for approximations based on a random subset of columns of the original kernel matrix, the rank p may be chosen to be linear in the degrees of freedom associated with the problem, a quantity which is classically used in the statistical analysis of such methods, and is often seen as the implicit number of parameters of non-parametric estimators. This result enables simple algorithms that have subquadratic running time complexity, but provably exhibit the same predictive performance than existing algorithms.", "recorded": "2012-12-08T15:30:00", "title": "Sharp analysis of low-rank kernel matrix approximations"}, {"url": "icml08_gordon_nrl", "desc": "Quite a bit is known about minimizing different kinds of regret in experts problems, and how these regret types relate to types of equilibria in the multiagent setting of repeated matrix games. Much less is known about the possible kinds of regret in online convex programming problems (OCPs), or about equilibria in the analogous multiagent setting of repeated convex games. This gap is unfortunate, since convex games are much more expressive than matrix games, and since many important machine learning problems can be expressed as OCPs. In this paper, we work to close this gap: we analyze a spectrum of regret types which lie between external and swap regret, along with their corresponding equilibria, which lie between coarse correlated and correlated equilibrium. We also analyze algorithms for minimizing these regret types. As examples of our framework, we derive algorithms for learning correlated equilibria in polyhedral convex games and extensive-form correlated equilibria in extensive-form games. The former is exponentially more efficient than previous algorithms, and the latter is the first of its type.", "recorded": "2008-07-08T15:15:00", "title": "No-Regret Learning in Convex Games"}, {"url": "mitworld_liskov_kaelbling_brooks_schmidt_zue_ehmr", "desc": "If you feel inseparable from your laptop and cell phone today, just wait a few years: the connection between people and machines is about to get much, much closejavascript:DateTimeShortcuts.handleCalendarQuickLink(1,%200);r. This theme underlay much of the panel discussion. Victor Zue\u2019s computers can take subtle visual and conversational cues from a speaker, and respond appropriately (even switching languages at the drop of a hat!). Leslie Pack Kaelbling described the perfect office mate, the \u201cEnduring Personal Cognitive Assistant,\u201d which will someday remind you to press forward on a neglected project or to soothe the ruffled feathers of a snubbed colleague. Martin Schmidt\u2019s lab is finding ways to plug the tiniest circuitry into a boot or a sneaker to harvest the body\u2019s energy for lightweight battery power out in the field. And Rodney Brooks showcased Kismet, a robot who can babble like an appreciative baby and bat her eyes in response to compliments. He also showed a remarkable video of another invention: a prosthetic leg that understands how it\u2019s being walked on and adapts. As a woman tries the leg out for the first time, she laughs with sheer delight.", "recorded": "2003-05-23T11:00:00", "title": "Engineering Human-Machine Relationships"}, {"url": "icml08_walder_spg", "desc": "Most existing sparse Gaussian process (g.p.) models seek computational advantages by basing their computations on a set of m basis functions that are the covariance function of the g.p. with one of its two inputs fixed. We generalise this for the case of Gaussian covariance function, by basing our computations on m Gaussian basis functions with arbitrary diagonal covariance matrices (or length scales). For a fixed number of basis functions and any given criteria, this additional flexibility permits approximations no worse and typically better than was previously possible. We perform gradient based optimisation of the marginal likelihood, which costs O(m2n) time where n is the number of data points, and compare the method to various other sparse g.p. methods. Although we focus on g.p. regression, the central idea is applicable to all kernel based algorithms, and we also provide some results for the support vector machine (s.v.m.) and kernel ridge regression (k.r.r.). Our approach outperforms the other methods, particularly for the case of very few basis functions, i.e. a very high sparsity ratio.", "recorded": "2008-07-07T14:19:00", "title": "Sparse Multiscale Gaussian Process Regression"}, {"url": "mitworld_kaelbling_brooks_schmidt_zue_ehmr", "desc": "If you feel inseparable from your laptop and cell phone today, just wait a few years: the connection between people and machines is about to get much, much closer. This theme underlay much of the panel discussion. Victor Zue\u2019s computers can take subtle visual and conversational cues from a speaker, and respond appropriately (even switching languages at the drop of a hat!). Leslie Pack Kaelbling described the perfect office mate, the \u201cEnduring Personal Cognitive Assistant,\u201d which will someday remind you to press forward on a neglected project or to soothe the ruffled feathers of a snubbed colleague. Martin Schmidt\u2019s lab is finding ways to plug the tiniest circuitry into a boot or a sneaker to harvest the body\u2019s energy for lightweight battery power out in the field. And Rodney Brooks showcased Kismet, a robot who can babble like an appreciative baby and bat her eyes in response to compliments. He also showed a remarkable video of another invention: a prosthetic leg that understands how it\u2019s being walked on and adapts. As a woman tries the leg out for the first time, she laughs with sheer delight.", "recorded": "2003-05-23T18:21:25", "title": "Engineering Human-Machine Relationships"}, {"url": "ppsn2014_eiben_vivo_veritas", "desc": "Evolutionary Computing (EC) is the research field concerned with artificial evolutionary processes in digital spaces, inside computers. In about three decades the EC community learned the \u2018art of taming evolution\u2019 and developed several evolutionary algorithm variants to solve optimization, design, and machine learning problems. In all these applications, the reproductive entities are digital. This holds even in evolutionary robotics where the evolved code is ported to a robot body and in evolutionary design where the evolved design is constructed physically after the evolutionary process terminates. In this talk I present a vision about the next big breakthrough: the creation of artificial evolutionary processes in physical spaces. In other words, I envision the \"Evolution of Things\", rather than just the evolution of digital objects, leading to a new field of Embodied Artificial Evolution. After presenting this vision I elaborate on some of the technical challenges and relate the main algorithmic/technical requirements to the current know-how in EC. Finally, I will speculate about possible applications, their societal impacts, and argue that these developments will radically change our lives.", "recorded": "2014-09-17T09:00:00", "title": "In Vivo Veritas: Towards the Evolution of Things"}, {"url": "icml09_taylor_kvfa", "desc": "A recent surge in research in kernelized approaches to reinforcement learning has sought to bring the benefits of kernelized machine learning techniques to reinforcement learning. Kernelized reinforcement learning techniques are fairly new and different authors have approached the topic with different assumptions and goals. Neither a unifying view nor an understanding of the pros and cons of different approaches has yet emerged. In this paper, we offer a unifying view of the different approaches to kernelized value function approximation for reinforcement learning. We show that, except for different approaches to regularization, Kernelized LSTD (KLSTD) is equivalent to a model based approach that uses kernelized regression to find an approximate reward and transition model, and that Gaussian Process Temporal Difference learning (GPTD) returns a mean value function that is equivalent to these other approaches. We also demonstrate the relationship between our model based approach and the earlier Gaussian Processes in Reinforcement Learning (GPRL). Finally, we decompose the Bellman error into the sum of transition error and reward error terms, and demonstrate through experiments that this decomposition can be helpful in choosing regularization parameters.", "recorded": "2009-06-16T11:30:00", "title": "Kernelized Value Function Approximation for Reinforcement Learning       "}, {"url": "mbc07_cemgil_hbm", "desc": "In recent years, there has been an increasing interest in statistical approaches and tools from machine learning for the analysis of audio and music signals, driven partially by applications in music information retrieval, computer aided music education and interactive music performance systems.\r The application of statistical techniques is quite natural: acoustical time series can be conveniently modelled using hierarchical signal models by incorporating prior knowledge from various sources: from physics or studies of human cognition and perception. Once a realistic hierarchical model is constructed, many audio processing tasks such as coding, restoration, transcription, separation, identification or resynthesis can be formulated consistently as Bayesian posterior inference problems.\r \\\\ \r In this talk, we will review recent advances in various signal models for audio and music signal analysis. In particular, factorial switching state space models, Gamma-Markov random fields will be discussed. Some models admit exact inference, otherwise efficient algorithms based on variational or stochastic approximation methods can be developed. We will illustrate applications on music transcription, tempo tracking, restoration and source separation applications.", "recorded": "2007-12-08T09:50:00", "title": "Hierarchical Bayesian Models for Audio and Music Processing"}, {"url": "kdd2014_ikbal_student_risks", "desc": "Poor academic performance in K-12 is often a precursor to unsatisfactory educational outcomes such as dropout, which are associated with significant personal and social costs. Hence, it is important to be able to predict students at risk of poor performance, so that the right personalized intervention plans can be initiated. In this paper, we report on a large-scale study to identify students at risk of not meeting acceptable levels of performance in one state-level and one national standardized assessment in Grade 8 of a major US school district. An important highlight of our study is its scale - both in terms of the number of students included, the number of years and the number of features, which provide a very solid grounding to the research. We report on our experience with handling the scale and complexity of data, and on the relative performance of various machine learning techniques we used for building predictive models. Our results demonstrate that it is possible to predict students at-risk of poor assessment performance with a high degree of accuracy, and to do so well in advance. These insights can be used to pro-actively initiate personalized intervention programs and improve the chances of student success.", "recorded": "2014-08-26T11:30:00", "title": "Predicting Student Risks Through Longitudinal Analysis"}, {"url": "kdd2010_tang_oei", "desc": "At Google, experimentation is practically a mantra; we evaluate almost every change that potentially affects what our users experience. Such changes include not only obvious user-visible changes such as modifications to a user interface, but also more subtle changes such as different machine learning algorithms that might affect ranking or content selection. Our insatiable appetite for experimentation has led us to tackle the problems of how to run more experiments, how to run experiments that produce better decisions, and how to run them faster. In this paper, we describe Google's overlapping experiment infrastructure that is a key component to solving these problems. In addition, because an experiment infrastructure alone is insufficient, we also discuss the associated tools and educational processes required to use it effectively. We conclude by describing trends that show the success of this overall experimental environment. While the paper specifically describes the experiment system and experimental processes we have in place at Google, we believe they can be generalized and applied by any entity interested in using experimentation to improve search engines and other web applications.\r\n", "recorded": "2010-07-26T16:00:00", "title": "Overlapping Experiment Infrastructure: More, Better, Faster Experimentation"}, {"url": "aml08_bunau_ssa", "desc": "Non-stationarities are an ubiquitous phenomenon in real-world data, yet they challenge standard Machine Learning methods: if training and test distributions differ we cannot, in principle, gen- eralise from the observed training sample to the test distribution. This affects both supervised and unsupervised learning algorithms. In a classification problem, for instance, we may infer spurious dependen- cies between data and label from the the training sample that are mere artefacts of the non-stationarities. Conversely, identifying the sources of non-stationary behaviour in order to better understand the analyzed system often lies at the heart of a scientific question. To this end, we propose a novel unsupervised paradigm: Stationary Subspace Analysis (SSA). SSA decomposes a multi-variate time-series into a stationary and a non-stationary subspace. We derive an efficient algorithm that hinges on an optimization procedure in the Special Orthogonal Group. By exploiting the Lie group structure of the optimization manifold, we can explicitly factor out the inherent symmetries of the problem and thereby reduce the number of parameters to the exact degrees of freedom. The practical utility of our approach is demonstrated in an application to Brain Computer-Interfacing (BCI).", "recorded": "2008-12-12T10:00:00", "title": "Stationary Subspace Analysis"}, {"url": "kolokviji_ekaykin_drilling", "desc": "On the 5th of February 2012 the hardened hands of drill masters turned a new page of the chronicle of the Antarctic exploration. In the heart of the snow desert at Russian Vostok Station, the head of a boring machine pierced the glacier to touch the water of the huge lake that had been hidden under 4 km of ice for millions of years. A long way led to this event, full of mystery, tragedy, disappointment and luck. An overview of Russian activities with focus on deep ice drilling at Vostok Station and the investigations of the subglacial Lake Vostok will be presented. How the Lake was discovered? Why is it important to explore it? What do we know about the Lake now? What shall we know after penetrating it? These and other questions will be addressed here. In addition, the climatic record over the past 420.000 years, as the main achievement of the Vostok deep ice coring, will be shortly discussed. Finally, the drilling operations that were carried out during the last summer expedition from November 2011 to February 2012 will be shown and some unique materials will be for the first time presented to the public to demonstrate the moment of the penetration to the Lake.", "recorded": "2012-04-18T13:00:00", "title": "Drilling the Ice from the Past to the Future"}, {"url": "machine_narasimhan_binary_classification", "desc": "We investigate the relationship between three fundamental problems in machine learning: binary classification, bipartite ranking, and binary class probability estimation (CPE). It is known that a good binary CPE model can be used to obtain a good binary classification model (by thresholding at 0.5), and also to obtain a good bipartite ranking model (by using the CPE model directly as a ranking model); it is also known that a binary classification model does not necessarily yield a CPE model. However, not much is known about other directions. Formally, these relationships involve regret transfer bounds. In this paper, we introduce the notion of weak regret transfer bounds, where the mapping needed to transform a model from one problem to another depends on the underlying probability distribution (and in practice, must be estimated from data). We then show that, in this weaker sense, a good bipartite ranking model can be used to construct a good classification model (by thresholding at a suitable point), and more surprisingly, also to construct a good binary CPE model (by calibrating the scores of the ranking model).", "recorded": "2014-01-20T12:00:00", "title": "On the Relationship Between Binary Classification, Bipartite Ranking, and Binary Class Probability Estimation"}, {"url": "ocwc2014_translectures_workshop", "desc": "**[[http://conference.ocwconsortium.org/2014/pre-conference-workshops/workshop-translectures-transcription-and-translation-of-video-lectures/|transLectures - Transcription and translation of Video Lectures]]**\r\n\r\ntransLectures is a European project whose academic and industrial partners represent the fields of language technologies, machine learning and audiovisual services. Online collections of video material are fast becoming a staple feature of the Internet and a key educational resource. What we are working on at transLectures is a set of easy-to-use tools that will allow users to add multilingual subtitles to these videos. In doing so, they will make the content of these videos available to a much wider audience in a way that is cost-effective and sustainable over the vast collections of online video lectures being generated. Its case studies are VideoLectures.Net and poliMedia, and the languages/language pairs being targeted in this project are EN, ES and SL for transcription, and EN<>ES, EN<>SL, EN>FR and EN>DE for translation. The end result are subtitles -transcriptions and translations- and, in a later phase, personalisation, contextualisation, descriptions, time alignment, fragmentation, recommendation and more for a dataset of 20k academic talks.", "recorded": "2014-04-22T09:00:00", "title": "Workshop: transLectures"}, {"url": "uai2011_domingos_kersting_combining", "desc": "AI problems are characterized by high degrees of complexity and uncertainty. Complexity is well handled by first-order logic, and uncertainty by probability. Combining the two in one language would be highly desirable, and the last decade has seen rapid progress in this direction. Many probabilistic logical languages have been proposed, and efficient inference and learning algorithms for them are available, often in open source software. Probabilistic logical techniques have been successfully applied to a wide variety of problems in natural language processing, vision, robotics, planning, social networks, the Web, and other areas. This tutorial begins with an overview of the key issues in this area and the solutions that have been proposed, from representation to learning and inference. As an example, we then focus on Markov logic, which attaches weights to first-order formulas and treats them as templates for features of log-linear models. We look in particular at the application of lifting techniques to probabilistic inference in relational domains, the combination of statistical learning with inductive logic programming (a.k.a. statistical relational learning), and the application of these techniques to machine reading.", "recorded": "2011-07-14T02:00:00", "title": "Combining Logic and Probability: Languages, Algorithms and Applications"}, {"url": "russir2012_yaroslavl", "desc": "The 6th Russian Summer School in Information Retrieval (RuSSIR 2012) was held on August 6-10, 2012 in Yaroslavl, Russia. The school was co-organized by Yaroslavl Demidov State University and Russian Information Retrieval Evaluation Seminar (ROMIP) with support from the MUMIA network.\r\n\r\nThe mission of the RuSSIR school series is to teach students about modern problems and methods in information retrieval and related disciplines, to stimulate scientific research and collaboration in the field; and to create environment for informal contacts between scientists, students and industry professionals. RuSSIR 2012 will focus on multilingual information access, cross-language information retrieval, and machine translation.\r\n\r\nThe target audience of the school is advanced graduate and PhD students, post-doctoral researchers, academic and industrial researchers, and developers. The working language of the school is English. RuSSIR 2012 will offer up to seven courses and host approximately 150 participants.\r\n\r\nTo find out more please visit the [[http://romip.ru/russir2012/index.php|RuSSIR 2012 website]].\r\n\r\n**//Disclaimer:// VideoLectures.NET emphasizes we are not the authors of these recordings.", "recorded": "2012-08-06T09:00:00", "title": "6th Russian Summer School in Information Retrieval (RuSSIR), Yaroslavl 2012"}, {"url": "kdd09_last_idmups", "desc": "Overall performance of the data mining process depends not just on the value of the induced knowledge but also on various costs of the process itself such as the cost of acquiring and pre-processing training examples, the CPU cost of model induction, and the cost of committed errors. Recently, several progressive sampling strategies for maximizing the overall data mining utility have been proposed. All these strategies are based on repeated acquisitions of additional training examples until a utility decrease is observed. In this paper, we present an alternative, projective sampling strategy, which fits functions to a partial learning curve and a partial run-time curve obtained from a small subset of potentially available data and then uses these projected functions to analytically estimate the optimal training set size. The proposed approach is evaluated on a variety of benchmark datasets using the RapidMiner environment for machine learning and data mining processes. The results show that the learning and run-time curves projected from only several data points can lead to a cheaper data mining process than the common progressive sampling methods.\r\n", "recorded": "2009-06-30T14:00:00", "title": "Improving Data Mining Utility with Projective Sampling"}, {"url": "mlss06au_muller_bci", "desc": "Brain Computer Interfacing (BCI) aims at making use of brain signals for e.g. the control of objects, spelling, gaming and so on. This tutorial will first provide a brief overview of the current BCI research activities and provide details in recent developments on both invasive and non-invasive BCI systems. In a second part -- taking a physiologist point of view -- the necessary neurological/neurophysical background is provided and medical applications are discussed. The third part -- now from a machine learning and signal processing perspective -- shows the wealth, the complexity and the difficulties of the data available, a truely enormous challenge. In real-time a multi-variate very noise contaminated data stream is to be processed and classified. Main emphasis of this part of the tutorial is placed on feature extraction/selection and preprocessing which includes among other techniques CSP and also ICA methods. Finally, I report in more detail about the Berlin Brain Computer (BBCI) Interface that is based on EEG signals and take the audience all the way from the measured signal, the preprocessing and filtering, the classification to the respective application. BCI communication is discussed in a clincial setting and for gaming.", "recorded": "2006-02-07T00:00:00", "title": "Brain Computer Interfaces"}, {"url": "mla09_preisach_ioeivd", "desc": "Vibration is the response of a system to an internal or external stimulus causing it to oscillate. Vibration causes dynamic stress if the system is excited at the same frequency as the so called Eigenmodes and this can damage the system [2]. Thus, the identi\fcation of Eigenmodes in vibration data is an important issue in the aerospace industry, e.g. jet engines need to be certi\fed before going into service and any dangerous vibration has to be detected. This data is usually analyzed manually, since this a time consuming process, machine learning can be applied in order to support engineers in their work. The vibration data is usually visualised as 2D images (campbell plots) and the Eigenmodes are displayed as lines.\r\n   We introduce an iterative algorithm using background knowledge for the identi\fcation of Eigenmodes. Our algorithms extends the original Hough Transform [3, 1], an image processing algorithm used for detection of lines and other parametrisable shapes. Finally we show in our evaluation that our approach for identifying Eigenmodes, applied on a data set provided by a major European jet engine manufacturer, outperforms the prediction of the Finite Element Model and is competitive to the base model using lab measurements.", "recorded": "2009-07-04T11:55:00", "title": "Identification of Eigenmodes in Vibration Data"}, {"url": "icml09_dasgupta_langford_actl", "desc": "Active learning is defined by contrast to the passive model of supervised learning where all the labels for learning are obtained without reference to the learning algorithm, while in active learning the learner interactively chooses which data points to label. The hope of active learning is that interaction can substantially reduce the number of labels required, making solving problems via machine learning more practical. This hope is known to be valid in certain special cases, both empirically and theoretically.\r\n\r\nVariants of active learning have been investigated over several decades and fields. The focus of this tutorial is on general techniques which are applicable to many problems. At a mathematical level, this corresponds to approaches with provable guarantees under weakest-possible assumptions since real problems are more likely to fit algorithms which work under weak assumptions.\r\n\r\nWe believe this tutorial should be of broad interest. People working on or using supervised learning are often confronted with the need for more labels, where active learning can help. Similarly, in reinforcement learning, generalizing while interacting in more complex ways is an active research topic. Please join us.", "recorded": "2009-06-14T16:00:00", "title": "Active Learning"}, {"url": "aistats2011_talwalkar_matrix", "desc": "Matrix coherence has recently been used to\r\ncharacterize the ability to extract global information\r\nfrom a subset of matrix entries in\r\nthe context of low-rank approximations and\r\nother sampling-based algorithms. The significance\r\nof these results crucially hinges upon\r\nthe possibility of efficiently and accurately\r\ntesting this coherence assumption. This paper\r\nprecisely addresses this issue. We introduce\r\na novel sampling-based algorithm for estimating\r\ncoherence, present associated estimation\r\nguarantees and report the results of\r\nextensive experiments for coherence estimation.\r\nThe quality of the estimation guarantees\r\nwe present depends on the coherence\r\nvalue to estimate itself, but this turns\r\nout to be an inherent property of samplingbased\r\ncoherence estimation, as shown by our\r\nlower bound. In practice, however, we find\r\nthat these theoretically unfavorable scenarios\r\nrarely appear, as our algorithm efficiently\r\nand accurately estimates coherence across a\r\nwide range of datasets, and these estimates\r\nare excellent predictors of the effectiveness of\r\nsampling-based matrix approximation on a\r\ncase-by-case basis. These results are significant\r\nas they reveal the extent to which coherence\r\nassumptions made in a number of recent\r\nmachine learning publications are testable.", "recorded": "2011-04-13T11:00:00", "title": "Can matrix coherence be efficiently and accurately estimated?"}, {"url": "clsp_chiang_syntactic", "desc": "Minimum-error-rate training (MERT) is a bottleneck for current development in statistical machine translation (MT) because it has difficulty estimating more than a dozen or two parameters. I will present two classes of features that address deficiencies in the Hiero hierarchical phrase-based translation model but cannot practically be trained using MERT. Instead, we use the MIRA algorithm, introduced by Crammer et al and previously applied to MT by Watanabe et al. Building on their work, we show that by parallel processing and utilizing more of the parse forest, we can obtain results using MIRA that match those of MERT in terms of both translation quality and computational requirements. We then test the method on the new features: first, simultaneously training a large number of Marton and Resnik's soft syntactic constraints, and, second, introducing a novel structural distortion model based on a large number of features. In both cases we obtain significant improvements in translation performance over the baseline.\r\n\r\nThis talk represents joint work with Yuval Marton and Philip Resnik of the University of Maryland.", "recorded": "2008-09-16T15:57:33", "title": "Online Large-Margin Training of Syntactic and Structural Translation Features"}, {"url": "esslli2011_mladenic_characters", "desc": "People use natural language and write texts to express themselves. For the purpose of text processing, text can be represented in different ways ranging from simply characters to capturing knowledge from the text in a form of logic. One of the key properties of natural languages is redundancy in the encoded information and the structure used. As a consequence, different techniques can extract different aspects of information from text. They range from simple techniques, such as character counting, to more sophisticated, such as linear algebra, to the advanced techniques which exploit the structural aspects of text. Many of these techniques deliver something useful and solve somebody\u2019s problem. Examples of such problems are: language identification (solved with character counting), document categorization (solved with linear algebra methods), question-answering (solved typically with shallow linguistic methods), and reasoning (solved typically using logic). The talk will present different text representations from the view of automatic text processing. In the second half of the talk we will take a look at some research results based on using machine learning methods and we will see demos of the corresponding prototype systems.", "recorded": "2011-08-11T19:00:30", "title": "Representing Text \u2013 from characters to logic"}, {"url": "colt2014_samadi_herding", "desc": "Herding is an algorithm of recent interest in the machine learning community, motivated by inference in Markov random fields. It solves the following sampling problem: given a set X\u2282Rd with mean \u03bc, construct an infinite sequence of points from X such that, for every t\u22651, the mean of the first t points in that sequence lies within Euclidean distance O(1/t) of \u03bc. The classic Perceptron boundedness theorem implies that such a result actually holds for a wide class of algorithms, although the factors suppressed by the O(1/t) notation are exponential in d. Thus, to establish a non-trivial result for the sampling problem, one must carefully analyze the factors suppressed by the O(1/t) error bound.\r\n\r\nThis paper studies the best error that can be achieved for the sampling problem. Known analysis of the Herding algorithm give an error bound that depends on geometric properties of X but, even under favorable conditions, this bound depends linearly on d. We present a new polynomial-time algorithm that solves the sampling problem with error O(d\u221alog2.5|X|/t) assuming that X is finite. Our algorithm is based on recent algorithmic results in discrepancy theory. We also show that any algorithm for the sampling problem must have error \u03a9(d\u221a/t). This implies that our algorithm is optimal to within logarithmic factors.", "recorded": "2014-06-15T09:00:00", "title": "Near-Optimal Herding"}, {"url": "mlmi04uk_oviatt_tai", "desc": "Techniques for information fusion are at the heart of multimodal system design. In this talk, I'll summarize recent work on predictive modeling of users' multimodal integration patterns, including that (1) there are large individual differences in users' dominant speech and pen multimodal integration patterns, (2) these patterns can be identified almost immediately and remain highly consistent for individual users over time, (3) they are highly resistant to change, even when users are given strong selective reinforcement or explicit instructions to switch patterns, and (4) these distinct patterns appear to derive from enduring differences among users in cognitive style. I'll also discuss findings on systematic entrenchment of users' dominant multimodal integration pattern when under load, including as task difficulty increases and during error handling. I'll conclude by highlighting work we are now pursuing that combines predictive user modeling with machine learning techniques to accelerate, generalize, and improve the reliability of information fusion during multimodal system processing. Implications of this research will be discussed for the design of adaptive multimodal systems with substantially improved performance characteristics.", "recorded": "2005-07-12T09:00:00", "title": "Toward Adaptive Information Fusion in Multimodal Systems"}, {"url": "icml09_jin_nlhmmblfwom", "desc": "Merchants selling products on the Web often ask their customers to share their opinions and hands-on experiences on products they have \r\npurchased. As e-commerce is becoming more and more popular, the number of customer reviews a product receives grows rapidly. This \r\nmakes it dif\ufb01cult for a potential customer to read them to make an informed decision on whether to purchase the product. In this research, \r\nwe aim to mine customer reviews of a product and extract highly speci\ufb01c product related entities on which reviewers express their opinions. \r\nOpinion expressions and sentences are also identi\ufb01ed and opinion orientations for each recognized product entity are classi\ufb01ed as positive \r\nor negative. Different from previous approaches that have mostly relied on natural language processing techniques or statistic information, \r\nwe propose a novel machine learning framework using lexicalized HMMs. The approach naturally integrates linguistic features, such as \r\npart-of-speech and surrounding contextual clues of words into automatic learning. The experimental results demonstrate the effectiveness \r\nof the proposed approach in web opinion mining and extraction from product reviews.", "recorded": "2009-06-16T14:25:00", "title": "A Novel Lexicalized HMM-Based Learning Framework for Web Opinion Mining"}, {"url": "mlg07_xu_fiiih", "desc": "Relational learning is an area of growing interest in machine learning (Dzeroski & Lavrac, 2001; Friedman et al.,\r 1999; Raedt & Kersting, 2003). Xu et al. (2006) introduced the infinite hidden relational model (IHRM)\r which views relational learning in context of the entity-relationship database model with entities, attributes and\r relations (compare also (Kemp et al., 2006)). In the IHRM, for each entity a latent variable is introduced. The\r latent variable is the only parent of the other entity attributes and is a parent of relationship attributes. The\r number of states in each latent variable is entity class specific. Therefore it is sensible to work with Dirichlet\r process (DP) mixture models in which each entity class can optimize its own representational complexity in a\r self-organized way. For our discussion it is sufficient to say that we integrate a DP mixture model into the IHRM\r by simply letting the number of hidden states for each entity class approach infinity. Thus, a natural outcome\r of the IHRM is a clustering of the entities providing interesting insight into the structure of the domain.", "recorded": "2007-08-02T17:30:00", "title": "Fast Inference in Infinite Hidden Relational Models"}, {"url": "acml2013_lin_cost_sensitive_classification", "desc": "Classification is an important problem in machine learning. It can be used in a variety of applications, such as distinguishing apples, oranges, and bananas automatically. Traditionally, the regular classification problem aims at minimizing the number of future mis-prediction errors. Nevertheless, many real-world applications require varying costs for different types of mis-classification errors. For instance, a false-negative prediction for a spam classification system only takes the user an extra second to delete the email, while a false-positive prediction can mean a huge loss when the email actually carries important information; in bacteria classification, mis-classifying a Gram-positive species as a Gram-negative one leads to totally ineffective treatments and is hence more serious than mis-classifying a Gram-positive species as another Gram-positive one; when classifying a patient as healthy, cold-infected, or H1N1-infected, predicting an H1N1-infected patient as healthy is significantly more serious than predicting a predicting a healthy patient as H1N1-infected. Such a cost-sensitive classification problem can be very different from the regular classification one, and can be used by applications like targeted marketing, information retrieval, medical decision making, object recognition and intrusion detection. In fact, cost-sensitive classification can be used to express any finite-choice and bounded-loss supervised learning problems, and connects to popular machine learning problems including ranking, structured learning and online decision making.\r\n\r\nBinary cost-sensitive classification problem considers only two kinds of costs: mis-predicting the first class as the second; mis-predicting the second class as the first. The studies on the problem can be traced back to the works of Elkan (2001) and Zadrozny (2003), which embeds the costs into the learning procedure by the technique of re-weighting the importance of each example. Multiclass cost-sensitive classification problem, on the other hand, can be more difficult than the binary one. There are several families of approaches:\\\\\r\n[1] by properly considering the costs during decision making rather than training, which tackles the problem from the Bayesian perspective (Domingos, 1999)\\\\\r\n[2] by re-weighting, which extends from the approach in binary classification (Zhou, 2006)\\\\\r\n[3] by reducing to regular classification, which generally needs re-weighting and re-labeling the examples (Abe, 2004; Lin, 2008)\\\\\r\n[4] by reducing to binary classification based on some different decomposition structures (Lin, 2008; Baygelzimer, 2005; Langford, 2005; Beygelzimer, 2007)\\\\\r\n[5] by reducing to regression by embedding the costs in the labels (Tu and Lin, 2010)\r\n\r\nIn addition to the algorithmic developments discussed above, cost-sensitive classification is being used in more and more applications in recent years. Selected applications that the speaker have first-hand experience include improving the performance in a real-world bacteria classification system (Jan, 2012), improving the ranking performance for information retrieval (Ruan, 2013), and constructing a useful model for recommender systems (Chen, 2011) which is part of the winning solution of the National Taiwan University team in KDD Cup 2011. The applications stimulate new needs for cost-sensitive classification, such as using the costs to approximate the true evaluation criteria of interest (Ruan, 2013), or being both cost-sensitive and error-sensitive (Jan, 2012).\r\n\r\nIn this tutorial, we review existing approaches of cost-sensitive classification, for both binary classification and multi-class classification. The approaches range from the Bayesian perspective of including costs during decision making to the reduction-based approaches that transform the cost-sensitive classification task to regular classification or regression tasks. We discuss the theoretical guarantees behind the approaches as well as their practical uses. We will also introduce some recent advances of cost-sensitive classification, including its success in applications. The tutorial assumes the basic background (techniques in classification and regression) in machine learning, and nothing more.", "recorded": "2013-11-13T08:30:00", "title": "Cost-sensitive Classification: Algorithms and Advances"}, {"url": "mitworld_willson_nt", "desc": "n a lecture that dips into both the anatomy and history of the semiconductor, Grant Willson offers some provocative thoughts on whether industry can continue improving on this most useful of inventions.\n\nHe describes how steady advances in miniaturization enabled the astonishing progress of microchips over the past 40 years. Today, says Willson, you can \u201cbuy a transistor for less than the cost of a single written character in your local newspaper.\u201d When he began at IBM in the 1970s, the silicon wafers produced were only 1 \u00bc inches in diameter; now \u201cthey\u2019re bigger than pizzas.\u201d\n\nWillson delves into the technological changes that both enabled printing on circuits to grow smaller, and the final product to grow larger. He details the original process of photolithography, involving designing a circuit pattern, then using a $25 million printer with a focused electron beam to reproduce the pattern on special glass, called a mask. It\u2019s the mask\u2019s pattern, etched onto a silicon wafer that forms the basis of the microchip. Layer after layer of these patterns get laid down on a single chip.\n\nThe machines behind these processes cost tens of millions of dollars. Just the lens for focusing laser light onto the wafer through the mask has 40 optical elements and weighs as much as a car. Over time, explains Wilson, \u201cPeople try to make bigger lenses to make smaller structures. The bigger the lens, the shorter the wavelength of light.\u201d In the \u201870s, recounts Wilson, machines were printing hundreds of miles of stuff the size of a bacterium. Today, with the help of chemical catalysts, printing has been reduced to less than 100 nanometers in diameter.\n\nBut there\u2019s a problem in reaching the next generation of super-small, mass-produced chips, believes Willson. Major manufacturers are investing hundreds of millions to figure out the right method to enable light to burn ever more Lilliputian lines on chips. \u201cEven if they succeed in building this tool, they will lose. Chemistry will defeat them in the end, and the machine will never work.\u201d According to Willson, the chemical catalyst diffuses and there\u2019s blurring of lines that should be sharp. Furthermore, a single machine of this type would cost $80 million, says Willson, putting production costs ludicrously high.\n\nSo has the march of improvement in semiconductor technology ended? Willson sees hope yet, in the form of Step and Flash Imprint lithography (S-FIL), a new approach to high resolution patterning, which can replicate shapes as small as 10 nanometers and at reasonable cost.", "recorded": "2007-08-30T19:24:11", "title": "Nanofabrication Technology: A View of the Future"}, {"url": "mitworld_willson_ntwf", "desc": "In a lecture that dips into both the anatomy and history of the semiconductor, Grant Willson offers some provocative thoughts on whether industry can continue improving on this most useful of inventions.\r\n\r\nHe describes how steady advances in miniaturization enabled the astonishing progress of microchips over the past 40 years. Today, says Willson, you can \u201cbuy a transistor for less than the cost of a single written character in your local newspaper.\u201d When he began at IBM in the 1970s, the silicon wafers produced were only 1 \u00bc inches in diameter; now \u201cthey\u2019re bigger than pizzas.\u201d\r\n\r\nWillson delves into the technological changes that both enabled printing on circuits to grow smaller, and the final product to grow larger. He details the original process of photolithography, involving designing a circuit pattern, then using a $25 million printer with a focused electron beam to reproduce the pattern on special glass, called a mask. It\u2019s the mask\u2019s pattern, etched onto a silicon wafer that forms the basis of the microchip. Layer after layer of these patterns get laid down on a single chip.\r\n\r\nThe machines behind these processes cost tens of millions of dollars. Just the lens for focusing laser light onto the wafer through the mask has 40 optical elements and weighs as much as a car. Over time, explains Wilson, \u201cPeople try to make bigger lenses to make smaller structures. The bigger the lens, the shorter the wavelength of light.\u201d In the \u201870s, recounts Wilson, machines were printing hundreds of miles of stuff the size of a bacterium. Today, with the help of chemical catalysts, printing has been reduced to less than 100 nanometers in diameter.\r\n\r\nBut there\u2019s a problem in reaching the next generation of super-small, mass-produced chips, believes Willson. Major manufacturers are investing hundreds of millions to figure out the right method to enable light to burn ever more Lilliputian lines on chips. \u201cEven if they succeed in building this tool, they will lose. Chemistry will defeat them in the end, and the machine will never work.\u201d According to Willson, the chemical catalyst diffuses and there\u2019s blurring of lines that should be sharp. Furthermore, a single machine of this type would cost $80 million, says Willson, putting production costs ludicrously high.\r\n\r\nSo has the march of improvement in semiconductor technology ended? Willson sees hope yet, in the form of Step and Flash Imprint lithography (S-FIL), a new approach to high resolution patterning, which can replicate shapes as small as 10 nanometers and at reasonable cost.", "recorded": "2007-08-07T10:48:37", "title": "Nanofabrication Technology: A View of the Future"}, {"url": "yaleecon159f07_polak_lec19", "desc": "We analyze three games using our new solution concept, subgame perfect equilibrium (SPE). The first game involves players' trusting that others will not make mistakes. It has three Nash equilibria but only one is consistent with backward induction. We show the other two Nash equilibria are not subgame perfect: each fails to induce Nash in a subgame. The second game involves a matchmaker sending a couple on a date. There are three Nash equilibria in the dating subgame. We construct three corresponding subgame perfect equilibria of the whole game by rolling back each of the equilibrium payoffs from the subgame. Finally, we analyze a game in which a firm has to decide whether to invest in a machine that will reduce its costs of production. We learn that the strategic effects of this decision - its effect on the choices of other competing firms - can be large, and if we ignore them we will make mistakes.\r\n\r\n**Reading assignment:**\r\n\r\nStrategies and Games: Theory And Practice. (Dutta): Chapter 13\r\n\r\nStrategy: An Introduction to Game Theory. (Watson): Chapter 16\r\n\r\n**Resources:**\r\n\r\n;[[http://oyc.yale.edu/sites/default/files/blackboard19_0.pdf|Blackboard Notes Lecture 19 [PDF] ]]", "recorded": "2007-09-09T09:00:00", "title": "Lecture 19 -  Subgame perfect equilibrium: matchmaking and strategic investments"}, {"url": "icml2015_xu_scale_free_networks", "desc": "Learning network structure underlying data is an important problem in machine learning. This paper presents a novel degree prior to study the inference of scale-free networks, which are widely used to model social and biological networks. In particular, this paper formulates scale-free network inference using Gaussian Graphical model (GGM) regularized by a node degree prior. Our degree prior not only promotes a desirable global degree distribution, but also exploits the estimated degree of an individual node and the relative strength of all the edges of a single node. To fulfill this, this paper proposes a ranking-based method to dynamically estimate the degree of a node, which makes the resultant optimization problem challenging to solve. To deal with this, this paper presents a novel ADMM (alternating direction method of multipliers) procedure. Our experimental results on both synthetic and real data show that our prior not only yields a scale-free network, but also produces many more correctly predicted edges than existing scale-free inducing prior, hub-inducing prior and the l1 norm.", "recorded": "2015-07-08T14:46:53", "title": "Learning Scale-Free Networks by Dynamic Node Specific Degree Prior"}, {"url": "ecmlpkdd2011_khurshid_divergences", "desc": "Fault localization, i.e., identifying erroneous lines of code in a buggy program, is a tedious process, which often requires considerable manual effort and is costly. Recent years have seen much progress in techniques for automated fault localization, specifically using program spectra - executions of failed and passed test runs provide a basis for isolating the faults. Despite the progress, fault localization in large programs remains a challenging problem, because even inspecting a small fraction of the lines of code in a large problem can require substantial manual effort. This paper presents a novel framework for fault localization based on latent divergences - an effective method for feature selection in machine learning. Our insight is that the problem of fault localization can be reduced to the problem of feature selection, where lines of code correspond to features. We also present an experimental evaluation of our framework using the Siemens suite of subject programs, which are a standard benchmark for studying fault localization techniques in software engineering. The results show that our framework enables more accurate fault localization than existing techniques.", "recorded": "2011-09-06T15:20:00", "title": "A Novel Framework for Locating Software Faults Using Latent Divergences"}, {"url": "nipsworkshops09_learning_multiple_sources", "desc": "**Learning from Multiple Sources with Applications to Robotics**\r\n\r\nLearning from multiple sources denotes the problem of jointly learning from a set of (partially) related learning problems / views / tasks. This general concept underlies several subfields receiving increasing interest from the machine learning community, which differ in terms of the assumptions made about the dependency structure between learning problems. In particular, the concept includes topics such as data fusion, transfer learning, multitask learning, multiview learning, and learning under covariate shift. Several approaches for inferring and exploiting complex relationships between data sources have been presented, including both generative and discriminative approaches.\r\n\r\nThe workshop will provide a unified forum for cutting edge research on learning from multiple sources; the workshop will examine the general concept, theory and methods, and will also examine robotics as a natural application domain for learning from multiple sources. The workshop will address methodological challenges in the different subtopics and further interaction between them. The intended audience is researchers working in fields of multi-modal learning, data fusion, and robotics.\r\n----\r\nThe Workshop homepage can be found at http://www.dcs.gla.ac.uk/~srogers/lms09/index.htm\r\n----", "recorded": "2009-12-12T07:30:00", "title": "Learning from Multiple Sources"}, {"url": "nipsworkshops2011_recht_lockfree", "desc": "Stochastic Gradient Descent (SGD) is a very popular optimization algorithm for solving data-driven machine learning problems. SGD is well suited to processing large amounts of data due to its robustness against noise, rapid convergence rates, and predictable memory footprint. Nevertheless, SGD seems to be impeded by many of the classical barriers to scalability: (1) SGD appears to be inherently sequential, (2) SGD assumes uniform sampling from the underlying data set resulting in poor locality, and (3) current approaches to parallelize SGD require performance-destroying, fine-grained communication.\r\nThis talk aims to refute the conventional wisdom that SGD inherently suffers from these impediments. Specifically, I will show that SGD can be implemented in parallel with minimal communication, with no locking or synchronization, and with strong spatial locality. I will provide both theoretical and experimental evidence demonstrating the achievement of linear speedups on multicore workstations on several benchmark optimization problems. Finally, I will close with a discussion of a challenging problem raised by our implementations relating arithmetic and geometric means of matrices.\r\nJoint work with Feng Niu, Christopher Re, and Stephen Wright. ", "recorded": "2011-12-16T16:00:00", "title": "Lock-Free Approaches to Parallelizing Stochastic Gradient Descent"}, {"url": "classconference2012_di_martino_project", "desc": "Cloud vendor lock-in and interoperability gaps arise (among many reasons) when semantics of resources and services, and of Application Programming Interfaces is not shared. Standards and techniques borrowed from SOA and Semantic Web Services areas might help in gaining shared, machine readable description of Cloud offerings (resources, Services at Platform and Application level, and their API groundings), thus allowing automatic discovery, matchmaking, and thus supporting selection, brokering, interoperability end even composition of Cloud Services among multiple Clouds. The EU funded mOSAIC project (http://www.mosaic-cloud.eu) aims at designing and developing an innovative open-source API and platform that enables applications to be Cloud providers' neutral and to negotiate Cloud services as requested by their users. Using the mOSAIC Cloud ontology and Semantic Engine, cloud applications' developers will be able to specify their services and resources requirements and communicate them to the mOSAIC Platform and Cloud Agency. The mOSAIC Cloud Agency will implement a multi-agent brokering mechanism that will search for Cloud services matching the applications\u2019 request, and possibly compose the requested service.", "recorded": "2012-10-24T12:20:00", "title": "Portability and Interoperability in Clouds: contributions from the mOSAIC Project"}, {"url": "ecmlpkdd2011_grbovic_loss", "desc": "Fault localization, i.e., identifying erroneous lines of code in a buggy program, is a tedious process, which often requires considerable manual effort and is costly. Recent years have seen much progress in techniques for automated fault localization, specifically using program spectra - executions of failed and passed test runs provide a basis for isolating the faults. Despite the progress, fault localization in large programs remains a challenging problem, because even inspecting a small fraction of the lines of code in a large problem can require substantial manual effort. This paper presents a novel framework for fault localization based on latent divergences - an effective method for feature selection in machine learning. Our insight is that the problem of fault localization can be reduced to the problem of feature selection, where lines of code correspond to features. We also present an experimental evaluation of our framework using the Siemens suite of subject programs, which are a standard benchmark for studying fault localization techniques in software engineering. The results show that our framework enables more accurate fault localization than existing techniques.", "recorded": "2011-09-06T14:00:00", "title": "Tracking Concept Change with Incremental Boosting by Minimization of the Evolving Exponential Loss"}, {"url": "solomon_rousu_sopef", "desc": "Enzyme function prediction is an important problem in post-genomic\r\nbioinformatics. There are two general methods for solving the problem:\r\ntransfer of annotation from a similar, already annotated protein, and\r\nmachine learning approaches that treat the problem as classification\r\nagainst a fixed taxonomy, such as Gene Ontology or the EC hierarchy.\r\nThese methods are suitable in cases where the function has been\r\npreviously characterized and included in the taxonomy. However, given a\r\nnew function that is not previously described, existing approaches\r\narguably do not offer adequate support for the human expert.\r\n\r\nIn this presentation, we I will present a structured output learning\r\napproach, where the enzyme function, an enzymatic reaction, is described\r\nin fine-grained fashion with so called reaction kernels which allow\r\ninterpolation and extrapolation in the output (reaction) space. A\r\nstructured output model is learned to predict enzymatic reactions from\r\nsequence motifs. We bring forward several choices for constructing\r\nreaction kernels and experiment with them in the remote homology case\r\nwhere the functions in the test set have not been seen in the training\r\nphase. Our experiments demonstrate the viability of our approach.", "recorded": "2009-09-04T10:00:00", "title": "Structured Output Prediction of Enzyme Function via Reaction Kernels"}, {"url": "nipsworkshops2013_koedinger_removing_barriers", "desc": "We have developed analytic methods to discover barriers to student learning from data for educational technology use (see learnlab.org). Such discoveries can guide the redesign of instruction and our online experiments demonstrate enhanced learning outcomes. Our analytic methods span issues of student skill acquisition, metacognition, and motivation. Focusing on the first, I will illustrate how alternative cognitive models of learning can be evaluated by translating them to statistical models and predicting learning curve data for model comparison. We have used machine learning in a couple of ways to generate alternative cognitive models, one more practical and other more cutting edge. The second involves a computational model of student learning, SimStudent, that learns as students do by using an intelligent tutoring system. The cognitive models SimStudent acquires have been demonstrated to yield empirically-verified discoveries not present in the human-designed cognitive models behind the intelligent tutoring systems. In other words, with SimStudent is the potential to not only create intelligent tutoring systems without AI programming, but to also produce systems that are pedagogically more effective than human-built tutoring systems.", "recorded": "2013-12-10T07:30:00", "title": "Discovering and removing barriers to learning"}, {"url": "kdd07_ye_imds", "desc": "The proliferation of malware has presented a serious threat to the security of computer systems. Traditional signature-based antivirus systems fail to detect polymorphic and new, previously unseen malicious executables. In this paper, resting on the analysis of Windows API execution sequences called by PE files, we develop the Intelligent Malware Detection System (IMDS) using Objective Oriented Association (OOA) mining based classification. IMDS is an integrated system consisting of three major modules: PE parser, OOA rule generator, and rule based classifier. An OOA Fast FPGrowth algorithm is adapted to efficiently generate OOA rules for classification. A comprehensive experimental study on a large collection of PE files obtained from the anti-virus laboratory of King Soft Corporation is performed to compare various malware detection approaches. Promising experimental results demonstrate that the accuracy and efficiency of our IMDS system outperform popular anti-virus software such as Norton AntiVirus and McAfee VirusScan, as well as previous data mining based detection systems which employed Naive Bayes, Support Vector Machine (SVM) and Decision Tree techniques.", "recorded": "2007-08-14T15:00:00", "title": "IMDS: Intelligent Malware Detection System"}, {"url": "kdd2014_bengio_deep_learning", "desc": "Deep learning has rapidly moved from a marginal approach in the machine learning community less than ten years ago to one that has strong industrial impact, in particular for high-dimensional perceptual data such as speech and images, but also natural language. The demand for experts in deep learning is growing very fast (faster than we can graduate PhDs), thereby considerably increasing their market value. Deep learning is based on the idea of learning multiple levels of representation, with higher levels computed as a function of lower levels, and corresponding to more abstract concepts automatically discovered by the learner. Deep learning arose out of research on artificial neural networks and graphical models and the literature on that subject has considerably grown in recent years, culminating in the creation of a dedicated conference (ICLR). The tutorial will introduce some of the basic algorithms, both on the supervised and unsupervised sides, as well as discuss some of the guidelines for successfully using them in practice. Finally, it will introduce current research questions regarding the challenge of scaling up deep learning to much larger models that can successfully extract information from huge datasets.", "recorded": "2014-08-24T14:30:00", "title": "Scaling Up Deep Learning"}, {"url": "mlsb2010_rousu_mpo", "desc": "Machine learning has become increasingly important in drug discovery where\r\nviable molecular structures are searched or designed for therapeutic efficacy. In\r\nparticular, the costly pre-clinical in vitro and in vivo testing of drug candidates\r\ncan be focused to the most promising molecules, if accurate in silico models are\r\navailable [7]. During the last decade kernel methods [3, 7, 2, 1, 10] have emerged\r\nas an effective way for modelling the activity of candidate drug molecules.\r\nHowever, classification methods focusing on a single target variable at a time\r\nare not optimally suited to drug screening applications where a large number of\r\ntarget cell lines are to be handled. In this paper we propose, to our knowledge,\r\nthe first multilabel learning approach for molecular classification. Our method\r\nbelongs to the structured output prediction family [6, 8, 4, 5], where graphical\r\nmodels and kernels have been successfully married in recent years. In our approach,\r\nthe drug targets (cancer cell lines) are organized in a network, drug\r\nmolecules are represented by kernels and discriminative max-margin training is\r\nused to learn the parameters. We demonstrate the benefits of the multilabel\r\nclassification approach on a dataset of 60 cancer cell lines and 4554 candidate\r\nmolecules.", "recorded": "2010-10-16T15:30:00", "title": "Multilabel prediction of drug activity "}, {"url": "kdd07_pan_dcssm", "desc": "Accurate localization of mobile objects is a major research problem in sensor networks and an important data mining application. Specifically, the localization problem is to determine the location of a client device accurately given the radio signal strength values received at the client device from multiple beacon sensors or access points. Conventional data mining and machine learning methods can be applied to solve this problem. However, all of them require large amounts of labeled training data, which can be quite expensive. In this paper, we propose a probabilistic semi-supervised learning approach to reduce the calibration effort and increase the tracking accuracy. Our method is based on semi-supervised conditional random fields which can enhance the learned model from a small set of training data with abundant unlabeled data effectively. To make our method more efficient, we exploit a Generalized EM algorithm coupled with domain constraints. We validate our method through extensive experiments in a real sensor network using Crossbow MICA2 sensors. The results demonstrate the advantages of methods compared to other state-of-the-art objecttracking algorithms.", "recorded": "2007-08-14T14:00:00", "title": "Domain-Constrained Semi-Supervised Mining of Tracking Models in Sensor Networks "}, {"url": "acl07_hovy_onp", "desc": "Many natural language processing (NLP) applications could benefit from a richer model of text meaning than the bag-of-words and n-gram models that currently predominate. Despite theoretical interest since the 1960s, however, no large-scale model exists; in fact, it is not even clear what such a model should minimally include.  However, the introduction of large-scale public resources such as the Penn TreeBank and WordNet have generated a great deal of progress in the NLP community, and so it seems increasingly important to create some kind of meaning-oriented model and build a corresponding corpus that is large enough to support adequate machine learning.\n\nThis talk argues for the necessity of (even shallow) semantics-based NLP, describes the contents and operation of the OntoNotes project, and in so doing introduces and explains the general issues facing annotation projects.  Our hope is that other people not only try to use the OntoNotes corpus in their own work, but also create their own annotations on the same material, so that more layers of shallow semantics can be included into OntoNotes.", "recorded": "2007-06-23T08:00:00", "title": "The OntoNotes Project: Building a Large Corpus of Semantically Annotated Text"}, {"url": "kdd09_li_otbpud", "desc": "In data publishing, anonymization techniques such as generalization and bucketization have been designed to provide privacy protection. In the meanwhile, they reduce the utility of the data. It is important to consider the tradeoff between privacy and utility. In a paper that appeared in KDD 2008, Brickell and Shmatikov proposed an evaluation methodology by comparing privacy gain with utility gain resulted from anonymizing the data, and concluded that \"even modest privacy gains require almost complete destruction of the data-mining utility\". This conclusion seems to undermine existing work on data anonymization. In this paper, we analyze the fundamental characteristics of privacy and utility, and show that it is inappropriate to directly compare privacy with utility. We then observe that the privacy-utility tradeoff in data publishing is similar to the risk-return tradeoff in financial investment, and propose an integrated framework for considering privacy-utility tradeoff, borrowing concepts from the Modern Portfolio Theory for financial investment. Finally, we evaluate our methodology on the Adult dataset from the UCI machine learning repository. Our results clarify several common misconceptions about data utility and provide data publishers useful guidelines on choosing the right tradeoff between privacy and utility.\r\n", "recorded": "2009-06-29T11:10:00", "title": "On the Tradeoff Between Privacy and Utility in Data Publishing "}, {"url": "kdd2010_li_mpn", "desc": "It is a big challenge to guarantee the quality of discovered relevance features in text documents for describing user preferences because of the large number of terms, patterns, and noise. Most existing popular text mining and classification methods have adopted term-based approaches. However, they have all suffered from the problems of polysemy and synonymy. Over the years, people have often held the hypothesis that pattern-based methods should perform better than term-based ones in describing user preferences, but many experiments do not support this hypothesis. The innovative technique presented in paper makes a breakthrough for this difficulty. This technique discovers both positive and negative patterns in text documents as higher level features in order to accurately weight low-level features (terms) based on their specificity and their distributions in the higher level features. Substantial experiments using this technique on Reuters Corpus Volume 1 and TREC topics show that the proposed approach significantly outperforms both the state-of-the-art term-based methods underpinned by Okapi BM25, Rocchio or Support Vector Machine and pattern based methods on precision, recall and F measures.\r\n", "recorded": "2010-07-27T11:00:00", "title": "Mining Positive and Negative Patterns for Relevance Feature Discovery"}, {"url": "aaai2012_von_ahn_duolingo", "desc": "I want to translate the web into every major language: every web page, every video, and, yes, even Justin Bieber's tweets. With its content split up into hundreds of languages \u2014 and with over 50 percent of it in English \u2014 most of the web is inaccessible to most people in the world. This problem is pressing, now more than ever, with millions of people from China, Russia, Latin America and other quickly developing regions entering the web. In this talk, I introduce my new project, called Duolingo, which aims at breaking the language barrier, and thus making the web truly world wide.\r\n\r\nWe have all seen how systems such as Google Translate are improving every day at translating the gist of things written in other languages. Unfortunately, they are not yet accurate enough for my purpose: Even when what they spit out is intelligible, it's so badly written that I can't read more than a few lines before getting a headache. This is why you don't see machine-translated books. With Duolingo, our goal is to encourage people, like you and me, to translate the web into their native languages.", "recorded": "2012-07-24T13:25:49", "title": "Duolingo: Translating the Web with Millions of People"}, {"url": "wsdm2011_hoi_msi", "desc": "With the popularity of various social media applications, massive social images associated with high quality tags have been made available in many social media web sites nowadays. Mining social images on the web has become an emerging important research topic in web search and data mining. In this paper, we propose a machine learning framework for mining social images and investigate its application to automated image tagging. To effectively discover knowledge from social images that are often associated with multimodal contents (including visual images and textual tags), we propose a novel Unified Distance Metric Learning (UDML) scheme, which not only exploits both visual and textual contents of social images, but also effectively unifies both inductive and transductive metric learning techniques in a systematic learning framework. We further develop an efficient stochastic gradient descent algorithm for solving the UDML optimization task and prove the convergence of the algorithm. By applying the proposed technique to the automated image tagging task in our experiments, we demonstrate that our technique is empirically effective and promising for mining social images towards some real applications.", "recorded": "2011-02-10T18:00:00", "title": "Mining Social Images with Distance Metric Learning for Automated Image Tagging"}, {"url": "lsoldm2013_nickel_tensor_factorization", "desc": "Relational data, i.e. data where information is represented via the relationships between entities, has become ubiquitous in many fields of application such as social network analysis, bioinformatics, and artificial intelligence. Furthermore, it is generated in an unprecedented amount in projects like the Semantic Web and Linked Open Data and is expected to drive next generation approaches to IR such as Google's Knowledge Graph. Learning from relational data, and in particular learning from large-scale relational data, is therefore an important task in machine learning. In this talk, we discuss a novel approach to relational learning which is based on the factorization of a third-order tensor. Due to its structure, the factorization exhibits a strong relational learning effect, what enables the efficient exploitation of contextual information that might be distant in the relational graph. Moreover, the computational complexity of the factorization scales linearly with the size of the data, what enables its application to complete knowledge bases consisting of millions of entities and billions of known facts -- even on commodity hardware. We will demonstrate the state-of-the-art performance of the approach for various relational learning tasks such as the prediction of unknown relationships, the deduplication of entities, or the unsupervised learning of taxonomies.", "recorded": "2013-09-23T15:35:41", "title": "Tensor Factorization for Large-Scale Relational Learning"}, {"url": "icml08_grunwald_mld", "desc": "We give a self-contained tutorial on the Minimum Description Length (MDL) approach to modeling, learning and prediction. We focus on the recent (post 1995) formulations of MDL, which can be quite different from the older methods that are often still called 'MDL' in the machine learning and UAI communities.\r \r In its modern guise, MDL is based on the concept of a `universal model'. We explain this concept at length. We show that previous versions of MDL (based on so-called two-part codes), Bayesian model selection and predictive validation (a variation of cross-validation) can all be interpreted as approximations to model selection based on 'universal models'. Modern MDL prescribes the use of a certain `optimal' universal model, the so-called `normalized maximum likelihood model' or `Shtarkov distribution'. This is related to (yet different from) Bayesian model selection with non-informative priors. It leads to a penalization of `complex' models that can be given an intuitive differential-geometric interpretation. Roughly speaking, the complexity of a parametric model is directly related to the number of distinguishable probability distributions that it contains. We also discuss some recent extensions such as the 'luckiness principle', which can be used if the Shtarkov distribution is undefined, and the 'switch distribution', which allows for a resolution of the AIC-BIC dilemma.", "recorded": "2008-07-09T09:00:00", "title": "MDL Tutorial"}, {"url": "ecmlpkdd2011_chen_hierarchical", "desc": "Hierarchical modeling and reasoning are fundamental in machine intelligence, and for this the two-parameter Poisson-Dirichlet Process (PDP) plays an important role. The most popular MCMC sampling algorithm for the hierarchical PDP and hierarchical Dirichlet Process is to conduct an incremental sampling based on the Chinese restaurant metaphor, which originates from the Chinese restaurant process (CRP). In this paper, with the same metaphor, we propose a new table representation for the hierarchical PDPs by introducing an auxiliary latent variable, called table indicator, to record which customer takes responsibility for starting a new table. In this way, the new representation allows full exchangeability that is an essential condition for a correct Gibbs sampling algorithm. Based on this representation, we develop a block Gibbs sampling algorithm, which can jointly sample the data item and its table contribution. We test this out on the hierarchical Dirichlet process variant of latent Dirichlet allocation (HDP-LDA) developed by Teh, Jordan, Beal and Blei. Experiment results show that the proposed algorithm outperforms their \"posterior sampling by direct assignment\" algorithm in both out-of-sample perplexity and convergence speed. The representation can be used with many other hierarchical PDP models.", "recorded": "2011-09-07T15:00:00", "title": "Sampling Table Configurations for the Hierarchical Poisson-Dirichlet Process"}, {"url": "sicgt07_blum_atosf", "desc": "Kernel methods have proven to be very powerful tools in machine learning. In addition, there is a well-developed theory of sufficient conditions for a kernel to be useful for a given learning problem. However, while a kernel function can be thought of as just a pairwise similarity function that satisfies additional mathematical properties, this theory requires viewing kernels as implicit (and often difficult to characterize) maps into high-dimensional spaces. In this talk I will describe a more general theory that applies to more general similarity functions (not just legal kernels) and furthermore describes the usefulness of a given similarity function in terms of more intuitive, direct properties of the induced weighted graph. An interesting feature of the proposed framework is that it can also be applied to learning from purely unlabeled data, i.e., clustering. In particular, one can ask how much stronger the properties of a similarity function should be (in terms of its relation to the unknown desired clustering) so that it can be used to *cluster* well. Investigating this question leads to a number of interesting graph-theoretic properties, and their analysis in the inductive setting uses regularity-lemma type results of [FK99,AFKK03]. This work is joint with Maria-Florina Balcan and Santosh Vempala.", "recorded": "2007-09-07T18:45:56", "title": "A theory of similarity functions for learning and clustering"}, {"url": "acai05_todorovski_em", "desc": "There is a variety of learning methods capable of inducing predictive models from data. In order to be able to decide which method to use on a particular data set of interest, we need systematic way to evaluate and compare the performance of different methods. This talk describes and illustrates the key criteria and methods for performance assessment and comparison. It first introduces predictive error as the most widely used criteria for evaluating predictive models. Since we want to evaluate predictive error of the model on independent test data, unseen in the learning process, the first part of the talk focuses on methods for evaluating predictive error on test data and resolving the well known bias-variance trade-off in machine learning. We also overview techniques for pair-wise comparison of learning methods' performance. While first part of the talk deals with classification task (i.e., predicting discrete variables) only, the second part of the talk provides wider perspective on evaluating methods for predicting class probability distribution, numeric variables, and dealing with situations where the error depends on type of the misclassification. Finally, we learn how to assess other aspects of predictive performance, such as complexity of the induced models and their comprehensibility.", "recorded": "2005-07-05T09:00:00", "title": "Evaluation Methodology"}, {"url": "colt2013_bach_matrix", "desc": "We consider supervised learning problems within the positive-definite kernel framework, such as kernel ridge regression, kernel logistic regression or the support vector machine. With kernels leading to infinite-dimensional feature spaces, a common practical limiting difficulty is the necessity of computing the kernel matrix, which most frequently leads to algorithms with running time at least quadratic in the number of observations n, i.e., O(n2). Low-rank approximations of the kernel matrix are often considered as they allow the reduction of running time complexities to O(p2n), where p is the rank of the approximation. The practicality of such methods thus depends on the required rank p. In this paper, we show that for approximations based on a random subset of columns of the original kernel matrix, the rank p may be chosen to be linear in the degrees of freedom associated with the problem, a quantity which is classically used in the statistical analysis of such methods, and is often seen as the implicit number of parameters of non-parametric estimators. This result enables simple algorithms that have sub-quadratic running time complexity, but provably exhibit the same predictive performance than existing algorithms, for any given problem instance, and not only for worst-case situations.", "recorded": "2013-06-13T09:45:00", "title": "Sharp analysis of low-rank kernel matrix approximations"}, {"url": "nips09_hsu_duin", "desc": "A classic debate in cognitive science revolves around understanding how children learn complex linguistic rules, such as those governing restrictions on verb alternations, without negative evidence. Traditionally, formal learnability arguments have been used to claim that such learning is impossible without the aid of innate language-specific knowledge. However, recently, researchers have shown that statistical models are capable of learning complex rules from only positive evidence. These two kinds of learnability analyses differ in their assumptions about the role of the distribution from which linguistic input is generated. The former analyses assume that learners seek to identify grammatical sentences in a way that is robust to the distribution from which the sentences are generated, analogous to discriminative approaches in machine learning. The latter assume that learners are trying to estimate a generative model, with sentences being sampled from that model. We show that these two learning approaches differ in their use of implicit negative evidence -- the absence of a sentence -- when learning verb alternations, and demonstrate that human learners can produce results consistent with the predictions of both approaches, depending on the context in which the learning problem is presented.", "recorded": "2009-12-08T16:10:00", "title": "Differential Use of Implicit Negative Evidence in Generative and Discriminative Language Learning"}, {"url": "eswc2012_kolcz_twitter", "desc": "Twitter represents a large complex network of users with diverse and continuously evolving interests. Discussions and interactions range from very small to very large groups of people and most of them occur in the public. Interests are both long and short term and are expressed by the content generated by the users as well as via the Twitter follow graph, i.e. who is following whose content. Understanding user interests is crucial to providing good Twitter experience by helping users to connect to others, find relevant information and interesting information sources. The manner in which information is spread over the network and communication attempts are made can also help in identifying spammers and other service abuses. Understanding users and their preferences is also a very challenging problem due to the very large volume information, the fast rate of change and the short nature of the tweets. \r\nLarge scale machine learning as well as graph and text mining have been helping us to tackle these problems and create new opportunities to better understand our users. In the talk I will describe a number of challenging modeling problems addressed by the Twitter team as well as our approach to creating frameworks and infrastructure to make learning at scale possible.", "recorded": "2012-05-31T14:00:00", "title": "Large Scale Learning at Twitter"}, {"url": "nipsworkshops2011_vanderplas_processing", "desc": "Wide-field probes of weak gravitational lensing have the potential to address fundamental questions about the nature of the universe. Measures such as the correlation function, power spectrum, or statistics of shear peaks can be compared with theoretical predictions to answer substantive question about the nature of dark matter, dark energy, gravity, and primordial perturbations. Comparison of the data to the theoretical model, however, can be subject to systematic effects due to survey geometry, selection functions, and other biases. This can be framed as a machine learning problem: given a sparse set of noisy observations, how can one best recover the underlying signal of interest? We propose to address these challenges using a compressed-sensing approach based on a Karhunen-Loeve (KL) model of the signal. This approach can efficiently recover the shear signal from\r\nnoisy data with arbitrary masking and survey geometry. The signal-to-noise-ranked KL vectors allow effective noise filtration, leading to a 30% decrease in B-mode contamination for simulated data. Furthermore, because the KL model is based on covariance matrices, it naturally encapsulates the two-point information of the field and provides a framework for efficient Bayesian likelihood analysis of the two-point statistics of a cosmological shear", "recorded": "2011-12-16T08:36:00", "title": "Processing Shear Maps with Karhunen-Loeve Analysis"}, {"url": "icml2015_gu_model_selection", "desc": "Model selection with cross validation (CV) is very popular in machine learning. However, CV with grid and other common search strategies cannot guarantee to find the model with minimum CV error, which is often the ultimate goal of model selection. Recently, various solution path algorithms have been proposed for several important learning algorithms including support vector classification, Lasso, and so on. However, they still do not guarantee to find the model with minimum CV error.In this paper, we first show that the solution paths produced by various algorithms have the property of piecewise linearity. Then, we prove that a large class of error (or loss) functions are piecewise constant, linear, or quadratic w.r.t. the regularization parameter, based on the solution path. Finally, we propose a new generalized error path algorithm (GEP), and prove that it will find the model with minimum CV error for the entire range of the regularization parameter. The experimental results on a variety of datasets not only confirm our theoretical findings, but also show that the best model with our GEP has better generalization error on the test data, compared to the grid search, manual search, and random search.", "recorded": "2015-07-09T14:46:53", "title": "A New Generalized Error Path Algorithm for Model Selection"}, {"url": "kdd07_castano_oba", "desc": "Analyzing data on-board a spacecraft as it is collected enables several advanced spacecraft capabilities, such as prioritizing observations to make the best use of limited bandwidth and reacting to dynamic events as they happen.\n\nIn this paper, we describe how we addressed the unique challenges associated with on-board mining of data as it is collected: uncalibrated data, noisy observations, and severe limitations on computational and memory resources. The goal of this effort, which falls into the emerging application area of spacecraft-based data mining, was to study three specific science phenomena on Mars.\n\nFollowing previous work that used a linear support vector machine (SVM) on-board the Earth Observing 1 (EO-1) spacecraft, we developed three data mining techniques for use on-board the Mars Odyssey spacecraft. These methods range from simple thresholding to state-of-the-art reduced-set SVM technology. We tested these algorithms on archived data in a flight software testbed.\n\nWe also describe a significant, serendipitous science discovery of this data mining effort: the confirmation of a water ice annulus around the north polar cap of Mars. We conclude with a discussion on lessons learned in developing algorithms for use on-board a spacecraft.", "recorded": "2007-08-14T11:30:00", "title": "On-board Analysis of Uncalibrated Data for a Spacecraft at Mars "}, {"url": "kdd2010_liu_lci", "desc": "Existing cost-sensitive learning methods require that the unequal misclassification costs should be given as precise values. In many real-world applications, however, it is generally difficult to have a precise cost value since the user maybe only knows that one type of mistake is much more severe than another type, yet it is infeasible to give a precise description. In such situations, it is more meaningful to work with a cost interval instead of a precise cost value. In this paper we report the first study along this direction. We propose the CISVM method, a support vector machine, to work with cost interval information. Experiments show that when there are only cost intervals available, CISVM is significantly superior to standard cost-sensitive SVMs using any of the minimal cost, mean cost and maximal cost to learn. Moreover, considering that in some cases other information about costs can be obtained in addition to cost intervals, such as the distribution of costs, we propose a general approach CODIS for using the distribution information to help improve performance. Experiments show that this approach can reduce 60% more risks than the standard cost-sensitive SVM which assumes the expected cost is the true value.\r\n", "recorded": "2010-07-26T14:00:00", "title": "Learning with Cost Intervals"}, {"url": "sahd2014_teh_mondrian_forests", "desc": "Ensembles of randomized decision trees are widely used for \r\nclassification and regression tasks in machine learning and statistics. \r\nThey achieve competitive predictive performance and are computationally \r\nefficient to train (batch setting) and test, making them excellent \r\ncandidates for real world prediction tasks. However, the most popular \r\nvariants (such as Breiman's random forest and extremely randomized trees) \r\nwork only in the batch setting and cannot handle streaming data easily. In \r\nthis talk, I will present Mondrian Forests, where random decision trees are \r\ngenerated from a Bayesian nonparametric model called a Mondrian process \r\n(Roy and Teh, 2009). Making use of the remarkable consistency properties of \r\nthe Mondrian process, we develop a variant of extremely randomized trees \r\nthat can be constructed in an incremental fashion efficiently, thus making \r\ntheir use on streaming data simple and efficient. Experiments on real world \r\nclassification tasks demonstrate that Mondrian Forests achieve competitive \r\npredictive performance comparable with existing online random forests and \r\nperiodically retrained batch random forests, while being more than an order \r\nof magnitude faster, thus representing a better computation vs accuracy \r\ntradeoff.", "recorded": "2014-09-04T12:30:00", "title": "Mondrian forests: Efficient random forests for streaming data via Bayesian nonparametrics"}, {"url": "ecmlpkdd2011_furnkranz_iteration", "desc": "This paper makes a first step toward the integration of two subfields of machine learning, namely preference learning and reinforcement learning (RL). An important motivation for a \"preference-based\" approach to reinforcement learning is a possible extension of the type of feedback an agent may learn from. In particular, while conventional RL methods are essentially confined to deal with numerical rewards, there are many applications in which this type of information is not naturally available, and in which only qualitative reward signals are provided instead. Therefore, building on novel methods for preference learning, our general goal is to equip the RL agent with qualitative policy models, such as ranking functions that allow for sorting its available actions from most to least promising, as well as algorithms for learning such models from qualitative feedback. Concretely, in this paper, we build on an existing method for approximate policy iteration based on roll-outs. While this approach is based on the use of classification methods for generalization and policy learning, we make use of a specific type of preference learning method called label ranking. Advantages of our preference-based policy iteration method are illustrated by means of two case studies.", "recorded": "2011-09-08T09:50:00", "title": "Preference-based policy iteration: Leveraging preference learning for reinforcement learning"}, {"url": "mlg07_nowozin_wsm", "desc": "In web-related applications of image categorization, it is\r desirable to derive an interpretable classification rule with\r high accuracy. Using the bag-of-words representation and\r the linear support vector machine, one can partly fulfill the\r goal, but the accuracy of linear classifiers is not high and\r the obtained features are not informative for users. We propose\r to combine item set mining and large margin classifiers\r to select features from the power set of all visual words. Our\r resulting classification rule is easier to browse and simpler\r to understand, because each feature has richer information.\r As a next step, each image is represented as a graph where\r nodes correspond to local image features and edges encode\r geometric relations between features. Combining graph mining\r and boosting, we can obtain a classification rule based\r on subgraph features that contain more information than the\r set features. We evaluate our algorithm in a web-retrieval\r ranking task where the goal is to reject outliers from a set of\r images returned for a keyword query. Furthermore, it is evaluated\r on the supervised classification tasks with the challenging\r VOC2005 data set. Our approach yields excellent accuracy\r in the unsupervised ranking task and competitive results\r in the supervised classification task.", "recorded": "2007-08-03T11:30:00", "title": "Weighted Substructure Mining for Image Analysis"}, {"url": "pmnp07_grzegorczyk_regp", "desc": "One of the major goals in systems biology is to infer the architecture of\r biochemical pathways and regulatory networks from postgenomic data, such as\r microarray gene expression and cytometric protein expression data. Various\r reverse engineering Machine Learning methods have been proposed in the literature,\r and it is important to understand their relative merits and shortcomings.\r In the talk the learning performances of three different graphical models machine\r learning methods, namely Relevance networks, Gaussian Graphical Models, and\r Bayesian networks, are cross-compared on real cytometric protein data and simulated\r data from the RAF signalling pathway. Relevance networks are based\r on pairwise association scores and straightforward to implement. But the inference\r is not done in the context of the whole system and there is no possibility\r to distinguished between direct and indirect associations. Both shortcomings\r are addressed by Gaussian graphical models, where the partial correlation between\r two variables, conditional on all the other domain variables, is employed\r as association score. Bayesian networks are more flexible probabilistic graphical\r models for conditional dependence and independence relations. Bayesian\r networks are based on directed acyclic graphs and can be exploited to analyse\r interventional data for identifying putative causal interactions. The empirical\r results were obtained by applying the shrinkage estimator of Schaefer and\r Strimmer (2005) to compute the inverse covariance matrix for Gaussian Graphical\r Models, and Bayesian network inference was done by sampling BNs from the\r posterior distribution with order Markov chain Monte Carlo (MCMC), as proposed\r by Friedman and Koller (2003). The experimental results were obtained\r by analysing data from the RAF protein signalling network reported in Sachs\r et al. (2005); which describes the interaction of eleven phosphorylated proteins\r and phospholipids in human immune system cells. Thereby it was distinguished\r between real cytometric protein activity measurements reported in Sachs et al.\r (2005) and synthetically generated data as well as between pure observational\r and interventional data. Observational data are obtained by passively monitoring\r the system without any interference while interventional data are obtained\r by actively manipulating variables, e.g. using gene knock-out experiments. Detailed results of this empirical study have been published in Werhli et al. (2006)\r and Grzegorczyk (2007). The three main findings can be summarized as follows.\r First, exclusively on Gaussian observational data, Bayesian networks and Gaussian\r graphical models were found to outperform Relevance networks. Second,\r for observational data no significant difference between Bayesian networks and\r Gaussian Graphical models was observed. Third, only for interventional data\r Bayesian networks clearly performed superior to the other two approaches.", "recorded": "2007-09-04T15:30:13", "title": "Reverse engineering gene and protein regulatory networks using graphical models: A comparative evaluation study"}, {"url": "bmvc2013_zhang_preserving_projections", "desc": "Linear projection for reducing data dimensionality is a common practice in various\r\ndata processing applications. Among the existing projection methods, Principal Component\r\nAnalysis (PCA) is arguably the most popular one. Standard PCA used in image\r\npreprocessing pursues the projection directions by minimizing the reconstruction error\r\nin a least square sense. However, since PCA does not adapt to the data or any specific\r\ndomains, it may lead to severe loss of certain discriminative features during the projection,\r\nand damage the performance of either human perception (e.g. stimulus in the visual\r\ncortex, as modeled by Gabor wavelets), or machine perceptions (e.g. recognizing the images\r\nbased on a certain type of visual features), or both. In this paper, we propose a novel\r\nPerception Preserving Projections (PPP) method to preserve the information for specific\r\nperception systems. In particular, PPP incorporates domain-specific feature extractor into\r\nthe standard PCA formulation for the projection learning procedure. This enables PPP to\r\nmake more sensible projections for feature based perception systems while retaining the\r\nsimplicity and unsupervised manner of PCA. In experimental studies, PPP shows clear\r\neffectiveness and improvement over PCA in terms of two performance metrics: feature\r\nextraction deviation and the pattern recognition accuracy.", "recorded": "2013-09-10T12:20:00", "title": "Perception Preserving Projections"}, {"url": "nipsworkshops09_graphical_models", "desc": "**Approximate Learning of Large Scale Graphical Models: Theory and Applications**\r\n\r\nUndirected graphical models provide a powerful framework for representing dependency structure between random variables. Learning the parameters of undirected models plays a crucial role in solving key problems in many machine learning applications, including natural language processing, visual object recognition, speech perception, information retrieval, computational biology, and many others. Learning in undirected graphical models of large treewidth is difficult because of the hard inference problem induced by the partition function for maximum likelihood learning, or by finding the MAP assignment for margin-based loss functions. Over the last decade, there has been considerable progress in developing algorithms for approximating the partition function and MAP assignment, both via variational approaches (e.g., belief propagation) and sampling algorithms (e.g., MCMC). More recently, researchers have begun to apply these methods to learning large, densely-connected undirected graphical models that may contain millions of parameters. A notable example is the learning of Deep Belief Networks and Deep Boltzmann Machines, that employ MCMC strategy to greedily learn deep hierarchical models.\r\n----\r\nThe Workshop homepage can be found at http://www.cs.toronto.edu/~rsalakhu/workshop_nips2009/index.html.\r\n----", "recorded": "2009-12-12T07:30:00", "title": "Large Scale Graphical Models"}, {"url": "aistats2011_wainwright_convex", "desc": "Problems that require estimating high-dimensional matrices from noisy observations arise frequently in statistics and machine learning.  Examples include dimensionality reduction methods (e.g., principal components and canonical correlation), collaborative filtering and matrix completion (e.g., Netflix and other recommender systems), multivariate regression, estimation of time-series models, and graphical model learning.  When the sample size is less than the matrix dimensions, all of these problems are ill-posed, so that some type of structure is required in order to obtain interesting results.\r\n\r\nIn recent years, relaxations based on the nuclear norm and other types of convex matrix regularizers have become popular.  By framing a broad class of problems as special cases of matrix regression, we present a single theoretical result that provides guarantees on the accuracy of such convex relaxations.  Our general result can be specialized to obtain various non-asymptotic bounds, among them sharp rates for noisy forms of matrix completion, matrix compression, and matrix decomposition.  In all of these cases, information-theoretic methods can be used to show that our rates are minimax-optimal, and thus cannot be substantially improved upon by any algorithm, regardless of computational complexity.", "recorded": "2011-04-13T08:00:00", "title": "Convex Relaxation and Estimation of High-Dimensional Matrices"}, {"url": "nipsworkshops09_pednault_hmli", "desc": "Hadoop is an open-source implementation of Google's Map-Reduce programming model. Over the past few years, it has evolved into a popular platform for parallelization in industry and academia. Furthermore, trends suggest that Hadoop will likely be the analytics platform of choice on forthcoming Cloud-based systems. Unfortunately, implementing parallel machine learning/data mining (ML/DM) algorithms on Hadoop is complex and time consuming. To address this challenge, we present Hadoop-ML, an infrastructure to facilitate the implementation of parallel ML/DM algorithms on Hadoop. Hadoop-ML has been designed to allow for the specification of both task-parallel and data-parallel ML/DM algorithms. Furthermore, it supports the composition of parallel ML/DM algorithms using both serial as well as parallel building blocks -- this allows one to write reusable parallel code. The proposed abstraction eases the implementation process by requiring the user to only specify computations and their dependencies, without worrying about scheduling, data management, and communication. As a consequence, the codes are portable in that the user never needs to write Hadoop-specific code. This potentially allows one to leverage future parallelization platforms without rewriting one's code. ", "recorded": "2009-12-11T10:02:00", "title": "Hadoop-ML: An Infrastructure for the Rapid Implementation of Parallel Reusable Analytics"}, {"url": "kdd09_liu_lsslr", "desc": "Logistic Regression is a well-known classification method that has been used widely in many applications of data mining, machine learning, computer vision, and bioinformatics. Sparse logistic regression embeds feature selection in the classification framework using the L1-norm regularization, and is attractive in many applications involving high-dimensional data. In this paper, we propose Lassplore for solving large-scale sparse logistic regression. Specifically, we formulate the problem as the L1-ball constrained smooth convex optimization, and propose to solve the problem using the Nesterov's method, an optimal first-order black-box method for smooth convex optimization. One of the critical issues in the use of the Nesterov's method is the estimation of the step size at each of the optimization iterations. Previous approaches either applies the constant step size which assumes that the Lipschitz gradient is known in advance, or requires a sequence of decreasing step size which leads to slow convergence in practice. In this paper, we propose an adaptive line search scheme which allows to tune the step size adaptively and meanwhile guarantees the optimal convergence rate. Empirical comparisons with several state-of-the-art algorithms demonstrate the efficiency of the proposed Lassplore algorithm for large-scale problems.\r\n", "recorded": "2009-06-29T14:45:00", "title": "    \t Large-Scale Sparse Logistic Regression "}, {"url": "kdd2010_kumar_dmppeh", "desc": "Health insurance costs across the world have increased alarmingly in recent years. A major cause of this increase are payment errors made by the insurance companies while processing claims. These errors often result in extra administrative effort to re-process (or rework) the claim which accounts for up to 30% of the administrative staff in a typical health insurer. We describe a system that helps reduce these errors using machine learning techniques by predicting claims that will need to be reworked, generating explanations to help the auditors correct these claims, and experiment with feature selection, concept drift, and active learning to collect feedback from the auditors to improve over time. We describe our framework, problem formulation, evaluation metrics, and experimental results on claims data from a large US health insurer. We show that our system results in an order of magnitude better precision (hit rate) over existing approaches which is accurate enough to potentially result in over $15-25 million in savings for a typical insurer. We also describe interesting research problems in this domain as well as design choices made to make the system easily deployable across health insurance companies.", "recorded": "2010-07-27T11:02:00", "title": "Data Mining to Predict and Prevent Errors in Health Insurance Claims Processing"}, {"url": "ccss09_sornette_fbreb", "desc": "The financial crisis of 2008, which started with an initially well-defined epicenter focused on mortgage backed securities (MBS), has been cascading into a global economic recession, whose increasing severity and uncertain duration had led and is continuing to lead to massive losses and damage for billions of people. Heavy central bank interventions and government spending programs have been launched worldwide and especially in the USA and Europe, in the hope to unfreeze credit and boltster consumption. Here, I present evidence and articulate a general framework that allows one to diagnose the fundamental cause of the unfolding financial and economic crisis: the accumulation of several bubbles and their interplay and mutual reinforcement has led to an illusion of a perpetual money machine allowing financial institutions to extract wealth from an unsustainable artificial process. Taking stock of this diagnostic, I conclude that many of the intervention to address the so-called liquidity crisis and to encouragemore consumption are ill-advised and even dangerous, given the lack of precautionary reserves that have been unaccumulated in the good times and the huge liabilities. The most interesting presents times constitute unique opportunities but also great challenges, for which I offer a few recommendations.", "recorded": "2009-06-09T16:25:00", "title": "Financial Bubbles, Real Estate Bubbles, Derivative Bubbles, and the Financial and Economic Crisis"}, {"url": "nipsworkshops2011_bovy_classification", "desc": "Quasars\u2014actively accreting supermassive black holes\u2014are among the most luminous objects in the Universe. Large samples of quasars can be used to study topics including inflationary cosmology, the evolution of black hole growth over the course of cosmic history, and the physics of astrophysical black hole accretion. One of the major challenges for the peta-scale surveys of the future is to classify and estimate the distances to quasars without the need for expensive spectroscopic follow-up. I will present currently used techniques to classify quasars from broadband photometry, focusing on the XDQSO method\u2014a probabilistic method that uses the extreme-deconvolution density estimation technique to handle missing and highly uncertain data\u2014and a critical appraisal of other machine learning methods currently used. Going forward the major challenges will be to\r\n(1) incorporate variability and astrometric data into the currently used color selection for optimal quasar selection, (2) separate quasars from galaxies (as opposed to stars) as we go to fainter magnitudes, and (3) strike a balance between data-driven, non-parametric methods\u2014which work well for bright quasars\u2014and template-based techniques\u2014necessary for faint quasars where host-galaxy contamination of the observed flux is significant.", "recorded": "2011-12-16T08:25:00", "title": "Quasar classification and characterization from broadband multi-filter, multi-epoch data sets"}, {"url": "kdd09_yan_fasc", "desc": "Spectral clustering refers to a flexible class of clustering procedures that can produce high-quality clusterings on small data sets but which has limited applicability to large-scale problems due to its computational complexity of O(n3) in general, with n the number of data points. We extend the range of spectral clustering by developing a general framework for fast approximate spectral clustering in which a distortion-minimizing local transformation is first applied to the data. This framework is based on a theoretical analysis that provides a statistical characterization of the effect of local distortion on the mis-clustering rate. We develop two concrete instances of our general framework, one based on local k-means clustering (KASP) and one based on random projection trees (RASP). Extensive experiments show that these algorithms can achieve significant speedups with little degradation in clustering accuracy. Specifically, our algorithms outperform k-means by a large margin in terms of accuracy, and run several times faster than approximate spectral clustering based on the Nystrom method, with comparable accuracy and significantly smaller memory footprint. Remarkably, our algorithms make it possible for a single machine to spectral cluster data sets with a million observations within several minutes.", "recorded": "2009-06-28T11:00:00", "title": "Fast Approximate Spectral Clustering"}, {"url": "kdd2014_lan_learning", "desc": "We propose SPARFA-Trace, a new machine learning-based framework for time-varying learning and content analytics for educational applications. We develop a novel message passing-based, blind, approximate Kalman filter for sparse factor analysis (SPARFA) that jointly traces learner concept knowledge over time, analyzes learner concept knowledge state transitions (induced by interacting with learning resources, such as textbook sections, lecture videos, etc., or the forgetting effect), and estimates the content organization and difficulty of the questions in assessments. These quantities are estimated solely from binary-valued (correct/incorrect) graded learner response data and the specific actions each learner performs (e.g., answering a question or studying a learning resource) at each time instant. Experimental results on two online course datasets demonstrate that SPARFA-Trace is capable of tracing each learner's concept knowledge evolution over time, analyzing the quality and content organization of learning resources, and estimating the question--concept associations and the question difficulties. Moreover, we show that SPARFA-Trace achieves comparable or better performance in predicting unobserved learner responses compared to existing collaborative filtering and knowledge tracing methods.", "recorded": "2014-08-25T15:00:00", "title": "Time-Varying Learning and Content Analytics via Sparse Factor Analysis"}, {"url": "acmmm2010_milani_cae", "desc": "Reliable delivery of 3D video contents to a wide set of users\r\nis expected to be the next big revolution in multimedia ap-\r\nplications provided that it is possible to grant a certain level\r\nof Quality-of-Experience (QoE) to the end user. During the\r\nlast years, several cross-layer solutions have proved to be ex-\r\ntremely effective in tuning the transmission parameters at\r\nthe different layers of the protocol stack and in maximiz-\r\ning the perceptual quality of the reconstructed 3D scene.\r\nAmong these, Cognitive Source Coding (CSC) schemes (de-\r\nfined in analogy with Cognitive Radio systems) make possi-\r\nble to improve the quality of the 3D QoE at the receiver by\r\nadapting the source coding strategy according to the state\r\nof the transmission channel and to the characteristics of the\r\ncoded signal. This knowledge also permits an optimization\r\nof the computational complexity required at the encoder.\r\nThe paper presents a CSC architecture that analyzes the\r\n3D scene, identifies the different elements, and chooses the\r\nmost appropriate coding strategy via a classification of the\r\nfeatures of each element based on Support Vector Machine\r\ntheory. Experimental results show that the proposed ap-\r\nproach permits improving the quality of the received 3D\r\nsignal with respect to traditional cross-layer techniques and\r\nreducing the computational complexity of coding operation.", "recorded": "2010-10-27T11:10:00", "title": "A Cognitive Approach for Effective Coding and Transmission of 3D Video"}, {"url": "kdd2013_zafarani_behavioral_modeling", "desc": "People use various social media for different purposes. The\r\ninformation on an individual site is often incomplete. When\r\nsources of complementary information are integrated, a better pro\fle of a user can be built to improve online services\r\nsuch as verifying online information. To integrate these\r\nsources of information, it is necessary to identify individuals across social media sites. This paper aims to address\r\nthe cross-media user identifi\fcation problem. We introduce\r\na methodology (MOBIUS) for \ffinding a mapping among\r\nidentities of individuals across social media sites. It consists of three key components: the \ffirst component identiti\fes\r\nusers' unique behavioral patterns that lead to information\r\nredundancies across sites; the second component constructs\r\nfeatures that exploit information redundancies due to these\r\nbehavioral patterns; and the third component employs machine learning for e\u000bffective user identi\fcation. We formally\r\ndefi\fne the cross-media user identi\ffication problem and show\r\nthat MOBIUS is e\u000bffective in identifying users across social\r\nmedia sites. This study paves the way for analysis and mining across social media sites, and facilitates the creation of\r\nnovel online services across sites.", "recorded": "2013-08-12T10:30:46", "title": "Connecting Users across Social Media Sites: A Behavioral-Modeling Approach"}, {"url": "snnsymposium2010_roy_bgbm", "desc": "In the last few years, how robots operate in the world has advanced considerably. Examples include the autonomous vehicles in the DARPA Grand Challenges and Urban Challenge, the considerable work in robot mapping, and the growing interest in home and service robots. However, these example technologies and systems are still mostly restricted to research prototypes. One obstacle to getting more widely useful robots is that the way robots reason about their world is still pretty different to how people reason. Robots think in terms of point features, dense occupancy grids and action cost maps. People think in terms of landmarks, segmented objects and tasks (among other representations). There are good reasons why these are different, and robots are unlikely to ever reason about the world in the same way that people do. But, there has been recent work in bridging the gap between low-level geometry and control, and higher-level semantic representations. I will talk about how machine learning is being used to develop more capable robots that can operate in populated environments and perform complex tasks. I will discuss the state of the art, what the open challenges are and the potential impact of solving these challenges. ", "recorded": "2010-11-17T09:57:00", "title": "Bridging the gap between machines and people"}, {"url": "mmdss07_artieres_ulpum", "desc": "User modeling is progressively becoming an important and generic component of\r many applications and services. The mains reasons that explain this phenomenon are\r the tasks increasing complexity and the wide variety of users.\r \\\\ \r nformation systems, hypermedia, websites, and application software are becoming\r more and more complex, hence difficult to use efficiently. Also, the amount of on-line\r information available to a user through Internet is huge and is still increasing everyday\r so that recovering information is becoming harder and harder. Finally, together\r with the huge development of Internet, more and more on-line commercial websites\r and services are proposed to Internet users. The aim and interest of user modeling\r consists in these situations in helping the user to efficiently use the systems he is offered\r and to retrieve the information he is looking for by filtering the information according\r to his will and needs. Furthermore, while many software, hypermedia, websites\r and services are potentially used by a variety of users, these systems have been\r traditionally developed in a \u201cone size fits all\u201d manner. Consequently, they are often\r not adapted to most of the users, with various knowledge, preferences, and needs. In\r this context user modeling allows personalizing such systems, their content or presentation,\r in order to fit the individual.", "recorded": "2007-09-11T11:15:23", "title": "User logs processing using machine learning techniques "}, {"url": "icml2015_zhou_safe_subspace_screening", "desc": "Nuclear norm regularization has been shown very promising for pursing a low rank matrix solution in various machine learning problems. Many efforts have been devoted to develop efficient algorithms for solving the optimization problem in nuclear norm regularization. Solving it for large-scale matrix variables, however, is still a challenging task since the complexity grows fast with the size of matrix variable. In this work, we propose a novel method called safe subspace screening (SSS), to improve the efficiency of the solver for nuclear norm regularized least squares problems. Motivated by the fact that the low rank solution can be represented by a few subspaces, the proposed method accurately discards a predominant percentage of inactive subspaces prior to solving the problem to reduce problem size. Consequently, a much smaller problem is required to solve, making it more efficient than optimizing the original problem. The proposed SSS is safe, in that its solution is identical to the solution from the solver. In addition, the proposed SSS can be used together with any existing nuclear norm solver since it is independent of the solver. Extensive results on several synthetic and real data sets show that the proposed SSS is very effective in inactive subspace screening.", "recorded": "2015-07-07T12:57:30", "title": "Safe Subspace Screening for Nuclear Norm Regularized Least Squares Problems"}, {"url": "aaai2010_atlanta", "desc": "AAAI-10 is the Twenty-Fourth AAAI Conference on Artificial Intelligence (AI). The purpose of this conference is to promote research in AI and scientific exchange among AI researchers, practitioners, scientists, and engineers in related disciplines. AAAI-10 will have multiple technical tracks, student abstracts, poster sessions, invited speakers, and exhibit programs, all selected according to the highest reviewing standards.\r\n\r\nAAAI-10 will feature technical papers on mainstream AI topics as well as novel cross-cutting work in related areas. Topics include but are not limited to the following:\r\n\r\n    * Agents\r\n    * Cognitive modeling and human interaction\r\n    * Commonsense reasoning\r\n    * Constraint satisfaction and optimization\r\n    * Evolutionary computation\r\n    * Game playing and interactive entertainment\r\n    * Information integration and extraction\r\n    * Knowledge acquisition and ontologies\r\n    * Knowledge representation and reasoning\r\n    * Machine learning and data mining\r\n    * Model-based systems\r\n    * Multiagent systems\r\n    * Natural language processing\r\n    * Planning and scheduling\r\n    * Probabilistic reasoning\r\n    * Robotics\r\n    * Search\r\n\r\nDetailed information on the conference can be found at http://www.aaai.org/Conferences/AAAI/aaai10.php.\r\n\r\n**//Disclaimer:// VideoLectures.NET emphasizes that the quality of these videos was notably improved, because of low light and sound quality conditions provided in the lecture auditorium.**", "recorded": "2010-07-11T09:00:00", "title": "24th AAAI Conference on Artificial Intelligence, Atlanta 2010"}, {"url": "icwsm2013_momeni_media_objects", "desc": "User-generated comments in online social media have recently been gaining increasing attention as a viable source of general-purpose descriptive annotations for digital objects like photos or videos. Because users have different levels of expertise, however, the quality of their comments can vary from very useful to entirely useless. Our aim is to provide automated support for the curation of useful user-generated comments from public collections of digital objects. After constructing a crowd-sourced gold standard of useful and not useful comments, we use standard machine learning methods to develop a usefulness classifier, exploring the impact of surface-level, syntactic, semantic, and topic-based features in addition to extra-linguistic attributes of the author and his or her social media activity. We then adapt an existing model of prevalence detection that uses the learned classifier to investigate patterns in the commenting culture of two popular social media platforms. We find that the prevalence of useful comments is platform-specific and is further influenced by the entity type of the media object being commented on (person, place, event), its time period (e.g., year of an event), and the degree of polarization among commenters.", "recorded": "2013-07-08T12:23:00", "title": "Properties, Prediction, and Prevalence of Useful User-Generated Comments for Descriptive Annotation of Social Media Objects"}, {"url": "ecmlpkdd2011_akrour_robot", "desc": "Many machine learning approaches in robotics, based on reinforcement learning, inverse optimal control or direct policy learning, critically rely on robot simulators. This paper investigates a simulatorfree direct policy learning, called Preference-based Policy Learning (PPL). PPL iterates a four-step process: the robot demonstrates a candidate policy; the expert ranks this policy comparatively to other ones according to her preferences; these preferences are used to learn a policy return estimate; the robot uses the policy return estimate to build new candidate policies, and the process is iterated until the desired behavior is obtained. PPL requires a good representation of the policy search space be available, enabling one to learn accurate policy return estimates and limiting the human ranking effort needed to yield a good policy. Furthermore, this representation cannot use informed features (e.g., how far the robot is from any target) due to the simulator-free setting. As a second contribution, this paper proposes a representation based on the agnostic exploitation of the robotic log.\r\n\r\nThe convergence of PPL is analytically studied and its experimental validation on two problems, involving a single robot in a maze and two interacting robots, is presented.", "recorded": "2011-09-08T09:10:00", "title": "Direct Policy Ranking with Robot Data Streams"}, {"url": "icml09_jebara_gcm", "desc": "Graph based semi-supervised learning (SSL)\r\nmethods play an increasingly important role\r\nin practical machine learning systems. A\r\ncrucial step in graph based SSL methods\r\nis the conversion of data into a weighted\r\ngraph. However, most of the SSL literature\r\nfocuses on developing label inference algorithms\r\nwithout extensively studying the\r\ngraph building method and its effect on performance.\r\nThis article provides an empirical\r\nstudy of leading semi-supervised methods\r\nunder a wide range of graph construction\r\nalgorithms. These SSL inference algorithms\r\ninclude the Local and Global Consistency\r\n(LGC) method, the Gaussian Random\r\nField (GRF) method, the Graph Transduction\r\nvia Alternating Minimization (GTAM)\r\nmethod as well as other techniques. Several\r\napproaches for graph construction, sparsification\r\nand weighting are explored including\r\nthe popular k-nearest neighbors method\r\n(kNN) and the b-matching method. As opposed\r\nto the greedily constructed kNN graph,\r\nthe b-matched graph ensures each node in the\r\ngraph has the same number of edges and produces\r\na balanced or regular graph. Experimental\r\nresults on both artificial data and real\r\nbenchmark datasets indicate that b-matching\r\nproduces more robust graphs and therefore\r\nprovides significantly better prediction accuracy\r\nwithout any significant change in computation\r\ntime.", "recorded": "2009-06-15T14:55:00", "title": "Graph Construction and b-Matching for Semi-Supervised Learning"}, {"url": "colt2013_guha_thakurta_lasso", "desc": "We design differentially private algorithms for statistical model selection. Given a data set and a large, discrete collection of \u201cmodels\u201d, each of which is a family of probability distributions, the goal is to determine the model that best \u201cfits\u201d the data. This is a basic problem in many areas of statistics and machine learning. We consider settings in which there is a well-defined answer, in the following sense: Suppose that there is a nonprivate model selection procedure f, which is the reference to which we compare our performance. Our differentially private algorithms output the correct value f(D) whenever f is stable on the input data set D. We work with two notions, perturbation stability and sub-sampling stability. We give two classes of results: generic ones, that apply to any function with discrete output set; and specific algorithms for the problem of sparse linear regression. The algorithms we describe are efficient and in some cases match the optimal non-private asymptotic sample complexity. Our algorithms for sparse linear regression require analyzing the stability properties of the popular LASSO estimator. We give sufficient conditions for the LASSO estimator to be robust to small changes in the data set, and show that these conditions hold with high probability under essentially the same stochastic assumptions that are used in the literature to analyze convergence of the LASSO.", "recorded": "2013-06-13T11:25:00", "title": "Differentially Private Feature Selection via Stability Arguments, and the Robustness of the Lasso"}, {"url": "icml07_tenenbaum_bmhi", "desc": "In everyday learning and reasoning, people routinely draw successful generalizations from very limited evidence. Even young children can infer the meanings of words, hidden properties of objects, or the existence of causal relations from just one or a few relevant observations -- far outstripping the capabilities of conventional learning machines. How do they do it? And how can we bring machines closer to these human-like learning abilities? I will argue that people's everyday inductive leaps can be understood as approximations to Bayesian computations operating over structured representations of the world, what cognitive scientists have called \"intuitive theories\" or \"schemas\". For each of several everyday learning tasks, I will consider how appropriate knowledge representations are structured and used, and how these representations could themselves be learned via Bayesian methods. The key challenge is to balance the need for strongly constrained inductive biases -- critical for generalization from very few examples -- with the flexibility to learn about the structure of new domains, to learn new inductive biases suitable for environments which we could not have been pre-programmed to perform in. The models I discuss will connect to several directions in contemporary machine learning, such as semi-supervised learning, structure learning in graphical models, hierarchical Bayesian modeling, and nonparametric Bayes.", "recorded": "2007-06-23T09:00:00", "title": "Bayesian models of human inductive learning"}, {"url": "colt2015_ma_sparse_coding", "desc": "Sparse coding is a basic task in many fields including signal processing, neuroscience and machine learning where the goal is to learn a basis that enables a sparse representation of a given set of data, if one exists. Its standard formulation is as a non-convex optimization problem which is solved in practice by heuristics based on alternating minimization. Recent work has resulted in several algorithms for sparse coding with provable guarantees, but somewhat surprisingly these are outperformed by the simple alternating minimization heuristics. Here we give a general framework for understanding alternating minimization which we leverage to analyze existing heuristics and to design new ones also with provable guarantees. Some of these algorithms seem implementable on simple neural architectures, which was the original motivation of Olshausen and Field \\cite{OF} in introducing sparse coding. We also give the first efficient algorithm for sparse coding that works almost up to the information theoretic limit for sparse recovery on incoherent dictionaries. All previous algorithms that approached or surpassed this limit run in time exponential in some natural parameter. Finally, our algorithms improve upon the sample complexity of existing approaches. We believe that our analysis framework will have applications in other settings where simple iterative algorithms are used.", "recorded": "2015-07-04T09:20:00", "title": "Analyzing Non-Convex Optimization for Sparse Coding"}, {"url": "kdd2014_dalessandro_transfer_learning", "desc": "Internet display advertising is a critical revenue source for publishers and online content providers, and is supported by massive amounts of user and publisher data. Targeting display ads can be improved substantially with machine learning methods, but building many models on massive data becomes prohibitively expensive computationally. This paper presents a combination of strategies, deployed by the online advertising firm Dstillery, for learning many models from extremely high-dimensional data efficiently and without human intervention. This combination includes: (i)~A method for simple-yet-effective transfer learning where a model learned from data that is relatively abundant and cheap is taken as a prior for Bayesian logistic regression trained with stochastic gradient descent (SGD) from the more expensive target data. (ii)~A new update rule for automatic learning rate adaptation, to support learning from sparse, high-dimensional data, as well as the integration with adaptive regularization. We present an experimental analysis across 100 different ad campaigns, showing that the transfer learning indeed improves performance across a large number of them, especially at the start of the campaigns. The combined \"hands-free\" method needs no fiddling with the SGD learning rate, and we show that it is just as effective as using expensive grid search to set the regularization parameter for each campaign.", "recorded": "2014-08-27T13:45:00", "title": "Scalable Hands Free Transfer Learning for Online Advertising"}, {"url": "mmdss07_raez_uli", "desc": "We report on some experiences using linguistic information as additional features\r in a classical Vector Space Model[10]. Extracted information of every word like\r the Part Of Speech and stem, lexical root have been combined in different ways\r for experimenting on a possible improvement in the classification performance\r and on several algorithms, like SVM [3], BBR [] and PLAUM [6].\r Automatic Text Classification, or Automatic Text Categorization as is also\r known, tries to related documents to predefined set of classes. Extensive research\r has been carried out on this subject [11] and a wide range of techniques are appliable\r to solve this task: feature extraction [5], feature weighting, dimensionality\r reduction [4], machine learning algorithms and more. Besides, the classification\r task can be either binary (one out of two possible classes to select), multi-class\r (one out of set of possible classes) or multi-label (a set of classes from a larger set\r of potential candidates). In most cases, the latter two can be reduced to binary\r decisions [1], as the used algorithm does in our experiments [8].\r In order to verify the contribution of the new features, we have combined\r them to be included into the vector space model by preprocessing the Reuters-\r 215781 collection, a well known set of data by the research community devoted\r to text categorization problems [2].", "recorded": "2007-09-14T16:15:48", "title": "Using linguistic information as features for text categorization "}, {"url": "nipsworkshops2011_rowe_lensing", "desc": "One of the most profound mysteries in modern cosmology is the accelerated expansion of the universe (the discovery of which led to the 2011 physics Nobel Prize). Weak gravitational lensing, an observational method that has the potential to shed the most light on this mystery, relies on accurate measurement of the shapes of millions of galaxies to uncover tiny distortions caused by matter between the galaxies and us. However, accurately inferring the true galaxy shapes is complicated due to large distortions from the atmosphere, telescope optics, detector and pixel noise. As data arrives in greater quantities, requirements on measurement accuracy become more stringent, and weak lensing must now meet unprecedented image analysis challenges. This need has driven ongoing improvements to shape measurement algorithms, and led to the creation of public data analysis challenges, of which the STEP1, STEP2, GREAT08 and GREAT10 challenges are recent examples. Some approaches have been successfully honed and tested by astronomers, but winning entrants have also been found from the machine learning community. In this poster we summarize what has been learned about shape measurement systematics from previous challenges, and highlight critical issues for the field in the near future, which will be tested in the next weak lensing data challenge (currently under development).", "recorded": "2011-12-16T10:08:00", "title": "GREAT3: The next weak lensing data challenge"}, {"url": "uai08_klein_ul", "desc": "Given the abundance of text data, unsupervised approaches are very appealing for natural language processing. We present three latent variable systems which achieve state-of-the-art results in domains previously dominated by fully supervised systems. For syntactic parsing, we describe a grammar induction technique which begins with coarse syntactic structures and iteratively refines them in an unsupervised fashion. The resulting coarse-to-fine grammars admit efficient coarse-to-fine inference schemes and have produced the best parsing results in a variety of languages. For co reference resolution, we describe a discourse model in which entities are shared across documents using a hierarchical Dirichlet process. In each document, entities are repeatedly rendered into mention strings by a sequential model of attentional state and anaphoric constraint. Despite being fully unsupervised, this approach is competitive with the best supervised approaches. Finally, for machine translation, we present a model which learns translation lexicons from non-parallel corpora. Alignments between word types are modeled by a prior over matchings. Given any fixed alignment, a joint density over word vectors derives from probabilistic canonical correlation analysis. This approach is capable of discovering high-precision translations, even when the underlying corpora and languages are divergent.", "recorded": "2008-07-11T14:30:00", "title": "Unsupervised Learning for Natural Language Processing"}, {"url": "lsoldm2013_deisenroth_autonomous_systems", "desc": "Autonomous learning has been a promising direction in control and robotics for more than a decade since learning models and controllers from data allows us to reduce the amount of engineering knowledge that is otherwise required. Due to their flexibility, autonomous reinforcement learning (RL) approaches typically require many interactions with the system to learn controllers. However, in real systems, such as robots, many interactions can be impractical and time consuming. To address this problem, current learning approaches typically require task-specific knowledge in form of expert demonstrations, pre-shaped policies, or specific knowledge about the underlying dynamics. \\n We follow a different approach and speed up learning by efficiently extracting information from sparse data. In particular, we learn a probabilistic, non-parametric Gaussian process dynamics model. By explicitly incorporating model uncertainty into long-term planning and controller learning our approach reduces the effects of model errors, a key problem in model-based learning. Bayesian inference leads to an automatic exploration/exploitation trade-off, such that our model-based policy search method achieves an unprecedented speed of learning compared to state-of-the art RL. We demonstrate its applicability to autonomous learning in real robot and control tasks.", "recorded": "2013-09-24T12:40:18", "title": "Bayesian Machine Learning for Controlling Autonomous Systems"}, {"url": "russir08_saggion_nlpiaa", "desc": "This course focus on the development of practical applications which involve the use of natural language technology. The course will introduce NLP concepts which will be reinforced by the development, testing, and evaluation of technology in demonstration sessions. Applications to be studied in the course include: Information Extraction, Question Answering, and Text Summarization. None of the applications will be studied in detail, the main objective of the course is to promote the use of NLP and to facilitate access to available technology which can be adapted to specific application domains so that students can go home motivated to develop their own tools/systems.\r\nDetailed content:\r\n\u2013 Overview of Natural Language Processing technologies including parts of speech tagging, named entity recognition, parsing, semantic interpretation and coreference resolution.\r\n\u2013 Natural Language Technology for Information access: existent systems and projects combining advanced NLP will be presented (e.g. Cubreporter project).\r\n\u2013 Information Extraction: named entity recognition, relation extraction, event extraction, rule-based and machine learning approaches, evaluation, MUC.\r\n\u2013 Question Answering: QA architecture, questions and answers, passage selection, answer identification, evaluation, TREC/QA.\r\n\u2013 Text Summarization: sentence extraction, superficial features for sentence extraction, feature combination, multi-document summarization, evaluation, Document Understanding Conference. ", "recorded": "2008-09-02T17:15:00", "title": "Hands-on Natural Language Processing for Information Access Applications (NLPIAA)"}, {"url": "acl07_dagan_tte", "desc": "Recognizing Textual Entailment is the task of determining, for example, that the sentence: \"Google files for its long awaited IPO\" entails that \"Google goes public\". Determining whether the meaning of a given text passage entails that of another or whether they have the same meaning is a fundamental problem in natural language understanding that requires the ability to abstract over the inherent syntactic and semantic variability in natural language. This challenge is at the heart of many natural language understanding tasks including Question Answering, Information Retrieval and Extraction, Machine Translation, and others that attempt to reason about and capture the meaning of linguistic expressions. The task has attracted significant interest over the last couple of years mainly fostered by the PASCAL Recognizing Textual Entailment Challenge (RTE). A substantial number of papers on these topics have been published in major conferences and workshops in the last couple of years. The primary goals of this tutorial are to review the framework of applied Textual Entailment and motivate it as a generic paradigm for natural language semantics.\n\nWe will present some of the key computational approaches proposed and some of the obstacles identified by the research community in this area, as a way to promote further research. The tutorial will thus be useful for many of the senior and junior researchers that have prior or new interest in this area, providing a concise overview of recent perspectives and research results.", "recorded": "2007-06-24T08:00:00", "title": "Tutorial on Textual Entailment"}, {"url": "wsdm2010_leela_lup", "desc": "Presence of duplicate documents in the World Wide Web ad- versely affects crawling, indexing and relevance, which are the core building blocks of web search. In this paper, we present a set of techniques to mine rules from URLs and utilize these rules for de-duplication using just URL strings without fetching the content explicitly. Our technique is composed of mining the crawl logs and utilizing clusters of similar pages to extract transformation rules, which are used to normalize URLs belonging to each cluster. Preserv- ing each mined rule for de-duplication is not efficient due to the large number of such rules. We present a machine learning technique to generalize the set of rules, which re- duces the resource footprint to be usable at web-scale. The rule extraction techniques are robust against web-site spe- cific URL conventions. We compare the precision and scal- ability of our approach with recent efforts in using URLs for de-duplication. Experimental results demonstrate that our approach achieves 2 times more reduction in duplicates with only half the rules compared to the most recent previ- ous approach. Scalability of the framework is demonstrated by performing a large scale evaluation on a set of 3 Billion URLs, implemented using the MapReduce framework.", "recorded": "2010-02-06T14:11:48", "title": "Learning URL Patterns for Webpage De-duplication"}, {"url": "nipsworkshops2010_widmer_mmk", "desc": "The lack of sufficient training data is the limiting factor for\r\nmany Machine Learning applications in Computational Biology.\r\nIf data is available for several different but related problem\r\ndomains, Multitask Learning algorithms can be used to learn a\r\nmodel based on all available information. However, combining\r\ninformation from several tasks requires careful consideration of\r\nthe degree of similarity between tasks. We propse to use the\r\nrecently published q-Norm Multiple Kernel Learning algorithm\r\nto simultaneously learn or refine the similarity matrix between\r\ntasks along with the Multitask Learning classifier by formulating\r\nthe Multitask Learning problem as Multiple Kernel Learning. We\r\ndemonstrate the performance of our method on two problems\r\nfrom Computational Biology. First, we show that our method is\r\nable to improve performance on a splice site dataset with given\r\nhierarchical task structure by refining the task relationships.\r\nSecond, we consider an MHC-I dataset, for which we assume\r\nno knowledge about the degree of task relatedness. Here, we\r\nare able to learn the task similarity ab initio. Our framework is\r\nvery general as it allows to incorporate prior knowledge about\r\ntasks relationships if available, but is also able to identify task\r\nsimilarities in absence of such prior information. Both variants\r\nshow promising results in applications from Computational\r\nBiology.", "recorded": "2010-12-11T10:10:00", "title": "Multitask Multiple Kernel Learning (MT-MKL)"}, {"url": "colt2014_urner_labels", "desc": "With the emergence of Machine Learning tools that allow handling data with a huge number of features, it becomes reasonable to assume that, over the full set of features, the true labeling is (almost) fully determined. That is, the labeling function is deterministic, but not necessarily a member of some known hypothesis class. However, agnostic learning of deterministic labels has so far received little research attention. We investigate this setting and show that it displays a behavior that is quite different from that of the fundamental results of the common (PAC) learning setups. First, we show that the sample complexity of learning a binary hypothesis class (with respect to deterministic labeling functions) is not fully determined by the VC-dimension of the class. For any d, we present classes of VC-dimension d that are learnable from O~(d/\u03f5)-many samples and classes that require samples of size \u03a9(d/\u03f52). Furthermore, we show that in this setup, there are classes for which any proper learner has suboptimal sample complexity. While the class can be learned with sample complexity O~(d/\u03f5), any proper (and therefore, any ERM) algorithm requires \u03a9(d/\u03f52) samples. We provide combinatorial characterizations of both phenomena, and further analyze the utility of unlabeled samples in this setting. Lastly, we discuss the error rates of nearest neighbor algorithms under deterministic labels and additional niceness-of-data assumptions.", "recorded": "2014-06-13T15:10:00", "title": "The sample complexity of agnostic learning under deterministic labels"}, {"url": "nipsworkshops2011_grunwald_guts", "desc": "A remarkable variety of problems in machine learning and statistics can be recast as data compression\r\nunder constraints: (1) sequential prediction with arbitrary loss functions can be transferred to equivalent log loss (data compression) problems. The worst-case optimal regret for the original loss is determined by Vovk\u2019s mixability, which in fact measures how many bits we lose if we are not allowed to use mixture codes in the compression formulation. (2) in classification, we can map each set of candidate classifiers C to a corresponding probability model M. Tsybakov\u2019s condition (which determines the optimal convergence rate) turns out to measure how much more we can compress data by coding it using the convex hul of M rather than just M. (3) hypothesis testing in the applied sciences is usually based on p-values, a brittle and much-criticized approach. Berger and Vovk independently proposed calibrated p-values, which are much more robust. Again we show these have a data compression interpretation. (4) Bayesian nonparametric approaches usually work well, but fail dramatically in Diaconis and Freedman\u2019s pathological cases. We show that in these cases (and only in these) the Bayesian predictive distribution does not compress the data. We speculate that all this points towards a general theory that goes beyond standard MDL and Bayes.", "recorded": "2011-12-16T16:00:00", "title": "We need a BIT more GUTS (Grand Unified Theory of Statistics)"}, {"url": "mlss06au_schraudolph_aml", "desc": "The incorporation of online learning capabilities into real-time computing systems has been hampered by a lack of efficient, scalable optimization algorithms for this purpose: second-order methods are too expensive for large, nonlinear models, conjugate gradient does not tolerate the noise inherent in online learning, and simple gradient descent, evolutionary algorithms, etc., are unacceptably slow to converge. I am addressing this problem by developing new ways to accelerate stochastic gradient descent, using second-order gradient information obtained through the efficient computation of curvature matrix-vector products. In the stochastic meta-descent (SMD) algorithm, this cheap curvature information is built up iteratively into a stochastic approximation of Levenberg-Marquardt second-order gradient steps, which are then used to adapt individual gradient step sizes. SMD handles noisy, correlated, non-stationary signals well, and approaches the rapid convergence of second-order methods at only linear cost per iteration, thus scaling up to extremely large nonlinear systems. To date it has enabled new adaptive techniques in computational fluid dynamics and computer vision. Our most recent development is a version of SMD operating in reproducing kernel Hilbert space.", "recorded": "2006-02-06T00:00:00", "title": "Rapid Stochastic Gradient Descent: Accelerating Machine Learning"}, {"url": "kdd2010_liu_sss", "desc": "In plenty of scenarios, data can be represented as vectors and then mathematically abstracted as points in a Euclidean space. Because a great number of machine learning and data mining applications need proximity measures over data, a simple and universal distance metric is desirable, and metric learning methods have been explored to produce sensible distance measures consistent with data relationship. However, most existing methods suffer from limited labeled data and expensive training. In this paper, we address these two issues through employing abundant unlabeled data and pursuing sparsity of metrics, resulting in a novel metric learning approach called semi-supervised sparse metric learning. Two important contributions of our approach are: 1) it propagates scarce prior affinities between data to the global scope and incorporates the full affinities into the metric learning; and 2) it uses an efficient alternating linearization method to directly optimize the sparse metric. Compared with conventional methods, ours can effectively take advantage of semi-supervision and automatically discover the sparse metric structure underlying input data patterns. We demonstrate the efficacy of the proposed approach with extensive experiments carried out on six datasets, obtaining clear performance gains over the state-of-the-arts.", "recorded": "2010-07-28T13:30:00", "title": "Semi-Supervised Sparse Metric Learning Using Alternating Linearization Optimization"}, {"url": "mit901f03_schneider_lec01", "desc": "1) How may Descartes\u2019 encounter with a toy in the French Royal Gardens have influenced the history of science?\n\n2) Is the nervous system like a machine?\n\n3) How was Descartes wrong and how was he right, in his description of a reflex?\n\n4) We \u201cbelieve in\u201d reflexes, but most modern neuroscientists are not \u201creflexologists\u201d. What is a reflexologist? \nIn their view, what gives rise to human uniqueness?\n\n5) What was it about Ivan Sechenov\u2019s writings about reflexes that caused problems with the censors in 19th century Russia?\n\n6) What major factor did Sechenov\u2019s student Ivan Pavlov add to human knowledge about reflexes that made reflexology a much more adequate theory of behavior?\n\n7) What is the \u201claw of roots\u201d? Who were the people that discovered it? How was one of them more modern and convincing in his experiments?\n\n8) How did the Spanish neuroanatomist Ramon y Cajal specify the first complete neural circuit underlying behavior? What kind of behavior?\n\n9) What did the British neurophysiologist Charles Scott Sherrington add to this picture? What was the word he coined that is so important in modern neuroscience?\n\n10) How did Donald M. MacKay argue that one can accept physiological determination of behavior and still argue that a person is responsible for her/his actions?", "recorded": "2003-09-03T09:00:00", "title": "Lecture 1: Introduction to Brain-behavior Studies"}, {"url": "rldm2015_tomlin_hybrid_systems", "desc": "Hybrid systems are a modeling tool allowing for the composition of continuous and discrete state dynamics. They can be represented as continuous systems with modes of operation modeled by discrete dynamics, with the two kinds of dynamics influencing each other. Hybrid systems have been essential in modeling a variety of important problems, such as aircraft flight management, air and ground transportation systems, robotic vehicles and human-automation systems.  These systems use discrete logic in control because discrete abstractions make it easier to manage complexity and discrete representations more naturally accommodate linguistic and qualitative information in controller design. A great deal of research in recent years has focused on the synthesis of controllers for hybrid systems. For safety specifications on the hybrid system, namely to design a controller that steers the system away from unsafe states, we will present a synthesis and computational technique based on optimal control and game theory. We will briefly review these methods and their application to collision avoidance and avionics design in air traffic management systems, and networks of manned and unmanned aerial vehicles.  Then, we will present a toolbox of methods combining reachability with machine learning techniques, to enable performance improvement while maintaining safety.  We will illustrate these \u201csafe learning\u201d methods on a quadrotor UAV experimental platform which we have at Berkeley.", "recorded": "2015-06-09T08:30:00", "title": "Reachability and Learning for Hybrid Systems"}, {"url": "ocwc2014_juan_turro_translectures", "desc": "Online collections of video material are fast becoming a staple feature of the Internet and a key educational resource. What we are working on at transLectures is a set of easy-to-use tools that will allow users to add multilingual subtitles to these videos. In doing so, they will make the content of these videos available to a much wider audience in a way that is cost-effective and sustainable over the vast collections of online video lectures being generated. Automatic transcription tools will provide verbatim subtitles of the talks recorded on video, thereby allowing the hard-of-hearing to access this content. Language learners and other non-native speakers will also benefit from these monolingual subtitles. Meanwhile, machine translation tools will make these subtitles available in languages other than that in which the video was recorded. Specifically, we will be developing tools for use on VideoLectures.NET, a collection of videos recorded at various academic events set up by JSI\u2019s Centre for Knowledge Transfer, and for poliMedia, a lecture capture system designed and implemented at the UPVLC. Our tools will also be fully compatible with Matterhorn, a free, open-source platform for the management of educational audio and video content. The language pairs being targeted in this project are English, Spanish and Slovenian for transcription, and English<>Spanish, English<>Slovenian, English>French and English>German for translation.", "recorded": "2014-04-23T13:00:00", "title": "transLectures"}, {"url": "licsb08_dalche_slb", "desc": "Identification of biological networks such as signalling pathways, gene regulatory networks, protein-protein interaction networks and metabolic networks is considered as a key challenge in computational biology. Using machine learning framework, this problem can be addressed using different points of view, depending of course on the nature of the biological interactions to be inferred but also on the level of abstraction of the chosen modeling and the amount of prior knowledge available. Since 2000, research in statistical learning of biological networks have given rise to a rich panel of approaches whose interest overcomes the field of computational biology. Network identification has been tackled using large scale data-mining approaches, supervised predictive approaches and reverse-modeling approaches. In this sole last family, it is very instructive to focus on the numerous graphical models that have been proposed so far such as Graphical Gaussian Models, Bayesian networks, Dynamical Bayesian networks and state-space models. I will present a short review of these methods discussing among other issues model complexity, relevance to biology, ability to deal with hidden variables and scalability. I will also plead for the construction of a benchmark repository devoted to examples of relevant test problems even if the true relevant test has always to be made in vivo or in vitro. ", "recorded": "2008-03-26T15:20:00", "title": "Statistical learning of biological networks: a brief overview"}, {"url": "pim07_glasgow", "desc": "Within the context of Systems Biology there is a growing requirement for the development of mechanistic models to support reasoning about the structures of biochemical pathways. In addition to defining such models, a means of objectively assessing the validity of competing hypotheses regarding pathway structures based on experimental data and prior knowledge is essential. As the models themselves will have been identified from experimental observations for which there is significant variability it is advisable to adopt a consistent grammar for scientific reasoning that will take account of this uncertainty.\r\n\r\nThe Bayesian perspective is highly appropriate to enable consistent reasoning over mechanistic models of biological systems. However, given the intractable nature of the integrals required for an analytical Bayesian solution we are required to turn to Markov chain Monte Carlo or other approximating techniques to perform system identification and model-based reasoning. Practical solutions to these problems are critical to the utility and realism of systems biology models.\r\n\r\nThe goal of this workshop explores the main methodological and technical issues associated with performing Bayesian inference over mechanistic biochemical pathway models. It brings together experts in systems biology, statistical inference and machine learning (through the PASCAL networks).\r\n\r\nFor more information visit the [[http://www.dcs.gla.ac.uk/pimms/|Workshop website]].", "recorded": "2007-09-18T13:00:00", "title": "Workshop on Practical Inference Methods for Mechanistic Modelling of Biological Systems (PIMMS), Glasgow 2007"}, {"url": "mlss03_burges_smtml", "desc": "These are lectures on some fundamental mathematics underlying many approaches and algorithms in machine learning. They are not about particular learning algorithms; they are about the basic concepts and tools upon which such algorithms are built. Often students feel intimidated by such material: there is a vast amount of \"classical mathematics\", and it can be hard to find the wood for the trees. The main topics of these lectures are Lagrange multipliers, functional analysis, some notes on matrix analysis, and convex optimization. I've concentrated on things that are often not dwelt on in typical CS coursework. Lots of examples are given; if it's green, it's a puzzle for the student to think about. These lectures are far from complete: perhaps the most significant omissions are probability theory, statistics for learning, information theory, and graph theory. I hope eventually to turn all this into a series of short tutorials. Please let me know of any errors, etc. ; ://from Chris Burges homepage : [[http://research.microsoft.com/~cburges]] **Lecture contains:**\\\\ Lagrange multipliers: * Lagrange the Mathematician * Lagrange multipliers: an indirect approach can be easier * Multiple Equality Constraints * Multiple Inequality Constraints * Two points on a d-sphere * The Largest Parallelogram * Resource allocation * A convex combination of numbers is maximized by choosing the largest * The Isoperimetric problem * For fixed mean and variance, which univariate distribution has maximum entropy? * An exact solution for an SVM living on a simplex Notes on some Basic Statistics * Probabilities can be Counter-Intuitive (Simpson's paradox; the Monty Hall puzzle) * IID-ness: Measurement Error decreases as 1/sqrt{n} * Correlation versus Independence * The Ubiquitous Gaussian: ** Product of Gaussians is Gaussian ** Convolution of two Gaussians is a Gaussian ** Projection of a Gaussian is a Gaussian ** Sum of Gaussian random variables is a Gaussian random variables ** Uncorrelated Gaussian variables are also independent ** Maximum Likelihood Estimates for mean and covariance (prove required matrix identities) ** Aside: For 1-dim Laplacian, max. likelihood gives the median * Using cumulative distributions to derive densities Principal Component Analysis and Generalizations * Ordering by Variance * Does Grouping Change Things? * PCA Decorrelates the Samples * PCA gives Reconstruction with Minimal Mean Squared Error * PCA preserves Mutual Information on Gaussian data * PCA directions lie in the span of the data * PCA: second order moments only * The Generalized Rayleigh Quotient ** Non-orthogonal principal directions ** OPCA ** Fisher Linear Discriminant ** Multiple Discriminant Analysis Elements of Functional Analysis * High Dimensional Spaces * Is Winning Transitive? * Most of the Volume is Near the Surface: Cubes * Spheres in n-dimensions * Banach Spaces, Hilbert Spaces, Compactness * Norms * Useful Inequalities (Minkowski and Holder) * Vector Norms * Matrix Norms * The Hamming Norm * L1, L2, L_infty norms - is L0 a norm? * Example: Using a Norm as a Constraint in Kernel Algorithms", "recorded": "2003-08-05T10:00:00", "title": "Some Mathematical Tools for Machine Learning"}, {"url": "turingslais2012_ljubljana", "desc": "[[http://en.wikipedia.org/wiki/Alan_Turing|Alan Mathison Turing]] (1912 - 1954) was an English mathematician, logician, cryptanalyst, and computer scientist. He was highly influential in the development of computer science, providing a formalisation of the concepts of \"algorithm\" and \"computation\" with the Turing machine, which played a significant role in the creation of the modern computer.Turing is widely considered to be the father of computer science and artificial intelligence.\r\n\r\n[[http://slais.ijs.si/|Slovenian Artificial Intelligence Society (SLAIS)]] is an association of researchers and practitioners in the field of Artificial Intelligence in Slovenia. Most of them come from universities and research institutes, but there are members from industrial and commercial organizations as well. The society promotes theoretical and applied research as well as the transfer of AI technology to industrial and commercial environments. SLAIS was founded in 1992 and is a member society of ECCAI (European Coordinating Committee for Artificial Intelligence).\r\n\r\nThe conference is centered on the centenary of the Alan Turing Year and the 20-years anniversary of the Slovenian Artificial Intelligence Society.\r\n\r\nThe theme of the conference is related to the Turing's unique impact on mathematics, computing, computer science, informatics, artificial intelligence, philosophy and computational aspects of physics, biology, linguistics, economics and the wider scientific world. Several authors consider Turing the father of modern computer science, artificial intelligence and computational biology. Papers related to Turing are welcome, either to his life or his achievements. Research papers should be related to Turing's work in a narrow or broad sense, e.g. novel hypercomputers proposals or new versions of Turing test are welcome as well as classical papers. Each paper should state the relation to one or many Turing's influential and long-lasting contribution. In addition, we also welcome papers related to [[http://en.wikipedia.org/wiki/Donald_Michie|Donald Michie]] (1923 - 2007), Alan Turing's collaborator during 2nd World War at the Bletchley laboratory. They both also became famous for their joint work on breaking German military codes produced by the encoding machine Enigma. Later Donald Michie did considerable amount of joint work in AI with Slovenian researchers, which was acknowledged by him becoming a member of SAZU (Slovenian Academy of Sciences and Arts), and an associate member of J. Stefan Institute.\r\n\r\nThe theme of the conference is also related to the 20-years anniversary of SLAIS, the Slovenian AI society. We invite papers about important achievements of the Slovenian AI. We welcome papers on the achievements that importantly contributed to the field of AI in the national and international context, both in theory and practice. Especially welcome are papers that present an achievement from the historical perspective, its contributions to the field of AI, its impacts to information society and possible impacts to further development of AI.\r\n\r\nDetailed information can be found at [[http://ailab.ijs.si/dunja/TuringSLAIS-2012/TuringSLAIS2012.html|TuringSLAIS 2012]].", "recorded": "2012-10-11T09:00:00", "title": "Conference on 100 Years of Alan Turing and 20 Years of SLAIS, Ljubljana 2012"}, {"url": "eswc2014_dutta_knowledge_sources", "desc": "Open Information Extraction (OIE) systems like Nell and ReVerb have achieved impressive results by harvesting massive amounts of machine-readable knowledge with minimal supervision. However, the\r\nknowledge bases they produce still lack a clean, explicit semantic data model. This, on the other hand, could be provided by full- edged semantic networks like DBpedia or Yago, which, in turn, could benefi\ft\r\nfrom the additional coverage provided by Web-scale IE. In this paper, we bring these two strains of research together, and present a method to align terms from Nell with instances in DBpedia. Our approach is unsupervised in nature and relies on two key components. First, we automatically acquire probabilistic type information for Nell terms given a set of matching hypotheses. Second, we view the mapping task as the statistical inference problem of \ffinding the most likely coherent mapping i.e., the maximum a posteriori (MAP) mapping based on the outcome of the fi\frst component used as soft constraint. These two steps are highly intertwined: accordingly, we propose an approach that iteratively re\ffines type acquisition based on the output of the mapping generator, and vice versa. Experimental results on gold-standard data indicate that our approach outperforms a strong baseline, and is able to produce ever-improving mappings consistently across iterations", "recorded": "2014-05-28T11:25:00", "title": "A Probabilistic Approach for Integrating Heterogeneous Knowledge Sources"}, {"url": "coinplanetdataschool2011_ladwig_data", "desc": "Today, the World Wide Web is a global information space of interlinked documents. Linked Data describes a method of publishing not only documents, but also structured data so that it can be interlinked and become more useful. Linked Data builds on Web standards, such as HTTP and URIs, and allows sharing of information in machine readable formats. In the first part of the tutorial we will discuss the underlying RDF data model and the Linked Data principles for publishing RDF data on the Web. In particular, we will cover:\r\n- Motivation: Why do we need Linked Data?\r\n- Linked Data principles\r\n- RDF data model\r\n- SPARQL query language for RDF\r\nHowever, the remarkable semantics-based interoperations that are enabled by these technologies, are limited by the static aspect of the data. In order to introduce dynamics and executable functionalities in this environment we have to look beyond fixed datasets. It has to be considered how the semantics of Web Services and APIs can be described. Additionally the dynamically created data from these services should be interlinked with existing Linked Data sets. In the second part of the tutorial will introduce Linked Services/APIs as an approach to achieve these goals by combining LOD principles with those of RESTful services. In particular, we will cover:\r\n- service descriptions based on SPARQL graph patterns\r\n- communication of RDF via RESTful content negotiation\r\n- best practices to lift non-RDF services to a semantic level \r\n", "recorded": "2011-12-01T09:00:00", "title": "Linked Data and APIs"}, {"url": "kdd09_you_lpitd", "desc": "Our dynamic graph-based relational mining approach has been developed to learn structural patterns in biological networks as they change over time. The analysis of dynamic networks is important not only to understand life at the system-level, but also to discover novel patterns in other structural data. Most current graph-based data mining approaches overlook dynamic features of biological networks, because they are focused on only static graphs. Our approach analyzes a sequence of graphs and discovers rules that capture the changes that occur between pairs of graphs in the sequence. These rules represent the graph rewrite rules that the first graph must go through to be isomorphic to the second graph. Then, our approach feeds the graph rewrite rules into a machine learning system that learns general transformation rules describing the types of changes that occur for a class of dynamic biological networks. The discovered graph-rewriting rules show how biological networks change over time, and the transformation rules show the repeated patterns in the structural changes. In this paper, we apply our approach to biological networks to evaluate our approach and to understand how the biosystems change over time. We evaluate our results using coverage and prediction metrics, and compare to biological literature.", "recorded": "2009-06-30T16:15:00", "title": "Learning Patterns in the Dynamics of Biological Networks"}, {"url": "abi07_winther_lsb", "desc": "The Netflix prize problem provides an excellent testing ground for machine learning. The problem is large scale and the data complex and noisy. It is therefore likely that relatively complex models with careful regularization are needed in order to get reasonable predictions. A Bayesian modeling approach seems ideal for the task if it is possible to scale it up to the size of the Netflix data set, where extremely high-dimensional Bayesian expectations will possibly have to be approximated. In this talk, an ordinal regression low-rank matrix decomposition model is presented. We use a variational Bayes (VB) inference algorithm to demonstrate that it is possible to make a large scale Bayesian algorithm. This model also highlight some of the general limitations of VB. The more accurate expectation propagation/expectation consistent (EP/C) inference cannot be applied to this bi-linear model without further approximations. We therefore propose a hybrid approach with EP/C inspired modifications of the VB algorithm. We compare the different variational approximations with a Laplace approximation, a MAP approximation and a Hamiltonian MCMC. In the latter one sample takes around 6 hours of computing time on a 1GHz processor, with fast C++ code, so there is a very clear case to be made for deterministic approximate inference. Another good feature of the Netflix data is the magnitude of the the test set which makes even small differences in the performance significant.", "recorded": "2007-12-07T09:00:00", "title": "Large-scale Bayesian Inference for Collaborative Filtering"}, {"url": "rease_noy_oma", "desc": "This is a one-hour video recording of the presentation of Natasha Noy at the KnowledgeWeb summer school 2005. It comprises either the video synchronized with the slides (but requires Quicktime, hence Windows or MacOS, otherwise the slides have to be switched manually). It provide a high-level overview on ontology mapping while the presentation of Steffen Staab provides more details and example systems.\r\n\r\nTable of Contents: \r\n\r\nOntology Mapping and Alignment\r\nLots of Overlapping Ontologies on the Semantic Web\r\nExample Definitions of School\r\nCreating Correspondences Between Ontologies\r\nSemantic Integration Tasks\r\nReasons for Mismatches\r\nTypes of Mismatches\r\nLanguage-level Mismatches\r\nOntology-level Mismatches\r\nOntology-level Mismatches: Examples\r\nSome of the Differences\r\nCategories of Mappings\r\nMapping Discovery: Information Sources\r\nUsing a Common Reference Ontology\r\nSolve the problem before it arises\r\nUsing reference ontologies: Problems\r\nUsing Lexical Information\r\nUsing Ontology Structure\r\nUsing External Sources\r\nUser Input\r\nUsing Prior Matches\r\nMapping Composition\r\nUsing Corpus of Matches\r\nMapping Discovery: Information Sources\r\nMapping Methods\r\nRule-Based and Graph-Analysis Methods\r\nGraph-based Methods\r\nAnchorPrompt: Analyzing Graph Structure\r\nMachine Learning Approaches\r\nProbabilistic Approaches\r\nReasoning and Theorem Proving\r\nUsing Mappings\r\nData Transformation\r\nQuery Answering\r\nGeneration of Ontology Extensions\r\nChallenges/Issues", "recorded": "2006-12-06T00:00:00", "title": "Ontology Mapping and Alignment"}, {"url": "mitworld_donald_bvsm", "desc": "Philosophers and AI researchers may argue the point, but Bruce Donald believes his microscopic invention qualifies as a robot. Donald\u2019s machine is about as wide as a strand of human hair. He likens it to a car, because it\u2019s controllable: \u201cYou can steer it anywhere on a flat surface, and drive it wherever you want to go.\u201d Unlike previous attempts at such a microelectromechanical system, Donald\u2019s robot has no tether, but operates via electrical charges on a silicon grid. It\u2019s a real speed demon, proceeding in nano-sized hops (one billionth of a meter, 20,000 times per second), ultimately achieving two millimeters per second, or the equivalent on a more human scale of 80 kilometers per hour. To the tunes of a Strauss waltz, Donald demonstrates two robots dancing in straight and wavy lines around each other, and then coupling to form a single system.\n\nDonald envisions many possible applications for this work. Since his robots can push and shove things in their path, and can also latch onto each other, they might prove quite useful assisting in techniques involving protein design, manipulation of cells and biomedical engineering. The next five to 10 years, Donald predicts, will see an even smaller generation of robots, which \u201cwill be doing useful things in the lab.\u201d", "recorded": "2007-04-19T19:17:39", "title": "Building Very Small Mobile Micro-Robots"}, {"url": "rldm2015_thomaz_human_teachers", "desc": "In this talk I present recent work from the Socially Intelligent Machines Lab at Georgia Tech. The vision of our research is to enable robots to function in real human environments; such as, service robots helping at home, co-worker robots to revolutionize manufacturing, and assistive robots empowering healthcare workers and enabling aging adults to live longer in their homes.  To do this, we need to build intelligent robots that can be embedded into human environments to interact with everyday people.   Many of the successes of robotics to date rely on structured environments and repeatable tasks, but what all of these visions have in common is deploying robots into dynamic human environments where pre-programmed controllers won\u2019t be an option.  These robots will need to interact with end users in order to learn what they need to do on-the-job. Our research aims to computationally model mechanisms of human social learning in order to build robots and other machines that are intuitive for people to teach.  We take Machine Learning interactions and redesign interfaces and algorithms to support the collection of learning input from end users instead of ML experts.  This talk covers results on building models of reciprocal interactions for high-level task goal\r\nlearning, low-level skill learning, and active learning interactions using humanoid robot platforms.", "recorded": "2015-06-09T15:50:00", "title": "Robots learning from human teachers"}, {"url": "nipsworkshops2013_sha_visual_domain", "desc": "Statistical machine learning has become an important driving force behind many application fields. By large, however, its theoretical underpinning has hinged on the stringent assumption that the learning environment is stationary. In particular, the data distribution on which statistical models are optimized is the same as the distribution to which the models are applied.\r\n\r\nReal-world applications are far more complex than the pristine condition. For instance, computer vision systems for recognizing objects in images often suffer from significant performance degradation if they are evaluated on image datasets that are different from the dataset on which they are designed.\r\n\r\nIn this talk, I will describe our efforts in addressing this important challenge of building intelligent systems that are robust to distribution disparity. The central theme is to learn invariant features, cast as learning kernel functions and adapt probabilistic models across different distributions (i.e., domains). To this end,our key insight is to discover and exploit hidden structures in the data. These structures, such as manifolds and discriminative clusters,are intrinsic and thus resilient to distribution changes due to exogenous factors. I will present several learning algorithms we have proposed and demonstrate their effectiveness in pattern recognition tasks from computer vision.\r\n\r\nThis talk is based on the joint work with my students (Boqing Gong and Yuan Shi, both from USC) and our collaborator Prof. Kristen Grauman (U. of Texas, Austin).", "recorded": "2013-12-10T10:55:00", "title": "Learning kernels for visual domain adaptation"}, {"url": "mitworld_donald_bvsmmr", "desc": "Philosophers and AI researchers may argue the point, but Bruce Donald believes his microscopic invention qualifies as a robot. Donald\u2019s machine is about as wide as a strand of human hair. He likens it to a car, because it\u2019s controllable: \u201cYou can steer it anywhere on a flat surface, and drive it wherever you want to go.\u201d Unlike previous attempts at such a microelectromechanical system, Donald\u2019s robot has no tether, but operates via electrical charges on a silicon grid. It\u2019s a real speed demon, proceeding in nano-sized hops (one billionth of a meter, 20,000 times per second), ultimately achieving two millimeters per second, or the equivalent on a more human scale of 80 kilometers per hour. To the tunes of a Strauss waltz, Donald demonstrates two robots dancing in straight and wavy lines around each other, and then coupling to form a single system.\r\n\r\nDonald envisions many possible applications for this work. Since his robots can push and shove things in their path, and can also latch onto each other, they might prove quite useful assisting in techniques involving protein design, manipulation of cells and biomedical engineering. The next five to 10 years, Donald predicts, will see an even smaller generation of robots, which \u201cwill be doing useful things in the lab.\u201d", "recorded": "2007-04-19T10:55:25", "title": "Building Very Small Mobile Micro-Robots"}, {"url": "slsfs05_bohinj", "desc": "The workshop examines and invites discussion on a range of methods that have been developed for dimension reduction and feature selection. This is a core topic which has been addressed theoretically in many guises from the perspectives of boosting, eigenanalysis, optimisation, latent structure analysis, bayesian methods and traditional statistical approaches to name a few. As an applied technique many algorithms exist for feature selection and all real-world applications of machine learning include some aspect of this in their implementation.\r\n\r\nIn line with the Thematic Programme 'Linking Learning and Statistics with Optimisation' the workshop focuses on the integration between for example the statistical (frequentist and Bayesian) aspects as well as optimisation issues raised by subspace identification. We feel the workshop provides a real opportunity for interaction between different areas of research and its focus on a strongly applicable family of methods will promote active discussion between different areas of the research community.\r\n\r\nTopics considered and contributions are sought in the following areas:\r\n\r\n    * Dimension reduction techniques, subspace methods\r\n    * Random projection methods\r\n    * Boosting\r\n    * Statistical analysis methods\r\n    * Bayesian approaches to feature selection\r\n    * Latent structure analysis/Probabilistic LSA\r\n    * Optimisation methods\r\n    * Novel applications of feature selection algorithms\r\n    * Open problems in the domain\r\n\r\nMore information can be found [[http://pascallin.ecs.soton.ac.uk/Workshops/SLSFS05/|here]].\r\n", "recorded": "2005-02-23T00:00:00", "title": "Workshop on Subspace, Latent Structure and Feature Selection Techniques: Statistical and Optimisation Perspectives, Bohinj 2005"}, {"url": "ecmlpkdd2011_stojanova_ceci_regression", "desc": "Regression inference in network data is a challenging task in machine learning and data mining. Network data describe entities represented by nodes, which may be connected with (related to) each other by edges. Many network data sets are characterized by a form of auto correlation where the values of the response variable at a given node depend on the values of the variables (predictor and response) at the nodes connected to the given node. This phenomenon is a direct violation of the assumption of independent (i.i.d.) observations: At the same time, it offers a unique opportunity to improve the performance of predictive models on network data, as inferences about one entity can be used to improve inferences about related entities. In this paper, we propose a data mining method that explicitly considers auto correlation when building regression models from network data. The method is based on the concept of predictive clustering trees (PCTs), which can be used both for clustering and predictive tasks: PCTs are decision trees viewed as hierarchies of clusters and provide symbolic descriptions of the clusters. In addition, PCTs can be used for multi-objective prediction problems, including multi-target regression and multi-target classification. Empirical results on real world problems of network regression show that the proposed extension of PCTs performs better than traditional decision tree induction when auto correlation is present in the data.", "recorded": "2011-09-07T17:50:00", "title": "Network Regression with Predictive Clustering Trees"}, {"url": "kdd2010_han_mhin", "desc": "With the ubiquity of information networks and their broad applications, there have been numerous studies on the construction, online analytical processing, and mining of information networks in multiple disciplines, including social network analysis, World-Wide Web, database systems, data mining, machine learning, and networked communication and information systems. Moreover, with a great demand of research in this direction, there is a need of a systematic introduction of methods for analysis of information networks from multiple disciplines. Recently there have been some tutorials on structures and laws of homogeneous information networks and graphs. However, there are few systematic tutorials on mining a more important kind of networks, heterogeneous information networks, where information networks are formed by interconnected, multi-typed nodes and links. In this tutorial, we will present an organized picture on scalable mining of heterogeneous information networks, which complements existing tutorials on knowledge discovery in homogeneous information networks. The tutorial includes the following topics:\r\n\r\n   1. introduction: information networks and information network analysis,\r\n   2. data integration, data cleaning and data validation in heterogeneous information networks,\r\n   3. clustering and ranking in heterogeneous information networks\r\n   4. classification of heterogeneous information networks,\r\n   5. summarization, OLAP and multidimensional analysis in heterogeneous information networks,\r\n   6. evolution of dynamic heterogeneous information networks, and research challenges on mining heterogeneous information networks.\r\n", "recorded": "2010-07-25T14:00:00", "title": "Mining Heterogeneous Information Networks"}, {"url": "samt08_dasiopoulou_ufdl", "desc": "Research in image analysis has reached a point where detectors can be learned in a generic fashion for a significant number of conceptual entities. The obtained performance however exhibits versatile behaviour, reflecting implications over the training set selection, similarities in visual manifestations of distinct conceptual entities, and appearance variations of the conceptual entities. A factor partially accountable for these limitations relates to the fact that machine learning techniques realise the transition from visual features to conceptual entities based solely on information regarding perceptual features. Hence, a significant part of knowledge is missed. In this paper, we investigate the use of formal semantics in order to benefit from the logical associations between the conceptual entities, and thereby alleviate part of the challenges involved in extracting semantic descriptions. More specifically, a fuzzy DL based reasoning framework is proposed for the extraction of enhanced image descriptions based on an initial set of graded annotations, generated through generic image analysis techniques. Under the proposed reasoning framework, the initial descriptions are integrated at a semantic level, resolving inconsistencies emanating from conflicting descriptions. Furthermore, the descriptions are enriched by means of entailment, resulting in more complete image descriptions. Experimentation in the domain of outdoor images has shown very promising results, demonstrating the added value in terms of accuracy and completeness.", "recorded": "2008-12-05T14:20:00", "title": "Using Fuzzy DLs to Enhance Semantic Image Analysis"}, {"url": "samt08_doherty_vdec", "desc": "The Microsoft SenseCam is a small lightweight wearable camera used to passively capture photos and other sensor readings from a user's day-to-day activities. It can capture up to 3,000 images per day, equating to almost 1 million images per year. It is used to aid memory by creating a personal multimedia lifelog, or visual recording of the wearer's life. However the sheer volume of image data captured within a visual lifelog creates a number of challenges, particularly for locating relevant content. Within this work, we explore the applicability of semantic concept detection, a method often used within video retrieval, on the novel domain of visual lifelogs. A concept detector models the correspondence between low-level visual features and high-level semantic concepts (such as indoors, outdoors, people, buildings, etc.) using supervised machine learning. By doing so it determines the probability of a concept's presence. We apply detection of 27 everyday semantic concepts on a lifelog collection composed of 257,518 SenseCam images from 5 users. The results were then evaluated on a subset of 95,907 images, to determine the precision for detection of each semantic concept and to draw some interesting inferences on the lifestyles of those 5 users. We additionally present future applications of concept detection within the domain of lifelogging.", "recorded": "2008-12-05T13:45:00", "title": "Validating the Detection of Everyday Concepts in Visual Lifelogs"}, {"url": "iswc07_zhang_irash", "desc": "As an extension to the current Web, Semantic Web will not only contain structured data with machine understandable semantics but also textual information. While structured queries can be used to find information more precisely on the Semantic Web, keyword searches are still needed to help exploit textual information. It thus becomes very important that we can combine precise structured queries with imprecise keyword searches to have a hybrid query capability. In addition, due to the huge volume of information on the Semantic Web, the hybrid query must be processed in a very scalable way. In this paper, we define such a hybrid query capability that combines unary tree-shaped structured queries with keyword searches. We show how existing information retrieval (IR) index structures and functions can be reused to index semantic web data and its textual information, and how the hybrid query is evaluated on the index structure using IR engines in an efficient and scalable manner. We implemented this IR approach in an engine called Semplore. Comprehensive experiments on its performance show that it is a promising approach. It leads us to believe that it may be possible to evolve current web search engines to query and search the Semantic Web. Finally, we breifly describe how Semplore is used for searching Wikipedia and an IBM customer\u2019s product information.", "recorded": "2007-11-15T11:00:00", "title": "Semplore: An IR Approach to Scalable Hybrid Query of Semantic Web Data"}, {"url": "mlss09us_candes_ocsssrl1m", "desc": "In many applications, one often has fewer equations than unknowns. While this seems hopeless, the premise that the object we wish to recover is sparse or nearly sparse radically changes the problem, making the search for solutions feasible. This lecture will introduce sparsity as a key modeling tool together with a series of little miracles touching on many areas of data processing. These examples show that finding *that* solution to an underdetermined system of linear equations with minimum L1 norm, often returns the ''right'' answer. Further, there is by now a well-established body of work going by the name of compressed sensing, which asserts that one can exploit sparsity or compressibility when acquiring signals of general interest, and that one can design nonadaptive sampling techniques that condense the information in a compressible signal into a small amount of data - in fewer data points than were thought necessary. We will survey some of these theories and trace back some of their origins to early work done in the 50's. Because these theories are broadly applicable in nature, the tutorial will move through several applications areas that may be impacted such as signal processing, bio-medical imaging, machine learning and so on. Finally, we will discuss how these theories and methods have far reaching implications for sensor design and other types of designs.", "recorded": "2009-06-09T08:30:00", "title": "An Overview of Compressed Sensing and Sparse Signal Recovery via L1 Minimization"}, {"url": "colt2013_voss_signal", "desc": "A prototypical blind signal separation problem is the so-called cocktail party problem, with n people talking simultaneously and n different microphones within a room. The goal is to recover each speech signal from the microphone inputs. Mathematically this can be modeled by assuming that we are given samples from a n-dimensional random variable X=AS, where S is a vector whose coordinates are independent random variables corresponding to each speaker. The objective is to recover the matrix A\u22121 given random samples from X. A range of techniques collectively known as Independent Component Analysis (ICA) have been proposed to address this problem in the signal processing and machine learning literature. Many of these techniques are based on using the kurtosis or other cumulants to recover the components. In this paper we propose a new algorithm for solving the blind signal separation problem in the presence of additive Gaussian noise, when we are given samples from X=AS+\u03b7, where \u03b7 is drawn from an unknown, not necessarily spherical n-dimensional Gaussian distribution. Our approach is based on a method for decorrelating a sample with additive Gaussian noise under the assumption that the underlying distribution is a linear transformation of a distribution with independent components. Our decorrelation routine is based on the properties of cumulant tensors and can be combined with any standard cumulant-based method for ICA to get an algorithm that is provably robust in the presence of Gaussian noise. We derive polynomial bounds for sample complexity and error propagation of our method.", "recorded": "2013-06-12T16:00:00", "title": "Blind Signal Separation in the Presence of Gaussian Noise"}, {"url": "clsp_cortes_kernels", "desc": "Kernel methods are widely used in statistical learning techniques due to their excellent performance and their computational efficiency in high-dimensional feature space. However, text or speech data cannot always be represented by the fixed-length vectors that the traditional kernels handle. In this talk, we introduce a general framework, Rational Kernels, that extends kernel techniques to deal with variable-length sequences and more generally to deal with large sets of weighted alternative sequences represented by weighted automata. Far from being abstract and computationally complex objects, rational kernels can be readily implemented using general weighted automata algorithms that have been extensively used in text and speech processing and that we will briefly review. Rational kernels provide a general framework for the definition and design of similarity measures between word or phone lattices particularly useful in speech mining applications. Viewed as a similarity measure, they can also be used in Support Vector Machines and significantly improve the spoken-dialog classification performance in difficult tasks such as the AT&T 'How May I Help You' (HMIHY) system. We present several examples of rational kernels to illustrate these applications. We finally show that many string kernels commonly considered in computational biology applications are specific instances of rational kernels.", "recorded": "2007-08-15T15:15:14", "title": "Rational Kernels: A General Machine Learning Framework for the Analysis of Text, Speech and Biological Sequences"}, {"url": "bmvc2012_kohli_discrete_models", "desc": "Many problems in Computer Vision are formulated in form of a random filed of discrete variables. Examples range from low-level vision such as image segmentation, optical flow and stereo reconstruction, to high-level vision such as object recognition. The goal is typically to infer the most probable values of the random variables, known as Maximum a Posteriori (MAP) estimation. This has been widely studied in several areas of Computer Science (e.g. Computer Vision, Machine Learning, Theory), and the resulting algorithms have greatly helped in obtaining accurate and reliable solutions to many problems. These algorithms are extremely efficient and can find the globally (or strong locally) optimal solutions for an important class of models in polynomial time. Hence, they have led to a significant increase in the use of random field models in computer vision and information engineering in general. This tutorial is aimed at researchers who wish to use and understand these algorithms for solving new problems in computer vision and information engineering. No prior knowledge of probabilistic models or discrete optimization will be assumed. The tutorial will answer the following questions: (a) How to formalize and solve some known vision problems using MAP inference of a random field? (b) What are the different genres of MAP inference algorithms? (c) How do they work? (d) What are the recent developments and open questions in this field? ", "recorded": "2012-09-03T16:00:23", "title": "MAP inference in Discrete Models"}, {"url": "nipsworkshops2010_kushnir_amm", "desc": "Multigrid solvers for solving linear problems (structured or unstructured) are based on iterating between two processes:\r\n\r\nClassical relaxation schemes, which are generally slow to converge but fast to \"smooth\" the error function.\r\nApproximating the \"smooth\" error function on a coarser level (a coarser grid or, more generally, a smaller graph, typically having one quarter to one half the number of variables). This is done by solving coarse-level equations derived from the fine-level system and the residuals of its current approximate solution, and then interpolating that coarse-level solution to correct the fine-level approximation. The solution of the coarse level equations is obtained by using recursively the same two processes, employing still coarser levels.\r\n\r\nThe most basic Multigrid solvers are the Geometric solvers (GMG) in which the problem is defined on a geometric grid. Of more interest, in the context of Machine learning, are the Algebraic Multigrid (AMG) solvers designed for unstructured problems. In this tutorial I will introduce the AMG technique, and its adoption for solving numerical problems in the context of Data Analysis. In particular, I will describe several efficient AMG eigensolvers that can be used within spectral graph methods such clustering, image segmentation, and dimensionality reduction. It turns out that the Algebraic Multigrid coarsening procedure itself can yield efficient and accurate algorithms for data clustering and image segmentation without any explicit solution of the related equations.", "recorded": "2010-12-11T15:30:00", "title": "Algebraic Multigrid Methods with Applications to Data Analysis"}, {"url": "google_huttenlocher_lrvoc", "desc": "Over the past few years there has been substantial progress in the development of techniques for recognizing generic categories of objects in images, such as automobiles, bicycles, airplanes, and human faces. Much of this progress can be traced to two underlying technical advances: # detectors for locally invariant features of an image, and # the application of techniques from machine learning. Despite recent successes, however, there are some fundamental concerns about methods that rely heavily on feature detection, because the local image evidence used in detection decisions is often highly ambiguous due to the absence of contextual information. We are taking a different approach to learning and recognizing visual object categories, in which there is no separate feature detection stage. In our approach, objects are modeled as local image patches with spring-like connections that constrain the spatial relations between patches. Such models are intuitively natural, and their use dates back over 30 years. Until recently such models were largely abandoned due to computational challenges that are addressed by our work. Our approach can be used to learn models from weakly labeled training data, without any specification of the location of objects or their parts. The recognition accuracy for such models is better than when using techniques based on feature detection that encode similar forms of spatial constraint.", "recorded": "2007-01-19T00:00:00", "title": "Learning and Recognizing Visual Object Categories"}, {"url": "w3cworkshop2013_anders_nelson_global", "desc": "Companies are shifting from viewing the enterprise as a machine with mostly sequential workflows to seeing it as an interconnected complex ecosystem with a variety of feedback entities. Enterprises need to become more responsive to changing environments to better adapt and further evolve by operating as a learning entity that proactively interacts with its environment and continuously improves based on experiments and feedback. Accordingly, the currently static multilingual content (web, technical communication and documentation, service descriptions, etc.) offered by companies must also become more fluent and dynamic. This change which will have a dramatic impact on existing corporate language formalities and rules, which must become learning enabled entities within the connected enterprise. It also poses many challenges for language-processing and compliance-control systems, both internally and externally. As a result there will be not a single, static language model, but instead multiple models derived and learned from natural patterns identified in continuous data streams, shared across communities and combined with additional media elements. As a first attempt towards possible solutions, we discuss how the development of mobile applications for smartphones and tablets might contribute to help companies in successfully mastering the transition phase to a fully connected enterprise. The presentation is organized into: (1) the emerging connected enterprise, (2) arising language challenges, (3) contribution of mobile app development, and (4) projection and outlook.", "recorded": "2013-03-12T10:12:00", "title": "Going Global with Mobile App Development: Enabling the Connected Enterprise"}, {"url": "eswc2012_nikolov_data_linking", "desc": "As commonly accepted identifiers for data instances in semantic datasets (such as ISBN codes or DOI identifiers) are often not available, discovering links between overlapping datasets on the Web is generally realised through the use of fuzzy similarity measures. Configuring such measures, i.e. deciding which similarity function to apply to which data properties with which parameters, is often a non-trivial task that depends on the domain, ontological schemas, and formatting conventions in data. Existing solutions either rely on the user's knowledge of the data and the domain or on the use of machine learning to discover these parameters based on training data. In this paper, we present a novel approach to tackle the issue of data linking which relies on the unsupervised discovery of the required similarity parameters. Instead of using labeled training data, the method takes into account several desired properties which the distribution of output similarity values should satisfy. The method includes these features into a fitness criterion used in a genetic algorithm to establish similarity parameters that maximise the quality of the resulting linkset according to the considered properties. We show in experiments using benchmarks as well as real-world datasets that such an unsupervised method can reach the same levels of performance as manually engineered methods, and how the different parameters of the genetic algorithm and the fitness criterion affect the results for different datasets.", "recorded": "2012-05-31T16:00:00", "title": "Unsupervised Learning of Data Linking Configuration"}, {"url": "sfbayacm07_govindarajan_pebp", "desc": "ProteinGPS, the technology for navigation in protein space, addresses the shortcomings of existing protein engineering paradigms and takes advantage of the last 50 years of development in linear and nonlinear systems optimization. Protein engineering has classically been approached from two diametrically opposed directions: rational design and directed evolution. Rationalism attempts to understand protein structure and function at a complete mechanistic level so that the effect of any modification to the protein can be estimated by calculation from first principles. Directed evolution on the other hand follows the strict empirical tradition and attempts to find a desired solution by testing many many different solutions, typically using various evolution-based algorithms. ProteinGPS instead uses established machine learning and nonlinear systems optimization technologies to provide a standard convention for protein space navigation. The method calculates the specific location of a protein variant in multidimensional space and places unique information rich variants called infologs, at important crossroads within the space assessed. The resulting datasets are used to map the hyper space and calculate new protein variant sequences that fulfill the functional constraints needed. Application of technologies that the data mining society has established over the last 50 years, to Protein Engineering, results in far more functional protein improvement while needing far less samples to test.", "recorded": "2007-12-12T19:00:00", "title": "Putting Engineering Back in Protein Engineering"}, {"url": "translectures", "desc": "transLectures is a European project to provide improved technologies for the automatic transcription and translation of online educational videos. Funded under the EU's Seventh Framework Programme (FP7) for research and technological development, the project was launched on 1 November 2011 and will run for three years until 31 October 2014.\r\n\r\n\r\n====Project overview\r\n\r\nOnline collections of educational video material are fast becoming a staple feature of the Internet.  What we are working on at transLectures is a set of easy-to-use tools that will allow users to add multilingual subtitles to these videos and, by doing so, make their content available to a much wider audience.\r\n\r\nSpecifically, automatic transcription tools will provide verbatim subtitles, thereby allowing the hard-of-hearing and, for example, non-native speakers, access to their content. Meanwhile, machine translation tools will make these subtitles available in languages other than that in which the video was recorded.\r\n\r\nFurthermore, a key line of research at transLectures is that of intelligent interaction. This is where users are called upon to post-edit not all, but selected segments of automatic output as part of an interactive process designed to improve not only the current subtitles but all future subtitles generated using the transLectures tool.\r\n\r\nThe languages and language pairs being targeted are: English, Spanish and Slovenian for transcription; and English<>Spanish, English<>Slovenian, English>German and English>French for translation.\r\n", "recorded": "2013-02-07T13:17:53", "title": "TransLectures - transcription and translation of videolectures"}, {"url": "simbad2011_morvant_transfer", "desc": "Similarity functions are widely used in many machine learning\r\nor pattern recognition tasks. We consider here a recent framework\r\nfor binary classifi\fcation, proposed by Balcan et al., allowing to learn\r\nin a potentially non geometrical space based on good similarity functions.\r\nThis framework is a generalization of the notion of kernels used\r\nin support vector machines in the sense that allows one to use similarity\r\nfunctions that do not need to be positive semi-de\ffinite nor symmetric.\r\nThe similarities are then used to define an explicit projection space where\r\na linear classifi\fer with good generalization properties can be learned. In\r\nthis paper, we propose to study experimentally the usefulness of similarity\r\nbased projection spaces for transfer learning issues. More precisely,\r\nwe consider the problem of domain adaptation where the distributions\r\ngenerating learning data and test data are somewhat di\u000bfferent. We stand\r\nin the case where no information on the test labels is available. We show\r\nthat a simple renormalization of a good similarity function taking into\r\naccount the test data allows us to learn classifi\fers more performing on\r\nthe target distribution for difficult adaptation problems. Moreover, this\r\nnormalization always helps to improve the model when we try to regularize\r\nthe similarity based projection space in order to move closer the\r\ntwo distributions. We provide experiments on a toy problem and on a\r\nreal image annotation task.", "recorded": "2011-09-28T11:00:00", "title": "On the Usefulness of Similarity based Projection Spaces for Transfer Learning"}, {"url": "rldm2015_stone_practical_rl", "desc": "When scaling up Reinforcement Learning (RL) to large continuous domains with imperfect representations and hierarchical structure, we often try applying algorithm that are proven to converge in small finite domains, and then just hope for the best. This talk will advocate instead designing algorithms that adhere to the constraints, and indeed take advantage of the opportunities, that might come with the problem at hand. Drawing on several different research threads within the Learning Agents Research Group at UT Austin, I will touch on four types of issues that arise from these constraints and opportunities:  1) Representation -choosing the algorithm for the problem\u2019s representation and adapting the representation to fit the algorithm; 2) Interaction - with other agents and with human trainers; 3) Synthesis - of different algorithms for the same problem and of different concepts in the same algorithm; and 4) Mortality - dealing with the constraint that when the environment is large relative to the number of action opportunities available, one cannot explore\r\nexhaustively. Within this context,  I will focus on two specific RL approaches,  namely the TEXPLORE algorithm for real-time sample-efficient reinforcement learning for robots; and layered learning, a hierarchical machine learning paradigm that enables learning of complex behaviors by incrementally learning a series of sub-behaviors.  TEXPLORE has been implemented and tested on a full-size fully autonomous robot car, and layered learning was the key deciding factor in our RoboCup 2014 3D simulation league championship.", "recorded": "2015-06-10T08:30:00", "title": "Practical RL: Representation, interaction, synthesis, and morality (PRISM)"}, {"url": "nipsworkshops2012_germain_adaptation", "desc": "In machine learning, Domain Adaptation (DA) arises when the distribution generating the test (target) data differs from the one generating the learning (source) data. It is well known that DA is an hard task even under strong assumptions, among which the covariate-shift where the source and target distributions diverge only in their marginals, i.e. they have the same labeling function. Another popular approach is to consider an hypothesis class that moves closer the two distributions while implying a low-error for both tasks. This is a VC-dim approach that restricts the complexity of an hypothesis class in order to get good generalization. Instead, we propose a PAC-Bayesian approach that seeks for suitable weights to be given to each hypothesis in order to build a majority vote. We prove a new DA bound in the PAC-Bayesian context. This leads us to design the first DA-PAC-Bayesian algorithm based on the minimization of the proposed bound. Doing so, we seek for a \u03c1-weighted majority vote that takes into account a trade-off between three quantities. The first two quantities being, as usual in the PAC-Bayesian approach, (a) the complexity of the majority vote (measured by a Kullback-Leibler divergence) and (b) its empirical risk (measured by the \u03c1-average errors on the source sample). The third quantity is (c) the capacity of the majority vote to distinguish some structural difference between the source and target samples.", "recorded": "2012-12-07T09:25:00", "title": "PAC-Bayesian Learning and Domain Adaptation"}, {"url": "mlsb2010_taskova_pee", "desc": "The task of parameter estimation is regularly encountered in the context of constructing\r\nsystems biology models. These models often focus on the dynamics of\r\nbiological systems and have the form of ordinary differential equations (ODEs).\r\nThe dynamics modeled is typically highly nonlinear and constrained and thus\r\nthe corresponding parameter optimization problems are hard for traditional local\r\nsearch optimization methods The problem becomes even worse when we\r\nattempt to model the dynamics in an automated fashion, using approaches like\r\nthe machine learning approach of equation discovery [3], which consider both different\r\nequation structures and different parameter values. The effectiveness and\r\nefficiency of the parameter optimization method used then becomes paramount.\r\n\r\nIn this context, we investigate the effectiveness and efficiency of different optimization\r\nmethods for the task of estimating the parameters of ODE models\r\nin systems biology. We consider three optimization methods: the local search\r\nmethod Algorithm 717(ALG717) [1] and two meta-heuristic approaches: Differential\r\nEvolution (DE) [5] and Differential Ant-Stygmergy Algorithm (DASA)\r\n[4]. We compare them on the parameter estimation task in a nonlinear dynamic\r\nmodel of an important endocytotic regulatory system that switches between\r\ncargo transport and maturation in early, respectively late endosomes [2]. Both\r\nartificial and real data are used in the comparison, which shows that the recent\r\nDASA approach is the method of choice: It is both effective (in terms of the\r\nquality of the solutions) and efficient (in terms of the speed of convergence).", "recorded": "2010-10-15T13:00:00", "title": "Parameter Estimation in an Endocytosis Model"}, {"url": "iswc2011_urbani_reasoning", "desc": "Both materialization and backward-chaining as di\u000berent modes\r\nof performing inference have complementary advantages and disadvan-\r\ntages.\r\nMaterialization enables very e\u000ecient responses at query time, but at the\r\ncost of an expensive up front closure computation, which needs to be\r\nredone every time the knowledge base changes. Backward-chaining does\r\nnot need such an expensive and change-sensitive precomputation, and is\r\ntherefore suitable for more frequently changing knowledge bases, but has\r\nto perform more computation at query time.\r\nMaterialization has been studied extensively in the recent semantic web\r\nliterature, and is now available in industrial-strength systems. In this\r\nwork, we focus instead on backward-chaining, and we present an hybrid\r\nalgorithm to perform efficient backward-chaining reasoning on very large\r\ndatasets expressed in the OWL Horst (pD*) fragment.\r\nAs a proof of concept, we have implemented a prototype called QueryPIE\r\n(Query Parallel Inference Engine), and we have tested its performance on\r\ndiff\u000berent datasets of up to 1 billion triples. Our parallel implementation\r\ngreatly reduces the reasoning complexity of a naive backward-chaining\r\napproach and returns results for single query-patterns in the order of\r\nmilliseconds when running on a modest 8 machine cluster.\r\nTo the best of our knowledge, QueryPIE is the \ffirst reported backward-\r\nchaining reasoner for OWL Horst that efficiently scales to a billion triples.", "recorded": "2011-10-25T11:00:00", "title": "QueryPIE: Backward reasoning for OWL Horst over very large knowlege bases"}, {"url": "cernacademictraining09_filhol_rdtg", "desc": "Over the last decade, many 3rd generation storage ring light sources have been built and put into operation. Progressively, significant improvements have been brought to the machine performances and experiences developed at the first facilities have benefited to the most recently built ones.\r\n\r\nMost of the recent facilities are now featuring small emittances, high current together with high position stability. The small sizes of the electron beam at the source points impose achieving position stabilities in the sub micron range. The technology to build the insertion devices that produce the photon beams has reached a very mature state and enables 3 GeV medium energy /medium size machines to produce high brilliance beams up to the hard X-Ray range (10 - 30 keV).  The designing of the optical set-up of a beamline includes now the choice of the best suited undulator. All these facilities are operated as \u201cphoton factories\u201d and deliver their beams to many beamlines over several thousands hours per year.\r\n\r\nSome recent projects aim at achieving very small horizontal emittances by combining large circumference rings with damping wiggler devices.\r\n\r\nWe will review the main features which are required to achieve the best performances, putting some emphasis on the specific developments made at SOLEIL, or at other facilities, either recently built or under construction.\r\n", "recorded": "2009-03-05T12:07:00", "title": "Recent developments at 3rd generation storage ring light sources"}, {"url": "mlws04_josef_samcs", "desc": "One of the recently emerged paradigms in machine learning is multiple classifier fusion. A large number of methods for constructing multiple classifier systems (MCS) have been suggested in the literature. The majority of these draw on a parallel architecture, involving a fusion of multiple classifiers via some form of linear or nonlinear combination rule. Intuitively, one can look at parallel fusion as an attempt to improve the performance by combining several independent estimates of a class aposteriori probability and thereby reducing the variance of the combined estimate. For a given probability margin between two competing hypotheses, this reduced variance then results in a lower probability of incurring an additional classification error over and above the Bayes' error. Much less attention has been paid to multiple classifier system schemes that aim to enhance the performance by manipulating the margin between competing hypotheses. An increased margin can normally be achieved by class grouping. This approach often leads to serial multiple classifier system architectures. Depending on whether the grouping structure is fixed or created dynamically, the resulting multiple classifier is either a decision tree or a chain like multistage system. In this paper the theory underpinning this MCS approach will be overviewed and its implications discussed. It will be shown that the theory leads to diverse class grouping/margin manipulation strategies. Their relative advantages will be discussed. The effectiveness of some of these strategies will be illustrated on a practical problem of object recognition.", "recorded": "2004-09-09T09:00:00", "title": "On serial architectures for multiple classifier systems"}, {"url": "mlsb09_juty_mfsb", "desc": "The ease with which modern computational and theoretical tools can be applied to modeling has led to an exponential increase in the size and complexity of computational models in biology. At the same time, the accelerating pace of progress also highlights limitations in current approaches to modeling. One of these limitations is the insufficient degree to which the semantics and qualitative behaviour of models are systematised and expressed formally enough to support unambiguous interpretation by software systems. As a result, human intervention is required to interpret and connect a model's mathematical structures with information about the its meaning (semantics). Often, this critical information is usually communicated through free-text descriptions or non-standard annotations; however, free-text descriptions cannot easily be interpreted by current modeling tools.\r\nWe will describe three efforts to standardize the encoding of missing semantics for kinetic models. The overall approach involves connecting model elements to common, external sources of information that can be extended as existing knowledge is expanded and refined. These external sources are carefully managed public, free, consensus ontologies: the Systems Biology Ontology (SBO), the Kinetic Simulation Algorithm Ontology (KiSAO), and the Terminology for the Description of Dynamics (TeDDy). Together they provide a means for annotating a model with stable and perennial identifiers which reference machine readable regulated terms defining the semantics of the three facets of the modeling process 1) the relationship between the model and the biology it aims to describe, 2) the process used to simulate the model and obtain expected results, and 3) the results themselves.", "recorded": "2009-09-06T11:20:00", "title": "Metadata For Systems Biology"}, {"url": "wsdm2010_taneva_garp", "desc": "Knowledge-sharing communities like Wikipedia and auto- mated extraction methods like those of DBpedia enable the construction of large machine-processible knowledge bases with relational facts about entities. These endeavors lack multimodal data like photos and videos of people and places. While photos of famous entities are abundant on the Inter- net, they are much harder to retrieve for less popular entities such as notable computer scientists or regionally interesting churches. Querying the entity names in image search engines yields large candidate lists, but they often have low precision and unsatisfactory recall.\r\nOur goal is to populate a knowledge base with photos of named entities, with high precision, high recall, and diversity of photos for a given entity. We harness relational facts about entities for generating expanded queries to retrieve different candidate lists from image search engines. We use a weighted voting method to determine better rankings of an entity\u2019s photos. Appropriate weights are dependent on the type of entity (e.g., scientist vs. politician) and automatically computed from a small set of training entities. We also exploit visual similarity measures based on SIFT features, for higher diversity in the final rankings. Our experiments with photos of persons and landmarks show significant improvements of ranking measures like MAP and NDCG, and also for diversity-aware ranking.", "recorded": "2010-02-06T16:20:39", "title": "Gathering and Ranking Photos of Named Entities with High Precision, High Recall, and Diversity"}, {"url": "kdd07_seni_fttf_canceled", "desc": "**Ensemble methods** are one of the most influential developments in Machine\r\n Learning over the past decade. They perform extremely well in a variety\r\n of problem domains, have desirable statistical properties, and scale\r\n well computationally. By combining competing models into a committee,\r\n they can strengthen \u201cweak\u201d learning procedures.\r\n\r\nThis tutorial explains two recent developments with ensemble methods: \\\\\r\n**Importance Sampling** reveals \u201cclassic\u201d ensemble methods\r\n (bagging, random forests, and boosting) to be special cases of a single\r\n algorithm. This unified view clarifies the properties of these methods\r\n and suggests ways to improve their accuracy and speed.\\\\\r\n**Rule Ensembles**\r\n are linear rule models derived from decision tree ensembles. While\r\n maintaining (and often improving) the accuracy of the tree ensemble,\r\n the rule-based model is much more interpretable.\r\n\r\nThis tutorial is aimed\r\n at both novice and advanced data mining researchers and\r\n practitioners especially in Engineering, Statistics, and Computer\r\n Science. Users with little exposure to ensemble methods will gain a\r\n clear overview of each method. Advanced practitioners already employing\r\n ensembles will gain insight into this breakthrough way to create\r\n next-generation models.\r\n\r\n\r\n\r\n\r\n: In a Nutshell, Examples & Timeline\r\n: Predictive Learning\r\n: Decision Trees\r\n: Model Selection (Bias-Variance Tradeoff , Regularization via shrinkage)\r\n: Ensemble Learning & Importance Sampling (ISLE)\r\n: Generic Ensemble Generation\r\n: Bagging, Random Forest, AdaBoost, MART\r\n: Rule Ensembles\r\n: Interpretation", "recorded": "2007-08-12T09:00:00", "title": "From Trees to Forests and Rule Sets - A Unified Overview of Ensemble Methods - part 2"}, {"url": "kdd09_wang_cwltiwnaests", "desc": "Automatic news extraction from news pages is important in many Web applications such as news aggregation. However, the existing news extraction methods based on template-level wrapper induction have three serious limitations. First, the existing methods cannot correctly extract pages belonging to an unseen template. Second, it is costly to maintain up-to-date wrappers for a large amount of news websites, because any change of a template may invalidate the corresponding wrapper. Last, the existing methods can merely extract unformatted plain texts, and thus are not user friendly. In this paper, we tackle the problem of template-independent Web news extraction in a user-friendly way. We formalize Web news extraction as a machine learning problem and learn a template-independent wrapper using a very small number of labeled news pages from a single site. Novel features dedicated to news titles and bodies are developed. Correlations between news titles and news bodies are exploited. Our template-independent wrapper can extract news pages from different sites regardless of templates. Moreover, our approach can extract not only texts, but also images and animates within the news bodies and the extracted news articles are in the same visual style as in the original pages. In our experiments, a wrapper learned from 40 pages from a single news site achieved an accuracy of 98.1% on 3,973 news pages from 12 news sites.\r\n", "recorded": "2009-07-01T12:05:00", "title": "Can We Learn a Template-Independent Wrapper for News Article Extraction from a Single Training Site?"}, {"url": "rss2010_oudeyer_dca", "desc": "Learning motor control in robots, such as learning visual reaching or object manipulation in humanoid robots, is becoming a central topic both in \"traditional\" robotics and in developmental robotics. A major obstacle is that learning can become extremely slow or even impossible without adequate exploration strategies. Active learning techniques, also called intrinsically motivated learning in the developmental robotics literature, can be used to accelerate learning. Yet, many robotic spaces have properties which are not compatible with the standard assumptions of most active learning or intrinsic motivation algorithms. For example, they are typically much too large to be learnt entirely, they can even be open-ended, and they can also contain subspaces which are too complex to be learnt by given machine learning algorithms. Some approaches to active learning/intrinsic motivation have been proposed to address some of these difficulties, such as the explicit maximization of information gain or the explicit maximization of the decrease of prediction errors (as opposed to the maximization of uncertainty or prediction errors as in many active learning heuristics). Yet, even these approaches become quickly inefficient in realistic sensorimotor spaces. In this talk, I will argue that various kinds of developmental constraints should be considered to address properly those spaces, such as maturational constraints on sensorimotor channels, the use of motor primitives, constraints on the spaces on which active learning is performed, morphological constraints, and obviously social learning constraints.", "recorded": "2010-06-27T15:15:00", "title": "Developmental constraints on active learning for the acquisition of motor skills in high-dimensional robots"}, {"url": "kdd2014_lin_large_scale_enterprise", "desc": "Large enterprise IT (Information Technology) infrastructure components generate large volumes of alerts and incident tickets. These are manually screened, but it is otherwise difficult to extract information automatically from them to gain insights in order to improve operational efficiency. We propose a framework to cluster alerts and incident tickets based on the text in them, using unsupervised machine learning. This would be a step towards eliminating manual classification of the alerts and incidents, which is very labor intense and costly. Our framework can handle the semi-structured text in alerts generated by IT infrastructure components such as storage devices, network devices, servers etc., as well as the unstructured text in incident tickets created manually by operations support personnel. After text pre-processing and application of appropriate distance metrics, we apply different graph-theoretic approaches to cluster the alerts and incident tickets, based on their semi-structured and unstructured text respectively. For automated interpretation and read-ability on semi-structured text clusters, we propose a method to visualize clusters that preserves the structure and human-readability of the text data as compared to traditional word clouds where the text structure is not preserved; for unstructured text clusters, we find a simple way to define prototypes of clusters for easy interpretation. This framework for clustering and visualization will enable enterprises to prioritize the issues in their IT infrastructure and improve the reliability and availability of their services.", "recorded": "2014-08-26T14:00:00", "title": "Unveiling Clusters of Events for Alert and Incident Management in Large-Scale Enterprise IT"}, {"url": "mlss09us_blum_tsflc", "desc": "Kernel methods have become powerful tools in machine learning. They perform well in many applications, and there is also a well-developed theory of what makes a given kernel useful for a given learning problem. However, this theory requires viewing kernels as implicit (and often difficult to characterize) maps into high-dimensional spaces. In this talk I will describe work on developing a theory that just views a kernel as a measure of similarity between data objects, and describes the usefulness of a given kernel (or more general similarity function) in terms of fairly intuitive, direct properties of how the similarity function relates to the task at hand, without need to refer to any implicit spaces. I will also talk about an extension of this framework to learning from purely unlabeled data, i.e., clustering. In particular, one can ask how much stronger the properties of a similarity function should be (in terms of its relation to the unknown desired clustering) so that it can be used to cluster well: to learn well without any label information at all. We find that if we are willing to relax the objective a bit (for example, allow the algorithm to produce a hierarchical clustering that we will call successful if some pruning is close to the desired clustering), then this question leads to a number of interesting graph-theoretic and game-theoretic properties that are sufficient to cluster well. This work can be viewed defining a kind of PAC model for clustering. (This talk based on work joint with Maria-Florina Balcan, Santosh Vempala, and Nati Srebro). ", "recorded": "2009-06-08T14:00:00", "title": "On a Theory of Similarity Functions for Learning and Clustering "}, {"url": "nips2010_daw_rlh", "desc": "Algorithms from computer science can serve as detailed process-level hypotheses for how the brain might approach difficult information processing problems. This tutorial reviews how ideas from the computational study of reinforcement learning have been used in biology to conceptualize the brain's mechanisms for trial-and-error decision making, drawing on evidence from neuroscience, psychology, and behavioral economics. We begin with the much-debated relationship between temporal-difference learning and the neuromodulator dopamine, and then consider how more sophisticated methods and concepts from RL -- including partial observability, hierarchical RL, function approximation, and various model-based approaches -- can provide frameworks for understanding additional issues in the biology of adaptive behavior.\r\n\r\nIn addition to helping to organize and conceptualize data from many different levels, computational models can be employed more quantitatively in the analysis of experimental data. The second aim of this tutorial is to review and demonstrate, again using the example of reinforcement learning, recent methodological advances in analyzing experimental data using computational models. An RL algorithm can be viewed as generative model for raw, trial-by-trial experimental data such as a subject's choices or a dopaminergic neuron's spiking responses; the problems of estimating model parameters or comparing candidate models then reduce to familiar problems in Bayesian inference. Viewed this way, the analysis of neuroscientific data is ripe for the application of many of the same sorts of inferential and machine learning techniques well studied by the NIPS community in other problem domains. ", "recorded": "2010-12-06T00:13:00", "title": "Reinforcement Learning in Humans and Other Animals"}, {"url": "nips09_hinton_dlmi", "desc": "Deep networks can be learned efficiently from unlabeled data. The layers of representation are learned one at a time using a simple learning module that has only one layer of latent variables. The values of the latent variables of one module form the data for training the next module. The most commonly used modules are Restricted Boltzmann Machines or autoencoders with a sparsity penalty on the hidden activities. Although deep networks have been quite successful for tasks such as object recognition, information retrieval, and modeling motion capture data, the simple learning modules do not have multiplicative interactions which are very useful for some types of data. The talk will show how a third-order energy function can be factorized to yield a simple learning module that retains advantageous properties of a Restricted Boltzmann Machine such as very simple exact inference and a very simple learning rule based on pair-wise statistics. The new module contains multiplicative interactions that are useful for a variety of unsupervised learning tasks. Researchers at the University of Toronto have been using this type of module to extract oriented energy from image patches and dense flow fields from image sequences. The new module can also be used to allow the style of a motion to blend auto regressive models of motion capture data. Finally, the new module can be used to combine an eye-position with a feature-vector to allow a system that has a variable resolution retina to integrate information about shape over many fixations.", "recorded": "2009-12-08T14:00:00", "title": "Deep Learning with Multiplicative Interactions"}, {"url": "nipsworkshops09_smith_ainlp", "desc": "I'll start out by presenting an idealized version of the natural language processing problem of parsing. I will brazenly suggest that most of NLP is reducible to variations on parsing problems. I'll show how dynamic programming solves the idealized version of the problem, both for calculating modes and marginals over parse trees, exploiting some key independence assumptions about the structure of natural language sentences. \r\n\r\nI will then discuss two approximate inference methods that let us build more powerful models of parsing. Neither comes with strong theoretical guarantees, but both are demonstrated to perform strongly in experiments on real NLP data. The first method builds on the dynamic programming representation, combining max-product and sum-product methods to produce, approximately, the k-best parses and a residual sum over the rest of the parses, useful when incorporating features that violate the usual independence assumptions. Experiments validate the approach with a discriminative model for machine translation. \r\n\r\nThe second method turns a parsing problem instance into a concise integer linear program. Approximate inference is then accomplished using well-known linear program relaxation. This is embedded in a new online learning algorithm that tries to penalize uninterpretable fractional solutions (and therefore inference cost at evaluation time). We show that this approach leads to state-of-the-art parsing performance on seven languages, with improved speed for both exact and approximate inference and no significant performance loss.", "recorded": "2009-12-12T07:40:00", "title": "Approximate Inference in Natural Language Processing"}, {"url": "okt09_bengio_ldhr", "desc": "Whereas theoretical work suggests that deep architectures might be computationally and statistically more efficient at representing highly-varying functions, training deep architectures was unsuccessful until the recent advent of algorithms based on unsupervised pre-training of each level of a hierarchically structured model. Several unsupervised criteria and procedures were proposed for this purpose, starting with the Restricted Boltzmann Machine (RBM), which when stacked gives rise to Deep Belief Networks (DBN). Although the partition function of RBMs is intractable, inference is tractable and we review several successful learning algorithms that have been proposed, in particular those using weights that change quickly during learning instead of converging. In addition to being impressive as generative models, deep architectures based on RBMs and other unsupervised learning methods have made an impact by being used to initialize deep supervised neural networks. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. We attempt to shed some light on these questions by comparing different successful approaches to training deep architectures and through extensive simulations investigating explanatory hypotheses. Finally, we describe our current research program, objectives and challenges, regarding learning representations at multiple levels of abstraction, to compare web objects such as images, documents, and search engine requests, comparisons that are at the core of several information retrieval applications.", "recorded": "2009-09-23T11:00:00", "title": "Learning Deep Hierarchies of Representations"}, {"url": "solomon_brank_mpv2", "desc": "Predstavil bom metodo podpornih vektorjev (support vector machine, SVM). To je relativno nov algoritem (pravzaprav bolj dru\u017eina algoritmov) s podro\u010dja strojnega u\u010denja. \\\\ Prvotna razli\u010dica algoritma je zami\u0161ljena za klasifikacijske probleme z dvema razredoma, obstajajo pa tudi raz\u0161iritve za ve\u010d razredov in za regresijske probleme. \\\\ SVM ima dobro teoreti\u010dno podlago, obenem pa se dobro obnese tudi na realisti\u010dnih podatkih z raznih problemskih podro\u010dij. \\\\ \\\\ V prvem delu seminarja (v torek) bom opisal, kako si SVM zastavi u\u010denje kot optimizacijski problem in kako ta problem s pomo\u010djo prijemov iz matematike (teorija optimizacije) predelamo v bolj obvladljivo dualno obliko, ki jo lahko re\u0161ujemo numeri\u010dno. \\\\ \\\\ V drugem delu (v \u010detrtek) bom opisal, kako lahko z uporabo t.im. jeder i\u0161\u010demo namesto linearnih tudi nelinearne modele, pri tem pa ostane optimizacijsko ogrodje prakti\u010dno nespremenjeno. Predstavil bom se nekaj raz\u0161iritev SVMja (vec kot dva razreda, regresija, transdukcija), nekaj prosto dostopnih implementacij SVMja in nekaj literature o SVMjih. \\\\ \\\\ Namen seminarja je, da bi lahko poslu\u0161alec metodo podpornih vektorjev uporabljal ne zgolj kot \u010drno skatlico, ampak bi si tudi malo predstavljal, kako ta \u0161katlica deluje.", "recorded": "2003-03-20T13:00:00", "title": "Metoda podpornih vektorjev - 2"}, {"url": "lms08_rogers_tlim", "desc": "The combined, unsupervised analysis of coupled data sources is an open problem in machine learning.\r\nA particularly important example from the biological domain is the analysis of mRNA and protein profiles derived from the same set of genes (either over time or under different conditions). Such analysis has the potential to provide a far more comprehensive picture of the mechanisms of transcription and translation than the individual analysis of the separate data sets. The problem is similar to that attacked with traditional Canonical Correlation Analysis (CCA) but in many application areas, the CCA assumptions are too restrictive. Probabilistic CCA [1] and kernel CCA [2] have both been recently proposed but the former is still limited to linear relationships and the latter compromises the interpretability in the original space. In this work, we preset a nonparametric model for coupled data that provides an interpretable description of the shared variability in the data (as well as that that isn\u2019t shared) whilst being free of restrictive assumptions such as those found in CCA.\r\nThe hierarchical model is built from two marginal mixtures (one for each representation - generalisation to three or more is straightforward). Each object will be assigned to one component in each marginal and the contingency table describing these joint assignments is assumed to have been generated by a mixture of tables with independent margins. This top-level mixture captures the shared\r\nvariability whilst the marginal models are free to capture variation specific to the respective data sources. The number of components in all three mixtures is inferred from the data using a novel Dirichlet Process (DP) formulation. ", "recorded": "2008-12-13T16:45:00", "title": "Two-level infinite mixture for multi-domain data"}, {"url": "kdd07_elder_seni_fttf", "desc": "Ensemble methods are one of the most influential developments in Machine Learning over the past decade. They perform extremely well in a variety of problem domains, have desirable statistical properties, and scale well computationally. By combining competing models into a committee, they can strengthen \u201cweak\u201d learning procedures. \n\n;This tutorial explains two recent developments with ensemble methods:\n:**Importance Sampling** reveals \u201cclassic\u201d ensemble methods (bagging, random forests, and boosting) to be special cases of a single algorithm. This unified view clarifies the properties of these methods and suggests ways to improve their accuracy and speed.\n:**Rule Ensembles** are linear rule models derived from decision tree ensembles. While maintaining (and often improving) the accuracy of the tree ensemble, the rule-based model is much more interpretable. \n\nThis tutorial is aimed at both novice and advanced data mining researchers and practitioners especially in Engineering, Statistics, and Computer Science. Users with little exposure to ensemble methods will gain a clear overview of each method. Advanced practitioners already employing ensembles will gain insight into this breakthrough way to create next-generation models.\n\n;**John Elder's lecture**: \n: In a Nutshell, Examples & Timeline\n: Predictive Learning \n: Decision Trees\n\n;**Giovanni Seni's lecture**: \n: Model Selection (Bias-Variance Tradeoff , Regularization via shrinkage) \n: Ensemble Learning & Importance Sampling (ISLE)\n: Generic Ensemble Generation\n: Bagging, Random Forest, AdaBoost, MART\n: Rule Ensembles\n: Interpretation", "recorded": "2007-08-12T09:00:50", "title": "From Trees to Forests and Rule Sets - A Unified Overview of Ensemble Methods"}, {"url": "cikm08_elkan_llmacrf", "desc": "Log-linear models are a far-reaching extension of logistic regression, while con- ditional random fields (CRFs) are a special case of log-linear models suitable for so-called structured learning tasks. Structured learning means learning to predict outputs that have internal structure. For example, recognizing handwritten words is more accurate when the correlations between neighboring letters are used to re\u00dene predictions. This tutorial will provide a simple but thorough introduction to these new developments in machine learning that have great potential for many novel applications.\n\nThe tutorial will first explain what log-linear models are, with with concrete examples but also with mathematical generality. Next, feature-functions will be explained; these are the knowledge-representation technique underlying log-linear models. The tutorial will then present linear-chain CRFs, from the point of view that they are a special case of log-linear models. The Viterbi algorithm that makes inference tractable for linear-chain CRFs will be covered, followed by a discus- sion of inference for general CRFs. The presentation will continue with a general derivation of the gradient of log-linear models; this is the mathematical foundation of all log-linear training algorithms. Then, the tutorial will discuss two impor- tant special-case CRF training algorithms, one that is a variant of the perceptron method, and another one called contrastive divergence. Last but not least, the tu- torial will introduce publicly available software for training and using CRFs, and will explain a practical application of CRFs with hands-on detail.", "recorded": "2008-10-26T11:00:00", "title": "Log-linear Models and Conditional Random Fields"}, {"url": "nipsworkshops2012_dolan_grounded_language", "desc": "Language allows us endlessly creative ways to express the same basic meaning,\r\nwhether through monolingual paraphrasing or through bilingual translation. That\r\nexpressive power, though, poses huge challenges for computational approaches to\r\nlanguage understanding: how can we model the relationship between strings that are\r\nsuperficially dissimilar and yet \u201cmean the same thing\u201d? \r\n\r\nThis problem is becoming\r\nparticularly acute as software interfaces move toward simpler, more natural\r\ninteractions that often crucially rely on linguistic input. An intelligent interface needs to\r\nbe able to decide, for example, that the following utterances all describe\r\napproximately the same user intent:\r\n\r\nShow me some formal dress shoes\\\\\r\nI need some shoes for a job interview\\\\\r\nIk wil kleedschoenen\\\\\r\n\u0422\u044a\u0440\u0441\u044f \u043e\u0444\u0438\u0446\u0438\u0430\u043b\u043d\u0438 \u043e\u0431\u0443\u0432\u043a\u0438\\\\\r\n\u6211\u9700\u8981\u4e00\u4e9b\u6b63\u5f0f \udbc0\udc00\u5408\u7a7f\u7684\u978b\u5b50\\\\\r\n\r\nIn addition to the challenge of modeling such mono- and multi-lingual alternations on\r\na broad scale, we also need to learn to ground language in the real world \u2013 in this\r\ncase, to an inventory of shoe styles, for instance, and show a set of relevant\r\nimages/records to the user. How can we reliably map natural language utterances \u2013\r\nno matter how they are expressed \u2013 to appropriate changes in machine state? This\r\ntalk will describe and modeling program aimed at capturing how different expressions\r\nof the same meaning \u2013 whether within one language or across multiple languages -\r\nare grounded in the real world. In addition, we will describe and demonstrate the\r\nresults of crowdsourcing experiments aimed at building multilingual datasets that are\r\ngrounded in video segments, database objects, programming functions, and human\r\nmovement.", "recorded": "2012-12-07T07:30:46", "title": "Modeling Multilingual Grounded Language"}, {"url": "nipsworkshops09_gendisc", "desc": "**Generative and Discriminative Learning Interface**\r\n\r\nGenerative and discriminative learning are two of the major paradigms for solving prediction problems in machine learning, each offering important distinct advantages. They have often been studied in different sub-communities, but over the past decade, there has been increasing interest in trying to understand and leverage the advantages of both approaches. The goal of this workshop is to map out our current understanding of the empirical and theoretical advantages of each approach as well as their combination, and to identify open research directions.\r\n\r\nThe aim of this workshop is to provide a platform for both theoretical and applied researchers from different communities to discuss the status of our understanding on the interplay between generative and discriminative learning, as well as to identify forward-looking open problems of interest to the NIPS community. Examples of topics of interest to the workshop are as follows:\r\n\r\n    * Theoretical analysis of generative vs. discriminative learning\r\n    * Techniques for combining generative and discriminative approaches\r\n    * Successful applications of hybrids\r\n    * Empirical comparison of generative vs. discriminative learning\r\n    * Inclusion of prior knowledge in discriminative methods (semi-supervised approaches, generalized expectation criteria, posterior regularization, etc.)\r\n    * Insights into the role of generative/discriminative interface for deep learning\r\n    * Computational issues in discriminatively trained generative models/hybrid models\r\n    * Map of possible generative/discriminative approaches and combinations\r\n    * Bayesian approaches optimized for predictive performance\r\n    * Comparison of model-free and model-based approaches in statistics or reinforcement learning\r\n\r\n----\r\nThe Workshop homepage can be found at http://gen-disc2009.wikidot.com/.\r\n----\r\n", "recorded": "2009-12-12T07:30:00", "title": "Generative / Discriminative Interface"}, {"url": "icml09_raina_lsd", "desc": "The promise of unsupervised learning methods\r\nlies in their potential to use vast amounts\r\nof unlabeled data to learn complex, highly\r\nnonlinear models with millions of free parameters.\r\nWe consider two well-known unsupervised\r\nlearning models, deep belief networks\r\n(DBNs) and sparse coding, that have recently\r\nbeen applied to a flurry of machine learning\r\napplications (Hinton & Salakhutdinov, 2006;\r\nRaina et al., 2007). Unfortunately, current\r\nlearning algorithms for both models are too\r\nslow for large-scale applications, forcing researchers\r\nto focus on smaller-scale models, or\r\nto use fewer training examples.\r\nIn this paper, we suggest massively parallel\r\nmethods to help resolve these problems.\r\nWe argue that modern graphics processors\r\nfar surpass the computational capabilities of\r\nmulticore CPUs, and have the potential to\r\nrevolutionize the applicability of deep unsupervised\r\nlearning methods. We develop general\r\nprinciples for massively parallelizing unsupervised\r\nlearning tasks using graphics processors.\r\nWe show that these principles can\r\nbe applied to successfully scaling up learning\r\nalgorithms for both DBNs and sparse coding.\r\nOur implementation of DBN learning is up to\r\n70 times faster than a dual-core CPU implementation\r\nfor large models. For example, we\r\nare able to reduce the time required to learn a\r\nfour-layer DBN with 100 million free parameters\r\nfrom several weeks to around a single\r\nday. For sparse coding, we develop a simple,\r\ninherently parallel algorithm, that leads to a\r\n5 to 15-fold speedup over previous methods.", "recorded": "2009-06-17T14:55:00", "title": "Large-Scale Deep Unsupervised Learning Using Graphics Processors"}, {"url": "nips2010_bouchard_cote_vic", "desc": "Since the discovery of sophisticated fully polynomial randomized algorithms for a range of #P problems (Karzanov et al., 1991; Jerrum et al., 2001; Wilson, 2004), theoretical work on approximate inference in combinatorial spaces has focused on Markov chain Monte Carlo methods. Despite their strong theoretical guarantees, the slow running time of many of these randomized algorithms and the restrictive assumptions on the potentials have hindered the applicability of these algorithms to machine learning. Because of this, in applications to combinatorial spaces simple exact models are often preferred to more complex models that require approximate inference (Siepel et al., 2004). Variational inference would appear to provide an appealing alternative, given the success of variational methods for graphical models (Wainwright et al., 2008); unfortunately, however, it is not obvious how to develop variational approximations for combinatorial objects such as matchings, partial orders, plane partitions and sequence alignments. We propose a new framework that extends variational inference to a wide range of combinatorial spaces. Our method is based on a simple assumption: the existence of a tractable measure factorization, which we show holds in many examples.<br />Simulations on a range of matching models show that the algorithm is more general and empirically faster than a popular fully polynomial randomized algorithm. We also apply the framework to the problem of multiple alignment of protein sequences, obtaining state-of-the-art results on the BAliBASE dataset (Thompson et al., 1999).", "recorded": "2010-12-07T17:45:00", "title": "Variational Inference over Combinatorial Spaces"}, {"url": "mlg07_firenze", "desc": "Data Mining and Machine Learning are in the midst of a \"structured revolution\". After many decades of focusing on independent and identically-distributed (iid) examples, many researchers are now studying problems in which examples consist of collections of inter-related entities or are linked together into complex graphs. A major driving force is the explosive growth in the amount of heterogeneous data that is being collected in the business and scientific world. Example domains include bioinformatics, chemoinformatics, transportation systems, communication networks, social network analysis, link analysis, robotics, among others. The structures encountered can be as simple as sequences and trees (such as those arising in protein secondary structure prediction and natural language parsing) or as complex as citation graphs, the World Wide Web, and even relational data bases. In all these cases, structured representations can give a more informative view of the problem at hand, which is often crucial for the development of successful mining and learning algorithms.\r\n\r\n We believe this is an ideal time for a workshop that allows active researchers in this area to discuss and debate the unique challenges of mining and learning from structured data. The MLG 2007 workshop thus concentrates on mining and learning with structured data in general and its many appearances and facets such as interpretations, graphs, trees, sequences. Specifically, we seek to invite researchers in Statistical Relational Learning, Kernel Methods for Structured Inputs/Outputs, Graph Mining, (Multi-) Relational Data Mining, Inductive Logic Programming, among others.", "recorded": "2007-08-01T09:00:00", "title": "5th International Workshop on Mining and Learning with Graphs (MLG), Firenze 2007"}, {"url": "kdd2014_zhou_unifying_learning", "desc": "For document scoring, although learning to rank and domain adaptation are treated as two different problems in previous works, we discover that they actually share the same challenge of adapting keyword contribution across different queries or domains. In this paper, we propose to study the cross-task document scoring problem, where a task refers to a query to rank or a domain to adapt to, as the first attempt to unify these two problems. Existing solutions for learning to rank and domain adaptation either leave the heavy burden of adapting keyword contribution to feature designers, or are difficult to be generalized. To resolve such limitations, we abstract the keyword scoring principle, pointing out that the contribution of a keyword essentially depends on, first, its importance to a task and, second, its importance to the document. For determining these two aspects of keyword importance, we further propose the concept of feature decoupling, suggesting using two types of easy-to-design features: meta-features and intra-features. Towards learning a scorer based on the decoupled features, we require that our framework fulfill inferred sparsity to eliminate the interference of noisy keywords, and employ distant supervision to tackle the lack of keyword labels. We propose the Tree-structured Boltzmann Machine (T-RBM), a novel two-stage Markov Network, as our solution. Experiments on three different applications confirm the effectiveness of T-RBM, which achieves significant improvement compared with four state-of-the-art baseline methods.", "recorded": "2014-08-27T14:00:00", "title": "Unifying Learning to Rank and Domain Adaptation: Enabling Cross-Task Document Scoring"}, {"url": "dataforum2012_lindstaedt_code", "desc": "Linked Open Data (LOD) shows enormous potential in becoming the next big evolutionary step of the WWW. However, this potential remains largely untapped due to missing usage and commercialisation strategies.\r\nCODE\u2019s vision is to establish the foundation for a web-based, commercially oriented ecosystem for Linked Open Data. This ecosystem establishes a sustainable and commercial value-creation-chain among traditional (e.g. data provider and consumer) and non-traditional (e.g. data analyst) roles in data marketplaces. Monetary incentives will motivate people to analyse, organise and integrate LOD with unstructured information sources thereby increasing data quality and quantity.\r\nOur use case focuses on research papers as a source for mining facts and their integration into LOD repositories and light-weight ontologies. Hence, we will leverage the wealth of knowledge contained in research publications on a semantic, machine-readable level by creating the Linked Science Data cloud. This cloud will have an impact on innovation driven businesses by making scientific knowledge more accessible and transparent.\r\nTherefore, we will research and develop:\r\n\r\ncrowd-sourcing enabled semantic enrichment & integration techniques for integrating facts contained in unstructured information into the LOD cloud\r\nfederated, provenance-enabled querying methods for fact discovery in LOD repositories\r\nweb-based visual analysis interfaces to support human based analysis, integration and organisation of facts\r\nsocio-economic factors - roles, revenue-models and value chains - realisable in the envisioned ecosystem.", "recorded": "2012-06-06T12:00:00", "title": "Commercially empowered Linked Open Data Ecosystems in Research \u2013 The CODE Approach"}, {"url": "iswc2011_lin_rdfdata", "desc": "The increasing availability of large RDF datasets o\u000bffers an\r\nexciting opportunity to use such data to build predictive models using\r\nmachine learning algorithms. However, the massive size and distributed\r\nnature of RDF data calls for approaches to learning from RDF data in\r\na setting where the data can be accessed only through a query interface,\r\ne.g., the SPARQL endpoint of the RDF store. In applications where the\r\ndata are subject to frequent updates, there is a need for algorithms that\r\nallow the predictive model to be incrementally updated in response to\r\nchanges in the data. Furthermore, in some applications, the attributes\r\nthat are relevant for specifi\fc prediction tasks are not known a priori and\r\nhence need to be discovered by the algorithm. We present an approach\r\nto learning Relational Bayesian Classiffi\fers (RBCs) from RDF data that\r\naddresses such scenarios. Specifi\fcally, we show how to build RBCs from\r\nRDF data using statistical queries through the SPARQL endpoint of the\r\nRDF store. We compare the communication complexity of our algorithm\r\nwith one that requires direct centralized access to the data and hence\r\nhas to retrieve the entire RDF dataset from the remote location for processing.\r\nWe establish the conditions under which the RBC models can\r\nbe incrementally updated in response to addition or deletion of RDF\r\ndata. We show how our approach can be extended to the setting where\r\nthe attributes that are relevant for prediction are not known a priori,\r\nby selectively crawling the RDF data for attributes of interest. We provide\r\nopen source implementation and evaluate the proposed approach on\r\nseveral large RDF datasets.", "recorded": "2011-10-26T11:00:00", "title": "Learning relational bayesian classifiers from RDF data"}, {"url": "kdd09_wong_cduhms", "desc": "Many applications in surveillance, monitoring, scientific discovery, and data cleaning require the identification of anomalies. Although many methods have been developed to identify statistically significant anomalies, a more difficult task is to identify anomalies that are both interesting and statistically significant. Category detection is an emerging area of machine learning that can help address this issue using a \"human-in-the-loop\" approach. In this interactive setting, the algorithm asks the user to label a query data point under an existing category or declare the query data point to belong to a previously undiscovered category. The goal of category detection is to bring to the user's attention a representative data point from each category in the data in as few queries as possible. In a data set with imbalanced categories, the main challenge is in identifying the rare categories or anomalies; hence, the task is often referred to as {\\it rare} category detection. We present a new approach to rare category detection based on hierarchical mean shift. In our approach, a hierarchy is created by repeatedly applying mean shift with an increasing bandwidth on the data. This hierarchy allows us to identify anomalies in the data set at different scales, which are then posed as queries to the user. The main advantage of this methodology over existing approaches is that it does not require any knowledge of the dataset properties such as the total number of categories or the prior probabilities of the categories. Results on real-world data sets show that our hierarchical mean shift approach performs consistently better than previous techniques.\r\n", "recorded": "2009-06-29T16:50:00", "title": "Category Detection Using Hierarchical Mean Shift"}, {"url": "iswc2014_vidal_drug_target_interaction", "desc": "The ability to integrate a wealth of human-curated knowledge from scientific datasets and ontologies can benefit drug-target interaction prediction. The hypothesis is that similar drugs interact with the same targets, and similar targets interact with the same drugs. The similarities between drugs reflect a chemical semantic space, while similarities between targets reflect a genomic semantic space. In this paper, we present a novel method that combines a data mining framework for link prediction, semantic knowledge (similarities) from ontologies or semantic spaces, and an algorithmic approach to partition the edges of a heterogeneous graph that includes drug-target interaction edges, and drug-drug and target-target similarity edges. Our semantics based edge partitioning approach, semEP, has the advantages of edge based community detection which allows a node to participate in more than one cluster or community. The semEP problem is to create a minimal partitioning of the edges such that the cluster density of each subset of edges is maximal. We use semantic knowledge (similarities) to specify edge constraints, i.e., specific drug-target interaction edges that should not participate in the same cluster. Using a well-known dataset of drug-target interactions, we demonstrate the benefits of using semEP predictions to improve the performance of a range of state-of-the-art machine learning based prediction methods. Validation of the novel best predicted interactions of semEP against the STITCH interaction resource reflect both accurate and diverse predictions.", "recorded": "2014-10-21T12:00:00", "title": "Drug-Target Interaction Prediction Using Semantic Similarity and Edge Partition"}, {"url": "promo_franc_solina_eng", "desc": "Computer vision makes interpretations of information that comes in form of images. As people use vision to recognize other people, objects and their immediate surroundings so that they can move and act on it, computer vision offers this capability to computers and robots.\r\n\r\nThe research program Computer Vision exists at University of Ljubljana since 1999. It consists of two research laboratories at the Faculty of Computer and Information Science with almost 20 researchers. One of their researchers is employed at the Faculty of Arts.\r\n\r\nLately, computer vision is getting closer to other artificial intelligence areas, in particular to machine learning since our main goal is to develop intelligent systems that can act and work in unstructured everyday human environment.\r\n\r\nIn our research we are focused on finding the principles of interactive continuous visual learning and learning of object categories. We study these problems from a theoretical point as well as by implementing various practical ways of learning visual concepts.\r\n\r\nBeside basic research we are engaged in several applicative projects.\r\nOne of our application projects is on intelligent user interfaces to determine in real-time the characteristics of users such as their age, gender and 3D position.\r\n \r\nWe are working also on biometrical applications, in particular on human faces and fingerprints. \r\n\r\nA particular aspect of our work is the use of computer vision in the context of interactive art installations. \r\n\r\nOur research program is tightly connected into the European research network. In the last eight years we participated in nine European projects that brought in a total of 2.5 millions of Euros to our research program.", "recorded": "2012-04-25T12:59:32", "title": "Computer vision"}, {"url": "kdd2014_liu_chinese_restaurant", "desc": "Processing large volumes of streaming data in near-real-time is becoming increasingly important as the Internet, sensor networks and network traffic grow. Online machine learning is a typical means of dealing with streaming data, since it allows the classification model to learn one instance of data at a time. Although many online learning methods have been developed since the development of the Perceptron algorithm, existing online methods assume that the number of classes is available in advance of classification process. However, this assumption is unrealistic for large scale or streaming data sets. This work proposes an online Chinese restaurant process (CRP) algorithm, which is an online and nonparametric algorithm, to tackle this problem. This work proposes a relaxing function as part of the prior and updates the parameters with the likelihood function in terms of the consistency between the true label information and predicted result. This work presents two Gibbs sampling algorithms to perform posterior inference. In the experiments, the online CRP is applied to three massive data sets, and compared with several online learning and batch learning algorithms. One of the data sets is obtained from Wikipedia, which comprises approximately two million documents. The experimental results reveal that the proposed online CRP performs well and efficiently on massive data sets. Finally, this work proposes two methods to update the hyperparameter $\\alpha$ of the online CRP. The first method is based on the posterior distribution of $\\alpha$, and the second exploits the property of online learning, namely adapting to change, to adjust $\\alpha$ dynamically.", "recorded": "2014-08-25T11:55:00", "title": "Online Chinese Restaurant Process"}, {"url": "sip08_berlin", "desc": "Data sets with a very large number of explanatory variables are becoming more and more common as features of both applications and theoretical investigations. In economical applications for instance, the revealed preference of market players is observed, and the analyst tries to understand them by a complex model by which the players' behavior can be understood as an indirect observation.  State-of-the art statistical approaches often formulate such models as inverse problems, but the corresponding methods can suffer of the curse of dimensionality: when there are \"too many\" possible explanatory variables, additional regularization is needed. Inverse problem theory already offers sophisticated regularization methods for smooth models, but is just beginning to integrate sparsity concepts. For high-dimensional linear models, sparsity regularizations have proved to be a convincing  way to tackle the issue both in theory and practice, but there remains  a  vast ground to be explored. Paralleling the statistics community are also recent advances in machine learning methodology and statistical learning theory, where the themes of sparsity and inverse problems have been intertwined.\r\n\r\nThe workshop will focus on the different ways to attack a same question:  there are many potential models to choose from, but each of them is relatively simple - each model is parameterized by many variables, most of them are zero. Yet, the choice of the right model or regularization parameter is crucial to obtain stable and reliable results.\r\n\r\n\r\nFind out more about the workshop [[http://pluto.mscc.huji.ac.il/~yaacov/SparsityWorkshop/index.html|here]].", "recorded": "2008-12-05T09:00:00", "title": "Workshop on Sparsity and Inverse Problems in Statistical Theory and Econometrics,  Berlin 2008"}, {"url": "lmcv04_szedmak_lsmcb", "desc": "One of the most hard tasks in image classification to find a method being applicable on large scale multi-class problems where the sample size and number of the features are huge. Linear discriminant analysis as a classica l method for multi-class classification, which was introduced by Fisher (1936) [4], plays an important role in the machine learning society recently. The kernelized version of this method are discussed in several papers, however they generally deal with the two class version of this approach. Bartlett recognised, in 1938 [2], there is strong relationship between the Fisher Discriminant and the Canonical Correlation Analysis and this statement is valid for the multi-class case as well. Based on this work Barker et al. (2003) [1] and Rosipal et al. (2003) [10] discuss the details about this relationship and show the appropriate kernel approach to this problem. Using Canonical Correlation for multi-class classification in large scale problem suffers from the numerical difficulty to solve the generalised eigenvalue problem to provide the optimum. We present an analogue classificationprocedure based on linear optimisation which is able to extend the scale range of the solvable problems and to give sparse solution. Our method exploits the relationship between the L1 norm SVM and the boosting approach which were presented by Bennett et al. (2000) [3], Mangasarian (1999) [5] and Meir et al. (2003) [6]. Additionally, the formulation based on the soft margin SVM can solve the problem when the number of the features are less than the number of the observations in a given sample.", "recorded": "2004-05-04T00:00:00", "title": "Large scale multiclass classification based on linear optimization"}, {"url": "nipsworkshops09_posner_wwts", "desc": "The availability of continuous streams of data from multiple modalities covering the same workspace has long been recognised as a privilege by robotics researchers. Data fusion has a successful track record in the field leading to the by now routine generation of high-quality large scale metric and topological maps of unstructured environments. With this success, however, comes the realisation that prominent applications in robotics -- such as action selection and human machine interaction -- require information beyond mere metric or topological representations. As a result, researchers throughout the community are becoming increasingly interested in adding higher-order, semantic information to the maps obtained. In this context, the availability of a rich set of data from complimentary modalities once again comes into its own. In this talk we provide a snapshot of ongoing work aiming to enrich standard metric or topological maps as provided by a mobile robot with higher-order semantic information. Environmental cues are considered for classification at different scales. The first stage considers local scene properties using a probabilistic bag-of-words classifier. The second stage incorporates contextual information across a given scene (spatial context) and across several consecutive scenes (temporal context) via a Markov Random Field (MRF). Our approach is driven by data from an onboard camera and 3D laser scanner and uses a combination of visual and geometric features. We demonstrate the virtue of considering such spatial and temporal context during the classification task and analyse the performance of our technique on data gathered over 17 km of track through a city. ", "recorded": "2009-12-12T07:30:00", "title": "Where's What? - Towards Semantic Mapping of Urban Environments"}, {"url": "colt2013_awasthi_queries", "desc": "We introduce a new model of membership query (MQ) learning, where the learning algorithm is restricted to query points that are close to random examples drawn from the underlying distribution. The learning model is intermediate between the PAC model (Valiant,1984) and the PAC+MQ model (where the queries are allowed to be arbitrary points).\r\n\r\nMembership query algorithms are not popular among machine learning practitioners. Apart from the obvious difficulty of adaptively querying labellers, it has also been observed that querying unnatural points leads to increased noise from human labellers (Lang and Baum, 1992). This motivates our study of learning algorithms that make queries that are close to examples generated from the data distribution.\r\n\r\nWe restrict our attention to functions defined on the n-dimensional Boolean hypercube and say that a membership query is local if its Hamming distance from some example in the (random) training data is at most O(log(n)). We show the following results in this model:\r\n\r\n1. The class of sparse polynomials (with coefficients in R) over {0,1}n is polynomial time learnable under a large class of locally smooth distributions using O(log(n))-local queries. This class also includes the class of O(log(n))-depth decision trees.\r\n\r\n2. The class of polynomial-sized decision trees is polynomial time learnable under product distributions using O(log(n))-local queries.\r\n\r\n3. The class of polynomial size DNF formulas is learnable under the uniform distribution using O(log(n))-local queries in time nO(log(log(n))).\r\n\r\n4. In addition we prove a number of results relating the proposed model to the traditional PAC model and the PAC+MQ model.", "recorded": "2013-06-13T16:30:00", "title": "Learning Using Local Membership Queries"}, {"url": "bsciw08_whistler", "desc": "The WWW has reached the stage where it can be looked upon as a gigantic information copying and distribution mechanism. But when the problem of distributing and copying information is essentially solved, where do we go next? There are a number of values that can be derived from the mesh, that also have immediate relevancy for the ML community. Goal of the workshop is to link these areas, and encourage cross-boundary thinking and working. \r\n\r\nTopics will be:\r\n# Machine learning and probabilistic modeling: Recommendation systems and knowledge extraction are two immediate applications, with research required for large scale inference, modeling languages, and efficient decision making. \r\n# Game theory and mechanism design: When a large number of contributors is involved, how can tasks and incentive structures be made such that the desired goal is achieved? Research is required for solving very large games, and for mechanism design under uncertainty. \r\n# Knowledge representation and reasoning: Large parts of the web are currently stored in an unstructured way, making linking and evaluating knowledge a complex problem. Open points are the difficulty of reasoning, the tradeoff between efficiency of reasoning and power of the representation, and reasoning under uncertainty. \r\n# Social networks and collective intelligence: How does information flow in the web? Who is reading what, who is in touch with whom? These networks need to be analyzed, modeled, and made amenable to reasoning. (\r\n# Privacy preserving learning: What can be learned, and how can be learned, whilst only revealing a minimal set of information, or information that does not make users individually identifiable?\r\n\r\nMore information about workshop - http://research.microsoft.com/osa/adCenter/beyond_search/", "recorded": "2008-12-12T07:30:00", "title": "NIPS \u02d908 Workshop: Beyond Search - Computational Intelligence for the Web"}, {"url": "estc08_franz_etm", "desc": "ROCESSUS as a particular project of the German national funded high-tech-initiative THESEUS has the objective to create an IT-based corporate system that will allow companies to compare products, solutions and details of business associates, as well as locating the complex and sometimes obscure specialist information needed by employees whose work involves high-density knowledge bases.\r\n\r\nThe research teams are also aiming to develop a basic semantic platform that will integrate a company\u2019s internal planning of resources with management of the digital content of agile business processes. One specific scenario taken from the domain of mechanical engineering will demonstrate the requirements of an ontology based intra- and intererenterprise communication for solution and application retrieval in the Engineering domain. A prototypical demonstrator will illustrate the solution approach.\r\n\r\nempolis is a software company specialised in knowledge management and information access technologies. Knowledge based technologies form the basis for empolis\u2019 products. Starting from core Case-Based Reasoning technology, empolis has evolved its tools towards a semantic processing technology. It is a platform for the implementation of hybrid intelligent applications that can make use of a variety of AI technologies, including e.g., the CBR kernel, a rule engine, machine learning and user modelling techniques, to name a few. Today we offer a complete framework for setting up intelligent content management, knowledge management, retrieval and assistance systems. empolis is an arvato AG subsidiary, an international media service company and part of Bertelsmann AG. empolis employs 200 people in Germany and international divisions.", "recorded": "2008-09-26T10:32:05", "title": "empolis/tu munich \u2502 Semantic Enterprise: Unleashing Solution Knowledge in the Area of Mechanical Engineering"}, {"url": "icml09_gondzio_ituipm", "desc": "In this talk we shall discuss the issues of Interior Point Methods (IPMs) applied to solve optimization problems arising in the context of very large-scale Support Vector Machine (SVM) training. First, we will briefly introduce IPMs for linear and quadratic programming and comment on their advantages: (a) polynomial complexity, (b) ability to solve very large problems, (c) excellent practical behaviour (much better than that predicted by the worst-case complexity analysis), and (d) eligibility to parallelisation [1]. We will then address specific features of optimization problems arising in SVM training, in particular, the presence of large datasets which are stored as dense matrices and make problems very well-suited to the use of IPMs. We will survey numerical techniques applicable in this context such as suitable factorization methods and we will comment on several interesting developments made over the last decade which aimed at using IPMs for SVM training. We will demonstrate that the key to success in applying IPMs is the ability to re-formulate SVM problems as separable quadratic optimization ones which then can be successfully tackled by appropriate linear algebra tools of IPMs [2,3]. Finally, we will comment on the existing challenges for IPMs in SVM training context and touch a number of research issues which still remain open. In particular, we will discuss a need of developing new algorithms which may take advantage of new multi-core architectures, challenges of problems getting larger and larger, and the use of non-linear and indefinite kernels. This is joint work with Kristian Woodsend.", "recorded": "2009-06-18T16:00:00", "title": "Using Interior Point Methods for Optimization in Training Very Large Scale Support Vector Machines"}, {"url": "bio2014_debata_odprta_ustvarjalnost", "desc": "Na\u010drt, izdelan na odprtokodnem programu. Objekt, izdelan na 3d tiskalniku, ki je sestavljen iz del\u010dkov, stiskanih na drugem 3d tiskalniku. Objekt, ki \u0161ele v kombinaciji z drugimi objekti dobi funkcijo. \u0160tevilne funkcije, ki jo kombinacije objektov omogo\u010dajo in ki se rojevajo \u0161ele v prihodnosti z raznolikimi potrebami in vizijami najrazli\u010dnej\u0161ih uporabnikov.\r\n\r\nJe odprta ustvarjalnost zgolj igra\u010dkanje velikih otrok ali \u017ee \u017eivimo revolucijo produkcije? Kje in na kak\u0161en na\u010din je v tem kontekstu oblikovalska ter arhitekturna stroka in praksa? Kak\u0161ne so prilo\u017enosti in pasti odprtega ustvarjanja?\r\n\r\nO tem in \u0161e o marsi\u010dem so na okrogli mizi spregovorili:\r\n\r\n- dr. Maja Bogataj Jan\u010di\u010d, ustanoviteljica IPI, In\u0161tituta za intelektualno lastnino, ki je med drugim vodila projekt Creative Commons Slovenija,\\\\\r\n- industrijski oblikovalec Tilen Sepi\u010d, ki je na BIO50 mentoriral skupino Hekanje gospodinjskih aparatov in je tudi soustanovitelj Poligon Maker Lab-a, ter\\\\\r\n- predsednik Piratske stranke Slovenije Rok De\u017eelak, v preteklosti tudi \u010dlan glasbenega kolektiva The Stroj (danes Stroj Machine), kjer je spoznaval in dodeloval koncepte ponovne rabe stvari v namene umetni\u0161kega ustvarjanja.\r\n\r\nOkroglo mizo je vodila Eva Per\u010di\u010d, neodvisna raziskovalka novih ekonomskih modelov in kreativnih skupnosti ter soustanoviteljica iniciativ Slovenia Coworking in Slovenia Crowdfunding ter kreativnega centra Poligon.", "recorded": "2014-10-16T18:05:00", "title": "Odprta ustvarjalnost: modnost ali prihodnost?"}, {"url": "bbci2012_mueller_multimodal_imaging", "desc": "Each method for imaging brain activity has technical or physiological limits. Thus, combinations of neuroimaging modalities that can alleviate these limitations such as simultaneous recordings of neurophysiological and hemodynamic activity have become increasingly popular. Multimodal imaging setups can take advantage of complementary views on neural activity and enhance our understanding about how neural information processing is reflected in each modality. However, dedicated analysis methods are needed to exploit the potential of multimodal methods. The talk will first spend some time on the multimodal data fusion problem from the Machine Learning point of view and introduce useful algorithms. Then I will discuss a hybrid noninvasive Brain Computer Interface (BCI) that combines electroencephalography (EEG) and near-infrared spectroscopy (NIRS).  In particular I will show that near-infrared spectroscopy (NIRS) can be used to enhance the EEG-BCI approach. In our study both methods were applied simultaneously in a real-time Sensory Motor Rhythm (SMR)-based BCI paradigm, involving executed movements as well as motor imagery. We tested how the classification of NIRS data can complement ongoing real-time EEG classification. Our results show that simultaneous measurements of NIRS and EEG can significantly improve the classification accuracy of motor imagery in over 90% of considered subjects and increases performance by 5% on average. However, the long time delay of the hemodynamic response may hinder an overall increase of bit-rates. Furthermore we find that EEG and NIRS complement each other in terms of information content and are thus a viable multimodal imaging technique, suitable for BCI.", "recorded": "2012-09-18T09:45:00", "title": "Multimodal Imaging and BCI "}, {"url": "nips2010_jain_grm", "desc": "Minimizing the rank of a matrix subject to affine constraints is a fundamental problem with many important applications in machine learning and statistics. In this paper we propose a simple and fast algorithm SVP (Singular Value Projection) for rank minimization under affine constraints ARMP and show that SVP recovers the minimum rank solution for affine constraints that satisfy a Restricted Isometry Property} (RIP). Our method guarantees geometric convergence rate even in the presence of noise and requires strictly weaker assumptions on the RIP constants than the existing methods. We also introduce a Newton-step for our SVP framework to speed-up the convergence with substantial empirical gains. Next, we address a practically important application of ARMP - the problem of low-rank matrix completion, for which the defining affine constraints do not directly obey RIP, hence the guarantees of SVP do not hold. However, we provide partial progress towards a proof of exact recovery for our algorithm by showing a more restricted isometry property and observe empirically that our algorithm recovers low-rank Incoherent matrices from an almost optimal number of uniformly sampled entries. We also demonstrate empirically that our algorithms outperform existing methods, such as those of \\cite{CaiCS2008,LeeB2009b, KeshavanOM2009}, for ARMP and the matrix completion problem by an order of magnitude and are also more robust to noise and sampling schemes. In particular, results show that our SVP-Newton method is significantly robust to noise and performs impressively on a more realistic power-law sampling scheme for the matrix completion problem.", "recorded": "2010-12-08T11:57:00", "title": "Guaranteed Rank Minimization via Singular Value Projection"}, {"url": "mlsb2010_ljosa_aqs", "desc": "Microscopy-based high-throughput experiments can provide a view into biological responses and states at the resolution of singe cells.\r\n\r\nCellProfiler, our open-source image-analysis software, has become widely used by biologists to design custom analysis pipelines for complex high-throughput assays. I will discuss our work in progress to automatically quantify the prevalence of subtle cellular phenotypes in high-throughput samples of cultured cells I will also touch briedly on the use of machine learning to improve the accuracy and robustness of CellProfiler's image segmentation.\r\n\r\nOur classification tool, CellProfiler Analyst, enables a biologist to train a boosting classifier iteratively to detect rare, complex phenotypes, and its usefulness has been demonstrated in several high-throughput screens. Here, I will describe a method to learn phenotypes without requiring hand-labeled cells for training. Instead, a classifier is trained from negative and positive controls in the experiment, where the positives are known to be enriched in the phenotype of interest, even if only slightly (e.g., 55% vs. 45% penetrance). By nonlinearly projecting cells into a random feature space, we can use efficient linear methods but still benefit from nonlinear notions of similarity, and can overcome experimental noise by training on millions of cells. Using the resulting classifier to assign soft labels to each cell in the experiment, we can identify enriched samples (\"hits\") nonparametrically. Furthermore, we are developing techniques to automatically identify relevant cellular phenotypes in large-scale chemical profiling experiments. ", "recorded": "2010-10-16T11:20:00", "title": "Automatic quantification of subtle cellular phenotypes in microscopy-based high-throughput experiments"}, {"url": "eswc2015_both_linked_data", "desc": "The Web of Data puts a vast and ever-increasing amount of information at the disposal of its users. In the era of big data, interpreting and exploiting these information is both a highly active research area and a key issue for users in industry trying to gain a competitive edge. One current problem in industry with many potential application areas is  finding  a  common  theme  for  varying  features  by  generating  higher level summaries. We introduce the notion of motives to describe these common themes. Motives can be identified for all sorts of entities such as geo-spatial regions (e.g., \\cultural regions\") or holidays (e.g., \\winter holidays\", \\activity holidays\"). These motives are closer to common\r\nlanguage and human conversations than ordinary keywords. Since  users  prefer  formulating  their  information  needs  using  everyday language, which expresses their understanding of the world, the potential  for  a  strong  industrial  impact  for  search  applications  can  be  derived. However, capturing the users' often vaguely formulated intentions and matching them to appropriate retrieval operations on the available knowledge bases is a challenging issue. Yet, it is an important step on the way of providing the best possible search experience to users. This paper presents our work in progress on computing motives for geospatial  regions.  Following  a  long  term  agenda,  we  are  evaluating  the requirements for identifying such motives in large data sets. At this point, we can show that out-of-the-box machine learning methods can be used on Linked Data to train a model for computation of geo-spatial motives with good accuracy", "recorded": "2015-05-31T15:00:00", "title": "Computing Geo-Spatial Motives from Linked Data for Search-driven Applications"}, {"url": "licsb08_tamaddoni_aii", "desc": "A central challenge in computational systems biology is to build models that will allow us to \u201cread\r\nacross\u201d between different \u2018omics datasets and to use diverse biological prior knowledge. In this talk we\r\npresent a framework for integrative systems biology which uses inductive/abductive logic\r\nprogramming. In this talk we first give an overview of abduction and induction and discuss their\r\nsimilarities and differences within a logicbased framework. We will review two previous projects\r\nwhere abduction and induction have been used for learning metabolic network models from biological\r\ndata and prior knowledge. We will also report on our current research in which we use inductive/abductive logic programming to build predictive models of genotypephenotype\r\nrelations in Campylobacter jejuni. These models will be used to test the hypothesis that mutations in\r\nCampylobacter jejuni result in changes in the surface glycans and therefore changes the host responses to the infection. This modeling involves two main parts: (i) a machine learning technique which can infer hypotheses about the genome of the mutants from the observed changes in the surface glycome and (ii) a cycle of model generation/revision and experimentation which allows the predictions of the model to be experimentally tested and the model to be revised accordingly. Inductive/abductive logic programming will be used to infer hypotheses from observed changes in metabolites, glycans and gene expressions profiles (empirical data) together with background knowledge which includes known\r\nmetabolic networks, glycan structures and enzyme/gene functions extracted from biological databases\r\nsuch as KEGG. We present the results from an initial inductive/abductive model for genotypephenotype\r\nrelation in Campylobacter jejuni and discuss the future plans.", "recorded": "2008-03-27T09:40:00", "title": "Abductive and inductive inference for integrative Systems Biology"}, {"url": "russir09_shanahan_cabmtai", "desc": "Internet advertising revenues in the United States totaled $21 billion for 2007, up 25 percent versus 2006 revenues of $16.9 billion (according to the Interactive Advertising Bureau); this represents approximately half the worldwide revenue from online advertising. Fueled by these growth rates and the desire to provide added incentives and opportunities for both advertisers and publishers, alternative business models to online advertising are been developed. This tutorial will review the main business models of online advertising including: the pay-per-impression model (CPM); and the pay-per-click model (CPC); a relative new comer, the pay-per-action model (CPA), where an action could be a product purchase, a site visit, a customer lead, or an email signup; and dynamic CPM (dCPM) which optimizes a campaign towards the sites and site sections that perform best for the advertiser.\r\nThis tutorial will also discuss in detail the technology being leveraged to automatically target ads within these business models; this largely derives from the fields of machine learning (e.g., logistic regression, online learning), statistics (e.g., binomial maximum likelihood), information retrieval (vector space model, BM25), optimization theory (linear and quadratic programming), economics (auction mechanisms, game theory). Challenges such as click fraud (the spam of online advertising), deception, privacy and other open issues will also be discussed. Web 2.0 applications such as social networks, and video/photo-sharing pose new challenges for online advertising. These will also be discussed. ", "recorded": "2009-09-11T10:30:00", "title": "Computational advertising: business models, technologies and issues (CoAd)"}, {"url": "nipsworkshops2011_model_order_selection", "desc": "Model order selection, which is a trade-off between model resolution and its statistical reliability, is one of the fundamental questions in machine learning. It was studied in detail in the context of supervised learning with i.i.d. samples, but received relatively little attention beyond this domain. The goal of our workshop is to raise attention to the question of model order selection in other domains, share ideas and approaches between the domains, and identify perspective directions for future research. Our interest covers ways of defining model complexity in different domains, examples of practical problems, where intelligent model order selection yields advantage over simplistic approaches, and new theoretical tools for analysis of model order selection. The domains of interest span over all problems that cannot be directly mapped to supervised learning with i.i.d. samples, including, but not limited to, reinforcement learning, active learning, learning with delayed, partial, or indirect feedback, and learning with submodular functions.\r\n\r\nAn example of first steps in defining complexity of models in reinforcement learning, applying trade-off between model complexity and empirical performance, and analyzing it can be found in [1-4]. An intriguing research direction coming out of these works is simultaneous analysis of exploration-exploitation and model order selection trade-offs. Such an analysis enables to design and analyze models that adapt their complexity as they continue to explore and observe new data. Potential practical applications of such models include contextual bandits (for example, in personalization of recommendations on the web [5]) and Markov decision processes.\r\n\r\nWorkshop homepage: http://people.kyb.tuebingen.mpg.de/seldin/fimos.html", "recorded": "2011-12-16T07:30:00", "title": "New Frontiers in Model Order Selection"}, {"url": "colt2015_cheng_spectral_sparsification", "desc": "Motivated by a sampling problem basic to computational statistical inference, we develop a toolset based on spectral sparsification for a family of fundamental problems involving Gaussian sampling, matrix functionals, and reversible Markov chains. Drawing on the connection between Gaussian graphical models and the recent breakthroughs in spectral graph theory, we give the first nearly linear time algorithm for the following basic matrix problem: Given an n-by-n Laplacian matrix M and a constant -1<= p <= 1, provide efficient access to a sparse n-by-n linear operator C such that M^p \\approx C C^T, where \\approx denotes spectral similarity. When p is set to -1, this gives the first parallel sampling algorithm that is essentially optimal both in total work and randomness for Gaussian random fields with symmetric diagonally dominant (SDD) precision matrices. It only requires nearly linear work and 2n i.i.d. random univariate Gaussian samples to generate an n-dimensional i.i.d. Gaussian random sample in polylogarithmic depth.\r\nThe key ingredient of our approach is an integration of spectral sparsification with multilevel method: Our algorithms are based on factoring M^p into a product of well-conditioned matrices, then introducing powers and replacing dense matrices with sparse approximations. We give two sparsification methods for this approach that may be of independent interest. The first invokes Maclaurin series on the factors, while the second builds on our new nearly linear time spectral sparsification algorithm for random-walk matrix polynomials. We expect these algorithmic advances will also help to strengthen the connection between machine learning and spectral graph theory, two of the most active fields in understanding large data and networks.", "recorded": "2015-07-05T15:05:00", "title": "Efficient Sampling for Gaussian Graphical Models via Spectral Sparsification"}, {"url": "kdd09_chen_lsbt", "desc": "Best Application Paper Award Winner\r\nBehavioral targeting (BT) leverages historical user behavior to select the ads most relevant to users to display. The state-of-the-art of BT derives a linear Poisson regression model from fine-grained user behavioral data and predicts click-through rate (CTR) from user history. We designed and implemented a highly scalable and efficient solution to BT using Hadoop MapReduce framework. With our parallel algorithm and the resulting system, we can build above 450 BT-category models from the entire Yahoo's user base within one day, the scale that one can not even imagine with prior systems. Moreover, our approach has yielded 20% CTR lift over the existing production system by leveraging the well-grounded probabilistic model fitted from a much larger training dataset.\r\n\r\nSpecifically, our major contributions include: (1) A MapReduce statistical learning algorithm and implementation that achieve optimal data parallelism, task parallelism, and load balance in spite of the typically skewed distribution of domain data. (2) An in-place feature vector generation algorithm with linear time complexity O(n) regardless of the granularity of sliding target window. (3) An in-memory caching scheme that significantly reduces the number of disk IOs to make large-scale learning practical. (4) Highly efficient data structures and sparse representations of models and data to enable fast model updates. We believe that our work makes significant contributions to solving large-scale machine learning problems of industrial relevance in general. Finally, we report comprehensive experimental results, using industrial proprietary codebase and datasets.", "recorded": "2009-06-30T14:50:00", "title": " Large-Scale Behavioral Targeting "}, {"url": "ecmlpkdd09_iba_chrmgp", "desc": "We show the real-world applications of EC (evolutionary computation)\r\nto robotics, which is called \"evolutionary robotics\".\r\nMachine Learning techniques can be applied to\r\na robot in order to achieve a task for it if the appropriate\r\nactions are not predetermined. In such a situation, the robot can\r\nlearn the appropriate actions by using trial-and-error in a real\r\nenvironment. GP (Genetic Programming) can generate programs to control\r\na robot directly, and many studies have been done showing this.\r\nGA (Genetic Algorithms) in combination\r\nwith neural networks (NN) can also be used to control robots.\r\nRegardless of the method used, the evaluation of real robots\r\nrequires a significant amount of time partly due to their complex\r\nmechanical actions. Moreover, evaluations have to be repeated over\r\nseveral generations for many individuals in both GP and GA.\r\nTherefore, in most studies, the learning is conducted in\r\nsimulation, and the acquired results are applied to real robots.\r\nTo solve these difficulties, we propose an integrated technique of\r\ngenetic programming and reinforcement learning (RL) to enable a\r\nreal robot to adapt its actions in a real environment. Our\r\ntechnique does not require a precise simulator because learning is\r\nachieved through the real robot. In addition, our technique makes\r\nit possible for real robots to learn effective actions. Based on\r\nthis proposed technique, we evolve common programs using GP,\r\nwhich are applicable to various types of robots. Using this\r\nevolved program, we execute reinforcement learning in a real\r\nrobot. With our method, the robot can adapt to its own operational\r\ncharacteristics and learn effective actions. The effectiveness of\r\nour proposed approach is demonstrated by performing experiments\r\nwith real humanoid robots.", "recorded": "2009-09-07T09:30:00", "title": "Controlling Humanoid Robots by Means of Genetic Programming"}, {"url": "icml09_kakade_itscbs", "desc": "Research at the intersection of machine learning and economics has flourished in recent years due to the realization that many technological systems (such as the internet) are better understood and managed when they are viewed as economic systems rather than just merely technological ones. In particular, there is much recent work on understanding learning algorithms and models in settings where the strategic considerations of the participants must be taken into account (e.g. spam detection).\r\nThis talk will examine the this broader issue in the setting of online ad-auctions (in particular \"pay-per-click\" auctions, where advertisers are charged only those rounds when their ad is clicked on). Designing such an auction faces the classic explore/exploit dilemma: while gathering information about the \"click-through-rates\" of advertisers, the mechanism may loose revenue; however, this gleaned information may prove valuable in the future for a more profitable allocation. In this sense, such mechanisms are prime candidates to be designed using multi-armed bandit techniques. However, a naive application of multi-armed bandit algorithms would not take into account the strategic considerations of the players - players might manipulate their bids (which determine the auction's revenue) in a way as to maximize their own utility. Hence, we consider the natural restriction that the auction be truthful.\r\nThis work sharply characterizes what regret is achievable, under this truthful restriction. Interestingly, we show that this restriction imposes statistical limits on the achievable regret - that it is Theta(T^2/3), while for traditional bandit algorithms (without this truthful restriction) the achievable regret is O(T^1/2) (where T is the number of rounds). We term the extra T^1/6 factor, the \"price of truthfulness\".", "recorded": "2009-06-18T16:00:00", "title": "Strategic Considerations in Bandit Settings - The Price of Truthfulness in Online Ad Auctions"}, {"url": "mcvc08_dupont_ifl", "desc": "Information retrieval is a very wide domain which can involve various types of activities and tasks. Many complex factors are participating in a search for information and many systems have been experimented. Nowadays a general consensus has been established around a keyword/document matching process which appears to be efficient on large scale and have enough reliability to satisfy a significant part of the users. Btu this claim has to be limited and for some subjects, search is still a difficult task. Many reasons can be proposed to explain these phenomena, but the most salient ones are the difficulty for users to express their needs while searching for information and the limitation of shared knowledge between users and information retrieval systems, meaning that both users and machines don't really understand the information and knowledge space used as references by the other. This presentation try to provide an overview of one way to resolve those gaps: using feedback learning. The aim is to make the system learning on user behaviour in order to better define its current needs. Machine learning algorithms applied on signal coming from user while performing a search can lead to the understanding of what is really relevant to the users and then can be exploited to help him during its tasks. The work, engaged through the VITALAS1 project, is presented: study of users search logs and definition of a feedback learning framework. Then research on implicit relevance feedback and query optimisation is presented as a first attempt to exploit the feedback learning framework. Finally an overview of the next steps within those studies is presented and especially their impact on the VITALAS project.", "recorded": "2008-02-12T14:05:19", "title": "Implicit feedback learning in semantic and collaborative information retrieval systems"}, {"url": "kdd08_han_mmrfid", "desc": "With the wide availability of satellite, RFID, GPS, sensor, wireless, and video technologies, moving-object data has been collected in massive scale and is becoming increasingly rich, complex, and ubiquitous. There is an imminent need for scalable and flexible data analysis over moving-object information; and thus mining moving-object data has become one of major challenges in data mining. There have been considerable research efforts on data mining for RFID, trajectory, and traffic data sets. However, there has been no systematic tutorial on knowledge discovery from such moving-object data sets. This tutorial presents a comprehensive, organized, and state-of-the-art survey on methodologies and algorithms on analyzing different kinds of moving-object data sets, with an emphasis on several important mining tasks: clustering, classification, outlier analysis, and multidimensional analysis. Besides a thorough survey of the recent research work on this topic, we also show how real-world applications can benefit from data mining of RFID, trajectory, and traffic data sets. The tutorial consists of three parts: (1) RFID data mining, (2) trajectory data mining, and (3) traffic data mining. In the first part, warehousing, cleaning, and flow mining for RFID data are explored. In the second part, pattern mining, clustering, classification, and outlier detection for trajectory data are explored. In the third part, route discovery, destination prediction, and hot-route or outlier detection for traffic data are explored. This tutorial is prepared for data mining, database, and machine learning researchers who are interested in moving-object data. ", "recorded": "2008-08-24T09:00:00", "title": "Mining Massive RFID, Trajectory, and Traffic Data Sets"}, {"url": "xlike", "desc": "The goal of the **[[http://www.xlike.org/|X-LIKE project]]** is to develop technology to monitor and aggregate knowledge that is currently spread across global mainstream and social media, and to enable cross-lingual services for publishers, media monitoring and business intelligence.\r\n\r\nIn terms of research contributions, the aim is to combine scientific insights from several scientific areas to contribute in the area of cross-lingual text understanding. By combining modern computational linguistics, machine learning, text mining and semantic technologies we plan to deal with the following two key open research problems:\r\n#to extract and integrate formal knowledge from multilingual texts with cross-lingual knowledge bases, and\r\n#to adapt linguistic techniques and crowdsourcing to deal with irregularities in informal language used primarily in social media.\r\n\r\nAs an interlingua, knowledge resources from Linked Open Data cloud (http://linkeddata.org) will be used with special focus on general common sense knowledge base CycKB (http://www.cyc.com). For the languages where no required linguistic resources will be available, we will use a probabilistic interlingua representation trained from a comparable corpus drawn from the Wikipedia.\r\n\r\nThe solution will be applied on two case studies, both from the area of news. For the Bloomberg case study the domain will be financial news, while for the Slovenian Press Agency we will deal with general news. The technology developed in the project will be used to introduce cross-lingual and information from social media in services for publishers and end-users in the area of summarization, contextualization, personalization, and plagiarism detection. Special attention will be paid to analysing news reporting bias from multilingual sources.\r\n\r\nThe developed technology will be language-agnostic, while within the project we will specifically address English, German, Spanish, and Chinese as major world languages and Catalan and Slovenian as minority languages.", "recorded": "2012-01-30T15:42:54", "title": "XLike"}, {"url": "lsoldm2012_kakade_hidden_variable_models", "desc": "In many applications, we face the challenge of modeling the interactions between multiple\r\nobservations. A popular and successful approach in machine learning and AI is to hypothesize the\r\nexistence of certain latent (or hidden) causes which help to explain the correlations in the\r\nobserved data. The (unsupervised) learning problem is to accurately estimate a model with only\r\nsamples of the observed data. For example, in document modeling, we may wish to characterize\r\nthe correlational structure of the \"bag of words\" in documents. Here, a standard model is to posit\r\nthat documents are about a few topics (the hidden variables) and that each active topic\r\ndetermines the occurrence of words in the document. The learning problem is, using only the\r\nobserved words in the documents (and not the hidden topics), to estimate the topic probability\r\nvectors (i.e. discover the strength by which words tend to appear under different topcis). In\r\npractice, a broad class of latent variable models is most often fit with either local search heuristics\r\n(such as the EM algorithm) or sampling based approaches.\r\n\r\nThis talk will discuss how generalizations of standard linear algebra tools (e.g. spectral methods)\r\nto tensors provide provable and efficient estimation methods for various latent variable models\r\n(under appropriate assumptions), including mixtures of Gaussians models, hidden Markov\r\nmodels, topic models, latent Dirichlet allocation, latent parse tree models (PCFGs and\r\ndependency parsers), and models for communities in social networks. The talk will also briefly\r\ndiscuss how matrix and tensor decomposition methods can be used for the structure learning\r\nproblem of determining both the existence of certain hidden causes and the underlying graphical\r\nstructure between these hidden causes and the observed variables.", "recorded": "2012-09-18T09:00:00", "title": "Scalable Tensor Decompositions for Learning Hidden Variable Models"}, {"url": "solomon_dias_swdcr", "desc": "The amount of information on the web is growing so fast that\r it is becoming more and more difficult for classical search engines to\r find relevant information. Indeed, due to the frenetic increase of\r webpages written in different languages and sometimes in mis-interpreted\r languages, the degree of ambiguity of the human language has been\r constantly evolving to levels unseen so far. However, people still query\r the systems with no more than 2 words on average. As a consequence, new\r information retrieval systems need to be proposed to decrease the level\r of ambiguity of the queries. Such systems usually make use of query\r expansion techniques to solve this problem. In this talk, I will present\r a system based on the automatic discovery of terms that are related to\r the query as a means of helping the user to search for relevant \r information.\r \r This technique can be classified within Interactive Query Expansion\r systems. However, unlike other systems, we use Web Mining Techniques to\r discover related terms based on different features such as association\r measures, document similarity, document relevance, etc.\r In the second part of my talk, I will present the future extensions of\r our retrieval systems based on the automatic discovery of relations\r between related terms. So, by using agglomerative clustering techniques\r and an auto-fed WebWarehouse, we hope to be able to propose\r less ambiguous query expansion terms than in present systems\r where the user needs to sort out the terms he is interested in.\r \r ; [[http://webspy.di.ubi.pt/|Web spider]]\r : Web Spider is a system that returns all related terms and links from a given URL and a given query.\r : The Spider has been developped using C5.0 machine learning algorithm.", "recorded": "2004-11-16T13:00:00", "title": "Searching the Web by Discovering and Clustering Related Terms"}, {"url": "mlss05us_caruana_eclmc", "desc": "Decision trees may be intelligible, but can they cut the mustard? Have SVMs replaced neural nets, or are neural nets still best for regression, and SVMs best for classification? Boosting maximizes a margin much like SVMs, but can boosting compete with SVMs? And is it better to boost weak models, as theory suggests, or to boost stronger models? Bagging is much easier than boosting, so how well does bagging stack up against boosting? Bagging is supposed to be best with low bias high variance methods like decision trees, so if we bag lower variance models like neural nets are they as good as bagged trees? What happens if we do bagging with steroids, i.e. switch to random forests? And what about old friends like k-nearest neighbor \u2014 should they just be put out to pasture? In this lecture I'll compare the performance of a variety of popular machine learning methods on nine performance criteria: Accuracy, F-score, Lift, Precision/Recall Break-Even? Point, Area under the ROC, Average Precision, Squared Error, Cross-Entropy?, and Probabilistic Calibration. I'll show that while no one learning method does it all, it is possible to \"repair\" some of them so that they do well on all metrics. I'll then describe NACHOS, a new ensemble method that does even better by by building on top of these other learning methods. Finally, I'll discuss how the nine performance metrics relate to each other, and look at a few case-studies to show why it is important to use the right metric for each problem.", "recorded": "2005-05-27T00:00:00", "title": "Empirical Comparisons of Learning Methods & Case Studies"}, {"url": "colt2013_wang_ranking", "desc": "Ranking has been extensively studied in information retrieval, machine learning and statistics. A central problem in ranking is to design a ranking measure for evaluation of ranking functions. State of the art leaning to rank methods often train a ranking function by using a ranking measure as the objective to maximize. In this paper we study, from a theoretical perspective, the widely used NDCG type ranking measures. We analyze the behavior of these ranking measures as the number of objects to rank getting large. We first show that, whatever the ranking function is, the standard NDCG which adopts a logarithmic discount, converges to 1 as the number of items to rank goes to infinity. On the first sight, this result seems to imply that NDCG cannot distinguish good and bad ranking functions, contradicting to the empirical success of NDCG in many applications. Our next main result is a theorem which shows that although NDCG converge to the same limit for all ranking functions, it has distinguishability for ranking functions in a strong sense. We then investigate NDCG with other possible discount. Specifically we characterize the class of feasible discount functions for NDCG. We also compare the limiting behavior and the power of distinguishability of these feasible NDCG type measures to the standard NDCG. We next turn to the cut-off version of NDCG, i.e., NDCG@k. The most popular NDCG@k uses a combination of a slow logarithmic decay and a hard cut-off as its discount. So a natural question is why not simply use a smooth discount with fast decay? We show that if the decay is too fast, then the NDCG measure does not have strong power of distinguishability and even not converge. Finally, feasible NDCG@k are also discussed.", "recorded": "2013-06-12T18:20:00", "title": "A Theoretical Analysis of NDCG Type Ranking Measures"}, {"url": "colt2014_moitra_learning", "desc": "In sparse recovery we are given a matrix A\u2208Rn\u00d7m (\u201cthe dictionary\u201d) and a vector of the form AX where X is sparse, and the goal is to recover X. This is a central notion in signal processing, statistics and machine learning. But in applications such as sparse coding, edge detection, compression and super resolution, the dictionary A is unknown and has to be learned from random examples of the form Y=AX where X is drawn from an appropriate distribution - this is the dictionary learning problem. In most settings, A is overcomplete: it has more columns than rows. This paper presents a polynomial-time algorithm for learning overcomplete dictionaries; the only previously known algorithm with provable guarantees is the recent work of Spielman et al. (2012) who who gave an algorithm for the undercomplete case, which is rarely the case in applications. Our algorithm applies to incoherent dictionaries which have been a central object of study since they were introduced in seminal work of Donoho and Huo (1999). In particular, a dictionary is \u03bc-incoherent if each pair of columns has inner product at most \u03bc/n\u2212\u221a.\r\n\r\nThe algorithm makes natural stochastic assumptions about the unknown sparse vector X, which can contain k\u2264cmin(n\u2212\u221a/\u03bclogn,m1/2\u2212\u03b7) non-zero entries (for any \u03b7>0). This is close to the best k allowable by the best sparse recovery algorithms even if one knows the dictionary A exactly. Moreover, both the running time and sample complexity depend on log1/\u03f5, where \u03f5 is the target accuracy, and so our algorithms converge very quickly to the true dictionary. Our algorithm can also tolerate substantial amounts of noise provided it is incoherent with respect to the dictionary (e.g., Gaussian). In the noisy setting, our running time and sample complexity depend polynomially on 1/\u03f5, and this is necessary.", "recorded": "2014-06-13T09:00:00", "title": "New Algorithms for Learning Incoherent and Overcomplete Dictionaries "}, {"url": "kdd2014_anagnostopoulos_value_iputations", "desc": "Solving the missing-value (MV) problem with small estimation errors in big data environments is a notoriously resource-demanding task. As datasets and their user community continuously grow, the problem can only be exacerbated. Assume that it is possible to have a single machine (`Godzilla'), which can store the massive dataset and support an ever-growing community submitting MV imputation requests. Is it possible to replace Godzilla by employing a large number of cohort machines so that imputations can be performed much faster, engaging cohorts in parallel, each of which accesses much smaller partitions of the original dataset? If so, it would be preferable for obvious performance reasons to access only a subset of all cohorts per imputation. In this case, can we decide swiftly which is the desired subset of cohorts to engage per imputation? But efficiency and scalability is just one key concern! Is it possible to do the above while ensuring comparable or even better than Godzilla's imputation estimation errors? In this paper we derive answers to these fundamentals questions and develop principled methods and a framework which offer large performance speed-ups and better, or comparable, errors to that of Godzilla, independently of which missing-value imputation algorithm is used. Our contributions involve Pythia, a framework and algorithms for providing the answers to the above questions and for engaging the appropriate subset of cohorts per MV imputation request. Pythia functionality rests on two pillars: (i) dataset (partition) signatures, one per cohort, and (ii) similarity notions and algorithms, which can identify the appropriate subset of cohorts to engage. Comprehensive experimentation with real and synthetic datasets showcase our efficiency, scalability, and accuracy claims.", "recorded": "2014-08-25T12:00:00", "title": "Scaling Out Big Data Missing Value Imputations"}, {"url": "nipsworkshops2012_xlite", "desc": "Automatic text understanding has been an unsolved research problem for many years. This partially results from the dynamic and diverging nature of human languages, which ultimately results in many different varieties of natural language. This variations range from the individual level, to regional and social dialects, and up to seemingly separate languages and language families.\r\n\r\nHowever, in recent years there have been considerable achievements in data driven approaches to computational linguistics exploiting the redundancy in the encoded information and the structures used. Those approaches are mostly not language specific or can even exploit redundancies across languages.\r\n\r\nThis progress in cross-lingual technologies is largely due to the increased availability of multilingual data in the form of static repositories or streams of documents. In addition parallel and comparable corpora like Wikipedia are easily available and constantly updated. Finally, cross-lingual knowledge bases like DBpedia can be used as an Interlingua to connect structured information across languages. This helps at scaling the traditionally monolingual tasks, such as information retrieval and intelligent information access, to multilingual and cross-lingual applications.\r\n\r\nFrom the application side, there is a clear need for such cross-lingual technology and services. Available systems on the market are typically focused on multilingual tasks, such as machine translation, and don't deal with cross-linguality. A good example is one of the most popular news aggregators, namely Google News that collects news isolated per individual language. The ability to cross the border of a particular language would help many users to consume the breadth of news reporting by joining information in their mother tongue with information from the rest of the world.\r\n\r\nWorkshop homepage: http://km.aifb.kit.edu/ws/xlite/", "recorded": "2012-12-07T07:30:00", "title": "xLiTe: Cross-Lingual Technologies"}, {"url": "xliteworkshop2012_lake_tahoe", "desc": "Automatic text understanding has been an unsolved research problem for many years. This partially results from the dynamic and diverging nature of human languages, which ultimately results in many different varieties of natural language. This variations range from the individual level, to regional and social dialects, and up to seemingly separate languages and language families.\r\n\r\nHowever, in recent years there have been considerable achievements in data driven approaches to computational linguistics exploiting the redundancy in the encoded information and the structures used. Those approaches are mostly not language specific or can even exploit redundancies across languages.\r\n\r\nThis progress in cross-lingual technologies is largely due to the increased availability of multilingual data in the form of static repositories or streams of documents. In addition parallel and comparable corpora like Wikipedia are easily available and constantly updated. Finally, cross-lingual knowledge bases like DBpedia can be used as an Interlingua to connect structured information across languages. This helps at scaling the traditionally monolingual tasks, such as information retrieval and intelligent information access, to multilingual and cross-lingual applications.\r\n\r\nFrom the application side, there is a clear need for such cross-lingual technology and services. Available systems on the market are typically focused on multilingual tasks, such as machine translation, and don't deal with cross-linguality. A good example is one of the most popular news aggregators, namely Google News that collects news isolated per individual language. The ability to cross the border of a particular language would help many users to consume the breadth of news reporting by joining information in their mother tongue with information from the rest of the world.\r\n\r\nWorkshop homepage: http://km.aifb.kit.edu/ws/xlite/", "recorded": "2012-12-07T07:30:00", "title": "xLiTe Workshop: Cross-Lingual Technologies, Lake Tahoe 2012"}, {"url": "dsb06_whistler", "desc": "The modelling of continuous-time dynamical systems from uncertain observations is an important task that comes up in a wide range of applications ranging from numerical weather prediction over finance to genetic networks and motion capture in video. Often, we may assume that the dynamical models are formulated by systems of differential equations. In a Bayesian approach, we may then incorporate a priori knowledge about the dynamics by providing probability distributions on the unknown functions, which correspond for example to driving forces and appear as coefficients or parameters in the differential equations. Hence, such functions become stochastic processes in a probabilistic Bayesian framework.\r\n\r\nGaussian processes (GPs) provide a natural and flexible framework in such circumstances. The use of GPs in the learning of functions from data is now a well-established technique in Machine Learning. Nevertheless, their application to dynamical systems becomes highly nontrivial when the dynamics is nonlinear in the (Gaussian) parameter functions. This happens naturally for nonlinear systems which are driven by a Gaussian noise process, or when the nonlinearity is needed to provide necessary constraints (e.g., positivity) for the parameter functions. In such a case, the prior process over the system's dynamics is non-Gaussian right from the start. This means, that closed form analytical posterior predictions (even in the case of Gaussian observation noise) are no longer possible. Moreover, their computation requires the entire underlying Gaussian latent process at all times (not just at the discrete observation times). Hence, inference of the dynamics would require nontrivial sampling methods or approximation techniques.\r\n\r\nDetailed information can be found at [[http://www.cs.ucl.ac.uk/staff/c.archambeau/dsb.htm|Dynamical Systems, Stochastic Processes and Bayesian Inference website]].", "recorded": "2006-12-09T08:00:00", "title": "NIPS Workshop on Dynamical Systems, Stochastic Processes and Bayesian Inference, Whistler 2006"}, {"url": "mitworld_kurzweil_ie", "desc": "Ray Kurzweil may be the closest thing we have to a crystal ball. And if anyone has the right to some credibility in the prognostication arena, this overachieving inventor can. With crackling speed, Kurzweil powerpoints through charts illustrating the growth of various technologies over the centuries. His main points: technology evolves exponentially; the rate of technical progress itself is accelerating, so expect to \u201csee 20,000 years of progress in the 21st century, about 1000 times greater than the 20th century.\u201d Before you can say, \u201cHold your horses,\u201d Kurzweil is off and running.\r\n\r\nSay goodbye to cancer and heart disease within 15 years, and hello to living way past 80. And try to survive until the year 2029, which according to Kurzweil\u2019s mathematical models, represents \u201c25 turns of the screw in terms of doubling the power of information technology in every aspect of our lives.\u201d We\u2019ll see reverse engineering of the human brain, and computers that \u201cwill combine the subtlety and pattern recognition of human intelligence with the speed, memory and knowledge sharing of machine intelligence.\u201d The marriage of nanotechnology and AI will bring us \u201ca killer app\u201d-- nanobots that can keep us healthy from the inside. These will also enable \u201cfull immersion virtual reality from within nervous systems\u201d and expand human intelligence, facilitating \u201cbrain to brain communication. As for human conflict, Kurzweil sees an end to starvation and energy concerns, but doesn\u2019t quite complete his utopia. New technologies may be used in anti-social ways, say, by a bioterrorist. \u201cI\u2019m less optimistic we can avoid all painful issues; we certainly did not do that in the 20th century,\u201d concludes Kurzweil.", "recorded": "2005-09-29T20:54:00", "title": "Innovation Everywhere - How the Acceleration of \u201cGNR\u201d (genetics, nanotechnology, robotics) Will Create a Flat and Equitable World"}, {"url": "kdd2014_schadt_treating_disease", "desc": "Throughout the biomedical and life sciences research community, advanced integrative biology algorithms are employed to integrate large scale data across many different high-dimensional datatypes to construct predictive network models of disease. The causal inference approaches we employ for this purpose well complement the types of natural artificial intelligence/machine learning approaches that have become nearly standard in the life and biomedical sciences for building classifiers for a range of problems, from disease classification and subtype stratification, to the identification of responders and non-responders for a given treatment strategy. By building a causal network model that spans multiple scales (from the molecular to the cellular, to the tissue/organ, to the organism and community) we can understand the flow of information and how best to modulate that flow to improve human wellbeing, whether better diagnosing and treating disease or improving overall health(1-4). More specifically, we have constructed predictive network models for Alzheimer's disease, along with other common human diseases such as obesity, diabetes, heart disease, and inflammatory bowel disease, and cancer, and demonstrated a causal network common across all of these diseases(3, 5-10). Not only do we demonstrate that our predictive models uncover important mechanisms of disease and mechanistic connections among different diseases, but that they have led to a natural way to prioritize therapeutic points of intervention and provide optimal molecular phenotypes for high throughput screening. Our application of these models in a number of disease areas has led to the identification of novel genes that are causal for disease and that may serve as efficacious points of therapeutic intervention, as well as to personalized treatment strategies that provide a more quantitative and accurate approach to tailoring treatments to specific forms of disease.", "recorded": "2014-08-27T09:00:00", "title": "A Data Driven Approach to Diagnosing and Treating Disease "}, {"url": "prib2010_lee_ssge", "desc": "In this paper, we present a new dimensionality reduction (DR) method (SSGEAL) which integrates Graph Embedding (GE) with semi-supervised  and active learning to provide a low dimensional data representation that allows for better class separation. Unsupervised DR methods such as Principal Component Analysis and GE have previously been applied to the classification of high  dimensional biomedical datasets (e.g. DNA microarrays and digitized histopathology) in the reduced dimensional space. However, these methods do not incorporate class label information, often leading to embeddings with significant overlap between the data classes. Semi-supervised dimensionality reduction (SSDR) methods have recently been proposed which utilize both labeled and unlabeled instances for learning the optimal low dimensional embedding. However, in several problems involving biomedical data, obtaining class labels may be difficult and/or expensive. SSGEAL utilizes labels from instances, identified as \u201chard to classify\u201d by a support vector machine based active learning algorithm, to drive an updated SSDR scheme  while reducing labeling cost. Real world biomedical data from 7 gene expression studies and 3900 digitized images of prostate cancer needle biopsies were used to show the superior performance of SSGEAL compared to both GE and SSAGE (a recently popular SSDR method) in terms of both the Silhouette Index (SI) (SI = 0.35 for GE, SI = 0.31 for SSAGE, and SI = 0.50 for SSGEAL) and the Area Under the Receiver Operating Characteristic Curve (AUC) for a Random Forest classifier (AUC = 0.85 for GE, AUC = 0.93 for SSAGE, AUC = 0.94 for SSGEAL). ", "recorded": "2010-09-22T16:30:00", "title": "Semi-Supervised Graph Embedding Scheme with Active Learning (SSGEAL): Classifying High Dimensional Biomedical Data"}, {"url": "icots2010_rosling_wshdw", "desc": "If you show the core numbers of statistics, rather than the meaning of the data, your audience will be small. Just imagine what would happen if you climb the stage in a concert hall just to show a  PowerPoint presentation of the musical notes of a famous piece. Even if it is from a great composer you will not get a standing ovation. Neither will you if you just display a newly constructed musical instrument to the audience. The success in music depends on a great composition being played on a good musical instrument that fits the composition by a musician that adds his personal interpretation but sticks to the notes of the composer. This is my vision for statistics. Get the great data sets with time series compiled by professional statisticians, let the IT industry develop new instruments for animated graphic displays, and then develop a new strain of statistical musicians. In other words just as meteorologists are doing every evening on prime time TV with their data. And those meteorologists with show biz talents become like rock stars in many countries. By serendipity Gapminder Foundation found that the beauty of a statistical database can be unveiled to a broad audience if the data set can be animated in newly developed software. Many things are needed and the last step is a presenter who manages to display the dataset in that software in a way not too different from how a musician can unveil the beauty of a composition. The value of a statistical database depends on how widely it can be used and understood and that in its turn depends on whether the database is made available in a uniform and machine readable format to faster innovation of graphical data display. Animated displays of time series do not replace any other form of data presentation. Its aim is to attract new user groups to the beauty of statistics. I will review what it will take to bring statistical databases into prime time TV.\r\n\r\n", "recorded": "2010-07-12T09:30:00", "title": "What showbiz has to do with it?"}, {"url": "mit801f99_lewin_lec29", "desc": "**1. Review of 1D Collisions:**\r\n\r\nTwo masses moving in one dimension collide in a completely inelastic manner. The equations for an elastic collision are also discussed.\r\n\r\n**2. Atwood's Machine:**\r\n\r\nA \"massless\" rope runs (without slipping) over a pulley (the moment of inertia of the pulley is not negligible). The rope has a pair of masses hanging from it. The equations of rotational (pulley) and translational (the 2 masses) motion are related, and solved for the acceleration of the masses.\r\n\r\n**3. SHO of a Suspended Rod:**\r\n\r\nA pendulum consists of a ruler/rod suspended from one end. The equation of motion is derived. The moment of inertia is determined using the parallel axis theorem. Rotational kinetic energy is also discussed. For small angles, the motion is that of a simple harmonic oscillator.\r\n\r\n**4. Conservation Laws for a Satellite's Orbit:**\r\n\r\nA satellite is launched from a planet. Using the conservation of angular momentum and mechanical energy, and the maximum distance between the planet and satellite, one can calculate the launch velocity and other orbital parameters.\r\n\r\n**5. Doppler Shift Review:**\r\n\r\nIf stars are receding from us, the starlight that we observe is red-shifted; the observed wavelengths are longer than those emitted by the star. The Doppler shift depends on the component of the velocity along the line-of-sight (so-called radial velocity). A somewhat similar equation applies to sound received from moving sound sources. However, in the case of sound, it DOES matter whether the sound source moves or whether the observer moves. For light, only the relative motion between source and observer matters.\r\n\r\n**6. Pure Roll:**\r\n\r\nThe acceleration of a cylinder and sphere is derived under the condition of pure roll. A sliding object accelerates faster down an incline plane than a rolling object. The static friction coefficient must be sufficient to support pure roll. Kinetic energy for a rolling object has both a translational and rotational term.", "recorded": "1999-11-19T10:00:00", "title": "Lecture 29: Exam Review 3"}, {"url": "xlike_kickoff2012_bled", "desc": "The goal of the X-LIKE project is to develop technology to monitor and aggregate knowledge that is currently spread across global mainstream and social media, and to enable cross-lingual services for publishers, media monitoring and business intelligence.\r\n\r\nIn terms of research contributions, the aim is to combine scientific insights from several scientific areas to contribute in the area of cross-lingual text understanding. By combining modern computational linguistics, machine learning, text mining and semantic technologies we plan to deal with the following two key open research problems: - to extract and integrate formal knowledge from multilingual texts with cross-lingual knowledge bases, and - to adapt linguistic techniques and crowdsourcing to deal with\r\nirregularities in informal language used primarily in social media.\r\n\r\nAs an interlingua, knowledge resources from [[http://linkeddata.org/|Linked Open Data]] cloud  will be used with special focus on general common sense knowledge base [[http://www.cyc.com/|CycKB]]. For the languages where no required linguistic resources will be available, we will use a probabilistic\r\ninterlingua representation trained from a comparable corpus drawn from the Wikipedia. The solution will be applied on two case\r\nstudies, both from the area of news. For the Bloomberg case study the domain will be financial news, while for the Slovenian Press\r\nAgency we will deal with general news. The technology developed in the project will be used to introduce cross-lingual and\r\ninformation from social media in services for publishers and end-users in the area of summarization, contextualization,\r\npersonalization, and plagiarism detection. Special attention will be paid to analysing news reporting bias from multilingual sources.\r\nThe developed technology will be language-agnostic, while within the project we will specifically address English, German,\r\nSpanish, and Chinese as major world languages and Catalan and Slovenian as minority languages.\r\n\r\nDetailed information about the project can be found at http://www.xlike.org.", "recorded": "2012-01-18T10:00:00", "title": "XLike - Cross-Lingual Knowledge Extraction Kickoff Meeting, Bled 2012"}, {"url": "colt2013_agarwal_bounds", "desc": "The area under the ROC curve (AUC) is a widely used performance measure in machine learning, and has been widely studied in recent years particularly in the context of bipartite ranking. A dominant theoretical and algorithmic framework for AUC optimization/bipartite ranking has been to reduce the problem to pairwise classification; in particular, it is well known that the AUC regret can be formulated as a pairwise classification regret, which in turn can be upper bounded using usual regret bounds for binary classification. Recently, Kotlowski et al. (2011) showed AUC regret bounds in terms of the regret associated with \u2018balanced\u2019 versions of the standard (non-pairwise) logistic and exponential losses. In this paper, we obtain such (non-pairwise) surrogate regret bounds for the AUC in terms of a broad class of proper (composite) losses that we term strongly proper. Our proof technique is considerably simpler than that of Kotlowski et al. (2011), and relies on properties of proper (composite) losses as elucidated recently by Reid and Williamson (2009, 2010, 2011) and others. Our result yields explicit surrogate bounds (with no hidden balancing terms) in terms of a variety of strongly proper losses, including for example logistic, exponential, squared and squared hinge losses. An important consequence is that standard algorithms minimizing a (non-pairwise) strongly proper loss, such as logistic regression and boosting algorithms (assuming a universal function class and appropriate regularization), are in fact AUC-consistent; moreover, our results allow us to quantify the AUC regret in terms of the corresponding surrogate regret. We also obtain tighter surrogate regret bounds under certain low-noise conditions via a recent result of Cl\u00e9menand Robbiano (2011).", "recorded": "2013-06-12T17:55:00", "title": "Surrogate Regret Bounds for the Area Under the ROC Curve via Strongly Proper Losses"}, {"url": "kdd2014_gong_feature_learning", "desc": "Multi-task feature learning has been proposed to improve the generalization performance by learning the shared features among multiple related tasks and it has been successfully applied to many real-world problems in machine learning, data mining, computer vision and bioinformatics. Most existing multi-task feature learning models simply assume a common noise level for all tasks, which may not be the case in real applications. Recently, a Calibrated Multivariate Regression (CMR) model has been proposed, which calibrates different tasks with respect to their noise levels and achieves superior prediction performance over the non-calibrated one. A major challenge is how to solve the CMR model efficiently as it is formulated as a composite optimization problem consisting of two non-smooth terms. In this paper, we propose a variant of the calibrated multi-task feature learning formulation by including a squared norm regularizer. We show that the dual problem of the proposed formulation is a smooth optimization problem with a piecewise sphere constraint. The simplicity of the dual problem enables us to develop fast dual optimization algorithms with low per-iteration cost. We also provide a detailed convergence analysis for the proposed dual optimization algorithm. Empirical studies demonstrate that, the dual optimization algorithm quickly converges and it is much more efficient than the primal optimization algorithm. Moreover, the calibrated multi-task feature learning algorithms with and without the squared norm regularizer achieve similar prediction performance and both outperform the non-calibrated ones. Thus, the proposed variant not only enables us to develop fast optimization algorithms, but also keeps the superior prediction performance of the calibrated multi-task feature learning over the non-calibrated one.", "recorded": "2014-08-27T13:30:00", "title": "Efficient Multi-Task Feature Learning with Calibration"}, {"url": "mlpmsummerschool2013_moreau_genomic_data_fusion", "desc": "NGS has rapidly increased our ability to discover the cause of many previously unresolved rare monogenic disorders by sequencing rare exomic variation. However, after standard filtering against nonsynonymous single nucleotide variants (nSNVs) and loss-of-function mutations that are not present in healthy populations or unaffected samples, many potential candidate mutations are often retained and we need predictive methods to prioritize variants for further validation. Several computational methods have been proposed that take into account biochemical, evolutionary and structural properties of mutations to assess their potential deleteriousness. However, most of these methods suffer from high false positive rates when predicting the impact of rare nSNVs. A plausible explanation for this poor performance is that many of these predicted variants are mildly deleterious, but in no way specific to the disease of interest. We therefore propose a genomic data fusion methodology that integrates multiple strategies to detect deleteriousness of mutations and prioritizes them in a phenotype-specific manner. A key innovation is that we incorporate into our strategy a computational method for gene prioritization, which scores mutated genes based on their similarity to known disease genes by fusing heterogeneous genomic information. We also integrate haploinsufficiency prediction scores that predict the probability that the function of a gene is affected if present in a functionally haploid state. To integrate or fuse these data sources, we develop a machine-learning model using the Human Genome Mutation Database (HGMD) of human disease-causing mutations compared to three control sets: common polymorphisms and two independent sets of rare variation. Benchmarking on HGMD demonstrates that this integrative phenotype-specific variant prioritization significantly outperforms state-of-the-art predictors, such as SIFT or PolyPhen-2.", "recorded": "2014-04-16T10:16:31", "title": "Variant prioritization by genomic data fusion"}, {"url": "elex2011_krek_language", "desc": "If the eighties brought the first extensive use of digitized dictionaries for linguistic querying, and the nineties were dedicated to collecting and exploring huge amounts of language data in digital format, such as corpora, lexicons, ontologies, lexical databases etc., the first decade in the new century saw the explosion of freely available (crowd-sourced) web contents such as Wikipedia and online dictionaries, every day use of NLP technologies, and the first move towards the abandonment of paper as the primary medium of written language transmission. At the beginning of the present decade it is therefore reasonable to ask what will fulfil the persisting human need to understand difficult parts of one's native language, or contribute to maintaining a common language standard. On the multilingual side, there is an equally important need to communicate with people or understand texts in languages other than one's own, the area where free web content and freely available statistical machine translation tools made a considerable step forward in recent years. While dictionaries on paper will continue to have an important role in the digitally underdeveloped environments, it is clear that in those parts of the world where access to the internet and mobile telephony is beginning to be understood as one or the basic human rights, paper format may be abandoned. Consequently, it is necessary to conceptualize a new format which will satisfy the same needs, but will deliberately break away from the 18th- and 19th-century dictionary concept and the codex format. We will try to guesstimate what the new format could be, taking into account language data and NLP technologies already available, as well as the maturing technologies. The format will be conceptualized as an interactive web portal where reliable information on all aspects of a particular language is available \u2013 an \"all-about\".", "recorded": "2011-11-10T09:15:00", "title": "Language data for digital natives: old wine in a new bottle or...?"}, {"url": "aispds08_cumberland_lodge", "desc": "The modelling of continuous-time stochastic processes from uncertain (discrete) observations is an important task that arises in a wide range of applications, such as in climate modelling, tracking, finance and systems biology. Although observations are in general only available at discrete times, the underlying system is often a continuous-time one. Hence, the physics or the dynamics are formulated by systems of differential equations, the observation noise and the process uncertainty being modelled by several stochastic sources. When dealing with stochastic processes, it is natural to take a probabilistic approach. For example, we may incorporate prior knowledge about the dynamics by providing probability distributions on the unknown functions. In contrast to models that are only data driven, it is hoped that incorporating domain knowledge in the inference process will improve performance in practice. The main challenges in this context are how to deal with continuous-time objects, how to do inference and how to be agnostic about the deterministic driving forces and the sources of uncertainty.\r\n\r\nThe workshop provides a forum for discussing the open problems arising in dynamical systems, and in particular continuous-time stochastic processes. It focuses both on the mathematical aspects/theoretical advances and the applications. Another important aim is to bridge the gap between the different communities (data assimilation, machine learning, optimal control, systems biology, finance, ...) and favour interactions. Hence, the workshop is of interest to researchers from statistics, computer science, mathematics, physics and engineering. We also hope that the workshop provides new insights in this exciting field and serve as a starting point for new research perspectives and future collaborations. The workshop is sponsored by **PASCAL2 network of excellence** and is one of six workshops in the Thematic Programme in Leveraging Complex Prior Knowledge for Learning.\r\n\r\nFor more inforamtion visit the [[http://www.cs.ucl.ac.uk/staff/C.Archambeau/AIS/ais.htm|Workshop website]].", "recorded": "2008-05-27T14:00:00", "title": "Workshop on Approximate Inference in Stochastic Processes and Dynamical Systems, Cumberland Lodge 2008"}, {"url": "roks2013_leuven", "desc": "One area of high impact both in theory and applications is kernel methods and support vector machines. Optimization problems, learning and representations of models are key ingredients in these methods. On the other hand considerable progress has also been made on regularization of parametric models, including methods for compressed sensing and sparsity, where convex optimization plays a prominent role. The aim of ROKS-2013 is to provide a multi-disciplinary forum where researchers of different communities can meet, to find new synergies along these areas, both at the level of theory and applications.\n\nThe scope includes but is not limited to:\n*Regularization: L2, L1, Lp, lasso, group lasso, elastic net, spectral regularization, nuclear norm, others\n*Support vector machines, least squares support vector machines, kernel methods, gaussian processes and graphical models\n*Lagrange duality, Fenchel duality, estimation in Hilbert spaces, reproducing kernel Hilbert spaces, Banach spaces, operator splitting\n*Optimization formulations, optimization algorithms\n*Supervised, unsupervised, semi-supervised learning, inductive and transductive learning\n*Multi-task learning, multiple kernel learning, choice of kernel functions, manifold learning\n*Prior knowledge incorporation\n*Approximation theory, learning theory, statistics\n*Matrix and tensor completion, learning with tensors\n*Feature selection, structure detection, regularization paths, model selection\n*Sparsity and interpretability\n*On-line learning and optimization\n*Applications in machine learning, computational intelligence, pattern analysis, system identification, signal processing, networks, datamining, others\n*Software\n\nFor more information visit the [[http://www.esat.kuleuven.be/sista/ROKS2013/|Workshop\u00b4s website]].", "recorded": "2013-07-08T13:00:00", "title": "International Workshop on Advances in  Regularization, Optimization, Kernel Methods and Support Vector Machines (ROKS): theory and applications, Leuven 2013"}, {"url": "pmnp07_sheffield", "desc": "Experimental advances in molecular biology are providing deeper understanding in the workings of living cells. High throughput functional genomic techniques are providing researchers with a reliable map of the complex networks underpinning the functioning of cells. Cellular processes often involve complex networking of several genes and transcription factors, and their temporal structure can often be accurately described in terms of pathways. A key problem in obtaining a computational understanding of these systems is the incomplete and noisy nature of most data: while certain relevant quantities, such as mRNA concentrations, can be measured accurately in a high throughput fashion, others, such as transcription factor concentrations, are difficult to measure quantitatively.\r\n\r\nProbabilistic machine learning techniques such as Bayesian Networks have emerged in recent years as one of the main computational tools. Starting from the pioneering work of Friedman et al. (J. of Comp. Biol., 2000), probabilistic models of gene networks have received considerable attention (for some more recent works, see e.g. Nachman et al 2004, Beal et al 2005, Sanguinetti et al 2006, Sabatti and James 2006, etc). Despite the success of this approach, outstanding tasks remain to be addressed. For example, it is very hard to formulate tractable models that take into account the combinatorial nature of gene regulation, and generalising genome-wide models to incorporate dynamical effects such as pathways presents formidable computational challenges.\r\n\r\nThe main aim of this workshop is to bring together researchers working on the many facets of these problem, providing a forum for discussion and giving focus to the future directions of research. We aim to involve some experimental biologists in order to foster collaborations between computational and experimental researchers.\r\n\r\nFind out more [[http://ml.dcs.shef.ac.uk/pmnp/|here]].", "recorded": "2007-09-04T09:00:41", "title": "Workshop on Probabilistic Modelling of Networks and Pathways, Sheffield 2007"}, {"url": "ecmlpkdd2010_bringmann_nijssen_vreeken_msp", "desc": "Pattern mining is one of the most important topics in data mining. The core idea is to extract relevant 'nuggets' of knowledge describing parts of a database. However, many traditional (frequent) pattern mining algorithms find patterns in numbers that are much too large to be of practical value: so many 'nuggets' of knowledge are found that they do not combine into a better global understanding of the data. In fact, the number of discovered patterns is often larger than the size of the original database!\r\n\r\nTo tackle this problem, in recent years many techniques have been developed for finding not all, but useful sets of patterns. The aim of this tutorial is to provide a general, comprehensive overview of the state-of-the-art of mining such high-quality sets of patterns.\r\n\r\nIn the tutorial, an important focus is on the tasks for which patterns can be mined, and how these tasks can influence both the pattern mining and pattern selection process. We make a distinction between patterns mined in an unsupervised setting, where patterns are intended to provide a description of the data and are often the end result of the mining process, and those mined in a supervised setting, where one usually is interested in the later use of patterns in predictive models. Both classes of problems come with distinctive problems, and allow us to discuss the possibilities and powers of using patterns for both classification and explorative data mining.\r\n\r\nThe main contributions of our tutorial will be that:\r\n\r\n    * we give a comprehensive overview of pattern set mining techniques.\r\n    * we thoroughly explore how classic machine learning tasks and recent pattern set mining techniques are related.\r\n    * we clarify the connections between a wide range of pattern and pattern set discovery algorithms.\r\n\r\nThe aim of the tutorial is to provide a broad overview of the key ideas studied in recent years; it will provide an overview of references for researchers and data mining practitioners interested in the more in-depth details.", "recorded": "2010-09-20T09:00:00", "title": "Mining Sets of Patterns"}, {"url": "elex2011_rundell_dante", "desc": "Dante (www.webDante.com) is a lexical database which provides a fine-grained, corpus-based description of the core vocabulary of English. Every fact recorded in the database is derived from, and explicitly supported by, evidence from a 1.7 billion-word corpus of current English. Almost all of these facts are machine-retrievable. Dante \u2013 the Database of ANalysed Texts of English \u2013 was designed and created for Foras na Gaeilge by the Lexicography Master Class and an 18-strong team of skilled lexicographers, using the Sketch Engine (www.sketchengine.co.uk) for corpus-querying, and IDM\u2019s Dictionary Production System (DPS: www.idm.fr) for entry-building. The resulting database records the semantic, grammatical, combinatorial, and text-type characteristics of over 42,000 single-word lemmas and 23,000 compounds and phrasal verbs, and includes over 27,000 idioms and phrases, underpinned by over 600,000 sentence examples from the corpus. The project pioneered new approaches in project management, software customisation, text origination, and quality control. Collectively, these initiatives enabled us to achieve significant levels of automation (hence cost saving) in the lexicographic process, as well as greater systematicity. Most of these innovations are transferable, so our experience on the Dante project has implications for lexicographic methodology as a whole. Though Dante began life as an \u2018English framework\u2019 destined for the development of a new English-Irish dictionary (http://www.focloir.ie/english.asp) it was designed to be a linguistic resource beyond this primary function. It offers publishers a launchpad for the development or updating of monolingual or bilingual dictionaries, and provides rich data for researchers, software developers, and materials writers. In this talk we will discuss the project\u2019s methodological innovations, demonstrate the wealth and range of data in Dante, and reflect on the long-term potential of this unique database.", "recorded": "2011-11-11T09:00:00", "title": "The DANTE database: what it is, how it was created, and what it can contribute to the dictionaries and lexicons of the future"}, {"url": "mitworld_negroponte_hdl", "desc": "magine a world where all school-age children own a laptop computer, even those living in villages lacking power and telephone service. Nicholas Negroponte has, and his One Laptop Per Child (OLPC) non-profit has propelled this vision into the real world. With backing from News Corporation and Google, among others, Negroponte has begun to line up millions of orders from Brazil, Thailand, Egypt, China and South Africa. Even the United Nations has bestowed its imprimatur on the concept. Negroponte\u2019s prototype computer is a \u201cskinnied down\u201d version of what he describes as the typical \u201cobese\u201d laptop. Remove sales and marketing costs, and set the machine up with a 7.5\u201d screen, Linux software, a hand crank for power, rugged rubber case, and super bright display so \u201cit can be taken into the sun and read like a book,\u201d and you\u2019ve got a very inexpensive tool for helping 800 million children explore, interact and create. Don\u2019t fret about connectivity; the Media Lab\u2019s got that covered: each laptop becomes a \u201cnode in the mesh\u201d of other local users, creating a novel network perfect for remote locations. For email and web browsing, just two MB can serve 1,000 kids, says Negroponte. The key to churning out these cheap educational devices is volume -- and the more countries that join the bandwagon, the sleeker and less expensive the computers are likely to be. Negroponte casts a wary eye on the potential grey market appeal of the machines, and is determined to make them so distinctive as a government-distributed, educational tool that taking one would \u201cbe like stealing a post office truck.\u201d Negroponte concludes, \u201cChanging education on the planet is a monumental challenge,\u201d taking decades. But OLPC will \u201cseed the change,\u201d and help \u201cinvent the future.\u201d", "recorded": "2005-09-28T21:00:00", "title": "The Hundred Dollar Laptop-Computing for Developing Nations"}, {"url": "mum09_carroll_cellphones", "desc": "Emerging out of an institutional collaboration between Parsons The New School for Design (New York\r\nCity) and the Academy of Arts and Design at Tsinghua University (Beijing), the mobile media\r\ninstallation, \u201c1000 Cell Phones,\u201d exposes the invisible conversations that constantly occur between\r\nthe networked devices we carry throughout our nomadic urban daily life. The installation consists of\r\nmultiple displays that playfully visualize and animate discovered Bluetooth devices within its situated\r\nspace. Devices are represented as abstract discs in dimensional screen space, colored by\r\ntranscribing the devices\u2019 unique identifiers to distinctive tone and hue values. This simple but\r\nevocative effect emphasizes how an ID number expressed as a one-of-a-kind color not only makes\r\nvisible a distinguishing feature of our portable networked device, but also reshapes its obfuscated\r\ntechnical datum into an aesthetic and coherent design object. It asks us if this machine identifier\r\nexpresses our persona and personality, perhaps without our knowledge and complete understanding\r\nof the implications. In addition, discovered device names animate across the screens, emphasizing\r\nthe transient nature of the tracking devices we carry, unwittingly broadcasting a unique identifier for\r\nanyone or anything willing to listen. By installing the work in a social space, such as a caf\u00e9 or lobby,\r\n\u201c1000 Cell Phones\u201d captures the unseen dialogue between mobile phones and laptops broadcasting\r\ntheir Bluetooth identities while owners lurk and socialize. When participants realize how their device\r\nnames render on the displays, they often engage in the intervention by altering their device settings to\r\naffect the textual content on the visualization. In these moments, the conversations between the\r\ninvisible and visible, technical and aesthetic, surveillance and dissemination, machines and people all\r\nbecome intertwined in a simple but enjoyable expression.", "recorded": "2009-11-23T18:00:00", "title": "1000 Cell Phones"}, {"url": "newtonschw03s08_owen_tis", "desc": "Sample reuse methods like the bootstrap and cross-validation are widely used in statistics and machine learning. They provide measures of accuracy with some face value validity that is not dependent on strong model assumptions.\r\n\r\nThese methods depend on repeating or omitting cases, while keeping all the variables in those cases. But for many data sets, it is not obvious whether the rows are cases and colunns are variables, or vice versa. For example, with movie ratings organized by movie and customer, both movie and customer IDs can be thought of as variables.\r\n\r\nThis talk looks at bootstrap and cross-validation methods that treat rows and columns of the matrix symmetrically. We get the same answer on X as on X'. McCullagh has proved that no exact bootstrap exists in a certain framework of this type (crossed random effects). We show that a method based on resampling both rows and columns of the data matrix tracks the true error, for some simple statistics applied to large data matrices.\r\n\r\nSimilarly we look at a method of cross-validation that leaves out blocks of the data matrix, generalizing a proposal due to Gabriel that is used in the crop science literature. We find empirically that this approach provides a good way to choose the number of terms in a truncated SVD model or a non-negative matrix factorization. We also apply some recent results in random matrix theory to the truncated SVD case.\r\n\r\nRelated Links\r\n\r\n    * http://stat.stanford.edu/~owen/reports - Page with research articles\r\n    * http://stat.stanford.edu/~owen/reports/cvsvd.pdf - Technical report on bi-cross-validation (in revision)\r\n    * http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&page=toc&handle=euclid.aoas/1196438015 - AOAS page with link to Pigeonhole boostrap paper\r\n\r\n**Homepage Link**\r\n* http://www.newton.ac.uk/programmes/SCH/seminars/062511301.html", "recorded": "2008-06-25T11:30:00", "title": "Transposably invariant sample reuse: the pigeonhole bootstrap and blockwise cross-validation"}, {"url": "kdd2014_liu_direct_optimization", "desc": "Linear regression is a widely used tool in data mining and machine learning. In many applications, fitting a regression model with only linear effects may not be sufficient for predictive or explanatory purposes. One strategy which has recently received increasing attention in statistics is to include feature interactions to capture the nonlinearity in the regression model. Such model has been applied successfully in many biomedical applications. One major challenge in the use of such model is that the data dimensionality is significantly higher than the original data, resulting in the small sample size large dimension problem. Recently, weak hierarchical Lasso, a sparse interaction regression model, is proposed that produces sparse and hierarchical structured estimator by exploiting the Lasso penalty and a set of hierarchical constraints. However, the hierarchical constraints make it a non-convex problem and the existing method finds the solution of its convex relaxation, which needs additional conditions to guarantee the hierarchical structure. In this paper, we propose to directly solve the non-convex weak hierarchical Lasso by making use of the GIST (General Iterative Shrinkage and Thresholding) optimization framework which has been shown to be efficient for solving non-convex sparse formulations. The key step in GIST is to compute a sequence of proximal operators. One of our key technical contributions is to show that the proximal operator associated with the non-convex weak hierarchical Lasso admits a closed form solution. However, a naive approach for solving each subproblem of the proximal operator leads to a quadratic time complexity, which is not desirable for large size problems. To this end, we further develop an efficient algorithm for computing the subproblems with a linearithmic time complexity. We have conducted extensive experiments on both synthetic and real data sets. Results show that our proposed algorithm is much more efficient and effective than its convex relaxation.", "recorded": "2014-08-25T11:20:00", "title": "An Efficient Algorithm For Weak Hierarchical Lasso"}, {"url": "acai05_bratko_hnd", "desc": "In the practice of machine learning, learning data typically contain errors. Imperfections in data can be due to various, often unavoidable causes: measurement errors, human mistakes, errors of expert judgement in classifying training examples etc. We refer to all of these as noise. Noise can also come from the treatment of missing values, when an example with unknown attribute value is replaced by a set of weighted examples corresponding to the probability distribution of the missing value. \r\n The typical consequences of noise in learning data are: (a) low prediction accuracy of induced hypotheses on new data, and (b) large hypotheses that are hard to interpret and to understand by the user. For example, decision trees with hundreds or thousands of nodes are not suitable for interpretation by the domain expert. We say that such complex hypotheses overfit the data. Overfitting occurs when the hypothesis not only reflects the genuine regularities in the domain, but it also traces noise in data. \r\n To alleviate the harmful effects of noise, we have to prevent overfitting. To do this, one common idea is to simplify induced hypotheses. In the learning of rules or decision trees, this leads to tree pruning or rule truncation. The main question in hypothesis simplification is: How can we know that our hypothesis is of \u201cthe right size\u201d, not too simple and not too complex? For example in tree pruning, when should we stop the pruning? The decision can be based on the estimated accuracy of a hypothesis before pruning and after pruning, and then the estimated accuracy is maximised. However, estimating the accuracy can be difficult, and involves the problem of estimating probabilities from small samples. Several methods for this will be discussed in this lecture, and the effects of simplification will be illustrated. A somewhat related approach of deciding about the \u201cright size\u201d of a hypothesis is based on the minimum description length principle (MDL). Another way of reducing the effects of noise is to use background or prior knowledge about the domain of learning. For example, in the learning from numerical data, a useful idea is to make the learning algorithm respect the known qualitative properties of the target concept. ", "recorded": "2005-07-04T11:00:00", "title": "Handling noisy data"}, {"url": "cerncompseminar_begin_slipstream", "desc": "Cloud technology is now everywhere. Beyond the hype, it provides a real opportunity to improve the engineering of software systems. Lately the DevOps movement has also gain momentum, which take an agile approach at bringing developers and system administrators closer together to better engineer software systems.\r\n\r\nIn this context, this presentation focuses on new tools for exploiting cloud services (private and public) in order to create a continuous flow between software commits and fully deployed and configured software systems, automatically and on-demand. To illustrate this, we present SlipStream and StratusLab.\r\n\r\nSlipStream is a new product developed by SixSq, able to create virtual machines and orchestrate multi-machine deployments.  SlipStream started from an idea developed in the context of the ETICS project, led by CERN. StratusLab is an open-source IaaS distribution, able to create public and private clouds.\r\n\r\nThis presentation will also describe a case study where SlipStream deploys an entire Mission Control System for the Operations Center of the European Space Agency.  We also describe how SlipStream can be integrated with management tools, such as Puppet and Chef.\r\nAbout the speaker\r\n\r\nMarc-Elian B\u00e9gin is co-founder of SixSq. At SixSq he is the lead developer of the SlipStream product, leads the Integration and Test Work Package of the StratusLab FP7 project, delivers regular agile training and is coach for different customers, including the Operation Centre of ESA (ESOC). He worked at CERN from 2004 to 2008 in the IT Department, as developer and technical lead on the EGEE and ETICS projects. Before joining CERN, he worked for a decade in the space industry, contributing to the delivery of several software systems, both for the space agencies and industry in Canada and Europe. Marc-Elian is a regular speaker at international events, such as Agile2011, XP2011 and DevOpsDays. In Helix Nebula, he leads the Architecture and Technical Group, responsible for short and longer term technical solutions for the cloud collaboration.", "recorded": "2012-02-01T11:00:00", "title": "SlipStream: automated provisioning and continuous deployment in the cloud"}, {"url": "nips2010_syed_ipr", "desc": "Cardiovascular disease is the leading cause of death globally, resulting in 17 million deaths each year. Despite the availability of various treatment options, existing techniques based upon conventional medical knowledge often fail to identify patients who might have benefited from more aggressive therapy. In this paper, we describe and evaluate a novel unsupervised machine learning approach for cardiac risk stratification. The key idea of our approach is to avoid specialized medical knowledge, and assess patient risk using symbolic mismatch, a new metric to assess similarity in long-term time-series activity. We hypothesize that high risk patients can be identified using symbolic mismatch, as individuals in a population with unusual long-term physiological activity. We describe related approaches that build on these ideas to provide improved medical decision making for patients who have recently suffered coronary attacks. We first describe how to compute the symbolic mismatch between pairs of long term electrocardiographic (ECG) signals. This algorithm maps the original signals into a symbolic domain, and provides a quantitative assessment of the difference between these symbolic representations of the original signals. We then show how this measure can be used with each of a one-class SVM, a nearest neighbor classifier, and hierarchical clustering to improve risk stratification. We evaluated our methods on a population of 686 cardiac patients with available long-term electrocardiographic data. In a univariate analysis, all of the methods provided a statistically significant association with the occurrence of a major adverse cardiac event in the next 90 days. In a multivariate analysis that incorporated the most widely used clinical risk variables, the nearest neighbor and hierarchical clustering approaches were able to statistically significantly distinguish patients with a roughly two-fold risk of suffering a major adverse cardiac event in the next 90 days.", "recorded": "2010-12-08T16:40:00", "title": "Identifying Patients at Risk of Major Adverse Cardiovascular Events Using Symbolic Mismatch"}, {"url": "eml07_parthasarathy_acd", "desc": "Over the past several years, architectural innovation in processor design has led to new capabilities in single-chip commodity processing and high end compute clusters. Examples include hardware prefetching, simultaneous multithreading (SMT), and more recently true chip multiprocessing. At the very high-end, systems area networking technologies like InfiniBand have spurred the development of affordable cluster-based supercomputers capable of storing and managing peta bytes of data. We contend that data mining and machine learning algorithms which often require significant computational, I/O and communication resources, stand to benefit from such innovations if appropriately leveraged. The challenges to do so are daunting.\r \\\\ \r First, a large number of state-of-the-art data mining algorithms grossly under-utilize modern processors, the building blocks of current generation commodity clusters. This is due to the widening gap between processor and memory performance and the memory and I/O intensive nature of these applications. Second, the emergence of multi-core architectures to the commodity market, bring with them further complications. Key challenges\r brought to the fore include the need to enhance available fine-grained parallelism and to alleviate memory bandwidth pressure. Third, parallelizing data mining algorithms on a multi-level cluster environment is a challenge given the need to share and communicate large sets of data and to balance the workload in the presence of data skew.\r \\\\ \r In this talk I will discuss progress made in the context of these challenges and attempt to demonstrate that ``architecture conscious\" solutions are both viable and necessary. I will attempt to separate general methodologies and techniques from specific instantiations whenever it makes sense. We will conclude with a discussion on future outlook, both in the context of systems support for next generation algorithms as well as in terms of educational objectives brought to the fore in this context.\r \\\\ \r This is joint work with my graduate students Gregory Buehrer, Amol Ghoting and Shirish Tatikonda.", "recorded": "2007-12-07T17:10:00", "title": "Architecture Conscious Data Analysis: Progress and Future Outlook"}, {"url": "solomon_skunca_pcifa", "desc": "Phylogenetic profiling is a genomic context method that predicts gene function by correlating gene occurrence patterns in selected organisms [1]. The intuition behind phylogenetic profiling is that genes found and lost together (i. e. inherited together) in different genomes are likely to share function, either by 1) being involved in the same biological pathway (which is therefore incomplete without all members in a given genome), or 2) being crucial for survival in a particular environment, so their presence is mandatory throughout the phenotype. \r\nWe have used a recently developed machine learning approach based on decision trees for Hierarchical Multi \u2013 label Classification (HMC) [2] to predict Gene Ontology (GO) assignments of Orthologous Matrix (OMA) groups [3]. The HMC extension of the decision tree classifier takes into account the hierarchical layout of GO and considerably improves computational efficiency and accuracy by taking into account a set of class labels simultaneously when constructing the decision trees, instead of learning each class label separately. A standard decision tree would recursively split the training data into subsets (\u2018branches\u2019) on values of an attribute in such a manner as to decrease a measure of entropy of a class label within the subsets after the split. The HMC approach has to deal with multiple class labels, and would compute a weighted average of decrease in entropy over all labels when deciding on a split point. The weights here are inversely proportional to the depth of a class in the GO, giving more significance to high-level, more general GO terms.\r\nWe have inspected the effects of stepwise addition of putative paralogs on computational learning of orthologous groups\u2019 (and consequentially gene) function. By introducing paralogous genes in the learning process, we substantially increase its success and show that gene function prediction from sequence information alone, when encoded as a paralog-containing phylogenetic profile, is a promising approach in narrowing of possible function space for a particular protein.", "recorded": "2010-02-15T13:00:00", "title": "Paralogs considerably improve accuracy of phylogenetic profiling for in silico functional annotation"}, {"url": "clsp_glass_regularities", "desc": "The development of an automatic speech recognizer is typically a highly supervised process involving the specification of phonetic inventories, lexicons, acoustic and language models, along with annotated training corpora. Although some model parameters may be modified via adaptation, the overall structure of the speech recognizer remains relatively static thereafter. While this approach has been effective for problems when there is adequate human expertise and labeled corpora, it is challenged by less-supervised or unsupervised scenarios. It also stands in stark contrast to human processing of speech and language where learning is an intrinsic capability. From a machine learning perspective, a complementary alternative is to discover unit inventories in an unsupervised manner by exploiting the structure of repeating acoustic patterns within the speech signal. In this work we use pattern discovery methods to automatically acquire lexical entities, as well as speaker and topic segmentations directly from an untranscribed audio stream. Our approach to unsupervised word acquisition utilizes a segmental variant of a widely used dynamic programming technique, which allows us to find matching acoustic patterns between spoken utterances. By aggregating information about these matching patterns across audio streams, we demonstrate how to group similar acoustic sequences together to form clusters corresponding to lexical entities such as words and short multi-word phrases. On a corpus of lecture material, we demonstrate that clusters found using this technique exhibit high purity and that many of the corresponding lexical identities are relevant to the underlying audio stream. We have applied the acoustic pattern matching and clustering methods to several important problems in speech and language processing. In addition to showing how this methodology applies across different languages, we demonstrate two methods to automatically determine the identify of speech clusters. Finally, we also show how it can be used to provide an unsupervised segmentation of speakers and topics. Joint work with Alex Park, Igor Malioutov, and Regina Barzilay.", "recorded": "2008-03-04T16:43:05", "title": "Finding Acoustic Regularities in Speech: From Words to Segments"}, {"url": "mitworld_smith_idf", "desc": "Imagine if thousands of Amy Smiths were unleashed on the world, providing simple, ingenious inventions to make life easier for those subsisting on less than $2 a day -- half of humanity. This MacArthur Award-winning inventor has been seeding such programs at MIT, and describes tangible results of efforts to inspire students to apply innovative thinking and technology to everyday problems in the developing world.\r\n\r\nThe Designs for Developing Countries Project, the MIT Program in Developmental Entrepreneurship and D (Development)-Lab have spawned a range of initiatives, spanning the fields of public health, labor, and agriculture. In Ghana and Ecuador, MIT students are helping provide safe drinking water, with low-cost water testing methods that can be applied in the field with no electricity.\r\n\r\nIn places like Haiti and Tibet, smoke from indoor cooking fires leads to high mortality rates among young children. Solar cookers have proven effective in some regions, but old models are very heavy and often slow to boil water in winter. So an MIT project came up with an inexpensive cooker made of canvas and Mylar, easily assembled by villagers, and highly portable \u2013 a major selling point with nomadic communities.\r\n\r\nSmith recounts other ventures: a bicycle pedal-powered, corn-shelling machine in Tanzania, which entrepreneurs can rent out, and which saves hours of drudgery for women who traditionally remove kernels of corn by hand; a backpack for storing hundreds of doses of vaccine that can be delivered as an inhaled powder and therefore require no refrigeration; cell phone services that allow Brazilian day laborers and bosses to vet each other in advance, and permit Indian health workers to follow up on TB patients.\r\n\r\nConcludes Smith, \u201cSomething like 90% of the world\u2019s resources creates products and technologies that serve only the wealthiest 10% of the worlds\u2019 population. There\u2019s a revolution afoot to promote R&D to get designers to work on technologies for the other 90%.\u201d", "recorded": "2008-10-03T14:54:08", "title": "The International Development Fair: The Human Factor at Work in the World"}, {"url": "mitworld_endy_isb", "desc": "There\u2019s no mistaking Drew Endy\u2019s profession: \u201cI like to make things -- that\u2019s what I do.\u201d From his engineer\u2019s perspective, the slow and painful methods of bioengineering demand a solution. Endy hopes to refine the tools necessary to move the field forward. \u201cWe\u2019re going from looking at the living world as only coming from nature, to a subset of the living world being produced by engineers who design and build hopefully useful living artifacts according to our specifications,\u201d says Endy.\n\nThirty years ago, scientists figured out how to use enzymes to cut and paste genetic material, leading to recombinant DNA technology. But the techniques involved are painfully slow, requiring very specific physical materials and \u201cknow-how via the guild-like structure of biology.\u201d Endy points to methods coming on line that will make it easier to design and build biological systems.\n\nOne is DNA synthesis, in which a machine fed information and sugars generates a physical piece of DNA. It reminds Endy of the \u201cmatter compilers\u201d seen on Star Trek, where \u201cfood materializes from a cubby in the wall.\u201d This technique will allow the economical production of long sequences of DNA. Another key ingredient in bioengineering will be the development of standards for making and measuring DNA, in the same way that machining hardware came to be governed by common standards in the 19th century. Endy also suggests that biotechnology will be increasingly informed by useful abstraction, so that scientists will manipulate raw materials less and refined and repackaged materials more, in order to make new things simply and more reliably. These advances will also enable bioengineers to \u201cbe experts in our own domains without having to be masters of everything.\u201d\n\nBut as bioengineering becomes easier, and \u201cpeople start to engineer biology,\u201d we\u2019ll need to worry about new issues, says Endy: Will people synthesize pathogens from scratch? Will groups pool knowledge legally? Will there be accreditation and oversight of those who create biological systems?", "recorded": "2006-03-21T11:25:00", "title": "The Implications of Synthetic Biology"}, {"url": "nipsworkshops2012_allen_prediction", "desc": "This research seeks to generate temporal event predictions using the sticky Hierarchical Dirichlet\r\nProcess - Hidden Markov Model (sticky HDP-HMM), a generalization of the infinite HMM.\r\nHidden Markov Models (HMMs) are one of the most widely used machine learning techniques for\r\nanalyzing temporal data. One significant limitation of this traditional approach is that the number of\r\nstates in the HMM, N, has to be decided a priori, but for a number of applications it is not possible\r\nto hypothesize this accurately. The nonparametric Bayesian solution to this is to remove the\r\ndependence on N by effectively making it infinite and specifying a prior over it; such as done in the\r\nHDP-HMM model. An extension to the HDP-HMM model, known as the sticky HDP-HMM\r\nmodel additionally contains a bias towards self-transitions.\r\nIn the sticky HDP-HMM is introduced with application to speaker diarization, where audio\r\nrecordings are segmented based on the speaker and simultaneously identify the number of speakers.\r\nFor that application inference is done to calculate the posterior of the hidden states corresponding to\r\nthe observations, where the hidden states are interpreted to correspond to the various speakers. The\r\ngoal is then to identify the number of speakers and which speaker corresponds to each observation.\r\nWe believe that another interesting question that can be asked about the HDP-HMM is how effective\r\nit is for making predictions for future observations. We consider in this paper an application of HDPHMM\r\nto the prediction of events based on stock market indexes. Economic predictions are a wellresearched\r\ntopic and an area where successful predictions have large impact, so the ability of a model\r\ndo well in this domain is very significant. We show that HDP-HMM can be successfully applied\r\nin such prediction tasks and that the utility of the model extends beyond the inference of hidden\r\nstates. In addition, we believe that the predictive power of the model is not fully exploited by the use\r\nof only economic data to predict economic events. In extensions to this work we are incorporating\r\nsignificantly more dimensions to the input data by using indicators from non-traditional sources such\r\nas web search trends, news articles, and social media (such as Tweets and blog posts).", "recorded": "2012-12-07T10:00:00", "title": "Sticky HDP-HMM for Prediction of Economic Events"}, {"url": "cancerbioinformatics2010_cambridge", "desc": "Substantial amounts of data are being generated within cancer research. Datasets range from gene expression and microRNA array data through to next generation sequence data. Data interpretation draws on mathematical and computational skills and thus the subject has engaged the interest of researchers in areas such as machine learning, statistics, bioinformatics and computer science. The goal of this cross-disciplinary Workshop is therefore to bring together researchers from these disciplines and cancer researchers who have an interest in data analysis, to explore and present innovative approaches to this subject. Presented papers should:\r\n\r\n   1. Propose novel data analysis methods applicable to this domain or:\r\n   2. Present bioinformatics-driven studies in which mathematical or computational methods played an important role in finding results of potential significance in cancer research.\r\n\r\nFor novel data analysis methods, a non-exhaustive list of suitable topics include:\r\n\r\n    * Unsupervised, semi-supervised and biclustering methods to highlight disease subtypes or dysregulated genes within these subtypes,\r\n    * Data integration/data fusion methods to integrate different types of data such as gene expression, microRNA expression and array CGH data,\r\n    * Inference of gene regulatory networks,\r\n    * Pathway modeling and probabilistic ranking of pathway models,\r\n    * Biomarker discovery,\r\n    * Genome-wide association studies,\r\n    * Rational drug design methods and chemoinformatics,\r\n    * Protein function, structure prediction and structural bioinformatics,\r\n    * microRNA target site prediction,\r\n    * Analysis of high throughput sequencing data,\r\n    * Gene expression and post-transcriptional regulation,\r\n    * Methods for the detection of fusion genes,\r\n    * Prediction of disease progression,\r\n    * Probabilistic inference, Bayesian methods and Kernel-based methods for classifier design with applications to cancer bioinformatics,\r\n    * Methods for the detection and quantification of copy number alterations and deletions.\r\n\r\nThe Workshop is principally focused on the intepretation of omics datasets and does not cover related areas such as cancer imaging or development of software tools unless in the context of novel methodology.\r\n\r\nMore about the workshop at http://www.enm.bris.ac.uk/cig/cb/.", "recorded": "2010-09-02T13:00:00", "title": "Cancer Bioinformatics Workshop, Cambridge 2010"}, {"url": "licsb08_damoulas_pmm", "desc": "The problems of protein fold recognition and remote homology detection have recently attracted a great deal of interest as they represent challenging multi-feature multi-class problems for which modern pattern recognition methods achieve only modest levels of performance. As with many pattern\r recognition problems, there are multiple feature spaces or groups of attributes available, such as global characteristics like the amino-acid composition (C), predicted secondary structure (S), hydrophobicity (H), van der Waals volume (V), polarity (P), polarizability (Z), as well as attributes derived from local sequence alignment such as the Smith-Waterman scores. This raises the need for a\r classification method that is able to assess the contribution of these potentially heterogeneous object descriptors while utilizing such information to improve predictive performance. To that end, we offer a single multi-class kernel machine that informatively combines the available feature groups and, as is demonstrated in this paper, is able to provide the state-of-the-art in performance accuracy on the fold recognition problem. Furthermore, the proposed approach provides some insight by assessing the significance of recently introduced protein features and string kernels. The proposed method is well-founded within a Bayesian hierarchical framework and a variational Bayes approximation is derived which allows for efficient CPU processing times.\r \r Results:\r The best performance which we report on the SCOP PDB-40D benchmark data-set is a 70% accuracy by combining all the available feature groups from global protein characteristics but also including sequence-alignment features.\r We offer an 8% improvement on the best reported performance that combines binary SVM classifiers while at the same time reducing computational costs and assessing the predictive power of the various available features. Furthermore, we examine the performance of our methodology on the SCOP 1.53 benchmark data-set that simulates remote homology detection and examine the combination\r of various state-of-the-art string kernels that have recently been proposed.", "recorded": "2008-03-27T11:20:00", "title": "Probabilistic multi-class multi-kernel learning: On protein fold recognition and remote homology detection"}, {"url": "solomon_caruana_mc", "desc": "Decision trees are intelligible, but do they perform well enough that you should use them? Have SVMs replaced neural nets, or are neural nets still best for regression, and SVMs best for classification? Boosting maximizes margins similar to SVMs, but can boosting compete with SVMs? And if it does compete, is it better to boost weak models, as theory might suggest, or to boost stronger models? Bagging is simpler than boosting -- how well does bagging stack up against boosting? Breiman said Random Forests are better than bagging and as good as boosting. Was he right? And what about old friends like logistic regression, KNN, and naive bayes? Should they be relegated to the history books, or do they still fill important niches? \\\\ In this talk we compare the performance of ten supervised learning methods on nine criteria: Accuracy, F-score, Lift, Precision/Recall Break-Even Point, Area under the ROC, Average Precision, Squared Error, Cross-Entropy, and Probability Calibration. The results show that no one learning method does it all, but some methods can be \"repaired\" so that they do very well across all performance metrics. In particular, we show how to obtain the best probabilities from max margin methods such as SVMs and boosting via Platt's Method and isotonic regression. We then describe a new ensemble method that combines select models from these ten learning methods to yield much better performance. Although these ensembles perform extremely well, they are too complex for many applications. We'll describe what we're doing to try to fix that. Finally, if time permits, we'll discuss how the nine performance metrics relate to each other, and which of them you probably should (or shouldn't) use. \\\\ During this talk I'll briefly describe the learning methods and performance metrics to help make the lecture accessible to non-specialists in machine learning.", "recorded": "2007-04-17T12:00:00", "title": "Model Compression"}, {"url": "mitworld_debate_climategate", "desc": "The hacking of emails from the University of East Anglia\u2019s Climate Research Unit in November rocked the world of climate change science, energized global warming skeptics, and threatened to derail policy negotiations at Copenhagen. These panelists, who differ on the scientific implications of the released emails, generally agree that the episode will have long-term consequences for the larger scientific community.\r\n\r\n\u201cWhat we have here,\u201d says Kerry Emanuel, are \u201cthousands of emails collectively showing scientists hard at work, trying to figure out the meaning of evidence that confronts them. Among a few messages, there are a few lines showing the human failings of a few scientists\u2026\u201d Emanuel believes that \u201cscientifically, it means nothing,\u201d because the controversy doesn\u2019t challenge the overwhelming evidence supporting anthropogenic warming. He is far more concerned with the well-funded \u201cpublic relations campaign\u201d to drown out or distort the message of climate science, which he links to \u201cinterests where billions, even trillions are at stake...\u201d This \u201cmachine \u2026 has been highly successful in branding climate scientists as a bunch of sandal-wearing, fruit-juice drinking leftist radicals engaged in a massive conspiracy to return us to agrarian society\u2026\u201d\r\n\r\nRichard Lindzen professes he has \u201cno idea\u201d what Emanuel is talking about -- if a \u201cmachine\u201d exists, it\u2019s on the \u201cother side,\u201d marginalizing those who disagree on the science. The release of emails is likely due \u201cto a whistleblower who couldn\u2019t take it anymore.\u201d Lindzen sees evidence in the correspondence of \u201cthings that are unethical and in many cases illegal,\u201d including the refusal to allow outsiders access to data, and the willingness to destroy data rather than release it. He believes that since it\u2019s hard to read the documents \u201cand not conclude that bad things are going on,\u201d this will have a negative impact on \u201cpopular support for science.\u201d There are \u201cscandals, cheating and arguments\u201d over research dealing with tiny increments of temperature change, Lindzen speculates, because so many scientists and ordinary people are invested in the idea of dramatic, human-based warming -- \u201cPeople are being thrown catastrophes.\u201d\r\n\r\n\u201cThe imprudent language in the email cache reflects scientists\u2019 enormous frustration with the tactics of their opponents,\u201d says Judith Layzer. Climate change poses a serious new challenge for scientists: \u201cOn the one hand, they perceive it as sufficiently urgent that they\u2019re willing to go to great lengths, use language they wouldn\u2019t ordinarily, to try to persuade the public. On the other hand, they face the most sophisticated campaign of skepticism ever assembled, and one that consistently violates protocols they\u2019re accustomed to.\u201d The moderate language of science, with its emphasis on the weight of evidence, can\u2019t compete with attacks that discredit models, \u201cwhich by their very nature are fishy to nonscientists.\u201d Careless email communications gave the public a harsh reminder that scientists \u201care human, fallible and not always judicious.\u201d\r\n\r\nThe email controversy, says Stephen Ansolabehere creates uncertainty about the scientific debate, and will lead to greater scrutiny by the public \u2013 which is \u201chealthy.\u201d Since climate change is a grand scale problem with impacts on multiple dimensions of society, the \u201cquestion we must ask ourselves now is, \u201cWho will police science and how can science maintain credibility as it gets into public debates?\u201d Scientists, as private citizens, are free to engage in political debates, but \u201cmust be especially careful about maintaining research standards and methods.\u201d Scientists will find in the future \u201cthey must be even more scrupulous about maintaining research standards because more is at stake than getting the next paper published\u2026\u201d\r\n\r\nAfter combing through the emails, Ronald Prinn has reached several conclusions: Some exchanges dealing with modeling natural variability in temperatures over hundreds of years were \u201cpersonal in nature,\u201d and \u201cunprofessional.\u201d The research of the scientists accused of manipulating data is not central to the argument for anthropogenic climate change, nor has it compromised the work of the Intergovernmental Panel on Climate Change, although the public perception of climate science has certainly been affected. Climate researchers, Prinn concludes, \u201cmust step back from the tendency to polarization.\u201d More important, they must communicate better to the public that multiple approaches and critical analyses are the norm in climate science and that legitimate science is found in peer-reviewed literature, \u201cnot in blogs or in opinion pieces that go into newspapers.\u201d", "recorded": "2009-12-10T10:48:38", "title": "The Great Climategate Debate"}, {"url": "sssw05_mladenic_kd2", "desc": "The basic idea of Knowledge discovery is to let a computer search for knowledge whereas the humans give just broad directions about where and how to search. Surprisingly, it is often the case that already relatively simple techniques are able to uncover useful hidden truth beneath the surface of the known facts and relationships. Knowledge discovery could be defined as a research area with several subfields with the most representative Machine Learning and Data Mining (Mitchell, 1997; Fayyad et al., 1996; Witten and Frank, 1999; Hand et al., 2001) and Data bases. Different real-life problems have been successfully addressed using Knowledge discovery methods including Data mining and Decision support (Mladenic et al., 2003; Mladenic and Lavrac, 2003). Semantic Web (Barnes-Lee and Fischetti, 1999) on the other hand, can be seen as mainly dealing with integration of many, already existing ideas and technologies with the specific focus of upgrading the existing nature of web-based information systems to a more \u201csemantic\u201d oriented nature. In this context Semantic Web could be viewed as a frontier of Knowledge Management with some emphasis on web-based applications. There are several dimensions along which Knowledge Discovery (KD) can bring important contributions to Semantic Web. Since KD techniques are mainly about discovering structure in the data, this can serve as one of the key mechanisms for structuring knowledge into an ontological structure being further used in Knowledge management process. An interesting aspect is that data and corresponding semantic structures change in time. As the consequence, we need to be able to adapt ontologies that are modeling the data accordingly. Sub-field of KD called \u201cstream mining\u201d deals with these kinds of problems. It is also important to point out that scalability is one of the central issues in KD, especially in the sub-areas such as Data mining where one needs to be able to deal with real-life datasets of the terra-byte sizes. Semantic Web is ultimately concerned with real-life data on the web which have exponential growth. ", "recorded": "2005-07-06T11:00:00", "title": "Knowledge Discovery - Part 2"}, {"url": "sssw05_grobelnik_kd1", "desc": "The basic idea of Knowledge discovery is to let a computer search for knowledge whereas the humans give just broad directions about where and how to search. Surprisingly, it is often the case that already relatively simple techniques are able to uncover useful hidden truth beneath the surface of the known facts and relationships. Knowledge discovery could be defined as a research area with several subfields with the most representative Machine Learning and Data Mining (Mitchell, 1997; Fayyad et al., 1996; Witten and Frank, 1999; Hand et al., 2001) and Data bases. Different real-life problems have been successfully addressed using Knowledge discovery methods including Data mining and Decision support (Mladenic et al., 2003; Mladenic and Lavrac, 2003). Semantic Web (Barnes-Lee and Fischetti, 1999) on the other hand, can be seen as mainly dealing with integration of many, already existing ideas and technologies with the specific focus of upgrading the existing nature of web-based information systems to a more \u201csemantic\u201d oriented nature. In this context Semantic Web could be viewed as a frontier of Knowledge Management with some emphasis on web-based applications. There are several dimensions along which Knowledge Discovery (KD) can bring important contributions to Semantic Web. Since KD techniques are mainly about discovering structure in the data, this can serve as one of the key mechanisms for structuring knowledge into an ontological structure being further used in Knowledge management process. An interesting aspect is that data and corresponding semantic structures change in time. As the consequence, we need to be able to adapt ontologies that are modeling the data accordingly. Sub-field of KD called \u201cstream mining\u201d deals with these kinds of problems. It is also important to point out that scalability is one of the central issues in KD, especially in the sub-areas such as Data mining where one needs to be able to deal with real-life datasets of the terra-byte sizes. Semantic Web is ultimately concerned with real-life data on the web which have exponential growth. ", "recorded": "2005-07-01T17:30:00", "title": "Knowledge Discovery - Part 1"}, {"url": "aaai2011_ferrucci_building", "desc": "Computer systems that can directly and accurately answer peoples\u2019 questions over a broad domain of human knowledge have been envisioned by\r\nscientists and writers since the advent of computers themselves. Open domain question answering holds tremendous promise for facilitating informed decision making over vast volumes of natural language content. Applications in business intelligence, healthcare, customer support, enterprise\r\nknowledge management, social computing, science and government would\r\nall beneit from deep language processing. The DeepQA project is aimed at exploring how advancing\r\nand integrating natural language processing, information retrieval, machine learning, massively parallel computation, and knowledge representation and reasoning can greatly advance open-domain automatic question answering. An exciting proof-point in this challenge is to develop a computer system\r\nthat can successfully compete against top human players at the Jeopardy! quiz show. Attaining champion-level performance Jeopardy!  requires a computer system to rapidly and accurately answer rich\r\nopen-domain questions, and to predict its own performance on any given category/question. The system must deliver high degrees of precision and conidence over a very broad range of knowledge and\r\nnatural language content with a 3-second response time. To do this DeepQA evidences and evaluates\r\nmany competing hypotheses. A key to success is automatically learning and combining accurate conidences across an array of complex algorithms and over different dimensions of evidence. Accurate\r\nconidences are needed to know when to \u201cbuzz in\u201d against your competitors and how much to bet.\r\nHigh precision and accurate conidence computations are just as critical for providing real value in\r\nbusiness settings where helping users focus on the right content sooner and with greater conidence\r\ncan make all the difference. The need for speed and high precision demands a massively parallel computing platform capable of generating, evaluating and combing 1000s of hypotheses and their associated evidence. In this talk I will introduce the audience to the Jeopardy! Challenge and how we tackled it using DeepQA.", "recorded": "2011-08-09T10:00:00", "title": "Building Watson: An Overview of DeepQA for the  Jeopardy! Challenge"}, {"url": "promogram_igor_mekjavic_eng", "desc": "The main goal is to create new knowledge and to transfer this knowledge to our users in industry, medicine and sports based on the development of original systems and technologies and on various activities in education of different types and levels.\r\nThe common title of reserach in this group is \"man and machine\". We investigate the functioning of humans, emphasis is on human motion, from the viewpoint of robotics and automation. We intend to implement these results in the development of advanced production systems and technologies, as well as biomedical devices and methods in medicine and sports. Within the programme, the following principal topics are investigated:\r\n\r\n# Integration of mobility and manipulation in industrial and service robots\\\\\r\nHere, we intend to integrate a mobile robot platform and a robot manipulator and equip this system with different types of sensors, including distance sensors, stereo vision, tacktile sensors, force sensors etc. The objective is to develop new methods of control of complex (futuristic) robots, including a whole spectrum from strategic to basic level of control. Special interest will also be given to the design and control of parallel and redundant robot mechanisms.\r\n# Humanoid robots\\\\\r\nTwo topics are investigated. First is concerned with the devlopent of a humanoid manipulator, in particular of a humanoid shoulder complex. The biomechanics of human manipulation is studied and the mathematics of humanoid motion will be synthesized. The second topic is related to the detection and validation of human motion by the use of computer stereo vision. The objective is to develop a system of robot teaching by showing.\r\n# Human physiology\\\\\r\nThis is a set of research topics in envirnomental and sports medicine with strong application impact, for instance, in biophysical evaluation of shoes, garment and various protection equipment or ergonomic analysis of workplaces in industry.\r\n# Development of new biomedical devices and methods\\\\\r\nWthin this area we investigate primarily the problems of electrical stimulation. Our functional and therapeutical stimulators are well known worldwide. We develop and transfer in practice different types of electrical stimulators.\r\n# Robotics, automation and informatization of production systems\\\\\r\nThe group is traditionally connected with a number of Slovenian industrial companies in advancing their production systems and technologies. In these years we plan to develop new and original production cells and lines.", "recorded": "2013-06-27T16:08:09", "title": "Automatics, robotics and biocybernetics"}, {"url": "solomon_caruana_kc2", "desc": "Decision trees are intelligible, but do they perform well enough that you should use them? Have SVMs replaced neural nets, or are neural nets still best for regression, and SVMs best for classification? Boosting maximizes margins similar to SVMs, but can boosting compete with SVMs? And if it does compete, is it better to boost weak models, as theory might suggest, or to boost stronger models? Bagging is simpler than boosting -- how well does bagging stack up against boosting? Breiman said Random Forests are better than bagging and as good as boosting. Was he right? And what about old friends like logistic regression, KNN, and naive bayes? Should they be relegated to the history books, or do they still fill important niches? \\\\ In this talk we compare the performance of ten supervised learning methods on nine criteria: Accuracy, F-score, Lift, Precision/Recall Break-Even Point, Area under the ROC, Average Precision, Squared Error, Cross-Entropy, and Probability Calibration. The results show that no one learning method does it all, but some methods can be \"repaired\" so that they do very well across all performance metrics. In particular, we show how to obtain the best probabilities from max margin methods such as SVMs and boosting via Platt's Method and isotonic regression. We then describe a new ensemble method that combines select models from these ten learning methods to yield much better performance. Although these ensembles perform extremely well, they are too complex for many applications. We'll describe what we're doing to try to fix that. Finally, if time permits, we'll discuss how the nine performance metrics relate to each other, and which of them you probably should (or shouldn't) use. \\\\ During this talk I'll briefly describe the learning methods and performance metrics to help make the lecture accessible to non-specialists in machine learning.", "recorded": "2007-04-17T12:00:00", "title": "KDD - CUP 2004"}, {"url": "solomon_caruana_ssms", "desc": "Decision trees are intelligible, but do they perform well enough that you should use them? Have SVMs replaced neural nets, or are neural nets still best for regression, and SVMs best for classification? Boosting maximizes margins similar to SVMs, but can boosting compete with SVMs? And if it does compete, is it better to boost weak models, as theory might suggest, or to boost stronger models? Bagging is simpler than boosting -- how well does bagging stack up against boosting? Breiman said Random Forests are better than bagging and as good as boosting. Was he right? And what about old friends like logistic regression, KNN, and naive bayes? Should they be relegated to the history books, or do they still fill important niches? \\\\ In this talk we compare the performance of ten supervised learning methods on nine criteria: Accuracy, F-score, Lift, Precision/Recall Break-Even Point, Area under the ROC, Average Precision, Squared Error, Cross-Entropy, and Probability Calibration. The results show that no one learning method does it all, but some methods can be \"repaired\" so that they do very well across all performance metrics. In particular, we show how to obtain the best probabilities from max margin methods such as SVMs and boosting via Platt's Method and isotonic regression. We then describe a new ensemble method that combines select models from these ten learning methods to yield much better performance. Although these ensembles perform extremely well, they are too complex for many applications. We'll describe what we're doing to try to fix that. Finally, if time permits, we'll discuss how the nine performance metrics relate to each other, and which of them you probably should (or shouldn't) use. \\\\ During this talk I'll briefly describe the learning methods and performance metrics to help make the lecture accessible to non-specialists in machine learning.", "recorded": "2007-04-17T12:00:00", "title": "Spooky Stuff in Metric Space"}, {"url": "solomon_caruana_wslmw", "desc": "Decision trees are intelligible, but do they perform well enough that you should use them? Have SVMs replaced neural nets, or are neural nets still best for regression, and SVMs best for classification? Boosting maximizes margins similar to SVMs, but can boosting compete with SVMs? And if it does compete, is it better to boost weak models, as theory might suggest, or to boost stronger models? Bagging is simpler than boosting -- how well does bagging stack up against boosting? Breiman said Random Forests are better than bagging and as good as boosting. Was he right? And what about old friends like logistic regression, KNN, and naive bayes? Should they be relegated to the history books, or do they still fill important niches? \\\\\r In this talk we compare the performance of ten supervised learning methods on nine criteria: Accuracy, F-score, Lift, Precision/Recall Break-Even Point, Area under the ROC, Average Precision, Squared Error, Cross-Entropy, and Probability Calibration. The results show that no one learning method does it all, but some methods can be \"repaired\" so that they do very well across all performance metrics. In particular, we show how to obtain the best probabilities from max margin methods such as SVMs and boosting via Platt's Method and isotonic regression. We then describe a new ensemble method that combines select models from these ten learning methods to yield much better performance. Although these ensembles perform extremely well, they are too complex for many applications. We'll describe what we're doing to try to fix that. Finally, if time permits, we'll discuss how the nine performance metrics relate to each other, and which of them you probably should (or shouldn't) use. \\\\\r During this talk I'll briefly describe the learning methods and performance metrics to help make the lecture accessible to non-specialists in machine learning.", "recorded": "2006-05-30T13:00:00", "title": "Which Supervised Learning Method Works Best for What? An Empirical Comparison of Learning Methods and Metrics"}, {"url": "nipsworkshops09_kumar_lrbssm", "desc": "In recent years, machine learning has seen the development of a series of algorithms for parameter learning that avoid estimating the partition function and instead, rely on accurate approximate MAP inference. Within this framework, we consider two new topics. \r\n\r\nIn the first part, we discuss parameter learning in a semi-supervised scenario. Specifically, we focus on a region-based scene segmentation model that explains an image in terms of its underlying regions (a set of connected pixels that provide discriminative features) and their semantic labels (such as sky, grass or foreground). While it is easy to obtain (partial) ground-truth labeling for the pixels of a training image, it is not possible for a human annotator to provide us with the best set of regions (those that result in the most discriminative features). To address this issue, we develop a novel iterative MAP inference algorithm which selects the best subset of regions from a large dictionary using convex relaxations. We use our algorithm to \"complete\" the ground-truth labeling (i.e. infer the regions) which allows us to employ the highly successful max-margin training regime. We compare our approach with the state of the art methods and demonstrate significant improvements. \r\n\r\nIn the second part, we discuss a new learning framework for general log-linear models based on contrastive objectives. A contrastive objective considers a set of \"interesting\" assignments and attempts to push up the probability of the correct instantiation at the expense of the other interesting assignments. In contrast to our approach, related methods such as pseudo-likelihood and contrastive divergence compare the correct instantiation only to nearby instantiations, which can be problematic when there is a high-scoring instantiation far away from the correct one. We present some of the theoretical properties and practical advantages of our method, including the ability to learn a log-linear model using only (approximate) MAP inference. We the theoretical properties and practical advantages of our method, including the ability to learn a log-linear model using only (approximate) MAP inference. We also show results of applying our method to some simple synthetic examples, where it significantly outperforms pseudo-likelihood.", "recorded": "2009-12-12T15:47:00", "title": "Learning a Region-based Scene Segmentation Model"}, {"url": "icml2010_allauzen_opke", "desc": "The OpenKernel library is an open-source software library for designing, combining, learning and using kernels for machine learning applications\r\n\r\nThe library supports the design and use of kernels defined over dense and sparse real vectors, as well as over sequences or distributions of sequences.\r\n\r\nFor dense and sparse features, the library provides implementation of the classical kernels: linear, polynomial, Gaussian and sigmoid.\r\n\r\nFor sequences and distributions of sequences, the library implements the rational kernel framework of Cortes et al. (JMLR, 2004). The library supplies the following sequence kernels:\r\n\r\nn -gram kernels,\r\ngappy n-gram kernels,\r\nmismatch kernels (Leslie et al., 2004),\r\nand gives the utilities for creating arbitrary rational kernels simply by providing the weighted finite-state transducers they are based on.\r\n\r\nKernels can be combined by taking their sum or their product, and can be composed with a polynomial, a Gaussian or a sigmoid. They support on-demand evaluation and caching. In addition to its own binary format, the library uses the ASCII format of LIBSVM/LIBLINEAR/SVMlight for representing features (and precomputed kernels for LIBSVM).\r\n\r\nFinally, the OpenKernel library also includes several options for using training data to automatically combine multiple kernels. This is particularly useful when the single best kernel for the task is not known. The algorithms implemented include\r\n\r\nL1-regularized linear combinations (Lanckriet et al. JMLR 2004);\r\nL2-regularized linear combinations (Cortes et al. UAI 2009);\r\nL2-regularized quadratic combinations (Cortes et al. NIPS 2009),\r\nas well as kernel correlation, or alignment (Cortes et al. ICML 2010), based combinations. Specialized efficient versions of these algorithms are also made available for weighting features and sparseness and can be used to further improve efficiency. The output kernels can be easily used in conjunction with LIBSVM, SVMlight and included kernel ridge regression implementations. Full reference documentation, tutorials and examples (with formatted datasets) are included.\r\n\r\nThe library is an open-source project distributed under the Apache license (2.0). This work has been partially supported by Google Inc. The library uses the OpenFst library for representing and manipulating weighted finite-state transducers.", "recorded": "2010-06-25T15:13:57", "title": "OpenKernel"}, {"url": "kdd2010_wang_dmdp", "desc": "Classification is one of the most essential tasks in data mining. Unlike other methods, associative classification tries to find all the frequent patterns existing in the input categorical data satisfying a user-specified minimum support and/or other discrimination measures like minimum confidence or information-gain. Those patterns are used later either as rules for rule-based classifier or training features for support vector machine (SVM) classifier, after a feature selection procedure which usually tries to cover as many as the input instances with the most discriminative patterns in various manners. Several algorithms have also been proposed to mine the most discriminative patterns directly without costly feature selection. Previous empirical results show that associative classification could provide better classification accuracy over many datasets. \r\nRecently, many studies have been conducted on uncertain data, where fields of uncertain attributes no longer have certain values. Instead probability distribution functions are adopted to represent the possible values and their corresponding probabilities. The uncertainty is usually caused by noise, measurement limits, or other possible factors. Several algorithms have been proposed to solve the classification problem on uncertain data recently, for example by extending traditional rule-based classifier and decision tree to work on uncertain data. In this paper, we propose a novel algorithm uHARMONY which mines discriminative patterns directly and effectively from uncertain data as classification features/rules, to help train either SVM or rule-based classifier. Since patterns are discovered directly from the input database, feature selection usually taking a great amount of time could be avoided completely. Effective method for computation of expected confidence of the mined patterns used as the measurement of discrimination is also proposed. Empirical results show that using SVM classifier our algorithm uHARMONY outperforms the state-of-the-art uncertain data classification algorithms significantly with 4% to 10% improvements on average in accuracy on 30 categorical datasets under varying uncertain degree and uncertain attribute number.", "recorded": "2010-07-27T15:40:00", "title": "Direct Mining of Discriminative Patterns for Classifying Uncertain Data"}, {"url": "mitworld_kanwisher_bbhv", "desc": "Nancy Kanwisher\u2019s breakthrough scanning research reveals \u201ca teeny part of an answer to the big question of what kinds of brains we have,\u201d she says. Her work depends on functional MRI, a way of imaging people\u2019s brains that detects areas of high neural activity. Kanwisher focuses on vision, to which almost 1/2 of the human cortex is dedicated. \u201cBefore fMRI, we knew almost nothing about how that part of the brain was organized,\u201d says Kanwisher. In some of her earliest work, she put her subjects in the fMRI machine, showed them pictures of faces and objects and scanned their heads. She found an area that lit up exclusively in response to the faces. She has found other regions since then, \u201ckind of mind-blowing, because nobody predicted them.\u201d There\u2019s brain circuitry devoted to places and spatial layouts, and another distinct region that responds selectively to body parts like feet, elbows and knees.\n\nKanwisher has shown that our \u201cminds contain at least a small number of very specialized mechanisms to process very specific kinds of information.\u201d There are lots of questions remaining, though, like determining which mental functions get \u201ctheir own private piece of cortex and which don\u2019t.\u201d Fruits and vegetables for instance, don't seem to merit their own special brain area. Kanwisher would like to know how these mechanisms arise during development -- whether in response to genetic wiring or environmental stimuli -- and how they change during adulthood.\n\nDuring exchanges with audience members, Kanwisher says she doesn\u2019t believe that \u201cevery mental function of interest happens in one little bit of the brain, because the range of human experience is too broad and varied to fit each into its own little patch.\u201d She dismisses as \u201cbaloney\u201d assertions about fundamental cognitive differences between men and women. She also answers questions about scanning in animals, infants and children; evolutionary pressure of brain development; and the limitations of fMRI.", "recorded": "2006-04-26T12:06:00", "title": "The Brain Basis of Human Vision"}, {"url": "kdd09_chakrabarti_agarwal_scca", "desc": "Many organizations now devote significant fractions of their advertising/outreach budgets to online advertising; ad-networks like Yahoo!, Google, MSN have responded by constructing new kinds of economic models and perform the fundamental task of matching the most relevant ads (selected from a large inventory) for a (query,user) pair in a given context. Nearly all of the challenges that arise are substantially data- or model-driven (or both). Computational Advertising is a relatively new scientific sub-discipline at the interesection of large scale search and text analysis, information retrieval, statistical modeling, machine learning, optimization and microeconomics that address this match-making problem and provides unprecedented opportunities to data miners.\r\n\r\nTopics covered include a comprehensive introduction to several advertising forms (sponsored search, contextual adverting, display advertising), revenue models (pay-per-click, pay-per-view, pay-per-conversion) and data mining challenges involved, along with an overview of state-of-the-art techniques in the area with a detailed discussion of open problems. We will cover information retrieval techniques and their limitations; data mining challenges involved in performing ad matching through clickstream data and challenging optimization issues that arise in display advertising. In particular, we will cover statistical modeling techniques for clickstream data and explore/exploit schemes to perform online experiments for better long-term performance using multi-armed bandit schemes. We also discuss the close relationship of techniques used in recommender systems to our problem but indicate several additional issues that needs to be addressed before they become routine in computational advertising.\r\n\r\nWe will only assume basic knowledge of statistical methods, no prior knowledge of online advertising is required. In fact, the first hour that provides an introduction to the area would be appropriate for all registered attendees of KDD 2009. The second half would require familiarity with basic concepts like regression, probability distributions and appreciation of issues involved in fitting statistical models to large scale applications. No prior knowledge of multi-armed bandits would be assumed.\r\n", "recorded": "2009-06-28T09:00:00", "title": "Statistical Challenges in Computational Advertising"}, {"url": "nipsworkshops09_kumar_pluamapi", "desc": "In recent years, machine learning has seen the development of a series of algorithms for parameter learning that avoid estimating the partition function and instead, rely on accurate approximate MAP inference. Within this framework, we consider two new topics. \r\n\r\nIn the first part, we discuss parameter learning in a semi-supervised scenario. Specifically, we focus on a region-based scene segmentation model that explains an image in terms of its underlying regions (a set of connected pixels that provide discriminative features) and their semantic labels (such as sky, grass or foreground). While it is easy to obtain (partial) ground-truth labeling for the pixels of a training image, it is not possible for a human annotator to provide us with the best set of regions (those that result in the most discriminative features). To address this issue, we develop a novel iterative MAP inference algorithm which selects the best subset of regions from a large dictionary using convex relaxations. We use our algorithm to \"complete\" the ground-truth labeling (i.e. infer the regions) which allows us to employ the highly successful max-margin training regime. We compare our approach with the state of the art methods and demonstrate significant improvements. \r\n\r\nIn the second part, we discuss a new learning framework for general log-linear models based on contrastive objectives. A contrastive objective considers a set of \"interesting\" assignments and attempts to push up the probability of the correct instantiation at the expense of the other interesting assignments. In contrast to our approach, related methods such as pseudo-likelihood and contrastive divergence compare the correct instantiation only to nearby instantiations, which can be problematic when there is a high-scoring instantiation far away from the correct one. We present some of the theoretical properties and practical advantages of our method, including the ability to learn a log-linear model using only (approximate) MAP inference. We the theoretical properties and practical advantages of our method, including the ability to learn a log-linear model using only (approximate) MAP inference. We also show results of applying our method to some simple synthetic examples, where it significantly outperforms pseudo-likelihood.", "recorded": "2009-12-12T15:30:00", "title": "Parameter Learning Using Approximate MAP Inference "}, {"url": "aaai2012_tenenbaum_grow_mind", "desc": "The fields of cognitive science and artificial intelligence grew up together, with the twin goals of understanding human minds and making machines smarter in more humanlike ways. Yet since the 1980s they have mostly grown apart, as cognitive scientists came to see AI as too focused on applications and technical engineering issues rather than big questions of intelligence, while AI researchers came to see cognitive science as too informal and concerned with peculiarities of human minds and brains rather than general principles. Just in the last few years, however, these fields appear poised to reconverge in exciting and deep ways. Cognitive scientists have begun to adopt the toolkit of modern probabilistic AI as a unifying framework for modeling natural intelligence, while many AI researchers are looking beyond immediate applications to some of the big picture questions that originally motivated the field, and both communities are increasingly aware of and even informed by the other's moves in these directions.\r\n\r\nThis talk will describe recent work at the center of the convergence: computational accounts of human intelligence that both draw on and advance state-of-the-art AI. I will focus on capacities for which even young children still far surpass machines: learning from very few examples, and common sense reasoning about the physical and social world. These abilities can be explained as approximate forms of probabilistic (Bayesian) inference over richly structured models \u2014 probabilistic models built on top of knowledge representations familiar from earlier, classic AI days, such as graphs, grammars, schemas, predicate logic, and functional programs. In many cases, sampling-based approximate inference with these models can be surprisingly tractable and can predict human judgments with high quantitative accuracy. Extended in a hierarchical nonparametric Bayesian framework, these models can explain how children learn to learn, bootstrapping adult-like intelligence from more primitive foundations. Using probabilistic programming languages, these models can be integrated into a unified cognitive architecture. Throughout the talk I will present concrete examples, along with a few more speculative predictions, of how these cognitive modeling efforts can inform the development of more intelligent machine systems.", "recorded": "2012-07-26T13:25:49", "title": "How to Grow a Mind: Statistics, Structure and Abstraction"}, {"url": "mitworld_moniz_kamen_ose", "desc": "Oil, Security, Environment, Technology\r\nAs an energy source, oil is hard to beat. In spite of reports to the contrary, there\u2019s still lots of it available\u20141 trillion barrels\u2014and the cost of extracting and harnessing it for use in transportation and industry is cheap. But, Ernest Moniz reminds us, the energy equation needs to include some important new factors: insecurity of supply and environmental stewardship. The price and convenience of fossil fuels decreases quickly when you take into account the costs of global warming and ensuring stability in the Middle East. If the U.S. ever develops a serious energy policy, says Moniz, here are some key objectives: Reduce oil demand by producing higher efficiency vehicles; target \u201cunconventional reservoirs of oil\u201d in places like Canada and Venezuela; and develop alternative liquid fuels from such ubiquitous sources as cellulose. Hydrogen power is hugely uneconomical, with viable technology decades away, so we \u201ccan\u2019t afford to have a focus on this direction impede serious approaches regarding security and the environment,\u201d concludes Moniz.\r\nThe Segway Alternative\r\nWhether you live in Bangkok or London, don\u2019t count on motoring along at more than 8 miles per hour -- the pace of 19th century horse carriages. Half the world\u2019s people -- around 3 billion -- will have moved to a city by 2020, says Dean Kamen, so \u201ccities will need cars like a fish needs a bicycle.\u201d Even if you make cars clean and efficient, he points out, \u201cwe don\u2019t need a 3-thousand pound pile of steel to move our 159 pound butt around town, and there are still the issues of congestion and parking.\u201d What if you could have a machine \u201cthat\u2019s non-polluting, fun to use, requires no change in infrastructure \u2013 just a change in thinking?\u201d Voila, the Segway, Kamen\u2019s invention: a self-balancing personal transportation device. If you live in a city, and don\u2019t need to travel more than two miles, try the Segway. For a block-long trip, pull on the sneakers, counsels Kamen. He has pedaled and peddled the Segway around the U.S. (as he does on Kresge\u2019s stage) to argue for its inclusion on sidewalks. Forty-five states have given the nod to Segway travel, with some hazing from the city of San Francisco and the bicycle lobby.", "recorded": "2004-06-05T10:08:00", "title": "Oil, Security, Environment, Technology - The Segway Alternative"}, {"url": "lms08_whistler", "desc": "While the machine learning community has primarily focused on analysing the output of a single data source, there has been relatively few attempts to develop a general framework, or heuristics, for analysing several data sources in terms of a shared dependency structure. Learning from multiple data sources (or alternatively, the data fusion problem) is a timely research area. Due to the increasing availability and sophistication of data recording techniques and advances in data analysis algorithms, there exists many scenarios in which it is necessary to model multiple, related data sources, i.e. in fields such as bioinformatics, multi-modal signal processing, information retrieval, sensor networks etc. \r\n\r\nThe open question is to find approaches to analyse data which consists of more than one set of observations (or view) of the same phenomenon. In general, existing methods use a discriminative approach, where a set of features for each data set is found in order to explicitly optimise some dependency criterion. However, a discriminative approach may result in an ad hoc algorithm, require regularisation to ensure erroneous shared features are not discovered, and it is difficult to incorporate prior knowledge about the shared information. A possible solution is to overcome these problems is a generative probabilistic approach, which models each data stream as a sum of a shared component and a private component that models the within-set variation. \r\n\r\nIn practice, related data sources may exhibit complex co-variation (for instance, audio and visual streams related to the same video) and therefore it is necessary to develop models that impose structured variation within and between data sources, rather than assuming a so-called 'flat' data structure. Additional methodological challenges include determining what is the 'useful' information to extract from the multiple data sources, and building models for predicting one data source given the others. Finally, as well as learning from multiple data sources in an unsupervised manner, there is the closely related problem of multitask learning, or transfer learning where a task is learned from other related tasks.\r\n\r\nMore information about workshop - http://web.mac.com/davidrh/LMSworkshop08/", "recorded": "2008-12-13T07:30:00", "title": "NIPS Workshop on Learning from Multiple Sources, Whistler 2008"}, {"url": "mla09_graening_kefad", "desc": "Applying numerical optimisation methods in the field of aerodynamic design optimisation normally leads to a huge amount of heterogeneous design data. While often only the most promising results are investigated and used to drive further optimisations, general methods for investigating the entire design data set are rare. It is our target to extract comprehensive knowledge from the design data concerning the interrelation between the shape and the performance of the design. The extracted knowledge is prepared in a way that it is usable for guiding further computational as well as manual design and optimisation processes.\r\n   For the design of complex aerodynamic shapes it is common to use different kinds of representations, what makes it difficult or even impossible to analyse the entire design data set. We suggest the transformation of the design data into discrete unstructured surface meshes and hence result in a homogeneous parametrisation of the designs. This makes it possible to analyse the design data independent of the representation used during the design and optimization process.\r\n   On the basis of discrete unstructured surface meshes we propose a displacement measure in order to analyse local differences between designs1. The measure provides information on the amount and the direction of surface modifications. We recently introduced a framework2 that uses the displacement data in conjunction with statistical methods and techniques from machine learning to provide meaningful knowledge from the dataset at hand. The framework comprises a number of approaches for the displacement analysis, sensitivity analysis, dimensionality reduction and rule extraction.\r\n   In order to demonstrate the feasibility of the suggested framework, we applied the proposed methods to a data set of a ultra-low aspect ratio transonic turbine stator blade of a small Honda turbofan engine that resulted from a computational optimisation run3. Decision trees have been formulated to generate a set of design rules which refer to a pre-defined blade design.\r\n   The results have been verified by means of modifying the turbine blade geometry using direct manipulation of free form deformation (DMFFD)4 techniques. The performance of the deformed blade design has been calculated by running computational fluid dynamic (CFD) simulations. It is shown that the suggested framework provides reasonable results which can directly be transformed into design modifications in order to guide the design process.", "recorded": "2009-07-03T14:20:00", "title": "Knowledge Extraction from Aerodynamic Design Data and its Application to 3D Turbine Blade Geometries"}, {"url": "reasecs_fuchs_ace", "desc": "Attempto Controlled English (ACE) is a controlled natural language,  \ni.e. a precisely defined subset of English that can automatically and  \nunambiguously be translated into first-order logic. ACE may seem to be  \ncompletely natural, but is actually a formal language, concretely it  \nis  a first-order logic language with an English syntax. Thus ACE is  \nhuman and machine understandable.\n\nACE was originally intended to specify software, but has since been  \nused as a general knowledge representation language in several  \napplication domains, most recently for the semantic web.\n\nACE is supported by a number of tools, predominantly the Attempto  \nParsing Engine (APE). Besides translating an ACE text into discourse  \nrepresentation structures, APE also offers translations into various  \nother forms of first-order logic, into OWL, SWRL, and into RuleML.  \nFurthermore, APE can generate ACE paraphrases of DRSs derived from ACE  \ntexts.\n\nTo support automatic reasoning in ACE, we have developed the Attempto  \nReasoner (RACE). RACE can prove that one ACE text is the logical  \nconsequence of another one, and give a justification for the success  \nor the failure of the proof in ACE.\n\nRecently, ACE has found several applications within the semantic web.  \nTherefore, we have developed translations of ACE into and from  \nsemantic web languages. Concretely, there are the translations ACE &lt;-&gt;  \nOWL/SWRL and ACE -&gt; rules. Also, there are various tools that use  \nthese translations: AceWiki (uses ACE -&gt; OWL/SWRL), ACE View (uses ACE  \n&lt;-&gt; OWL/SWRL), and AceRules (uses ACE -&gt; rules). The tool AceWiki  \ncombines controlled natural language with the ideas and technologies  \nof the semantic web and with the concepts of wikis. AceWiki also  \nincorporates a predictive editor that enables users to construct  \nsyntactically correct ACE sentences from a restricted vocabulary. ACE  \nView is a plug-in for the ontology editor Prot\u00e9g\u00e9. Finally, AceRules  \nis a forward chaining rule system that offers three different semantics.\n\nThe Attempto web-site (attempto.ifi.uzh.ch) contains much more  \ninformation on ACE and its tools: documentation, publications, web- \ninterfaces to the tools, software downloads, demos, talks and screen- \ncasts, and last but not least course material.", "recorded": "2008-05-14T00:00:00", "title": "Attempto Controlled English"}, {"url": "ssspr2010_special_session", "desc": "**Similarity-Based Pattern Recognition: Challenges and Prospects**\r\n\r\nTraditional pattern recognition techniques are centered around the notion of \"feature\". According to this view, the objects to be classified are represented in terms of properties that are intrinsic to the object itself. Hence, a typical pattern recognition system makes its decisions by simply looking at one or more feature vectors provided as input. The strength of this approach is that it can leverage a wide range of mathematical tools ranging from statistics, to geometry, to optimization. However, in many real-world applications a feasible feature-based description of objects might be difficult to obtain or inefficient for learning purposes. In these cases, it is often possible to obtain a measure of the (dis)similarity of the objects to be classified, and in some applications the use of dissimilarities (rather than features) makes the problem more viable. In the last few years, researchers in pattern recognition and machine learning are becoming increasingly aware of the importance of similarity information per se. Indeed, by abandoning the realm of vectorial representations one is confronted with the challenging problem of dealing with (dis)similarities that do not necessarily obey the requirements of a metric. This undermines the very foundations of traditional pattern recognition theories and algorithms, and poses totally new theoretical and computational questions.\r\n\r\nThe SIMBAD project is a EU FP7 project which aims at undertaking a thorough study of several aspects of purely similarity-based pattern analysis and recognition methods, from the theoretical, computational, and applicative perspective. It aims at covering a wide range of problems and perspectives, including supervised and unsupervised learning, generative and discriminative models, and its interest ranges from purely theoretical problems to real-world practical applications.\r\n\r\nTopics of interest for contributed papers include (but are not limited to):\r\n\r\n    * Foundational issues\r\n    * Embedding and embeddability\r\n    * Graph spectra and spectral geometry\r\n    * Indefinite and structural kernels\r\n    * Characterization of non-(geo)metric behavior\r\n    * Measures of (geo)metric violations\r\n    * Learning and combining similarities\r\n    * Multiple-instance learning\r\n    * Applications\r\n\r\nThe workshop aims to explore the spectrum of alternative approaches, methodologies and challenges in the area, rather than detailed techniques. Contributions can be of two kinds:\r\n\r\na) position papers that aim to stimulate discussion of the philosophy of approach underpinning the field,\r\n\r\nb) individual technical contributions on a focused topic.", "recorded": "2010-08-21T09:00:00", "title": "Special Session"}, {"url": "nipsworkshops2012_kernel_methods_graphical_models", "desc": "Kernel methods and graphical models are two important families of techniques for machine learning. Our community has witnessed many major but separate advances in the theory and applications of both subfields. For kernel methods, the advances include kernels on structured data, Hilbert-space embeddings of distributions, and applications of kernel methods to multiple kernel learning, transfer learning, and multi-task learning. For graphical models, the advances include variational inference, nonparametric Bayes techniques, and applications of graphical models to topic modeling, computational biology and social network problems. \r\n\r\nThis workshop addresses two main research questions: first, how may kernel methods be used to address difficult learning problems for graphical models, such as inference for multi-modal continuous distributions on many variables, and dealing with non-conjugate priors? And second, how might  kernel methods be advanced by bringing in  concepts from graphical models, for instance by incorporating sophisticated conditional independence structures, latent variables, and prior information?  \r\n\r\nKernel algorithms have traditionally had the advantage of being solved via convex optimization or eigenproblems, and having strong statistical guarantees on convergence. The graphical model literature has focused on modelling complex dependence structures in a flexible way, although approximations may be reuqired to make inference tractable. Can we develop a new set of methods which blend these strengths?\r\n\r\nThere has recently been a number of publications combining kernel and graphical model techniques, including kernel hidden Markov models, kernel belief propagation, kernel Bayes rule, kernel topic models, kernel variational inference, kernel herding as Bayesian quadrature, kernel beta processes, and a connection between kernel k-means and Bayesian nonparametrics. Each of these results deals with different inference tasks, and makes use of a range of RKHS propreties. We propose this workshop so as to \"connect the dots\" and develop a unified toolkit to address a broad range of learning problems, to the mutual benefit of reseachers in kernels and graphical models. The goals of the workshop are thus twofold: first, to provide an accessible review and synthesis of recent results combining graphical models and kernels. Second, to provide a discussion forum for open problems and technical challenges. \r\n\r\nWorkshop homepage: https://sites.google.com/site/kernelgraphical/", "recorded": "2012-12-08T07:45:00", "title": "Confluence between Kernel Methods and Graphical Models"}, {"url": "mitworld_prelec_neuroeconomics", "desc": "A pioneer in a \u201cdangerously hot research area,\u201d  Drazen Prelec  peers into the human brain while it makes decisions. In his corner of the new field of neuroeconomics, Prelec uses a functional magnetic resonance imaging (fMRI) machine to scan minds pondering the pros and cons of purchasing and selling products like Godiva chocolate and flash drives.\n\nPrelec first provides a brief background on the emergence of his discipline, made possible by technological advances in measuring brain activity, and the recent introduction of psychology into economics. The convergence (or perhaps collision) of behavioral approaches and economics has led to a \u201csustained criticism of the rationality assumption in economics,\u201d says Prelec, most prevalent in game theory. So much current research, he says, \u201cis a series of responses to the incorrect predictions of the rational normative model.\u201d\n\nSome Nobel Prize-winning work has emerged in the past few decades from studying the differences between the way human beings actually behave and the way classic economics suggests. Prelec describes prospect theory, which captures in a formula how there is \u201csomething about the way our mind deals with numbers (so that) if you look at positive things, you have one way of looking, and at negative, it\u2019s a different way.\u201d\n\nUsing three case studies, Prelec illustrates how \u201cneuroeconomics picks up some of these violations of rationality, trying to understand where in the brain we can get a deep understanding of what\u2019s going on.\u201d In a notable instance, subjects sipped different wines (through a straw) in the fMRI, and were asked to rate them. They were told they were drinking wine that ranged in price from $5 to $90. The \u201cdirty trick was the $5 and $45 wines were the same, as were the $10 and $90 wines.\u201d Not surprisingly, \u201cratings were massively influenced by price,\u201d so the $90 wine was considered exceptional.\n\nWhat was surprising, says Prelec, was that \u201cthe brain lies also.\u201d An area behind the forehead, the medial prefrontal cortex, which is associated with the perception of value, burst into more activity when the subject experienced the \u201c$90\u201d wine than with the exact same \u201c$10\u201d wine. It seems as if the very idea of quality, or value -- often a marketing ploy -- makes a product like wine more enjoyable.", "recorded": "2008-07-07T11:38:25", "title": "Neuroeconomics"}, {"url": "nipsworkshops2011_wortman_vaughan_convex", "desc": "A prediction market is a financial market designed to aggregate information. To facilitate trades, prediction markets are often operated by automated market makers. The market maker trades a set of securities with payoffs that depend on the outcome of a future event. For example, the market maker might offer a security that will pay off $1 if and only if a Democrat wins the 2012 US presidential election. A risk neutral trader who believes that the probability of a Democrat winning is p should be willing to purchase this security at any price below p, or sell it at any price above p. The current market price can then be viewed as the traders\u2019 collective estimate of how likely it is that a Democrat will win the election. Market-based estimates have proved to be accurate in a variety of domains, including business, entertainment, and politics. However, when the number of outcomes is very large, it is generally infeasible to run a simple prediction market over the full outcome space. We propose a general framework for the design of securities markets over combinatorial or infinite state or outcome spaces. Our framework enables the design of computationally efficient markets tailored to an arbitrary, yet relatively small, space of securities with bounded payoff. We prove that any market satisfying a set of intuitive conditions must price securities via a convex cost function, which is constructed via conjugate duality. Rather than deal with an exponentially large or infinite outcome space directly, our framework only requires optimization over a convex hull. By reducing the problem of automated market making to convex optimization, where many efficient algorithms exist, we arrive at a range of new polynomial-time pricing mechanisms for various problems.\r\nOur framework also provides new insights into the relationship between market design and machine learning. In particular, we show that the tools that have been developed for online linear optimization are strikingly similar to those we have constructed for selecting pricing mechanisms. This is rather surprising, as the problem of learning in an online environment is semantically quite distinct from the problem of pricing securities in a prediction market: a learning algorithm receives losses and selects weights whereas a market maker manages trades and sets prices. We show that although the two frameworks have very different semantics, they have nearly identical syntax in a very strong sense.", "recorded": "2011-12-16T09:15:00", "title": "Efficient Market Making via Convex Optimization & Connection to Online Learning"}, {"url": "mitworld_wozniak_iwoz", "desc": "Steve Wozniak tells the tale of Apple's early years with such illuminating details and brio that engineers (and ordinary mortals) will feel they\u2019d actually been on the scene. While lots of books recount this story, Wozniak says many of them \u201cgot it wrong.\u201d So he decided to set down his own version, by book and lecture.\n\nA ham radio licensee in 6th grade, Wozniak envisioned becoming an engineer, building \"radios, TVs or guidance systems.\" It was a time when one \u201ccouldn\u2019t hope to see a computer, and never own one because it cost as much as a house.\u201d\n\nWozniak put himself through U.C. Berkeley by working in electronics firms, including Hewlett Packard. All the while he was designing primitive computers. Then came the fateful day when he met Steve Jobs, with whom he had an immediate affinity. \"A lot of my life is driven by how you should live, your goals and values. A lot came from the pop music of the day, and we had similar tastes, like Bob Dylan.\" Both Wozniak and Jobs were fascinated by the early video games, like Pong, which had simple displays and controls. Wozniak stumbled on to the ARPANET, the precursor to the Internet, and was inspired by the idea of typing and seeing words appear on a video screen. \u2018I said, Wow, I can design my own computer and build it almost for free.\u201d\n\nWozniak devised a microprocessor with some memory and created the first \"local computer.\" Jobs set out to sell the invention. The Apple I was born, and in record time, they had an order for 100 computers, at $666.66 each. The famous garage was a staging area where Wozniak tested the machines for defects. He notes about this time, \u201cYou can do things amazingly fast when you don\u2019t have any lawyers.\u201d\n\nIn 1977, Wozniak\u2019s new and improved Apple II added basic programming language, as well as color and graphics, sound and paddles. This was the machine that convinced the world that computers didn\u2019t just belong in big companies, but in everyone\u2019s homes. \u201cIt was the biggest eureka moment of my life,\u201d Wozniak says, when he realized that with software on a computer \u201cyou could do in half an hour what would take you a lifetime in hardware.\u201d Whether with games, spreadsheet calculations or recipes, his computer had seized the imagination of an entire nation.", "recorded": "2006-09-28T16:41:17", "title": "iWoz: From Computer Geek to Cult Icon: How I Invented the Personal Computer, Co-Founded Apple, and Had Fun Doing It"}, {"url": "mitworld_bezos_okki", "desc": "One of the web\u2019s master entrepreneurs has devised a novel way to expand his domain. Jeff Bezos explains how Amazon, already home to 59 million active customers worldwide, hopes to beguile increasing numbers of developers to use web services that the company evolved for its own operations.\r\n\r\nBezos\u2019 plan involves renting out the \u201cguts of Amazon\u201d -- the servers and software code and networking behind the online shopping giant. He describes a trio of services. The first, Mechanical Turk, named for a 19th century chess automaton (actually run by a human) \u201cmakes it possible to encode human intelligence inside a software application,\u201d Bezos informs us. At Amazon, Mechanical Turk employs simple software to allow individuals to \u201cvote\u201d on product detail pages to help eliminate duplicate images and products. Work traditionally done by an in-house unit can be performed by a distributed group of Internet users, at their own convenience and for little cost. Bezos is making this software routine available to outsiders now, for such applications as podcasting transcription.\r\n\r\nAmazon\u2019s Simple Storage Service (S3) gives users access to Amazon\u2019s massive data storage capacity for an annual subscription fee. For small businesses worried about buying up to the next level of server capacity, S3 provides a welcome alternative, with its multiple data centers and relatively low cost, says Bezos. Businesses can also take advantage of this same digital network, through Elastic Compute Cloud, to run applications. This \u201callows for elastic scaling up\u201d of computing tasks, says Bezos. Many organizations need to test for bugs as they take applications to larger scales, and would prefer not to commit new resources if they don\u2019t have to. They also may require only intermittent machine time. Amazon\u2019s computers stand ready to serve, at 10 cents per CPU hour, and 20 cents per gigabyte of data transferred, says Bezos.\r\n\r\nPeople are excited, says Bezos, \u201cbecause they see a hint of what the future may be.\u201d The reality of taking an idea to a successful product involves lots of obstacles -- what Bezos describes as \u201cundifferentiated, heavy-lifting infrastructure.\u201d This \u201cmuck\u201d has to be of the highest quality, and often costs an arm and a leg. Amazon\u2019s web-scale computing enables users \u201cto get rid of as much of the muck as possible,\u201d says Bezos.", "recorded": "2006-09-27T14:30:00", "title": "Opening Keynote and Keynote Interview with Jeff Bezos"}, {"url": "kdd", "desc": "The annual **[[http://www.kdd.org/conferences|ACM SIGKDD conference]]** is the premier international forum for data mining researchers and practitioners from academia, industry, and government to share their ideas, research results and experiences. \r\n\r\n**Mission Statement**\r\n\r\n**[[http://www.kdd.org/|SIGKDD]]** aims to provide the premier forum for advancement and adoption of the \"science\" of knowledge discovery and data mining. SIGKDD will encourage:\r\n\r\n    * basic research in KDD (through annual research conferences, newsletter and other related activities)\r\n    * adoption of \"standards\" in the market in terms of terminology, evaluation, methodology\r\n    * interdisciplinary education among KDD researchers, practitioners, and users\r\n\r\nSIGKDD activities include the annual Conference on Knowledge Discovery and Data Mining and the SIGKDD Explorations Newsletter. You can also read about the charter, organization, or the annual SIGKDD awards using the navigation bar on the left.\r\n\r\nMembership benefits of joining SIGKDD include: a discount in the KDD conference registration, discounts in SIGKDD partner conferences, subscription to the SIGKDD newsletter, and a chance to make a difference in the KDD field. \r\n\r\n**Motivation**\r\n\r\n The field of Knowledge Discovery and Data Mining has grown rapidly in recent years. Massive data sets have driven research, applications, and tool development in business, science, government, and academia. The continued growth in data collection in all of these areas ensures that the fundamental problem which KDD addresses, namely how does one understand and use one's data, will continue to be of critical importance across a large swath of organizations.\r\n\r\nThe Knowledge Discovery and Data Mining field stands today where the database field stood about 15 years ago. There are over a 100 companies providing data-mining tools and applications, consulting and seminars, and even specialized hardware. The field is in the early adoption phase in the market, and we expect that within about 3-5 years, commercial products and the vendors will start entering the maturation phase. Within the next 10 years, in some form, the technology of data mining and knowledge discovery in data will become an integral part of the client/server enterprise information technology.\r\n\r\nComplementing business activities, there is considerable emerging research in the fields of databases, machine learning, applied statistics, visualization, and other fields relevant to data mining and knowledge discovery. This is evidenced by the recent emergence of successful KDD conferences in the US, Europe, and Asia as well as the successful launch in 1997 of the Data Mining and Knowledge Discovery Journal >>. ", "recorded": "2010-06-17T16:19:20", "title": "The ACM SIGKDD Conference Series - International Conference on Knowledge Discovery & Data Mining"}, {"url": "mitworld_christopher_nuclear", "desc": "\u201cThis machine of ours is running out of control\u201d is Thomas A. Christopher\u2019s sobering assessment of the consequences we face as a result of our insatiable appetite for energy. Christopher provides lucid and detailed descriptions of energy markets, nuclear and power plant design and operations, and a blunt message about our energy future.\r\n\r\nAs goes the cost of fuel, so goes the price of electricity, Christopher warns. And the cost of fuel, whether coal, which produces 60-70% of our electricity, or natural gas, is spiking upward. Christopher sees resource shortages today unlike any he\u2019s ever known, and electricity markets are consequently extremely volatile. Consumers are buffered \u2013 for the moment \u2013 by legislation many states impose on utilities. But fuel prices continue their rise, because China and India import U.S. coal; the U.S. increasingly imports natural gas; and utilities expensively retrofit plants to reduce emissions. Domestic demand increases 3% per year, so electricity costs will follow. Depending on where you live in the U.S., Christopher says to expect rate increases of 15% to 50% a year.\r\n\r\nAnd this is where nuclear power comes in; the economics make it inevitable, Christopher says. A combination of slimmed-down reactor construction designs and a less cumbersome federal permitting process will make possible a new generation of nuclear plants - the first to come online in decades. Christopher describes 17,000- page permitting documents detailing such safety features as how a plant will withstand the crash of a fully loaded jumbo jet into its reactor containment building or spent fuel pit. With the growing demand for renewable energy, the U.S. government is attempting to encourage the first handful of these $4- to $6-billion projects by backing up bank loans. After Seabrook and Shoreham led to protracted licensing processes and escalating construction costs, few banks want to be first to jump into the financing game, notes Christopher.\r\n\r\nWhile the new plants have larger capacities (1650 megawatts) than the 104 older plants running in the U.S., and even if several should come online by 2015 (the earliest projections), Christopher points out that U.S. demand is growing by 20 to 30 thousand megawatts per year, and energy conservation won't cut into this demand sufficiently to keep prices down. Because of spiraling costs, \"The country needs nuclear power as one part of the electricity generation mix.\" Nevertheless, he concludes, \"given that nuclear is only 20% of the content, the truth is our society is going to have to get adjusted to high electricity prices.\"", "recorded": "2007-11-05T09:07:47", "title": "The U.S. Energy Crisis and the Role of New Nuclear Plants"}, {"url": "mitworld_doya_cmbgf", "desc": "As a mathematical engineer, Kenji Doya approaches the goal of describing the most intricate brain mechanisms from a computational perspective. He constructs models of reinforcement learning involving the networked structures of the basal ganglia. His efforts are captured and expressed quantitatively as probabilities, regressions, and algorithms.\r\n\r\nIn this presentation, Doya covers basic concepts of reinforcement learning, then surveys the last decade of inquiry into the components of the basal ganglia circuit governing voluntary motion. Among the topics: action values, action candidates, and reward prediction involving the neurotransmitter dopamine; model-free versus model-based learning strategies; and the essential role of serotonin as modulator in the complex information loop.\r\n\r\nDoya\u2019s recent research is carried out via robots he calls \u201ccyber rodents.\u201d His dream as an undergraduate was to \u201cbuild a robot that learns the variety of behaviors on its own.\u201d That is, the computer, not the human engineer, teaches the robot to move. He accomplished this in designing a machine-creature exhibiting emotion-like attributes characterized as \u201cdepression,\u201d \u201cimpulsivity,\u201d \u201cgreed,\u201d and \u201cpatience.\u201d\r\n\r\nDoya believes the \u201cmetaparameters\u201d of reinforcement learning must be \u201ctuned appropriately\u2026Otherwise the performance of your learning is very, very poor.\u201d The iterative process involves three terms -- the reward itself, the expected reward for a new state based on choice of action, and memory of the reward gained in the previous state. In the comparison, any differential greater than zero can be exploited for learning. The tradeoff: \u201cNo pain, no gain.\u201d\r\n\r\nAs research advanced to increasing levels of structural specificity, Doya posited that \u201cthere seems to be spatial segregation in the function\u201d of basal ganglia components. Specialization in aspects of reinforcement learning is now seen, for instance, in ventral versus dorsal areas of the striatum.\r\n\r\nDifferentiation is also found in the cortico-basal ganglia information network: not a simple closed loop, but parallel electrical pathways conducting distinct neural operations. Further, the neuromodulators each have their respective missions. Dopamine encodes the temporal difference error -- the reward learning signal. Acetylcholine affects learning rate through memory updates of actions and rewards. Noradrenaline controls width or randomness of exploration. Serotonin is implicated in \u201ctemporal discounting,\u201d evaluating if a given action is worth the expected reward. Doya reminds us that clinically \u201cit is well known that the serotonin function is impaired in the depression patient.\u201d\r\n\r\nThe system of basal ganglia components and neuromodulators requires dynamic balancing. A delicate interplay determines outcomes for learning, actions, and affective states. Doya\u2019s synthetic models are proxies for human behavior, and his computational framework describing the moving parts ultimately has therapeutic implications for psychiatric and neurological disorders.", "recorded": "2009-05-07T13:30:00", "title": "Computational Models of Basal Ganglia Function"}, {"url": "eml07_sonnenburg_lsl", "desc": "In applications of bioinformatics and text processing, such as splice site recognition and spam detection, large amounts of training sequences are available and needed to achieve sufficiently high prediction performance on classification or regression tasks. Although kernel-based methods such as SVMs often achieve state-of-the-art results, training and evaluation times may be prohibitively large. When single kernel computation time is already linear (w.r.t. the input sequences) it seems difficult to achieve further speed ups. In this work we describe an efficient technique for computing linear combinations of string kernels using sparse data structures such as explicit maps, sorted arrays and suffix tries, trees or arrays [5]. As computing linear combinations of kernels make up the dominant part of SVM training and evaluation, speeding up their computation is essential. Considering the recently proposed and successfully used linear time string kernels, like the Spectrum kernel [2] and the Weighted Spectrum kernel [3] we show that one can accelerate SVM training by factors of 7 and 60 times, respectively, while requiring considerably less memory. Our method allows us to train string kernel SVMs on sets as large as 10 million sequences [4]. Moreover, using these techniques the evaluation on new sequences is often several thousand times faster, allowing us to apply the classifiers on genome-sized data sets with seven billion test examples [6]. The presented algorithms are implemented in our Machine Learning toolbox SHOGUN for which the source code is publicly available at http://www.shogun-toolbox.org. \\\\ References: [1] D. Gusfield. Algorithms on strings, trees, and sequences. Cambridge University Press, 1997. [2] C. Leslie, E. Eskin, and W. S. Noble. The spectrum kernel: A string kernel for SVM protein classification. In R. B. Altman, A. K. Dunker, L. Hunter, K. Lauderdale, and T. E. Klein, editors, Proceedings of the Pacific Symposium on Biocomputing, pages 564\u2013575, Kaua\u2019i, Hawaii, 2002. [3] G. R\u00a8atsch and S. Sonnenburg. Accurate Splice Site Prediction for Caenorhabditis Elegans, pages 277\u2013298. MIT Press series on Computational Molecular Biology. MIT Press, 2004. [4] S. Sonnenburg, P. Philips, G. Schweikert, and G. R\u00a8atsch. Accurate splice site prediction using support vector machines. BMC Bioinformatics, 8, 2007. [5] S. Sonnenburg, G. R\u00a8atsch, and K. Rieck. Large-scale learning with string kernels. In L. Bottou, O. Chapelle, D. DeCoste, and J. Weston, editors, Large Scale Kernel Machines, Neural Information Processing Series, pages 73\u2013104. MIT Press, Cambridge, MA, 2007. [6] S. Sonnenburg, A. Zien, and G. R\u00a8atsch. ARTS: Accurate Recognition of Transcription Starts in Human. Bioinformatics, 22(14):e472\u2013480, 2006.", "recorded": "2007-12-08T07:55:00", "title": "Large Scale Learning with String Kernels"}, {"url": "mitworld_dower_cultrueswar", "desc": "The Bush administration began its \u201cgreat misuse of history\u201d shortly after 9/11, says\nJohn Dower, when it seized upon Japan\u2019s 1941 Pearl Harbor attack as a useful analogy, a way to promote its own invasion of Iraq and subsequent occupation. Dower views as simplistic these \u201cpopular hooks to history\u201d and mercilessly slashes away at the Bush administration\u2019s continuing efforts to manipulate the public with historical imagery and example. Yet, with his more refined historical lens, Dower finds some unsettling areas of congruence between those days and our own times.\n\nReflecting on popular associations between 9/11 and Iraq, and Pearl Harbor and Japan, Dower offers two lines of analysis (and suggests he\u2019s got a few more up his sleeve): what he calls \u201ca Pearl Harbor code,\u201d and \u201cGround Zero 2001 and Ground Zero 1945.\u201d The first area involves comparing explanations of failures of intelligence that might have anticipated the attacks. Congressional and other investigations of the 1941 and 2001 attacks reveal that despite lots of \u201cnoise and chatter,\u201d intelligence agencies grossly miscalculated and missed enemy intentions. This represents \u201cnot just system breakdown, but a stunning failure of the imagination,\u201d says Dower. In both cases, the U.S. was caught unawares because it misjudged the enemy in a manner typical of \u201cwhite supremacists,\u201d simultaneously diminishing the other side\u2019s capabilities and casting it as irrational or illogical. In an ironic aside, Dower notes that the Japanese launched their war on \u201ca wish and a prayer, with no contingency planning and no serious contemplation of worst case scenarios.\u201d How like the \u201cU.S. strategic imbecility in the Iraqi invasion,\u201d he says.\n\nDower\u2019s second analytical line describes how a \u201cclash of civilizations\u201d argument has emerged powerfully since 9/11. Americans believe that Ground Zero 2001 marked the start of a new era -- the West opposing an Islamic culture that devalues human life. But Dower shows that a war machine targeting civilians and noncombatants went into high gear during World War II, with the U.S. and British air wars against Germany, then Japan. Airborne slaughter of innocents became standard operating procedure, part of an \u201cideological group think we associate with cultures of war.\u201d Victims are no longer individual civilians, but entire nations. Hiroshima and Pearl Harbor became \u201ccodes for mass destruction and psychological warfare,\u201d adopted by both bin Laden and the U.S. -- \u201cone side using this as a model for the horrors of 9/11, the other finding inspiration in what we call the cutting edge of shock and awe, tactics that were presumably to ensure victory in the invasion of Iraq.\u201d", "recorded": "2009-04-07T14:53:01", "title": "Cultures of War: Pearl Harbor/Hiroshima/9-11/Iraq"}, {"url": "mitworld_gellerman_breazeal_turkle_sr", "desc": "Cynthia Breazeal makes social robots, machines with the capacity to interact with people on psychological terms. She says they \u201copen up a new world of questions.\u201d But these increasingly sophisticated devices make Sherry Turkle uneasy, since they challenge the idea of human relationships and the very \u201cpurpose, importance, of living things.\u201d\n\nSince inventing her famously expressive, anthropomorphic Kismet, a robot that engages and learns from people through auditory, facial and social cues, Breazeal has evolved her work using robots as a scientific tool for social understanding. Her labs are putting robots through the paces of major child development milestones, such as appreciating the mental states of others. For instance, robot Leonardo has rudimentary object permanence, inferring from a tricky human\u2019s behavior where a Big Bird toy has been hidden.\n\nAnother project uses robots in home-based weight management studies, where they cue dieters to provide information on food intake, and provide moral support to wavering calorie counters. People form emotional attachments and name their robot partners, says Breazeal, and the robot method easily outperforms pen and paper, or computers, in helping people stick with their programs.\n\nAnother effort involves the Huggable, a teddy bear robot that acts via an internet connection to allow a distant grandparent to touch and play with the grandchildren -- \u201cas a new kind of communication media.\u201d And Breazeal provides a first-view of the MDS, a semi-autonomous robot that will combine state-of-the-art mobility, dexterity and social interaction.\n\nThis new species of extremely appealing, touchy, feely, humanoid machine puts Sherry Turkle on edge. She sees society on the verge of a \u201crobotic moment,\u201d as plugged in, instant messaging, virtual world denizens increasingly embrace machines as \u201ccreatures they feel a desire to connect with and nurture.\u201d She believes people are passionately attaching themselves to sociable robots, and fantasizing a reciprocal interest from these machines. \u201cYou care about them and want them to care about you. Nurturance turns out to be the killer app in robotics.\u201d She describes a graduate student who would gladly trade in her boyfriend for a robot exhibiting \u201ccaring human behavior.\u201d\n\nThere is a danger that we\u2019ll become accustomed to superficial cyber connections, and develop lower expectations for human to human interactions, says Turkle. Cyber intimacy may lead to cyber solitude. And you can turn off a robot when it bores you, or conversely, depend on it to \u201clive\u201d forever, while human relations come with endless baggage, complexities and sometimes unhappy endings. Says Turkle, \u201cRoboticists have come to speak of \u2018I Thou\u2019 relationships with machines, but what is the value of interactions that contain no understanding of us and that contribute nothing to the shared store of human meaning? These are not questions with ready made answers.\u201d", "recorded": "2008-04-29T08:31:00", "title": "Sociable Robots"}, {"url": "mitworld_belcher_nocera_rnt", "desc": "No single new technology can deliver limitless and clean energy, but Daniel Nocera and Angela Belcher are optimistic that they can harness the physical and natural worlds to move toward this goal.\n\nBelcher looks to ancient ocean organisms for her inspiration. The biocomposite materials that make up abalone shells or diatoms, which evolved over millions of years, are durable and exquisitely designed at the nano level. Belcher poses an \u201cinteresting question: Why didn\u2019t the organism make other materials, like solar cells, batteries, or traditional fuel cells? ....We say, they haven\u2019t had the opportunity yet, let\u2019s give them the opportunity.\u201d\n\nHer goal is to engineer these organisms so that their DNA codes for the synthesis of an efficient battery or solar cell, for instance. \u201cIt seems crazy,\u201d admits Belcher, but she points to a photo of her son, to whom she\u2019s passed on the genetic information that\u2019s given rise to his flesh and bones. Why not take the same principles and direct a microorganism to construct itself into a useful machine, Belcher suggests. \u201cWith the right ingredients, it would assemble itself,\u201d she says. Using natural materials would ensure \u201cenvironment-friendly processing\u201d that produces little waste. Indeed, the yeasts used in beer could \u201cbrew semiconductors for solar cells as well,\u2019 says Belcher.\n\n\u201cWhat will be the oil of the future, my nirvana?\u201d asks Daniel Nocera. The answer is deceptively simple: water plus light. Nocera is trying to emulate plants, which story the energy of sunlight: \u201cEvery time you eat a green leafy vegetable, you\u2019re literally chewing photons of the sun, releasing photons of the sun.\u201d Nocera \u201cdoes artificial photosynthesis\u201d, which he believes \u201cour future has to evolve to.\u201d\n\nThe challenge lies in how to capture and convert the energy created by splitting water with sunlight. Nocera says \u201cWe don\u2019t know how to make photovoltaics cheaply,\u201d but we must learn quickly.\n\nRight now humans globally require 13 trillion watts (or terawatts) of power. By 2050, we\u2019ll need 28 terawatts. Nocera pokes holes in some hypothetical scenarios offered to achieve this objective. If you gave over every square inch of cropland on the face of the earth to biomass production, you\u2019d only get 7 additional terawatts. Plus, \u201cyou couldn\u2019t eat anymore.\u201d You\u2019d still need to add 8,000 nuclear power plants, by building a new plant every 1.6 days for the next 45 years; put wind turbines everywhere; and dam every available river, to approach the 28 terawatt goal.\n\nThese technologies don\u2019t scale up realistically, says Nocera, so we must look to the sun, which in one hour puts out as much energy as humans use during an entire year.", "recorded": "2006-10-25T11:17:00", "title": "The Role of New Technologies in a Sustainable Energy Economy"}, {"url": "ecmlpkdd08_jain_dcyb", "desc": "The practice of classifying objects according to perceived similarities is the basis for much of science. Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms in to taxonomic ranks: domain, kingdom, phylum, class, etc.). Cluster analysis is the formal study of algorithms and methods for grouping objects according to measured or perceived intrinsic characteristics. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes cluster analysis (unsupervised learning) from discriminant analysis (supervised learning). The objective of cluster analysis is to simply find a convenient and valid organization of the data, not to establish rules for separating future data into categories. The development of clustering methodology has been a truly interdisciplinary endeavor. Taxonomists, social scientists, psychologists, biologists, statisticians, engineers, computer scientists, medical researchers, and others who collect and process real data have all contributed to clustering methodology. According to JSTOR, data clustering first appeared in the title of a 1954 article dealing with anthropological data. One of the most well-known, simplest and popular clustering algorithms is K-means. It was independently discovered by Steinhaus (1955), Lloyd (1957), Ball and Hall (1965) and McQueen (1967)! A search via Google Scholar found 22,000 entries with the word clustering and 1,560 entries with the words data clustering in 2007 alone. Among all the papers presented at CVPR, ECML, ICDM, ICML, NIPS and SDM in 2006 and 2007, 150 dealt with clustering. This vast literature speaks to the importance of clustering in machine learning, data mining and pattern recognition. A cluster is comprised of a number of similar objects grouped together. While it is easy to give a functional definition of a cluster, it is very difficult to give an operational definition of a cluster. This is because objects can be grouped into clusters with different purposes in mind. Data can reveal clusters of different shapes and sizes. Thus the crucial problem in identifying clusters in data is to specify or learn a similarity measure. In spite of thousands of clustering algorithms that have been published, a user still faces a dilemma regarding the choice of algorithm, distance metric, data normalization, number of clusters, and validation criteria. A familiarity with the application domain and clustering goals will certainly help in making an intelligent choice. This talk will provide background, discuss major challenges and key issues in designing clustering algorithms, summarize well known clustering methods, and point out some of the emerging research directions, including semi-supervised clustering that exploits pairwise constraints, ensemble clustering that combines results of multiple clusterings, learning distance metrics from side information, and simultaneous feature selection and clustering.", "recorded": "2008-09-14T18:00:00", "title": "Data Clustering: 50 Years Beyond K-means"}, {"url": "mitworld_belcher_nocera_tron", "desc": "No single new technology can deliver limitless and clean energy, but Daniel Nocera and Angela Belcher are optimistic that they can harness the physical and natural worlds to move toward this goal.\r\n\r\nBelcher looks to ancient ocean organisms for her inspiration. The biocomposite materials that make up abalone shells or diatoms, which evolved over millions of years, are durable and exquisitely designed at the nano level. Belcher poses an \u201cinteresting question: Why didn\u2019t the organism make other materials, like solar cells, batteries, or traditional fuel cells? ....We say, they haven\u2019t had the opportunity yet, let\u2019s give them the opportunity.\u201d\r\n\r\nHer goal is to engineer these organisms so that their DNA codes for the synthesis of an efficient battery or solar cell, for instance. \u201cIt seems crazy,\u201d admits Belcher, but she points to a photo of her son, to whom she\u2019s passed on the genetic information that\u2019s given rise to his flesh and bones. Why not take the same principles and direct a microorganism to construct itself into a useful machine, Belcher suggests. \u201cWith the right ingredients, it would assemble itself,\u201d she says. Using natural materials would ensure \u201cenvironment-friendly processing\u201d that produces little waste. Indeed, the yeasts used in beer could \u201cbrew semiconductors for solar cells as well,\u2019 says Belcher.\r\n\r\n\u201cWhat will be the oil of the future, my nirvana?\u201d asks Daniel Nocera. The answer is deceptively simple: water plus light. Nocera is trying to emulate plants, which story the energy of sunlight: \u201cEvery time you eat a green leafy vegetable, you\u2019re literally chewing photons of the sun, releasing photons of the sun.\u201d Nocera \u201cdoes artificial photosynthesis\u201d, which he believes \u201cour future has to evolve to.\u201d\r\n\r\nThe challenge lies in how to capture and convert the energy created by splitting water with sunlight. Nocera says \u201cWe don\u2019t know how to make photovoltaics cheaply,\u201d but we must learn quickly.\r\n\r\nRight now humans globally require 13 trillion watts (or terawatts) of power. By 2050, we\u2019ll need 28 terawatts. Nocera pokes holes in some hypothetical scenarios offered to achieve this objective. If you gave over every square inch of cropland on the face of the earth to biomass production, you\u2019d only get 7 additional terawatts. Plus, \u201cyou couldn\u2019t eat anymore.\u201d You\u2019d still need to add 8,000 nuclear power plants, by building a new plant every 1.6 days for the next 45 years; put wind turbines everywhere; and dam every available river, to approach the 28 terawatt goal.\r\n\r\nThese technologies don\u2019t scale up realistically, says Nocera, so we must look to the sun, which in one hour puts out as much energy as humans use during an entire year.", "recorded": "2006-10-25T11:47:45", "title": "The Role of New Technologies in a Sustainable Energy Economy"}, {"url": "mitworld_moss_boulanger_chin_geyer_jenkins_wnml", "desc": "Under new leadership, MIT\u2019s Media Lab has shifted gears significantly. This forum gives viewers a sense of the Lab\u2019s current priorities, via an overview by the director and three student presentations.\r\n\r\nFrank Moss initially laughed at the headhunter aiming to recruit him to the Media Lab, but reconsidered after reflecting on his kids\u2019 pointed comments: \u201cYou\u2019ve sold software to fat, white guys in IT departments all your life. When are you going to give something back to society?\u201d\r\n\r\nIn conversation with Henry Jenkins, Moss describes his vision of \u201cinventing a better future, in which technology can impact people at a deeper level, beginning with people who are disabled, disadvantaged, or disenfranchised.\u201d Targeting these groups will lead to inventions that impact society as a whole, believes Moss.\r\n\r\nMoss hopes Lab researchers will develop designs that enable more intimate interactions between humans and technology; that open up new ways for creativity and learning to change our lives; and that allow for a rethinking and simplification of \u201ccommon elements in our environment.\u201d\r\n\r\nHe introduces three young exemplars of the Media Lab\u2019s new focus. Adam Boulanger uses \u201cfacilitative technologies to break the mold,\u201d by handing music composition software to severely disabled patients in a Tewksbury, Massachusetts hospital. Hyperscore, says Boulanger, has enabled \u201cnew modes of interaction, new social interactions and empowerment\u201d among patients with psychiatric disorders, spina bifida, and Alzheimer\u2019s disease. He\u2019s working on broadening this software to provide useful interventions in autism, and to detect cognitive decline.\r\n\r\nRyan Chin\u2019s research focuses on ways to complement the increasing density of the world\u2019s cities with appropriate car design. City Car is a two-passenger electric vehicle that folds up (to four feet) so it can be conveniently stacked in small spaces in city centers and neighborhoods, and at commuter stations. Think shopping cart, says Chin. The concept challenges fundamental ideas of car ownership and function, since it\u2019s \u201cmore a computer on wheels,\u201d says Chin and is intended for shared, community use. But 504 of these vehicles fit on a city block that normally can accommodate only 82 parked cars, and when stationary, these cars can return some of their energy back to the grid.\r\n\r\nBiomechanical devices represent perhaps the ultimate in human-machine interaction. Hartmut Geyer works on ankle and knee prostheses, applying an understanding of the human gait -- the nerve signals and muscle actions required to move in different ways -- to create more responsive devices for amputees. Signals from the residual limb of the amputee tell the prosthesis how to respond during a particular activity like walking upstairs. Eventually, says Geyer, electrodes may be implanted into nerve fibers so that the brain can directly control the prosthesis, or the prosthesis can send signals to sensory fibers \u201cso maybe the amputee wearing it can feel what he\u2019s stepping on\u2014maybe sand, maybe concrete.\u201d", "recorded": "2007-03-01T09:20:11", "title": "What\u2019s New at the Media Lab?"}, {"url": "mitworld_malone_pentland_lakhani_ci", "desc": "Can human beings, with the help of smart machines, not merely avoid \u201ccollective idiocy\u201d (in Sandy Pentland\u2019s words), but actually achieve a degree of intelligence previously unattainable by either humans or machines alone? These three panelists study the possibilities from different angles.\r\n\r\nThomas Malone\u2019s Center for Collective Intelligence examines such evolving intelligent systems as Wikipedia, which relies on a veritable army of volunteers to \u201ccreate a high quality product with almost no centralized control,\u201d and Google, with its technology \u201charvesting knowledge\u201d and serving up answers to a vast audience of seekers. While a crowd doesn\u2019t guarantee the best solution to a problem, Malone sees opportunities in \u201cprediction markets,\u201d where humans, with the computational help of computers, predict things with greater accuracy than single experts, whether in electoral politics, or in medical diagnostics. Malone\u2019s research is also attempting to set up metrics to measure the intelligence of these new human group-machine hybrids, and ways of applying collective decision making to climate change policy.\r\n\r\nAlex (Sandy) Pentland performed a unique experiment in a large German bank, tagging its employees with special badges that tracked individuals\u2019 interactions, down to head nodding, body language, and tone of voice. His research, conducted over a month, looked at how face to face interactions played into the overall organizational flow. The patterns he uncovered in the data collected from his name badges and from email and more traditional documentation, demonstrated the significance of social dynamics in workplace productivity. Certain individuals acted as information bottlenecks; others as polarizers, group thinkers, or gossip mongers. Pentland shared information about these patterns of communication with individuals. \u201cRather than think of this as big brother,\u201d says Pentland, \u201cthink of this as a personal intelligence tool that collectively produces better results.\u201d Related technology might be able to detect depression by examining a person\u2019s patterns of socialization.\r\n\r\nKarim R. Lakhani says he \u201cstumbled into collective intelligence and distributed information systems as a puzzle.\u201d While trying to market his large corporation\u2019s medical imaging system, he discovered that a small Canadian group had \u201cleapfrogged\u201d his R&D team. A community of radiologists and physicists pooled their expertise to improve imaging technology, and beat a large, centralized lab. Since that time, Lakhani has pursued other examples of decentralized groups of people with a wide range of motivations, efficiently cracking complex problems-- from the open source software community, to biotech labs and entrepreneurial ventures. A T-shirt company, Threadless, asks its online community of a half million to submit T-shirt designs, and vote on them. The best scoring designs go into production. Sales are closing in on 1.5 million shirts at $20 a pop. Says Lakhani, \u201cOne hope of collective intelligence is that it takes the distributed and sticky pockets of knowledge that exist in the world and finds ways to aggregate them for us.\u201d", "recorded": "2007-10-04T11:39:22", "title": "Collective Intelligence"}, {"url": "mitworld_hockenberry_herr_ha", "desc": "These two MIT Museum speakers hope you\u2019ll walk away from their talk with a good case of augmentation envy \u2013 or at least a healthy respect for what technology can do for the human body and soul.\n\nJohn Hockenberry has used a wheelchair for 30 years, since a car accident left him a paraplegic. He tells us the public has viewed spinal cord injuries like his as \u201csomething horrific,\u201d or \u201cstaggeringly poignant.\u201d But in the last 10 years, disability has moved from being \u201can extraordinarily fringe activity\u201d to a central issue facing society, that of \u201cmarrying technology with humanity in a way that is organic to the body, appropriate to the spirit and sustainable to the community.\u201d Hockenberry believes that the needs and demands of disabled people are helping push science toward creating a set of design principles \u201cthat will allow this issue of human restoration and augmentation to merge into a kind of seamless unity.\u201d\n\nIn illustration of this claim, Hugh Herr describes the astonishing strides engineers are making in the development of \u201cHuman 2.0.\u201d He starts with himself -- a victim of frostbite during a 1982 mountain climbing accident. After losing both feet below the knee, Herr headed for the machine shop, and realized he didn\u2019t have to accept the version of his body provided by nature. So he cobbled together a pair of prostheses perfect for climbing (which made him over 7 feet tall), followed by other foot-ankle replacements made lightweight and responsive through carbon composite materials and computers. These designs are better than his originals, suggests Herr. \u201cWhat\u2019s fun about having part of your body artificial is that you can upgrade. It\u2019s depressing to me, too bad that you folks have biological limbs.\u201d\n\nWars in Iraq and Afghanistan have fueled the work in Herr\u2019s lab. He\u2019s now building robotic versions of arms and legs that restore capability, using computers and powered systems with sensors and motors. Stroke victims can use similar models, wrapped around an impaired limb, to restore symmetry between their left and right sides. The big prize will be a neural interface, a way of growing and reactivating an amputated nerve, so that it begins to convey sensory information through the complex networks of the brain. \u201cThe dream here is that one day I and other people with limb amputations will not only be able to walk across a sandy beach but feel the sand against their prosthesis,\u201d says Herr.\n\nResearchers haven\u2019t imposed limits on their attempts at augmentation \u2013 or improvement. An MIT lab has designed a \u201csocio-emotional prosthesis,\u201d Herr tells us \u2013 using deep brain stimulators that leave subjects feeling \u201chappy, calm, content.\u201d Hockenberry wonders in conclusion whether we are \u201cblowing away the notion of normal entirely and creating a completely improvisational notion of what it means to be human.\u201d Herr proposes that in the future, \u201cwhen we have many, many types of intimate technologies that are inside and attached to our bodies, it will unleash a renaissance in expression.\u201d", "recorded": "2008-04-09T11:43:00", "title": "Human Augmentation"}, {"url": "mitworld_eecs_department", "desc": "**Mission**\n\nThe mission of the Electrical Engineering and Computer Science Department is to produce graduates who are capable of taking a leadership position in the broad aspects of electrical engineering and computer science.  Our graduates:\n\n    *      Understand the basic principles that underlie modern electrical, electronic and computational technology;\n    *      Are able to apply creatively their understanding of science and engineering principles to the solution of problems arising in whatever career path they choose;\n    *      Are sensitive to the environmental, social, safety and economic context in which their work is done, and possess a strong commitment to ethical practice within that context;\n    *      Are able to communicate their ideas and positions clearly and concisely, both orally and in writing;\n    *      Are aware of the requirement for and possess the ability to engage in lifelong learning which will be necessary for continuing high performance in whatever career path they choose.\n\n**Academic Program**\n\nUndergraduate students in the department take a common core of two subjects that serves as their introduction to electrical engineering and computer science. They then systematically build up broad foundations and depth in selected intellectual theme areas that match their individual interests. This is done through a small set of foundation subjects, on top of which students select a set of header subjects that build on the foundations, and lay further foundation for exploration in a selected set of intellectual themes. Laboratory subjects, independent projects, and research opportunities provide engagement with principles and techniques of analysis, design, and experimentation in a variety of fields. The department also offers a range of programs that enable students to gain experience in industrial settings, ranging from collaborative industrial projects done on campus to term-long experiences at partner companies.\n\nGraduate study in the department moves students towards mastery of areas of individual interest, through course work and significant research, often defined in interdisciplinary areas that take advantage of the tremendous range of faculty expertise in the department and, more broadly, across MIT.\n\n**Research**\n\nGiven the breadth of faculty within EECS, the department pursues a wide range of research topics. These range from theoretical computer science, computer systems and architecture, graphics, robotics, computer vision, machine learning, computational applications in medicine, computational biology, communications, information theory, control systems, large scale systems analysis, circuits, devices, power and energy, numerical computing, novel materials for devices, nanotechnology, manufacturing, biotechnology, speech and hearing, prosthetic devices, analog and hybrid circuits and devices, and many, many more.\n\n**Faculty**\n\nThe department\u2019s faculty members are widely recognized as leading figures in their fields. Included are more than 40 members of the National Academy of Engineering, more than 10 members of the National Academy of Sciences, several National Medal of Technology winners, as well as many Fellows of professional societies, such as the IEEE, ACM, APS, AAAI and others.\n\n**About the Host** - **[[http://www.eecs.mit.edu/|Electrical Engineering and Computer Science Department]]**\n\n**This MIT World Series** is available at http://mitworld.mit.edu/host/view/27", "recorded": "2001-09-24T09:00:00", "title": "MIT Electrical Engineering and Computer Science Department"}, {"url": "mitworld_lloyd_fsl", "desc": "These nine panelists describe ways in which the Second Law of Thermodynamics can be stretched, or applied in less traditional ways. Adrian Bejan has constructed a law that \u201ccovers every configuration in physics, from animate, to inanimate, to us, the societal.\" Bejan demonstrates how his law describes and predicts the tree-shaped flow of all rivers, animal locomotion and human settlement distribution. With it, says Bejan, \u201cthermodynamics becomes a science of systems with configuration\u2026\u201d\r\n\r\nBjarne Andresen acknowledges \u201cmany fights about the Second Law,\u201d before declaring his belief that \u201centropy survives as a concept, and applies equally in the chemistry lab, to the quantum computer and to black holes.\u201d He discusses the importance of carefully defining the system under examination beforehand, \u201cotherwise you get into fights with your neighbors.\"\r\n\r\nMiguel Rubi discusses how to use the Second Law to extract information about the evolution of small systems. Unlike \u201ccanonical thermodynamics,\u201d which describe systems in terms of energy, volume and mass, mesoscopic thermodynamics focuses on systems in terms of positions and movement of particles. Some examples of processes explicable by this kind of thermodynamics include the translocation of ions, RNA unfolding under tension, and muscular contractions.\r\n\r\nSigne Kjelstrup argues that mesoscopic nonequilibrium thermodynamics (MNET) can address a longstanding problem in classical nonequilibrium thermodynamics, by addressing \u201cactivated processes.\u201d Biological systems have heat flow, says Kjelstrup, and \u201cthat is as of yet not included in the description of enzyme kinetics. It should be there to quantify lost work in these important systems.\u201d\r\n\r\n\u201cAn important question arising in nonequilibrium thermodynamics is not just entropy but temperature,\u201d says David Jou, in particular, \u201cthe physical meaning of temperature.\u201d Jou invokes the extended thermodynamics of viscoelastic systems, and looks for a simple model valid for a modest range of equations.\r\n\r\nMiroslav Grmela suggests that any time one goes from details to some kind of pattern, \u201cthere is an entropy involved\u2026by providing some kind of dissipation, some pattern recognition process.\u201d Grmela believes that thermodynamics \u2026 \u201cfind a natural formulation in the setting of contact geometry.\u201d\r\n\r\nLyndsay Gordon\u2019s talk involves Maxwellian valves. He discusses \u201ca machine based on an osmophoretic engine,\u201d a simple system with a liquid membrane, solvent and solute, \u201cthat is fluctuating completely forever,\u201d without information. \u201cThis thing goes by itself,\u201d he says.\r\n\r\nEric Schneider discerns \u201claws of ecology\u201d in such gradient systems as the energy flow between the sun and earth. \u201cWe can determine \u201c\u2026heat and entropy production in the system,\u201d as well as \u201cecological successions and directional processes that directly tie them to Darwinian evolution.\u201d He advises his colleagues \u201cto encourage policy makers to use exergy analyses on future energy development projects.\u201d\r\n\r\nSymposium organizer George Hatsopoulos wraps up by noting \u201cthat as far as I know in thermodynamics, there is no statement that says the Second Law implies the increase of entropy. The Second Law only says that the entropy cannot decrease, but there\u2019s nothing wrong with entropy staying put.\u201d We have evidence that in some cases it appears the entropy increases, but that\u2019s not the \u201cSecond Law.\u201d", "recorded": "2007-10-04T13:21:37", "title": "Frontiers of the Second Law"}, {"url": "mitworld_debate_bb", "desc": "MIT produces students who are \u201cdeep, entrepreneurial, passionate, diverse and active,\u201d says Phillip Clay, the kind of talented individuals who should play major parts on the world stage. MIT has begun a drive to ensure that its students fulfill their promise. Central to this mission, Richard Samuels says, is the kind of education that steeps students in the realities of globalization. In a world that\u2019s not so much flat as converging and increasingly complex and diverse, students must \u201cstep boldly and intelligently into the global market of ideas and commerce,\u201d says Samuels, lest they \u201cbecome cogs in a global machine.\u201d MIT hopes \u201cto create the people who design and operate those machines.\u201d\r\n\r\nThis means making international studies a core part of the MIT experience, and establishing MIT in an international context. At a time when MIT faces increased global competition, Subra Suresh worries that flat and reduced federal research funding will cut into MIT\u2019s research preeminence. So the School of Engineering is seeking out partnerships around the world for faculty, and looking to provide its undergraduates with exchange and practicum opportunities abroad.\r\n\r\nAll over the world, \u201ccountries want to reproduce MIT,\u201d says Marc Kastner. But MIT\u2019s unique culture is difficult to replicate: the Institute pours resources into the youngest students and faculty; promotes an egalitarian atmosphere; draws instructors from an international talent pool; and is \u201cgreat in everything\u201d -- science, engineering, liberal arts and business. As MIT seeks out international alliances, \u201cWe must think about how to communicate to our partners what\u2019s important about our culture,\u201d he says.\r\n\r\nThe \u201ccrown jewel\u201d of MIT\u2019s international programs is the MIT International Science and Technology Initiative (MISTI), says Deborah Fitzgerald. More than 300 MIT students each year get to spend time working in a company in another country, at no expense to them. A program that often requires two years of language, history and culture study, MISTI boosts confidence, says Fitzgerald, allowing students to see themselves \u201cas people who can solve any kind of problem, anywhere, in a foreign language\u201d -- a \u201cgreat vindication of all they\u2019ve worked so hard for.\u201d Fitzgerald\u2019s wish is to make MISTI possible for every student.\r\n\r\nMIT Sloan is committed to developing principled and innovative leaders who can improve the world, says Dave Schmittlein. The school has developed a Center for Leadership that emphasizes \u201cvalues, transparency, consistency in decision making,\u201d and provides its budding leaders with international experience through a global entrepreneurship lab that operates in 17 different countries.\r\n\r\nAdele Naud\u00e9 Santos declares herself \u201cpassionately opposed to outposts\u201d in foreign lands, because it would be impossible to clone MIT\u2019s collaborative, multidisciplinary, nonhierarchical ethos. Instead, \u201cwe partner,\u201d she says. Students and faculty work and study with colleagues abroad in projects like the Urbanization Laboratory, which develops sustainable designs for new cities in such nations as India, China and Japan. Graduates in architecture and planning migrate to all corners of the globe, carrying their unique experience and MIT\u2019s culture with them.", "recorded": "2008-10-03T14:57:28", "title": "Beyond the Bench: Preparing MIT Students for the Challenges of Global Leadership"}, {"url": "mitworld_hutchkinson_williams_coderre_jasanoff_dan", "desc": "This session goes a long way toward demonstrating the \u201chappy face of the atom,\u201d as moderator David Kaiser puts it, replacing the mushroom cloud image with a multidimensional picture of the uses of nuclear technology.\r\n\r\nAs a plasma physicist, Ian Hutchinson works on controlled fusion -- a very hot area of nuclear technology in more ways than one. By fusing together isotopes of hydrogen, you can achieve the energy source of stars, says Hutchinson. This promises infinite reserves of clean energy. These reactions are only possible at super high temperatures, and \u201cto bring these down to a human scale,\u201d the gases created must be contained by powerful magnets in machines called tokamaks. MIT and other labs have produced fusion energy and now a major international project to create a large fusion reactor is under way. The big challenge, says Hutchinson, is understanding the \u201cgreat stirrings and eddies inside the plasma\u201d that cause gas leaks and disruption to the fusion process.\r\n\r\nWe are now entering a time when \u201cangst seems to be subsiding and we are able to discuss the benefits of nuclear technology in the security arena,\u201d says Dwight Williams. He describes some major upgrades to the detection devices commonly used to prevent people from getting \u201cbad stuff on an airplane or through a port.\u201d Williams explains active system devices, which can induce a radioactive signature in something that was not originally radioactive, and thus signal an item\u2019s \u201celemental content.\u201d A machine using thermal neutron activation analysis can penetrate all kinds of shielding, to produce gamma rays and a 3D image of the contents of a bag. Since explosives share some of the features of jam, marzipan and chocolate, says Williams, advanced nuclear techniques will help inspectors distinguish between the benign and dangerous.\r\n\r\nMedical applications of nuclear technology deploy different types of radiation to kill tumor cells and spare healthy tissue. But, says Jeffrey Coderre, shielding healthy cells to prevent radiation\u2019s side effects turns out to be a tricky proposition. Coderre investigated the nature of radiation damage and determined it was a function of damage to stem cells (rather than damage to blood vessels). He describes how the radioisotopes used in medical radiation, virtually all of which come from Canadian reactors, can be used in a variety of ways: to view areas of rapid bone growth, or tumor sites in bone; to sterilize syringes and drapes used in hospitals; and in a radiation helmet called the gamma knife to get focused radiation into difficult brain tumors.\r\n\r\nAlan Jasanoff provides a one-stop tour of medical imaging techniques, differentiating between those scans that use high energy radiation (such as computed tomography and positron emission tomography); and low wavelength radiation, based on radio waves, such as nuclear magnetic resonance imaging. PET scans detect molecular tracers that have been consumed in a sugary drink to find areas where cells are rapidly dividing, for example. New applications for this well established imaging method include locating plaques in the brain that cause Alzheimer\u2019s disease. MRI, unlike CT or PET scans, has minimal destructive impact on tissues, and allows 3D mapping of blood vessels, and more recently, the tracing of microscopic fibers in the brain. Jasanoff\u2019s lab uses calcium-sensitive contrast agents to detect events in the brain.", "recorded": "2007-03-08T09:18:00", "title": "Diverse Applications of Nuclear Technology"}, {"url": "cogsys2012_belpaeme_strategies", "desc": "Social behaviour happens in the here and now but relationships depend also on the past. When we interact with others we respond to how they behave but we also bring to the interaction our social history i.e. the sum total of our previous experiences of similar situations. Researchers are very interested to understand how humans and robots can relate socially but to date most of the work that carried out focuses on minute-by-minute interactive behaviour.\n\nThe aim of the ALIZ-E project is to explore how human-robot interactions can be extended from minutes to the scale of days thus forging longer-term constructive bonds between robot and user. The ALIZ-E project will use the principles of embodied cognitive robotics to create agents capable of sustaining believable, any-depth social relationships with young users, over an extended, potentially discontinuous timeframe.\n\nThe ALIZ-E project will specifically explore robot-child interaction capitalising on children\u2019s open and imaginative responses to artificial \u2018creatures\u2019. Promising future applications include the development of educational companion robots for child users. The ALIZ-E project will innovate in taking robots out of the lab and putting them to the test in a health education role, with young diabetic patients, in a busy paediatric department at the Ospedale San Raffaele in Milan.\n\nOne central scientific goal of ALIZ-E involves implementing memory systems to enable robots to engage in self-sustaining interactions. This requires that they should have the capacity to store and recall experiences, to learn from them and to adapt their social behaviour on the basis of their previous experiences. A distributed, \u201cswitch board\u201d model, in which memory provides the substrate through which other cognitive modalities interact, will be used to provide socially coherent, long-term patterns of behaviour. The robots will learn online through unstructured interactions in dynamic environments and number of different machine learning approaches will be integrated to facilitate this functionality. Cloud computing techniques will be used to provide off-board computing resources for robots interacting autonomously.\n\nReal world social interaction hinges on making appropriate responses to the behaviour of others thus another key aspect of ALIZ-E concerns understanding emotion in human\u2013robot interaction. For a robot to sustain a social relationship with a child, it must be able to interpret the emotional and affective content of the interaction and be able to produce behaviour signalling appropriate affective responses back to the child. Non-verbal behaviour will play an important part in the robots\u2019 social repertoire but in order to engage the user fully the robots will require the capacity to understand and to produce speech. Verbal and non-verbal behaviour will necessarily be tightly coupled with learning and memory to support the longer-term functioning of a social bond between robot and child.\n\nALIZ-E will use Aldebaran Nao robots as an implementation platform, the Nao is a small, autonomous, humanoid robot already widely used in robot soccer. The project, coordinated by Dr. Tony Belpaeme at the University of Plymouth, involves a consortium of 7 academic partners comprising The University of Plymouth, Vrije Universiteit Brussel (Belgium), Deutsches Forschungzentrum f\u00fcr K\u00fcustliche Intelligenz (Germany), Imperial College (UK), The University of Hertfordshire (UK), National Research Council - Padova (Italy) and The Netherlands Organization for Applied Scientific Research (The Netherlands) plus commercial partners GOSTAI (France) and Fondazione Centro San Raffaele del Monte Tabor (Italy). Funded under the European Commission 7th Framework Programme the ALIZ-E project began in April 2010 and will run for a total of 4.5 years.", "recorded": "2012-02-23T11:30:00", "title": "ALIZ-E - Adaptive Strategies for Sustainable Long-Term Social Interaction"}, {"url": "classconference2014_juricic_distributed_maintance", "desc": "According to a recent survey the direct maintenance costs per year in European industry\r\namount to 450 billion \u20ac of which 70 billion \u20ac (!) are estimated to be wasted through\r\nineffective maintenance. Moreover, the indirect costs caused by degraded product quality,\r\nreduced production efficiency, loss of customers etc. are at least of the same range of\r\nmagnitude as direct costs. A remedy is to move from outdated maintenance practices to\r\nmodern ones based on automated condition monitoring (CM), able to timely detect the\r\nonset of fault and localize the root-cause. Since the traffic of data and information\r\nprocessing go in electronic format such a maintenance approach is referred to as emaintenance.\r\n\r\nWhy are then so few online condition monitoring systems implemented in practice in spite\r\nof tremendous advances in key enabling technologies, including sensors, processors,\r\ncommunication means and information technologies?\r\n\r\nThe answer is not as easy as is the question. Namely, the area of condition monitoring and\r\ncondition-based maintenance is at the crossroads between technology, information services\r\nand human resource management. A closer look at the market landscape reveals that the\r\nbulk of manufacturers of on-line CM systems have targeted equipment from high cost range\r\nin sectors like power plants, wind mills and oil refineries, where high implementation costs\r\ncan be more easily compensated by incomes typical for those branches. The same solutions,\r\nif applied to the equipment from middle or lower cost range (typical for \u201cordinary\u201d\r\nindustrial sectors), would encounter problems. Namely such investments turn not to be\r\neconomically justifiable due to poor return of investment rates, overwhelming\r\nimplementation costs as well as high costs of ownership.\r\n\r\nThese issues challenged the development of a CM platform referred to as MEMS-PHM for\r\ncondition monitoring, prognostics and health management of industrial asset. It comprises\r\na portfolio of MEMS sensors, smart nodes, communication interfaces, a MIMOSA database\r\nand reports generation modules (see Fig.1). The smart node (SN) is a low-cost, energy\r\nefficient and programmable unit, which can be installed on a broad range of assets\r\ndistributed not only within a plant but also geographically. They perform periodical data\r\nacquisition and execute the dedicated signal processing algorithms used for diagnosis and\r\nprognosis. The results of processing are features that reflect the condition of the monitored\r\nmachine. The results of processing as well as recorded data are transferred by various\r\ncommunication protocols to the central server within the company or via internet on\r\nremote server. A notable feature of the system is its ability to store the data in a way fully\r\ncompliant with MIMOSA OSA-EAI standard. This open source environment enables\r\nintegration with existing maintenance management systems, MES (Manufacturing\r\nExecution Systems) and ERP (Enterprise Resource Management) systems resident in the\r\ncompanies. Hence fully e-maintenance functionality is guaranteed to the end user.\r\nMinimization of the life-cycle costs is also due to a rich library of award winning algorithms,\r\nuse of automatic code generation and novel self-tuning and learning capabilities.\r\n\r\nExtending the MEMS-PHM portfolio with modules that allow for taking advantages of cloud\r\ntechnologies is obvious. Hence the cross-breeding of the technologies is conceived to\r\nadditionally reduce the costs of CM solutions owing to access to data from anywhere and to\r\nanyone with approved access rights (developers, operators, maintenance people,\r\nmanagement) without having to add additional expensive infrastructure.\r\n\r\nThere are some barriers that will have to be considered, e.g. company access policies and\r\nfirewalls. Some companies will be reluctant to the open access to business data within the\r\ncloud/ web environment. The risk/ challenge will be to combine business critical and technical\r\ndata to provide the advanced diagnosis and maintenance information required.", "recorded": "2014-09-25T11:30:00", "title": "Distributed E-maintenance over cloud: rationale and expectations"}, {"url": "snnsymposium2010_haring_pdmb", "desc": "Organizator ne zeli objave. Davor\r\n\r\nOn 29.11.2010 14:53, Bert Kappen wrote:\r\n> dear davor,\r\n> see below\r\n> \r\n> ANNET: Kan je de slides van gavrila opsturen naar davo orlic?\r\n> \r\n> bert\r\n> \r\n> On Nov 29, 2010, at 1:40 PM, Davor Orlic wrote:\r\n> \r\n>> Dear Mrs. Wanders and Mr. Kappen,\r\n>>\r\n>> the VideoLectures.NET team would like to notify you that the talks from\r\n>> the SNN Symposium 2010: Intelligent Machines are prepared and can be \r\n>> found at the following address \r\n>> http://videolectures.net/echallenges2010. For reviewing the talks you \r\n>> can log in with the green button on the right upper side of the page, \r\n>> the password/username are both snn2010.\r\n>>\r\n>> The videos are non-public, and will be published to the wider audience\r\n>> within 3 days after you receive this email, unless you approve it\r\n>> beforehand.\r\n>>\r\n>> I would kindly ask you to consider the few pending details below:\r\n>>\r\n>> (*) Publishing consent and authorship statement: We are missing the \r\n>> signatures of all the speakers for the \"Panel discussion moderated by \r\n>> Bas Haring\". I have prepared one document with all the names and \r\n>> suggest you sign it as the organizer and notify the speakers, this \r\n>> will be the easiest way, trust me. Because it is going to be almost \r\n>> impossible to get the signatures of all of them, especially now after \r\n>> the conference is past a few weeks.\r\n> I am not happy with the panel discussion. I suggest to delete it from \r\n> videolectures. Is that possible?\r\n>>\r\n>> (*) Slides: Could you please provide us with your slides and those of \r\n>> Mr. Dariu Gavrilaand?\r\n> My slides of the introduction you already have, I think. the slides of \r\n> Gavrila will be sent to you.\r\n>>\r\n>> (*) Authors info: Could you please provide us the information about \r\n>> which institution Mr. Bas Haring belongs to?\r\n> Not relevant.\r\n>>\r\n>> (*) Dissemination: If possible please disseminate the link through the \r\n>> conference mailing list and wider and via other social media and \r\n>> networks, so that all that could not attend the event could benefit \r\n>> from it. This is very important as You organizers and project managers \r\n>> are in a unique position to do so, whereas we can only disseminate via \r\n>> our own social channels such as Twitter and FaceBook.\r\n> We will do that.\r\n>>\r\n>> (*) VideoLectures.NET official blog: The other fairly important thing \r\n>> is we started a pilot news section on the VideoLectures.NET official \r\n>> blog on the topic of \"voice of the organizers\", and I was wondering if \r\n>> you would be able to write down a couple of sentences why you decided \r\n>> to go with the filming of the workshop in general, what you think will \r\n>> be the impact of the videos, who is the population target. I believe \r\n>> the community would greet such a public \"blog\" type of interaction \r\n>> between viewers and organizers, since Videolectures is actually a \r\n>> prolonged classroom for your conference. How do you feel about \r\n>> participating in this? Just a few sentences.\r\n> The Intelligent Machines meeting that we organized on November 17 in \r\n> Nijmegen, the Netherlands aimed to bring together academic researchers \r\n> on the field of machine learning with industrial researchers and \r\n> government policy makers. The meeting specifically aims to show the \r\n> present and future strength of the field in the form of attractive high \r\n> level talks and is directed to a very broad audience. The videolectures \r\n> initiative is an excellent way to further spread this message to all of \r\n> those interested in this topic. The videolectures team is very well \r\n> organized and recording these lectures has been done all by them without \r\n> any effort on my side.\r\n> \r\n> \r\n>>\r\n>> Please let me know if we need to change anything on the page.\r\n> Please remove Bas Haring from the pages.\r\n>>\r\n>> all the best,\r\n>>\r\n>> Davor Orlic\r\n>> Center for knowledge transfer in information technologies\r\n>> Jozef Stefan Institute\r\n>> Jamova 39\r\n>> SI - 1000 Ljubljana\r\n>> Slovenia\r\n>> Tel.: +386 1 477 3148\r\n>> Fax.: +386 1 477 3935\r\n>> Personal: +386 31 844 381\r\n>>\r\n>> Publication Manager\r\n>> VideoLectures.NET\r\n>> http://videolectures.net/\r\n>> <SNN2010_publishing_consent_bulk.pdf><videolecturesnet net \r\n>> logo.jpg><videolectures-net-logo.eps>\r\n> \r\n\r\n", "recorded": "2010-11-17T16:30:00", "title": "Panel discussion moderated by Bas Haring"}, {"url": "classconference2014_robnik_services_approach", "desc": "The business idea \"Platform services\" addresses aggregation of fragmented competences of\r\nSlovenian ICT companies and introduces cutting-edge technologies and business models for\r\ncloud computing and other technologies that go beyond Infrastructure as a Service and\r\nPlatform as a Service (IaaS/PaaS). Platform services approach with pre-integrated services\r\nsold via industrial partners' sales channels extends the classical business co-operation among\r\npartners and reaches a wider range of customers. The variety of platform services that run on\r\nopen source cloud infrastructure is also a perfect match for the proposed cloud solutions for\r\npublic sector and business customers in Slovenia and abroad.\r\n\r\nPre-integrated \u201cPlatform services\u201d run in harmony on carrier rather IT cloud infrastructure\r\nincluding also legacy applications. Telecommunications and IT services, services for privacy,\r\nsecurity and trust, services for business intelligence and data analytics on big and open data\r\nand domain specific services are essential as contextual services that can be used via open\r\ninterfaces and their orchestration for a variety of vertical sectors (public sectors, energy, safe\r\nsociety, transport, smart city and community etc.). By adding also other technology enablers -\r\nlike machine type communication (M2M, IoT), network functions virtualization and\r\nsoftwarisation among others - this approach strengthens value chain and business\r\nopportunities significantly and brings a long-term sustainable model for innovation, new jobs\r\nand business growth.\r\n\r\nIndustrial partners within ecosystem (Iskratel, d.o.o., Kranj, Alpineon razvoj in raziskave, d.o.o.,\r\nAstec d.o.o., NIL Podatkovne komunikacije d.o.o., RC IKT, d.o.o., SMART COM D.O.O., XLAB D.O.O.,\r\nSkupina FMC d.o.o., ZZI D.O.O., I-ROSE, d.o.o., PRONET, Kranj, d.o.o., INVIDA d.o.o., SETCCE,\r\nd.o.o.) will work together with the research organizations (the University of Ljubljana, the\r\nFaculty of Computer and Information Science and the Faculty of Electrical Engineering, the\r\nJo\u017eef Stefan Institut) and Zavod E-Oblak. With the sale of Platform services by industrial\r\npartners the ecosystem of above-mentioned partners will promote cloud computing within\r\npublic sector and also within the ICT and vertical sectors essentially. Beside cloud computing this programme contributes to usage and usefulness of other technologies and also to success\r\nand innovation of individual partner and our customers.\r\n\r\nThe programme Platform Services will offer with the upgrade of commercially-available open\r\nsource solution for infrastructure and platform and with the support for legacy services and\r\nrich set of pre-integrated services for public sector also a perfect match for the Slovenian public\r\nsector cloud solutions.\r\n\r\nIt is worth pointing out that a field of Platform services usage in vertical sectors has already\r\nbeen recognized within the programme \u201cSafe Society\u201d that will bring innovative solutions for\r\nthe operational communications, services and applications for the 112 operational centres\r\n(PSAPs) in Slovenia and abroad. It will also offer solutions for operational and tactical\r\ncommunications and applications of the first responders (policy, fire brigades, ambulance, civil\r\nprotection, etc.) and applications for general public, safe objects and transport.\r\n\r\nA representative example of such solutions is a vehicle emergency call system eCall which is\r\nbased on the pan-European emergency number 112. As a result of the research project eCall4All\r\nin Slovenia the proof of concept for the system 112 eCall has been carried out successfully, where\r\nthe eCall Node runs in a cloud and performs harmonization between public and private eCall\r\nvia APIs. The eCall service helps achieve EU goals to reduce serious injuries and casualties in\r\nroad traffic by 50%.\r\n\r\nBeside the ecosystem of above-mentioned partners within the programme \u201cPlatform Services\u201d\r\nanother ecosystem of the 20 Slovenian partners within the programme \u201cSafe Society\u201d has been\r\nset up practically at the same time and is very active. It consists of industrial and research\r\npartners and the ICT Technology Network.\r\n\r\nThe both programmes have strong support from the The Chamber of Commerce and Industry\r\nof Slovenia.", "recorded": "2014-09-25T11:00:00", "title": "Platform Services approach to strengthen Value Chain and Business Opportunities"}, {"url": "icaps2010_hoos_design", "desc": "High-performance algorithms can be found at the heart of many software systems; they often provide\r\nthe key to effectively solving the computationally difficult problems encountered in the application\r\nareas in which these systems are deployed. Examples of such problems include planning, scheduling,\r\ntimetabling, resource allocation, computer-aided design and software verification. Many of these\r\nproblems are NP-hard and considered computationally intractable; nevertheless, these `intractable'\r\nproblems arise in practice, and finding good solutions to them in many cases tends to become more\r\ndifficult as economic constraints tighten.\r\nIn most (if not all) cases, the key to solving such computationally challenging problems lies in the use\r\nof high-performance heuristic algorithms, that is, algorithms that make use of mechanisms whose\r\nefficacy can be demonstrated empirically, yet remains inaccessible to the analytical techniques used for\r\nproving theoretical complexity results.\r\nHigh-performance heuristic algorithms are typically constructed in an iterative, manual process in\r\nwhich the designer gradually introduces or modifies components or mechanisms whose performance is\r\nthen tested by empirical evaluation on one or more sets of benchmark problems. During this iterative\r\ndesign process, the algorithm designer has to make many decisions, ranging from choices of the\r\nheuristic mechanisms to be used and the details of these mechanisms to lower-level implementation\r\ndetails, such as data structures. Some of these choices take the form of parameters, whose values are\r\nguessed or determined based on limited experimentation.\r\nThis traditional approach for designing high-performance algorithms can and often does lead to\r\nsatisfactory results. However, it tends to be tedious and labour-intensive; furthermore, the resulting\r\nalgorithms are often unnecessarily complicated, yet fail to realise the full performance potential present\r\nin the space of designs that can be built using the same underlying set of components and mechanisms.\r\nAs an alternative to the traditional, manual algorithm design process, we advocate an approach that\r\nuses fully formalised procedures, implemented in software, to permit a human designer to explore large\r\ndesign spaces more effectively, with the aim of realising algorithms with desirable performance\r\ncharacteristics. Computer-aided algorithm design allows human designers to focus on the creative task\r\nof specifying a design space in terms of potentially useful components. This design space is then\r\nexplored using optimisation and machine learning techniques, in combination with significant amounts\r\nof computing power, in order to find algorithms that perform well on given sets or distributions of input\r\ninstances.\r\nAutomated parameter tuning, algorithm configuration, algorithm portfolios and per-instance algorithm\r\nselection are prominent special cases of computer-aided algorithm design and have recently played a\r\npivotal role in improving the state of the art in solving a broad range of challenging combinatorial\r\nproblems, ranging from propositional satisfiability (SAT) and mixed integer programming to protein\r\nstructure prediction, course timetabling and planning problems.\r\nIn this talk, I will introduce computer-aided algorithm design and discuss its main ingredients: design\r\npatterns, which provide ways of structuring potentially large spaces of candidate algorithms, and metaalgorithmic\r\noptimisation procedures, which are used for finding good designs within these spaces.\r\nAfter explaining how this algorithm design approach differs from and complements related approaches\r\nin program synthesis, genetic programming and so-called hyper-heuristics, I will illustrate its success\r\nusing examples from our own work in SAT-based software verification, timetabling and mixed integer\r\nprogramming. Furthermore, I will argue why this approach can be expected to be particularly useful\r\nand effective for building better solvers for rich and diverse classes of combinatorial problems, such as\r\nplanning and scheduling. Finally, I will outline out how programming by optimisation -- a design\r\nparadigm that emphasises the automated construction of performance-optimised algorithm by means of\r\nsearching large spaces of alternative designs -- has the potential to transform the design of highperformance\r\nalgorithm from a craft that is based primarily on experience and intuition into a principled\r\nand highly effective engineering effort.", "recorded": "2010-05-16T09:00:00", "title": "Computer-Aided Algorithm Design: Automated Tuning, Configuration, Selection and Beyond"}, {"url": "active_course_syllabus", "desc": "back to [[http://videolectures.net/active|ACTIVE training project ]]\n\n===\nThe goal of training activities in ACTIVE is to achieve the transfer of knowledge and best practices within the project as well as (principally) outside the project. ACTIVE training program is aimed at training individuals and groups on the topics that are relevant to the ACTIVE project. Each training program covers a general topic and is generated from several training courses that are focused on specific topics. Furthermore, each training program combines different types and forms of learning: traditional, ICT supported or blended training event. For each training course we provide textual materials, references to the sources used for training module preparation, a list of the topic relevant ACTIVE deliverables, video tutorials and additional materials/suggested readings.\n\n== 1. Theoretical Foundations and Conceptual Models\n\nThis course starts with the theoretical foundations on the organisational and collaboration forms, business process modelling, management and collaboration mechanisms, knowledge management and current governance principles in organisations. In contrast it provides basic information about traditional and emerging technologies that influence traditional business organisation and management context. Then it gives motivation for the complete ACTIVE training programme by providing incentives, future trends and business potentials.\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse1_TheoreticalFoundations.pdf|Theoretical foundations and conceptual models - course]]\n\nCourse topics:\n\n**Collaboration and collaborative knowledge creation**\n*[[http://videolectures.net/coinactivess2010_dengler_sm|Semantic MediaWiki]]\n*[[http://videolectures.net/semseach09_albakour_mcpse|Managing Collaboration Projects using Semantic Email]]\n*[[http://videolectures.net/iswc08_tudarache_scodp|Supporting Collaborative Ontology Development in Protege]]\n*[[http://videolectures.net/eswc08_schaffert_sw|Semantic Wikis - IkeWiki - A Semantic Wiki for Collaborative Knowledge Management]]\n*[[http://videolectures.net/ice08_kristensen_pcik|Productivity in Collaboration-intensive Knowledge Work: The Collaboration Management Imperative]]\n*[[http://videolectures.net/nano07_gadlin_rsc|Re-thinking scientific teams: competition, conflict and collaboration]]\n*[[http://videolectures.net/iswc06_auer_otssc|In-Use 1: OntoWiki - A Tool for Social, Semantic Collaboration]]\n\n**Knowledge processes and tasks**\n*[[http://videolectures.net/active09_tilly_ikp|Informal Knowledge Processes: The Long Tail of Business Processes]]\n*[[http://videolectures.net/eswc08_feldcamp_sb|Workshop on Semantic Business Process Management - GoMoKIT- Towards an applicable goal-oriented Business Process Modelling approach for knowledge-intensive Tasks]]\n\n**Process modelling**\n*[[http://videolectures.net/eswc08_feldcamp_sb|Workshop on Semantic Business Process Management - GoMoKIT- Towards an applicable goal-oriented Business Process Modelling approach for knowledge-intensive Tasks]]\n\n**Knowledge management**\n*[[http://videolectures.net/coinactivess2010_dengler_sm/|Semantic MediaWiki]]\n*[[http://videolectures.net/training06_davies_ka|Knowledge Access]]\n*[[http://videolectures.net/iswc08_bhagdev_cauoswilno|Creating and Using Organisational Semantic Webs in Large Networked Organisations]]\n*[[http://videolectures.net/cikm08_feldman_ak|Automating Knowledge]]\n*[[http://videolectures.net/eswc08_schaffert_sw|Semantic Wikis - IkeWiki - A Semantic Wiki for Collaborative Knowledge Management]]\n\n**Formalisms for (dynamic) aspects of knowledge worker context and enterprises**\n*[[http://videolectures.net/iswc06_grobelnik_cskrs|Context Sensitivity in Knowledge Rich Systems]]\n*[[http://videolectures.net/iswc06_witbrock_cskrs|Context Sensitivity in Knowledge Rich Systems]]\n*[[http://videolectures.net/iswc06_mozetic_cskrs|Context Sensitivity in Knowledge Rich Systems]]\n*[[http://videolectures.net/iswc06_haase_cop|Context Sensitivity in Knowledge Rich Systems - Contents of parts 2]]\n\n ----\n ----\n\n == 2. Knowledge Models and Structures\n\nThis course deals with the theoretical background on knowledge and semantic technologies and examines them from a technological, historical and scientific perspective. It starts with the basic facts about knowledge structures and models and their role in the knowledge formalisation, modelling, reasoning and adaptation. It provides comparison and contrasting of diverse knowledge models, structures and systems and understanding their participation in industry.\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse2_KnowledgeModels.pdf|Knowledge models and structures - course]]\n\nCourse topics:\n\n**Semantic languages**\n*[[http://videolectures.net/coinactivess2010_dengler_sm/|Semantic MediaWiki]]\n*[[http://videolectures.net/training06_sure_stsw/|A short Tutorial on Semantic Web]]\n*[[http://videolectures.net/eswc09_tappolet_atre|Applied Temporal RDF: Ef?cient Temporal Querying of RDF Data with SPARQL]]\n*[[http://videolectures.net/eswc09_vennekens_faaeod|FO(ID) as an Extension of DL with Rules]]\n*[[http://videolectures.net/iswc08_perez_nsparql|NSPARQL: A Navigational Language for RDF]]\n*[[http://videolectures.net/iswc08_hausenblas_bwdwd|RDFa - Bridging the Web of Documents and the Web of Data]]\n\n**Knowledge formalisation  and  representation**\n*[[http://videolectures.net/coinactivess2010_dengler_sm|Semantic MediaWiki]]\n*[[http://videolectures.net/akom08_krotzsch_fik|Formal and Informal knowledge representation]]\n*[[http://videolectures.net/iswc08_saggion_krebi|Knowledge Representation and Extraction for Business Intelligence]]\n\n**Reasoning and  probabilistic temporal models**\n*[[http://videolectures.net/iswc08_moller_itsr|Reasoning for Ontology Engineering and Usage]]\n*[[http://videolectures.net/ssll09_pagnucco_krr|Knowledge Representation and Reasoning]]\n*[[http://videolectures.net/bsciw08_schwaighofer_krrd|Knowledge Representation and Reasoning - Discussion]]\n\n**Knowledge structures**\n**Collaborative articulation of expressive knowledge**\n**Knowledge leveraging and repair models**\n**Knowledge-based adaptation**\n**Knowledge creation cycle**\n\nRelated talks:\n*[[http://videolectures.net/iswc08_hauer_asnrwpdehep|An architecture for semantic navigation and reasoning with patient data - experiences of the Health-e-Child project]]\n*[[http://videolectures.net/eswc08_blanco_sr|Semantic Reasoning: A Path To New Possibilities of Personalization]]\n*[[http://videolectures.net/akom08_krotzsch_fik/|Formal and Informal knowledge representation]]\n*[[http://videolectures.net/active09_ghani_rdekm/|Research Directions in Enterprise Knowledge Management]]\n\n ----\n ----\n\n == 3. Background Technologies\n\nThis course presents and explains core technologies from the area of knowledge technologies that ranges from the data driven methods to knowledge driven methods and are important for detecting, analysing and managing knowledge (tacit and explicit) in organisations. Recent developments in the particular areas are demonstrated through real software prototypes, successful market cases and real business implementations. The course provides many demos that are available as demonstrations online and could be supplemented with the hands-on session.\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse3_BackgroundTechnologies.pdf|Background technologies - course]]\n\nCourse topics:\n\n**Context mining**\n*[[http://videolectures.net/coinactivess2010_grobelnik_bpm|Business Process Mining and Formalization]]\n*[[http://videolectures.net/um05_loosli_ccdoc|Context changes detection by one-class svms]]\n*[[http://videolectures.net/ice08_lukowicz_crw|Context recognition in the wearIT@work project]]\n*[[http://videolectures.net/kdd09_zhu_mrciws|Mining Rich Session Context to Improve Web Search]]\n*[[http://videolectures.net/samt08_santini_cnod|Context as a non-ontological determinant of semantics]]\n\n**Stream mining**\n*[[http://videolectures.net/ecml07_gama_sad|State of the Art in Data Stream Mining]]\n*[[http://videolectures.net/ecml07_mohamed_sad|State of the Art in Data Stream Mining]]\n*[[http://videolectures.net/ecml07_mohamed_acac|An architecture for context-aware adaptive data stream mining]]\n*[[http://videolectures.net/ecml07_mohamed_mqgr|A Model for Quality Guaranteed Resource-Aware Stream Mining]]\n\n**Anomaly detection**\n*[[http://videolectures.net/ecmlpkdd08_lazarevic_dmfa|Data Mining for Anomaly Detection]]\n*[[http://videolectures.net/mmdss07_tishby_itam|Information Theo-retic and Alge-braic Methods for Network Anomaly Detection]]\n*[[http://videolectures.net/kdd09_jermaine_alrtffsad|A LRT Framework for Fast Spacial Anomaly Detection]]\n\n**Social network analysis**\n*[[http://videolectures.net/semseach09_graves_srrdsn|Searching and ranking in RDF documents and social networks]]\n*[[http://videolectures.net/icwsm09_agarwal_siaifs|A Social Identity Approach to Identify Familiar Strangers in a Social Network]]\n*[[http://videolectures.net/kdd09_jermaine_alrtffsad|A LRT Framework for Fast Spacial Anomaly Detection]]\n*[[http://videolectures.net/iswc07_aasman_usn|Using Social Network Analysis, Geotemporal Reasoning and RDFS++ Reasoning for Business Intelligence]]\n\n**Social software and Web 2.0**\n*[[http://videolectures.net/active09_mulvany_cwsc|A Cabinet of Web 2.0 Scientific Curiositics]]\n*[[http://videolectures.net/eswc08_gomes_cs|Collective Semantics: Collective Intelligence & the Semantic Web - Flickring Our World]]\n*[[http://videolectures.net/eccs07_huberman_bwe|Beyond Web 2.0]]\n*[[http://videolectures.net/icwsm09_sun_gmctfnf|Gesundheit! Modeling Contagion Through Facebook News Feed]]\n*[[http://videolectures.net/eswc08_halpin_sw|Panel II: Social Network Portability: Is the Semantic Web Ready?]]\n*[[http://videolectures.net/cikm08_perisic_usnfsw|Using Social Networks for Social Work]]\n*[[http://videolectures.net/www09_baezayates_mtwbs|Mining the Web 2.0 for Better Search]]\n*[[http://videolectures.net/samt08_baumann_wai|WhoAmI - A Web2.0 Platform for Faceted Identity Management through Aggregation of Social Media]]\n\n**Adaptive and context-aware systems**\n*[[http://videolectures.net/samt08_rodriguez_doncel_smac|A Semantic Model for the Authorisation of Context-Aware Content Adaptation]]\n\n**Semantic technologies and content**\n**Social software and Web 2.0**\n**Knowledge filters**\n**Meta learning**\n**Forecasting**\n\nRelated talks:\n*[[http://videolectures.net/eccs07_huberman_bwe|Beyond Web 2.0]]\n*[[http://videolectures.net/kdd07_fayyad_fmtw|From Mining the Web to Inventing the New Sciences Underlying the Internet ]]\n*[[http://videolectures.net/ecml07_baeza_yates_mwq|Mining Queries]]\n*[[http://videolectures.net/estc08_zaragoza_isst|Improving Search with Semantic Technologies: Current Research Directions]]\n*[[http://videolectures.net/active09_grobelnik_tmlws|Text Mining and Light Weight Semantics]]\n\n ----\n ----\n\n == 4. ACTIVE Innovative Solutions\n\nThis is one of the main courses in ACTIVE training programmes. It is aimed at presenting ACTIVE innovative solutions that will be developed in the frame of the project. Training starts with innovative business and organisational models that are consequence of newly introduced technologies. The main focus is on the explanation and demonstration of technologies developed in ACTIVE, their use in the industry contexts and on the discussion of their implications to the traditional business environment. Course addresses in detail the methods for identifying knowledge processes, hidden knowledge in organisations as well as new technology research streams. Furthermore it explains and demonstrates the methods for contextualisation that spans over three orthogonal dimensions: content, social network and time. [[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse4_InnovativeSolutions.pdf|ACTIVE innovative solutions - course]]\n\nCourse topics:\n\n**Tools and methods for collaborative and expressive knowledge articulation paradigms**\n*[[http://videolectures.net/akom08_warren_welcome|Welcome to the ACTIVE kick off meeting]]\n*[[http://videolectures.net/coinactivess2010_dengler_sm|Semantic MediaWiki]]\n\n**Ontology learning**\n*[[http://videolectures.net/coinactivess2010_grobelnik_bpm|Business Process Mining and Formalization]]\n*[[http://videolectures.net/training06_grobelnik_tmol|Text Mining for Ontology Learning]]\n*[[http://videolectures.net/koml04_grobelnik_olkds|Ontology Learning - Knowledge Discovery and the Semantic Web]]\n\n**Hybrid Web 2.0 - ontology infrastructure**\n*[[http://videolectures.net/coinactivess2010_imtiaz_aam|ACTIVE, Ali and More]]\n*[[http://videolectures.net/eswc08_hess_cs|Collective Semantics: Collective Intelligence & the Semantic Web - From Web 2.0 to Semantic Web - A Semi-Automated Approach]]\n\n**Semi-automated process refactoring**\n*[[http://videolectures.net/coinactivess2010_ruiz_moreno_leban_pak|Pro-Active Knowledge Processes Support]]\n\n**Optimization of knowledge models and pro-active support**\n*[[http://videolectures.net/coinactivess2010_ruiz_moreno_leban_pak|Pro-Active Knowledge Processes Support]]\n\n**Delivery of contextualized information**\n*[[http://videolectures.net/coinactivess2010_grobelnik_bpm|Business Process Mining and Formalization]]\n\n**Visualization of temporal enterprise model**\n*[[http://videolectures.net/coinactivess2010_ruiz_moreno_leban_pak|Pro-Active Knowledge Processes Support]]\n\n**Privacy preserving analysis of enterprise data**\n*[[http://videolectures.net/coinactivess2010_djordjevic_acs|Accenture Case Study: Enterprise Collaboration & Knowledge Management through Machine Learning and Semantic Technologies]]\n*[[http://videolectures.net/google_roughan_ppdm|Privacy Preserving DataMining]]\n\n**Inconsistency diagnosis and automatic repair of inconsistencies**\n**Complex rule interfaces**\n**Knowledge process mining**\n**Autonomous, context aware services for knowledge processes**\n**Dynamic adaptations of context-aware knowledge processes**\n**Security-aware knowledge processes**\n**Simultaneous analysis of multiple modalities**\n\nRelated talks:\n*[[http://videolectures.net/akom08_warren_welcome|Welcome to the ACTIVE kick off meeting]]\n*[[http://videolectures.net/iswc08_witbrock_fsc|Free Semantic Content: Using OpenCyc in Semantic Web Applications]]\n*[[http://videolectures.net/iswc08_dellaValle_rswa|Realizing a Semantic Web Application]]\n*[[http://videolectures.net/active09_warren_kmcfl|KM at the Customer Front-Line: The BT Case Study in ACTIVE]]\n\n ----\n ----\n\n == 5. Management and Problem Solving\n\nThis course is aimed at providing knowledge about the models, structures and mechanisms for management, coordination and problem solving. The coordination mechanisms are basic mechanisms in any networked organisation and as such the focal point of interest for any distributed, knowledge focused organisation. The course will analyse and study the approaches for distributed and collaborative decision-making including organizational aspects in the context of informal and knowledge processes. Support with ACTIVE models and solutions for decision making processes in inter-enterprise and collaborative environment is the main focus.\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse5_Management and problem solving.pdf|Management and problem solving - course]]\n\nCourse topics:\n\n**Context sensitive management**\n*[[http://videolectures.net/coinactivess2010_imtiaz_aam|ACTIVE, Ali and More]]\n\n**Pro-active knowledge process support**\n*[[http://videolectures.net/coinactivess2010_ruiz_moreno_leban_pak|Pro-Active Knowledge Processes Support]]\n\n ----\n ----\n\n == 6. Practical Examples and ACTIVE Prototypes\n\nIn this course the development process of the ACTIVE prototypes for three specific applications will be demonstrated. The development process and the applications with their specific benefits which are created by the ACTIVE research results will be shown. The course will be supplemented with a hands-on workshop to allow participants to deal with concrete examples from their work context. The focus will be on identifying areas of applications where the functionalities developed in the ACTIVE project can be applied.\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse6_PracticalExamples.pdf|Practical Examples and ACTIVE Prototypes - course]]\n\nCourse topics:\n\n**Case studies and best practices**\n*[[http://videolectures.net/active09_warren_kmcfl|KM at the Customer Front-Line: The BT Case Study in ACTIVE]]\n*[[http://videolectures.net/coinactivess2010_thurlow_bcs|ACTIVE\u000b BT Case Study]]\n*[[http://videolectures.net/coinactivess2010_djordjevic_acs|Accenture Case Study: Enterprise Collaboration & Knowledge Management through Machine Learning and Semantic Technologies]]\n\n**Demonstrators, demos, prototypes**\n*[[http://videolectures.net/eswc08_berges_smw|Semantic Web Technology for Agent Communication Protocols]]\n*[[http://videolectures.net/active09_dolinsek_iak|Introduction to the ACTIVE Knowledge Workspace SDK]]\n\n**Use scenarios**\n\n ----\n ----\n\n == 7. Economic Incentives for Web 2.0 and Semantic Applications\n\nThis course will in particular investigate aspects related to the economics and the incentives behind Web2.0 and semantic technologies. Issues of costs and benefits associated to developing, deploying or maintaining Web 2.0 and semantic systems at enterprise level will be presented. , and instruments to predict and use them for analytically describe knowledge creation processes are important pre-requisites for their large-scale. [[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse7_EconomicIncentives.pdf|Economic incentives for Web2.0 and semantic applications - course]]\n\nCourse topics:\n\n**Integration into management activities**\n*[[http://videolectures.net/coinactivess2010_imtiaz_aam|ACTIVE, Ali and More]]\n\n**Cost-benefit methods**\n*[[http://videolectures.net//akom08_popov_cbi|Costs, benefits and incentives (of semantic techologies)]]\n\n**Economic measurements**\n**Comprehensive models and tooling for economic measurements**\n**Business practices in knowledge technologies and social software**\n**Recommendations for tool design**\n\n Related taks:\n*[[http://videolectures.net/active09_sundaresan_tre|Trust and Reputation in eCommerce]]\n*[[http://videolectures.net/akom08_popov_cbi/|Costs, benefits and incentives (of semantic techologies)]]\n*[[http://videolectures.net/webstart08_mcgough_buan/|Business Angel]]\n*[[http://videolectures.net/eswc08_shadbolt_gst|Garlik: Semantic Technology for the Consumer]]\n\n ----\n ----\n\n == 8. Social, Ethical, Legal And Organizational Issues Related To Novel Semantic Technologies And Context Aware Systems\n\nIn this unit social, ethical, legal, and organizational issues are discussed and current trends pointed out. New business models and their applicability are discussed, namely through the introduction of examples. Marketing and sustainability, intellectual property management, systems of incentives, etc. are other relevant issues.\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse8_SocialIssues.pdf|Social, ethical, legal and organizational issues related to novel semantic technologies - course]]\n\nCourse topics:\n\n**Privacy models**\n*[[http://videolectures.net/coinactivess2010_djordjevic_acs|Accenture Case Study: Enterprise Collaboration & Knowledge Management through Machine Learning and Semantic Technologies]]\n*[[http://videolectures.net/ssms08_boehme_cpiimi|Concerns about Privacy & Innovation in ICT and Media Industries]]\n*[[http://videolectures.net/kdd09_li_otbpud|On the Tradeoff Between Privacy and Utility in Data Publishing ]]\n\n**Knowledge sharing and collaboration incentives**\n\nRelated talks:\n*[[http://videolectures.net/active09_zwegers_ficc/|On Future Internet, Cloud Computing and Semantics - You Name It]]\n*[[http://videolectures.net/iswc08_giannandrea_fowdw/|Freebase: An Open, Writable Database of the World\u2019s Information|]]\n*[[http://videolectures.net/iswc08_decker_mib/|Message in a Bottle or: How can the Semantic Web Community be more convincing?]]\n\n == 9. ACTIVE project - Introduction\n\nThe aim of this course is to present the aims, goals, structure and expected results of ACTIVE project. In addition this course provides information about the future plans, development and operations of the business development that will follow the ACTIVE project. In that respect the future directions in the research areas will be presented, their use potentials, development scenarios and business scenarios in the forms of business plans. [[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse9_Introduction.pdf|ACTIVE project - Introduction - course]]\n\n*[[http://videolectures.net/akom08_warren_welcome|Welcome to the ACTIVE kick off meeting]]\n*[[http://videolectures.net/active09_mladenic_warren_oai|Opening and Introduction of the 1st ACTIVE Summer School]]\n*[[http://videolectures.net/coinactivess2010_warren_ai|ACTIVE Introduction]]\n*[[http://videolectures.net/coinactivess2010_jermol_wel|Welcome to the Summer School on Advanced Technologies for Knowledge Intensive Networked Organizations 2010 - Aachen ]]\n\n == 10. Using ACTIVE solution\n\nThe aim of this training module is to train the potential users/adopters/developers on the ACTIVE solutions. The course is organised as a blended learning module that combines traditional two days event with the self-learning courses in LC. The main learning goals for this module are to teach about basics of ACTIVE solutions and infrastructure, provide technical specifications, development environment and cases. [[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse10_UsingACTIVESolution.pdf|Using ACTIVE solution - course]]\n\nCourse topics:\n\n**ACTIVE prototypes, ACTIVE Knowledge Workspace Desktop, ACTIVE SDK, ACTIVE Integrated Platform**\n*[[http://videolectures.net/active09_dolinsek_iak|Introduction to the ACTIVE Knowledge Workspace SDK]]\n*[[http://videolectures.net/coinactivess2010_dolinsek_akw|Active Knowledge Work Space demonstration]]", "recorded": "2009-01-23T15:04:44", "title": "The ACTIVE Project - Technologies and Application"}, {"url": "active_industry_training", "desc": "back to [[http://videolectures.net/active|ACTIVE training project ]]\r\n\r\n===\r\nThe goal of training activities in ACTIVE is to achieve the transfer of knowledge and best practices within the project as well as (principally) outside the project. ACTIVE training program is aimed at training individuals and groups on the topics that are relevant to the ACTIVE project. Each training program covers a general topic and is generated from several training courses that are focused on specific topics. Furthermore, each training program combines different types and forms of learning: traditional, ICT supported or blended training event. For each training course we provide textual materials, references to the sources used for training module preparation, a list of the topic relevant ACTIVE deliverables, video tutorials and additional materials/suggested readings.\r\n\r\n== 1. Theoretical Foundations and Conceptual Models\r\n\r\nThis course starts with the theoretical foundations on the organisational and collaboration forms, business process modelling, management and collaboration mechanisms, knowledge management and current governance principles in organisations. In contrast it provides basic information about traditional and emerging technologies that influence traditional business organisation and management context. Then it gives motivation for the complete ACTIVE training programme by providing incentives, future trends and business potentials.\r\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse1_TheoreticalFoundations.pdf|Theoretical foundations and conceptual models - course]]\r\n\r\nCourse topics:\r\n\r\n**Collaboration and collaborative knowledge creation**\r\n*[[http://videolectures.net/coinactivess2010_dengler_sm|Semantic MediaWiki]]\r\n*[[http://videolectures.net/semseach09_albakour_mcpse|Managing Collaboration Projects using Semantic Email]]\r\n*[[http://videolectures.net/iswc08_tudarache_scodp|Supporting Collaborative Ontology Development in Protege]]\r\n*[[http://videolectures.net/eswc08_schaffert_sw|Semantic Wikis - IkeWiki - A Semantic Wiki for Collaborative Knowledge Management]]\r\n*[[http://videolectures.net/ice08_kristensen_pcik|Productivity in Collaboration-intensive Knowledge Work: The Collaboration Management Imperative]]\r\n*[[http://videolectures.net/nano07_gadlin_rsc|Re-thinking scientific teams: competition, conflict and collaboration]]\r\n*[[http://videolectures.net/iswc06_auer_otssc|In-Use 1: OntoWiki - A Tool for Social, Semantic Collaboration]]\r\n\r\n**Knowledge processes and tasks**\r\n*[[http://videolectures.net/active09_tilly_ikp|Informal Knowledge Processes: The Long Tail of Business Processes]]\r\n*[[http://videolectures.net/eswc08_feldcamp_sb|Workshop on Semantic Business Process Management - GoMoKIT- Towards an applicable goal-oriented Business Process Modelling approach for knowledge-intensive Tasks]]\r\n\r\n**Process modelling**\r\n*[[http://videolectures.net/eswc08_feldcamp_sb|Workshop on Semantic Business Process Management - GoMoKIT- Towards an applicable goal-oriented Business Process Modelling approach for knowledge-intensive Tasks]]\r\n\r\n**Knowledge management**\r\n*[[http://videolectures.net/coinactivess2010_dengler_sm/|Semantic MediaWiki]]\r\n*[[http://videolectures.net/training06_davies_ka|Knowledge Access]]\r\n*[[http://videolectures.net/iswc08_bhagdev_cauoswilno|Creating and Using Organisational Semantic Webs in Large Networked Organisations]]\r\n*[[http://videolectures.net/cikm08_feldman_ak|Automating Knowledge]]\r\n*[[http://videolectures.net/eswc08_schaffert_sw|Semantic Wikis - IkeWiki - A Semantic Wiki for Collaborative Knowledge Management]]\r\n\r\n**Formalisms for (dynamic) aspects of knowledge worker context and enterprises**\r\n*[[http://videolectures.net/iswc06_grobelnik_cskrs|Context Sensitivity in Knowledge Rich Systems]]\r\n*[[http://videolectures.net/iswc06_witbrock_cskrs|Context Sensitivity in Knowledge Rich Systems]]\r\n*[[http://videolectures.net/iswc06_mozetic_cskrs|Context Sensitivity in Knowledge Rich Systems]]\r\n*[[http://videolectures.net/iswc06_haase_cop|Context Sensitivity in Knowledge Rich Systems - Contents of parts 2]]\r\n\r\n ----\r\n ----\r\n\r\n == 2. Knowledge Models and Structures\r\n\r\nThis course deals with the theoretical background on knowledge and semantic technologies and examines them from a technological, historical and scientific perspective. It starts with the basic facts about knowledge structures and models and their role in the knowledge formalisation, modelling, reasoning and adaptation. It provides comparison and contrasting of diverse knowledge models, structures and systems and understanding their participation in industry.\r\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse2_KnowledgeModels.pdf|Knowledge models and structures - course]]\r\n\r\nCourse topics:\r\n\r\n**Semantic languages**\r\n*[[http://videolectures.net/coinactivess2010_dengler_sm/|Semantic MediaWiki]]\r\n*[[http://videolectures.net/training06_sure_stsw/|A short Tutorial on Semantic Web]]\r\n*[[http://videolectures.net/eswc09_tappolet_atre|Applied Temporal RDF: Ef?cient Temporal Querying of RDF Data with SPARQL]]\r\n*[[http://videolectures.net/eswc09_vennekens_faaeod|FO(ID) as an Extension of DL with Rules]]\r\n*[[http://videolectures.net/iswc08_perez_nsparql|NSPARQL: A Navigational Language for RDF]]\r\n*[[http://videolectures.net/iswc08_hausenblas_bwdwd|RDFa - Bridging the Web of Documents and the Web of Data]]\r\n\r\n**Knowledge formalisation  and  representation**\r\n*[[http://videolectures.net/coinactivess2010_dengler_sm|Semantic MediaWiki]]\r\n*[[http://videolectures.net/akom08_krotzsch_fik|Formal and Informal knowledge representation]]\r\n*[[http://videolectures.net/iswc08_saggion_krebi|Knowledge Representation and Extraction for Business Intelligence]]\r\n\r\n**Reasoning and  probabilistic temporal models**\r\n*[[http://videolectures.net/iswc08_moller_itsr|Reasoning for Ontology Engineering and Usage]]\r\n*[[http://videolectures.net/ssll09_pagnucco_krr|Knowledge Representation and Reasoning]]\r\n*[[http://videolectures.net/bsciw08_schwaighofer_krrd|Knowledge Representation and Reasoning - Discussion]]\r\n\r\n**Knowledge structures**\r\n**Collaborative articulation of expressive knowledge**\r\n**Knowledge leveraging and repair models**\r\n**Knowledge-based adaptation**\r\n**Knowledge creation cycle**\r\n\r\nRelated talks:\r\n*[[http://videolectures.net/iswc08_hauer_asnrwpdehep|An architecture for semantic navigation and reasoning with patient data - experiences of the Health-e-Child project]]\r\n*[[http://videolectures.net/eswc08_blanco_sr|Semantic Reasoning: A Path To New Possibilities of Personalization]]\r\n*[[http://videolectures.net/akom08_krotzsch_fik/|Formal and Informal knowledge representation]]\r\n*[[http://videolectures.net/active09_ghani_rdekm/|Research Directions in Enterprise Knowledge Management]]\r\n\r\n ----\r\n ----\r\n\r\n == 3. Background Technologies\r\n\r\nThis course presents and explains core technologies from the area of knowledge technologies that ranges from the data driven methods to knowledge driven methods and are important for detecting, analysing and managing knowledge (tacit and explicit) in organisations. Recent developments in the particular areas are demonstrated through real software prototypes, successful market cases and real business implementations. The course provides many demos that are available as demonstrations online and could be supplemented with the hands-on session.\r\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse3_BackgroundTechnologies.pdf|Background technologies - course]]\r\n\r\nCourse topics:\r\n\r\n**Context mining**\r\n*[[http://videolectures.net/coinactivess2010_grobelnik_bpm|Business Process Mining and Formalization]]\r\n*[[http://videolectures.net/um05_loosli_ccdoc|Context changes detection by one-class svms]]\r\n*[[http://videolectures.net/ice08_lukowicz_crw|Context recognition in the wearIT@work project]]\r\n*[[http://videolectures.net/kdd09_zhu_mrciws|Mining Rich Session Context to Improve Web Search]]\r\n*[[http://videolectures.net/samt08_santini_cnod|Context as a non-ontological determinant of semantics]]\r\n\r\n**Stream mining**\r\n*[[http://videolectures.net/ecml07_gama_sad|State of the Art in Data Stream Mining]]\r\n*[[http://videolectures.net/ecml07_mohamed_sad|State of the Art in Data Stream Mining]]\r\n*[[http://videolectures.net/ecml07_mohamed_acac|An architecture for context-aware adaptive data stream mining]]\r\n*[[http://videolectures.net/ecml07_mohamed_mqgr|A Model for Quality Guaranteed Resource-Aware Stream Mining]]\r\n\r\n**Anomaly detection**\r\n*[[http://videolectures.net/ecmlpkdd08_lazarevic_dmfa|Data Mining for Anomaly Detection]]\r\n*[[http://videolectures.net/mmdss07_tishby_itam|Information Theo-retic and Alge-braic Methods for Network Anomaly Detection]]\r\n*[[http://videolectures.net/kdd09_jermaine_alrtffsad|A LRT Framework for Fast Spacial Anomaly Detection]]\r\n\r\n**Social network analysis**\r\n*[[http://videolectures.net/semseach09_graves_srrdsn|Searching and ranking in RDF documents and social networks]]\r\n*[[http://videolectures.net/icwsm09_agarwal_siaifs|A Social Identity Approach to Identify Familiar Strangers in a Social Network]]\r\n*[[http://videolectures.net/kdd09_jermaine_alrtffsad|A LRT Framework for Fast Spacial Anomaly Detection]]\r\n*[[http://videolectures.net/iswc07_aasman_usn|Using Social Network Analysis, Geotemporal Reasoning and RDFS++ Reasoning for Business Intelligence]]\r\n\r\n**Social software and Web 2.0**\r\n*[[http://videolectures.net/active09_mulvany_cwsc|A Cabinet of Web 2.0 Scientific Curiositics]]\r\n*[[http://videolectures.net/eswc08_gomes_cs|Collective Semantics: Collective Intelligence & the Semantic Web - Flickring Our World]]\r\n*[[http://videolectures.net/eccs07_huberman_bwe|Beyond Web 2.0]]\r\n*[[http://videolectures.net/icwsm09_sun_gmctfnf|Gesundheit! Modeling Contagion Through Facebook News Feed]]\r\n*[[http://videolectures.net/eswc08_halpin_sw|Panel II: Social Network Portability: Is the Semantic Web Ready?]]\r\n*[[http://videolectures.net/cikm08_perisic_usnfsw|Using Social Networks for Social Work]]\r\n*[[http://videolectures.net/www09_baezayates_mtwbs|Mining the Web 2.0 for Better Search]]\r\n*[[http://videolectures.net/samt08_baumann_wai|WhoAmI - A Web2.0 Platform for Faceted Identity Management through Aggregation of Social Media]]\r\n\r\n**Adaptive and context-aware systems**\r\n*[[http://videolectures.net/samt08_rodriguez_doncel_smac|A Semantic Model for the Authorisation of Context-Aware Content Adaptation]]\r\n\r\n**Semantic technologies and content**\r\n**Social software and Web 2.0**\r\n**Knowledge filters**\r\n**Meta learning**\r\n**Forecasting**\r\n\r\nRelated talks:\r\n*[[http://videolectures.net/eccs07_huberman_bwe|Beyond Web 2.0]]\r\n*[[http://videolectures.net/kdd07_fayyad_fmtw|From Mining the Web to Inventing the New Sciences Underlying the Internet ]]\r\n*[[http://videolectures.net/ecml07_baeza_yates_mwq|Mining Queries]]\r\n*[[http://videolectures.net/estc08_zaragoza_isst|Improving Search with Semantic Technologies: Current Research Directions]]\r\n*[[http://videolectures.net/active09_grobelnik_tmlws|Text Mining and Light Weight Semantics]]\r\n\r\n ----\r\n ----\r\n\r\n == 4. ACTIVE Innovative Solutions\r\n\r\nThis is one of the main courses in ACTIVE training programmes. It is aimed at presenting ACTIVE innovative solutions that will be developed in the frame of the project. Training starts with innovative business and organisational models that are consequence of newly introduced technologies. The main focus is on the explanation and demonstration of technologies developed in ACTIVE, their use in the industry contexts and on the discussion of their implications to the traditional business environment. Course addresses in detail the methods for identifying knowledge processes, hidden knowledge in organisations as well as new technology research streams. Furthermore it explains and demonstrates the methods for contextualisation that spans over three orthogonal dimensions: content, social network and time. [[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse4_InnovativeSolutions.pdf|ACTIVE innovative solutions - course]]\r\n\r\nCourse topics:\r\n\r\n**Tools and methods for collaborative and expressive knowledge articulation paradigms**\r\n*[[http://videolectures.net/akom08_warren_welcome|Welcome to the ACTIVE kick off meeting]]\r\n*[[http://videolectures.net/coinactivess2010_dengler_sm|Semantic MediaWiki]]\r\n\r\n**Ontology learning**\r\n*[[http://videolectures.net/coinactivess2010_grobelnik_bpm|Business Process Mining and Formalization]]\r\n*[[http://videolectures.net/training06_grobelnik_tmol|Text Mining for Ontology Learning]]\r\n*[[http://videolectures.net/koml04_grobelnik_olkds|Ontology Learning - Knowledge Discovery and the Semantic Web]]\r\n\r\n**Hybrid Web 2.0 - ontology infrastructure**\r\n*[[http://videolectures.net/coinactivess2010_imtiaz_aam|ACTIVE, Ali and More]]\r\n*[[http://videolectures.net/eswc08_hess_cs|Collective Semantics: Collective Intelligence & the Semantic Web - From Web 2.0 to Semantic Web - A Semi-Automated Approach]]\r\n\r\n**Semi-automated process refactoring**\r\n*[[http://videolectures.net/coinactivess2010_ruiz_moreno_leban_pak|Pro-Active Knowledge Processes Support]]\r\n\r\n**Optimization of knowledge models and pro-active support**\r\n*[[http://videolectures.net/coinactivess2010_ruiz_moreno_leban_pak|Pro-Active Knowledge Processes Support]]\r\n\r\n**Delivery of contextualized information**\r\n*[[http://videolectures.net/coinactivess2010_grobelnik_bpm|Business Process Mining and Formalization]]\r\n\r\n**Visualization of temporal enterprise model**\r\n*[[http://videolectures.net/coinactivess2010_ruiz_moreno_leban_pak|Pro-Active Knowledge Processes Support]]\r\n\r\n**Privacy preserving analysis of enterprise data**\r\n*[[http://videolectures.net/coinactivess2010_djordjevic_acs|Accenture Case Study: Enterprise Collaboration & Knowledge Management through Machine Learning and Semantic Technologies]]\r\n*[[http://videolectures.net/google_roughan_ppdm|Privacy Preserving DataMining]]\r\n\r\n**Inconsistency diagnosis and automatic repair of inconsistencies**\r\n**Complex rule interfaces**\r\n**Knowledge process mining**\r\n**Autonomous, context aware services for knowledge processes**\r\n**Dynamic adaptations of context-aware knowledge processes**\r\n**Security-aware knowledge processes**\r\n**Simultaneous analysis of multiple modalities**\r\n\r\nRelated talks:\r\n*[[http://videolectures.net/akom08_warren_welcome|Welcome to the ACTIVE kick off meeting]]\r\n*[[http://videolectures.net/iswc08_witbrock_fsc|Free Semantic Content: Using OpenCyc in Semantic Web Applications]]\r\n*[[http://videolectures.net/iswc08_dellaValle_rswa|Realizing a Semantic Web Application]]\r\n*[[http://videolectures.net/active09_warren_kmcfl|KM at the Customer Front-Line: The BT Case Study in ACTIVE]]\r\n\r\n ----\r\n ----\r\n\r\n == 5. Management and Problem Solving\r\n\r\nThis course is aimed at providing knowledge about the models, structures and mechanisms for management, coordination and problem solving. The coordination mechanisms are basic mechanisms in any networked organisation and as such the focal point of interest for any distributed, knowledge focused organisation. The course will analyse and study the approaches for distributed and collaborative decision-making including organizational aspects in the context of informal and knowledge processes. Support with ACTIVE models and solutions for decision making processes in inter-enterprise and collaborative environment is the main focus.\r\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse5_Management and problem solving.pdf|Management and problem solving - course]]\r\n\r\nCourse topics:\r\n\r\n**Context sensitive management**\r\n*[[http://videolectures.net/coinactivess2010_imtiaz_aam|ACTIVE, Ali and More]]\r\n\r\n**Pro-active knowledge process support**\r\n*[[http://videolectures.net/coinactivess2010_ruiz_moreno_leban_pak|Pro-Active Knowledge Processes Support]]\r\n\r\n ----\r\n ----\r\n\r\n == 6. Practical Examples and ACTIVE Prototypes\r\n\r\nIn this course the development process of the ACTIVE prototypes for three specific applications will be demonstrated. The development process and the applications with their specific benefits which are created by the ACTIVE research results will be shown. The course will be supplemented with a hands-on workshop to allow participants to deal with concrete examples from their work context. The focus will be on identifying areas of applications where the functionalities developed in the ACTIVE project can be applied.\r\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse6_PracticalExamples.pdf|Practical Examples and ACTIVE Prototypes - course]]\r\n\r\nCourse topics:\r\n\r\n**Case studies and best practices**\r\n*[[http://videolectures.net/active09_warren_kmcfl|KM at the Customer Front-Line: The BT Case Study in ACTIVE]]\r\n*[[http://videolectures.net/coinactivess2010_thurlow_bcs|ACTIVE\u000b BT Case Study]]\r\n*[[http://videolectures.net/coinactivess2010_djordjevic_acs|Accenture Case Study: Enterprise Collaboration & Knowledge Management through Machine Learning and Semantic Technologies]]\r\n\r\n**Demonstrators, demos, prototypes**\r\n*[[http://videolectures.net/eswc08_berges_smw|Semantic Web Technology for Agent Communication Protocols]]\r\n*[[http://videolectures.net/active09_dolinsek_iak|Introduction to the ACTIVE Knowledge Workspace SDK]]\r\n\r\n**Use scenarios**\r\n\r\n ----\r\n ----\r\n\r\n == 7. Economic Incentives for Web 2.0 and Semantic Applications\r\n\r\nThis course will in particular investigate aspects related to the economics and the incentives behind Web2.0 and semantic technologies. Issues of costs and benefits associated to developing, deploying or maintaining Web 2.0 and semantic systems at enterprise level will be presented. , and instruments to predict and use them for analytically describe knowledge creation processes are important pre-requisites for their large-scale. [[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse7_EconomicIncentives.pdf|Economic incentives for Web2.0 and semantic applications - course]]\r\n\r\nCourse topics:\r\n\r\n**Integration into management activities**\r\n*[[http://videolectures.net/coinactivess2010_imtiaz_aam|ACTIVE, Ali and More]]\r\n\r\n**Cost-benefit methods**\r\n*[[http://videolectures.net//akom08_popov_cbi|Costs, benefits and incentives (of semantic techologies)]]\r\n\r\n**Economic measurements**\r\n**Comprehensive models and tooling for economic measurements**\r\n**Business practices in knowledge technologies and social software**\r\n**Recommendations for tool design**\r\n\r\n Related taks:\r\n*[[http://videolectures.net/active09_sundaresan_tre|Trust and Reputation in eCommerce]]\r\n*[[http://videolectures.net/akom08_popov_cbi/|Costs, benefits and incentives (of semantic techologies)]]\r\n*[[http://videolectures.net/webstart08_mcgough_buan/|Business Angel]]\r\n*[[http://videolectures.net/eswc08_shadbolt_gst|Garlik: Semantic Technology for the Consumer]]\r\n\r\n ----\r\n ----\r\n\r\n == 8. Social, Ethical, Legal And Organizational Issues Related To Novel Semantic Technologies And Context Aware Systems\r\n\r\nIn this unit social, ethical, legal, and organizational issues are discussed and current trends pointed out. New business models and their applicability are discussed, namely through the introduction of examples. Marketing and sustainability, intellectual property management, systems of incentives, etc. are other relevant issues.\r\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse8_SocialIssues.pdf|Social, ethical, legal and organizational issues related to novel semantic technologies - course]]\r\n\r\nCourse topics:\r\n\r\n**Privacy models**\r\n*[[http://videolectures.net/coinactivess2010_djordjevic_acs|Accenture Case Study: Enterprise Collaboration & Knowledge Management through Machine Learning and Semantic Technologies]]\r\n*[[http://videolectures.net/ssms08_boehme_cpiimi|Concerns about Privacy & Innovation in ICT and Media Industries]]\r\n*[[http://videolectures.net/kdd09_li_otbpud|On the Tradeoff Between Privacy and Utility in Data Publishing ]]\r\n\r\n**Knowledge sharing and collaboration incentives**\r\n\r\nRelated talks:\r\n*[[http://videolectures.net/active09_zwegers_ficc/|On Future Internet, Cloud Computing and Semantics - You Name It]]\r\n*[[http://videolectures.net/iswc08_giannandrea_fowdw/|Freebase: An Open, Writable Database of the World\u2019s Information|]]\r\n*[[http://videolectures.net/iswc08_decker_mib/|Message in a Bottle or: How can the Semantic Web Community be more convincing?]]\r\n\r\n == 9. ACTIVE project - Introduction\r\n\r\nThe aim of this course is to present the aims, goals, structure and expected results of ACTIVE project. In addition this course provides information about the future plans, development and operations of the business development that will follow the ACTIVE project. In that respect the future directions in the research areas will be presented, their use potentials, development scenarios and business scenarios in the forms of business plans. [[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse9_Introduction.pdf|ACTIVE project - Introduction - course]]\r\n\r\n*[[http://videolectures.net/akom08_warren_welcome|Welcome to the ACTIVE kick off meeting]]\r\n*[[http://videolectures.net/active09_mladenic_warren_oai|Opening and Introduction of the 1st ACTIVE Summer School]]\r\n*[[http://videolectures.net/coinactivess2010_warren_ai|ACTIVE Introduction]]\r\n*[[http://videolectures.net/coinactivess2010_jermol_wel|Welcome to the Summer School on Advanced Technologies for Knowledge Intensive Networked Organizations 2010 - Aachen ]]\r\n\r\n == 10. Using ACTIVE solution\r\n\r\nThe aim of this training module is to train the potential users/adopters/developers on the ACTIVE solutions. The course is organised as a blended learning module that combines traditional two days event with the self-learning courses in LC. The main learning goals for this module are to teach about basics of ACTIVE solutions and infrastructure, provide technical specifications, development environment and cases. [[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse10_UsingACTIVESolution.pdf|Using ACTIVE solution - course]]\r\n\r\nCourse topics:\r\n\r\n**ACTIVE prototypes, ACTIVE Knowledge Workspace Desktop, ACTIVE SDK, ACTIVE Integrated Platform**\r\n*[[http://videolectures.net/active09_dolinsek_iak|Introduction to the ACTIVE Knowledge Workspace SDK]]\r\n*[[http://videolectures.net/coinactivess2010_dolinsek_akw|Active Knowledge Work Space demonstration]]", "recorded": "2011-02-23T15:18:56", "title": "ACTIVE - Training programme for industries"}, {"url": "active_academia_training", "desc": "back to [[http://videolectures.net/active|ACTIVE training project ]]\r\n\r\n===\r\nThe goal of training activities in ACTIVE is to achieve the transfer of knowledge and best practices within the project as well as (principally) outside the project. ACTIVE training program is aimed at training individuals and groups on the topics that are relevant to the ACTIVE project. Each training program covers a general topic and is generated from several training courses that are focused on specific topics. Furthermore, each training program combines different types and forms of learning: traditional, ICT supported or blended training event. For each training course we provide textual materials, references to the sources used for training module preparation, a list of the topic relevant ACTIVE deliverables, video tutorials and additional materials/suggested readings.\r\n\r\n== 1. Theoretical Foundations and Conceptual Models\r\n\r\nThis course starts with the theoretical foundations on the organisational and collaboration forms, business process modelling, management and collaboration mechanisms, knowledge management and current governance principles in organisations. In contrast it provides basic information about traditional and emerging technologies that influence traditional business organisation and management context. Then it gives motivation for the complete ACTIVE training programme by providing incentives, future trends and business potentials.\r\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse1_TheoreticalFoundations.pdf|Theoretical foundations and conceptual models - course]]\r\n\r\nCourse topics:\r\n\r\n**Collaboration and collaborative knowledge creation**\r\n*[[http://videolectures.net/coinactivess2010_dengler_sm|Semantic MediaWiki]]\r\n*[[http://videolectures.net/semseach09_albakour_mcpse|Managing Collaboration Projects using Semantic Email]]\r\n*[[http://videolectures.net/iswc08_tudarache_scodp|Supporting Collaborative Ontology Development in Protege]]\r\n*[[http://videolectures.net/eswc08_schaffert_sw|Semantic Wikis - IkeWiki - A Semantic Wiki for Collaborative Knowledge Management]]\r\n*[[http://videolectures.net/ice08_kristensen_pcik|Productivity in Collaboration-intensive Knowledge Work: The Collaboration Management Imperative]]\r\n*[[http://videolectures.net/nano07_gadlin_rsc|Re-thinking scientific teams: competition, conflict and collaboration]]\r\n*[[http://videolectures.net/iswc06_auer_otssc|In-Use 1: OntoWiki - A Tool for Social, Semantic Collaboration]]\r\n\r\n**Knowledge processes and tasks**\r\n*[[http://videolectures.net/active09_tilly_ikp|Informal Knowledge Processes: The Long Tail of Business Processes]]\r\n*[[http://videolectures.net/eswc08_feldcamp_sb|Workshop on Semantic Business Process Management - GoMoKIT- Towards an applicable goal-oriented Business Process Modelling approach for knowledge-intensive Tasks]]\r\n\r\n**Process modelling**\r\n*[[http://videolectures.net/eswc08_feldcamp_sb|Workshop on Semantic Business Process Management - GoMoKIT- Towards an applicable goal-oriented Business Process Modelling approach for knowledge-intensive Tasks]]\r\n\r\n**Knowledge management**\r\n*[[http://videolectures.net/coinactivess2010_dengler_sm/|Semantic MediaWiki]]\r\n*[[http://videolectures.net/training06_davies_ka|Knowledge Access]]\r\n*[[http://videolectures.net/iswc08_bhagdev_cauoswilno|Creating and Using Organisational Semantic Webs in Large Networked Organisations]]\r\n*[[http://videolectures.net/cikm08_feldman_ak|Automating Knowledge]]\r\n*[[http://videolectures.net/eswc08_schaffert_sw|Semantic Wikis - IkeWiki - A Semantic Wiki for Collaborative Knowledge Management]]\r\n\r\n**Formalisms for (dynamic) aspects of knowledge worker context and enterprises**\r\n*[[http://videolectures.net/iswc06_grobelnik_cskrs|Context Sensitivity in Knowledge Rich Systems]]\r\n*[[http://videolectures.net/iswc06_witbrock_cskrs|Context Sensitivity in Knowledge Rich Systems]]\r\n*[[http://videolectures.net/iswc06_mozetic_cskrs|Context Sensitivity in Knowledge Rich Systems]]\r\n*[[http://videolectures.net/iswc06_haase_cop|Context Sensitivity in Knowledge Rich Systems - Contents of parts 2]]\r\n\r\n ----\r\n ----\r\n\r\n == 2. Knowledge Models and Structures\r\n\r\nThis course deals with the theoretical background on knowledge and semantic technologies and examines them from a technological, historical and scientific perspective. It starts with the basic facts about knowledge structures and models and their role in the knowledge formalisation, modelling, reasoning and adaptation. It provides comparison and contrasting of diverse knowledge models, structures and systems and understanding their participation in industry.\r\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse2_KnowledgeModels.pdf|Knowledge models and structures - course]]\r\n\r\nCourse topics:\r\n\r\n**Semantic languages**\r\n*[[http://videolectures.net/coinactivess2010_dengler_sm/|Semantic MediaWiki]]\r\n*[[http://videolectures.net/training06_sure_stsw/|A short Tutorial on Semantic Web]]\r\n*[[http://videolectures.net/eswc09_tappolet_atre|Applied Temporal RDF: Ef?cient Temporal Querying of RDF Data with SPARQL]]\r\n*[[http://videolectures.net/eswc09_vennekens_faaeod|FO(ID) as an Extension of DL with Rules]]\r\n*[[http://videolectures.net/iswc08_perez_nsparql|NSPARQL: A Navigational Language for RDF]]\r\n*[[http://videolectures.net/iswc08_hausenblas_bwdwd|RDFa - Bridging the Web of Documents and the Web of Data]]\r\n\r\n**Knowledge formalisation  and  representation**\r\n*[[http://videolectures.net/coinactivess2010_dengler_sm|Semantic MediaWiki]]\r\n*[[http://videolectures.net/akom08_krotzsch_fik|Formal and Informal knowledge representation]]\r\n*[[http://videolectures.net/iswc08_saggion_krebi|Knowledge Representation and Extraction for Business Intelligence]]\r\n\r\n**Reasoning and  probabilistic temporal models**\r\n*[[http://videolectures.net/iswc08_moller_itsr|Reasoning for Ontology Engineering and Usage]]\r\n*[[http://videolectures.net/ssll09_pagnucco_krr|Knowledge Representation and Reasoning]]\r\n*[[http://videolectures.net/bsciw08_schwaighofer_krrd|Knowledge Representation and Reasoning - Discussion]]\r\n\r\n**Knowledge structures**\r\n**Collaborative articulation of expressive knowledge**\r\n**Knowledge leveraging and repair models**\r\n**Knowledge-based adaptation**\r\n**Knowledge creation cycle**\r\n\r\nRelated talks:\r\n*[[http://videolectures.net/iswc08_hauer_asnrwpdehep|An architecture for semantic navigation and reasoning with patient data - experiences of the Health-e-Child project]]\r\n*[[http://videolectures.net/eswc08_blanco_sr|Semantic Reasoning: A Path To New Possibilities of Personalization]]\r\n*[[http://videolectures.net/akom08_krotzsch_fik/|Formal and Informal knowledge representation]]\r\n*[[http://videolectures.net/active09_ghani_rdekm/|Research Directions in Enterprise Knowledge Management]]\r\n\r\n ----\r\n ----\r\n\r\n == 3. Background Technologies\r\n\r\nThis course presents and explains core technologies from the area of knowledge technologies that ranges from the data driven methods to knowledge driven methods and are important for detecting, analysing and managing knowledge (tacit and explicit) in organisations. Recent developments in the particular areas are demonstrated through real software prototypes, successful market cases and real business implementations. The course provides many demos that are available as demonstrations online and could be supplemented with the hands-on session.\r\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse3_BackgroundTechnologies.pdf|Background technologies - course]]\r\n\r\nCourse topics:\r\n\r\n**Context mining**\r\n*[[http://videolectures.net/coinactivess2010_grobelnik_bpm|Business Process Mining and Formalization]]\r\n*[[http://videolectures.net/um05_loosli_ccdoc|Context changes detection by one-class svms]]\r\n*[[http://videolectures.net/ice08_lukowicz_crw|Context recognition in the wearIT@work project]]\r\n*[[http://videolectures.net/kdd09_zhu_mrciws|Mining Rich Session Context to Improve Web Search]]\r\n*[[http://videolectures.net/samt08_santini_cnod|Context as a non-ontological determinant of semantics]]\r\n\r\n**Stream mining**\r\n*[[http://videolectures.net/ecml07_gama_sad|State of the Art in Data Stream Mining]]\r\n*[[http://videolectures.net/ecml07_mohamed_sad|State of the Art in Data Stream Mining]]\r\n*[[http://videolectures.net/ecml07_mohamed_acac|An architecture for context-aware adaptive data stream mining]]\r\n*[[http://videolectures.net/ecml07_mohamed_mqgr|A Model for Quality Guaranteed Resource-Aware Stream Mining]]\r\n\r\n**Anomaly detection**\r\n*[[http://videolectures.net/ecmlpkdd08_lazarevic_dmfa|Data Mining for Anomaly Detection]]\r\n*[[http://videolectures.net/mmdss07_tishby_itam|Information Theo-retic and Alge-braic Methods for Network Anomaly Detection]]\r\n*[[http://videolectures.net/kdd09_jermaine_alrtffsad|A LRT Framework for Fast Spacial Anomaly Detection]]\r\n\r\n**Social network analysis**\r\n*[[http://videolectures.net/semseach09_graves_srrdsn|Searching and ranking in RDF documents and social networks]]\r\n*[[http://videolectures.net/icwsm09_agarwal_siaifs|A Social Identity Approach to Identify Familiar Strangers in a Social Network]]\r\n*[[http://videolectures.net/kdd09_jermaine_alrtffsad|A LRT Framework for Fast Spacial Anomaly Detection]]\r\n*[[http://videolectures.net/iswc07_aasman_usn|Using Social Network Analysis, Geotemporal Reasoning and RDFS++ Reasoning for Business Intelligence]]\r\n\r\n**Social software and Web 2.0**\r\n*[[http://videolectures.net/active09_mulvany_cwsc|A Cabinet of Web 2.0 Scientific Curiositics]]\r\n*[[http://videolectures.net/eswc08_gomes_cs|Collective Semantics: Collective Intelligence & the Semantic Web - Flickring Our World]]\r\n*[[http://videolectures.net/eccs07_huberman_bwe|Beyond Web 2.0]]\r\n*[[http://videolectures.net/icwsm09_sun_gmctfnf|Gesundheit! Modeling Contagion Through Facebook News Feed]]\r\n*[[http://videolectures.net/eswc08_halpin_sw|Panel II: Social Network Portability: Is the Semantic Web Ready?]]\r\n*[[http://videolectures.net/cikm08_perisic_usnfsw|Using Social Networks for Social Work]]\r\n*[[http://videolectures.net/www09_baezayates_mtwbs|Mining the Web 2.0 for Better Search]]\r\n*[[http://videolectures.net/samt08_baumann_wai|WhoAmI - A Web2.0 Platform for Faceted Identity Management through Aggregation of Social Media]]\r\n\r\n**Adaptive and context-aware systems**\r\n*[[http://videolectures.net/samt08_rodriguez_doncel_smac|A Semantic Model for the Authorisation of Context-Aware Content Adaptation]]\r\n\r\n**Semantic technologies and content**\r\n**Social software and Web 2.0**\r\n**Knowledge filters**\r\n**Meta learning**\r\n**Forecasting**\r\n\r\nRelated talks:\r\n*[[http://videolectures.net/eccs07_huberman_bwe|Beyond Web 2.0]]\r\n*[[http://videolectures.net/kdd07_fayyad_fmtw|From Mining the Web to Inventing the New Sciences Underlying the Internet ]]\r\n*[[http://videolectures.net/ecml07_baeza_yates_mwq|Mining Queries]]\r\n*[[http://videolectures.net/estc08_zaragoza_isst|Improving Search with Semantic Technologies: Current Research Directions]]\r\n*[[http://videolectures.net/active09_grobelnik_tmlws|Text Mining and Light Weight Semantics]]\r\n\r\n ----\r\n ----\r\n\r\n == 4. ACTIVE Innovative Solutions\r\n\r\nThis is one of the main courses in ACTIVE training programmes. It is aimed at presenting ACTIVE innovative solutions that will be developed in the frame of the project. Training starts with innovative business and organisational models that are consequence of newly introduced technologies. The main focus is on the explanation and demonstration of technologies developed in ACTIVE, their use in the industry contexts and on the discussion of their implications to the traditional business environment. Course addresses in detail the methods for identifying knowledge processes, hidden knowledge in organisations as well as new technology research streams. Furthermore it explains and demonstrates the methods for contextualisation that spans over three orthogonal dimensions: content, social network and time. [[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse4_InnovativeSolutions.pdf|ACTIVE innovative solutions - course]]\r\n\r\nCourse topics:\r\n\r\n**Tools and methods for collaborative and expressive knowledge articulation paradigms**\r\n*[[http://videolectures.net/akom08_warren_welcome|Welcome to the ACTIVE kick off meeting]]\r\n*[[http://videolectures.net/coinactivess2010_dengler_sm|Semantic MediaWiki]]\r\n\r\n**Ontology learning**\r\n*[[http://videolectures.net/coinactivess2010_grobelnik_bpm|Business Process Mining and Formalization]]\r\n*[[http://videolectures.net/training06_grobelnik_tmol|Text Mining for Ontology Learning]]\r\n*[[http://videolectures.net/koml04_grobelnik_olkds|Ontology Learning - Knowledge Discovery and the Semantic Web]]\r\n\r\n**Hybrid Web 2.0 - ontology infrastructure**\r\n*[[http://videolectures.net/coinactivess2010_imtiaz_aam|ACTIVE, Ali and More]]\r\n*[[http://videolectures.net/eswc08_hess_cs|Collective Semantics: Collective Intelligence & the Semantic Web - From Web 2.0 to Semantic Web - A Semi-Automated Approach]]\r\n\r\n**Semi-automated process refactoring**\r\n*[[http://videolectures.net/coinactivess2010_ruiz_moreno_leban_pak|Pro-Active Knowledge Processes Support]]\r\n\r\n**Optimization of knowledge models and pro-active support**\r\n*[[http://videolectures.net/coinactivess2010_ruiz_moreno_leban_pak|Pro-Active Knowledge Processes Support]]\r\n\r\n**Delivery of contextualized information**\r\n*[[http://videolectures.net/coinactivess2010_grobelnik_bpm|Business Process Mining and Formalization]]\r\n\r\n**Visualization of temporal enterprise model**\r\n*[[http://videolectures.net/coinactivess2010_ruiz_moreno_leban_pak|Pro-Active Knowledge Processes Support]]\r\n\r\n**Privacy preserving analysis of enterprise data**\r\n*[[http://videolectures.net/coinactivess2010_djordjevic_acs|Accenture Case Study: Enterprise Collaboration & Knowledge Management through Machine Learning and Semantic Technologies]]\r\n*[[http://videolectures.net/google_roughan_ppdm|Privacy Preserving DataMining]]\r\n\r\n**Inconsistency diagnosis and automatic repair of inconsistencies**\r\n**Complex rule interfaces**\r\n**Knowledge process mining**\r\n**Autonomous, context aware services for knowledge processes**\r\n**Dynamic adaptations of context-aware knowledge processes**\r\n**Security-aware knowledge processes**\r\n**Simultaneous analysis of multiple modalities**\r\n\r\nRelated talks:\r\n*[[http://videolectures.net/akom08_warren_welcome|Welcome to the ACTIVE kick off meeting]]\r\n*[[http://videolectures.net/iswc08_witbrock_fsc|Free Semantic Content: Using OpenCyc in Semantic Web Applications]]\r\n*[[http://videolectures.net/iswc08_dellaValle_rswa|Realizing a Semantic Web Application]]\r\n*[[http://videolectures.net/active09_warren_kmcfl|KM at the Customer Front-Line: The BT Case Study in ACTIVE]]\r\n\r\n ----\r\n ----\r\n\r\n == 5. Management and Problem Solving\r\n\r\nThis course is aimed at providing knowledge about the models, structures and mechanisms for management, coordination and problem solving. The coordination mechanisms are basic mechanisms in any networked organisation and as such the focal point of interest for any distributed, knowledge focused organisation. The course will analyse and study the approaches for distributed and collaborative decision-making including organizational aspects in the context of informal and knowledge processes. Support with ACTIVE models and solutions for decision making processes in inter-enterprise and collaborative environment is the main focus.\r\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse5_Management and problem solving.pdf|Management and problem solving - course]]\r\n\r\nCourse topics:\r\n\r\n**Context sensitive management**\r\n*[[http://videolectures.net/coinactivess2010_imtiaz_aam|ACTIVE, Ali and More]]\r\n\r\n**Pro-active knowledge process support**\r\n*[[http://videolectures.net/coinactivess2010_ruiz_moreno_leban_pak|Pro-Active Knowledge Processes Support]]\r\n\r\n ----\r\n ----\r\n\r\n == 6. Practical Examples and ACTIVE Prototypes\r\n\r\nIn this course the development process of the ACTIVE prototypes for three specific applications will be demonstrated. The development process and the applications with their specific benefits which are created by the ACTIVE research results will be shown. The course will be supplemented with a hands-on workshop to allow participants to deal with concrete examples from their work context. The focus will be on identifying areas of applications where the functionalities developed in the ACTIVE project can be applied.\r\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse6_PracticalExamples.pdf|Practical Examples and ACTIVE Prototypes - course]]\r\n\r\nCourse topics:\r\n\r\n**Case studies and best practices**\r\n*[[http://videolectures.net/active09_warren_kmcfl|KM at the Customer Front-Line: The BT Case Study in ACTIVE]]\r\n*[[http://videolectures.net/coinactivess2010_thurlow_bcs|ACTIVE\u000b BT Case Study]]\r\n*[[http://videolectures.net/coinactivess2010_djordjevic_acs|Accenture Case Study: Enterprise Collaboration & Knowledge Management through Machine Learning and Semantic Technologies]]\r\n\r\n**Demonstrators, demos, prototypes**\r\n*[[http://videolectures.net/eswc08_berges_smw|Semantic Web Technology for Agent Communication Protocols]]\r\n*[[http://videolectures.net/active09_dolinsek_iak|Introduction to the ACTIVE Knowledge Workspace SDK]]\r\n\r\n**Use scenarios**\r\n\r\n ----\r\n ----\r\n\r\n == 7. Economic Incentives for Web 2.0 and Semantic Applications\r\n\r\nThis course will in particular investigate aspects related to the economics and the incentives behind Web2.0 and semantic technologies. Issues of costs and benefits associated to developing, deploying or maintaining Web 2.0 and semantic systems at enterprise level will be presented. , and instruments to predict and use them for analytically describe knowledge creation processes are important pre-requisites for their large-scale. [[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse7_EconomicIncentives.pdf|Economic incentives for Web2.0 and semantic applications - course]]\r\n\r\nCourse topics:\r\n\r\n**Integration into management activities**\r\n*[[http://videolectures.net/coinactivess2010_imtiaz_aam|ACTIVE, Ali and More]]\r\n\r\n**Cost-benefit methods**\r\n*[[http://videolectures.net//akom08_popov_cbi|Costs, benefits and incentives (of semantic techologies)]]\r\n\r\n**Economic measurements**\r\n**Comprehensive models and tooling for economic measurements**\r\n**Business practices in knowledge technologies and social software**\r\n**Recommendations for tool design**\r\n\r\n Related taks:\r\n*[[http://videolectures.net/active09_sundaresan_tre|Trust and Reputation in eCommerce]]\r\n*[[http://videolectures.net/akom08_popov_cbi/|Costs, benefits and incentives (of semantic techologies)]]\r\n*[[http://videolectures.net/webstart08_mcgough_buan/|Business Angel]]\r\n*[[http://videolectures.net/eswc08_shadbolt_gst|Garlik: Semantic Technology for the Consumer]]\r\n\r\n ----\r\n ----\r\n\r\n == 8. Social, Ethical, Legal And Organizational Issues Related To Novel Semantic Technologies And Context Aware Systems\r\n\r\nIn this unit social, ethical, legal, and organizational issues are discussed and current trends pointed out. New business models and their applicability are discussed, namely through the introduction of examples. Marketing and sustainability, intellectual property management, systems of incentives, etc. are other relevant issues.\r\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse8_SocialIssues.pdf|Social, ethical, legal and organizational issues related to novel semantic technologies - course]]\r\n\r\nCourse topics:\r\n\r\n**Privacy models**\r\n*[[http://videolectures.net/coinactivess2010_djordjevic_acs|Accenture Case Study: Enterprise Collaboration & Knowledge Management through Machine Learning and Semantic Technologies]]\r\n*[[http://videolectures.net/ssms08_boehme_cpiimi|Concerns about Privacy & Innovation in ICT and Media Industries]]\r\n*[[http://videolectures.net/kdd09_li_otbpud|On the Tradeoff Between Privacy and Utility in Data Publishing ]]\r\n\r\n**Knowledge sharing and collaboration incentives**\r\n\r\nRelated talks:\r\n*[[http://videolectures.net/active09_zwegers_ficc/|On Future Internet, Cloud Computing and Semantics - You Name It]]\r\n*[[http://videolectures.net/iswc08_giannandrea_fowdw/|Freebase: An Open, Writable Database of the World\u2019s Information|]]\r\n*[[http://videolectures.net/iswc08_decker_mib/|Message in a Bottle or: How can the Semantic Web Community be more convincing?]]\r\n\r\n == 9. ACTIVE project - Introduction\r\n\r\nThe aim of this course is to present the aims, goals, structure and expected results of ACTIVE project. In addition this course provides information about the future plans, development and operations of the business development that will follow the ACTIVE project. In that respect the future directions in the research areas will be presented, their use potentials, development scenarios and business scenarios in the forms of business plans. [[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse9_Introduction.pdf|ACTIVE project - Introduction - course]]\r\n\r\n*[[http://videolectures.net/akom08_warren_welcome|Welcome to the ACTIVE kick off meeting]]\r\n*[[http://videolectures.net/active09_mladenic_warren_oai|Opening and Introduction of the 1st ACTIVE Summer School]]\r\n*[[http://videolectures.net/coinactivess2010_warren_ai|ACTIVE Introduction]]\r\n*[[http://videolectures.net/coinactivess2010_jermol_wel|Welcome to the Summer School on Advanced Technologies for Knowledge Intensive Networked Organizations 2010 - Aachen ]]\r\n\r\n == 10. Using ACTIVE solution\r\n\r\nThe aim of this training module is to train the potential users/adopters/developers on the ACTIVE solutions. The course is organised as a blended learning module that combines traditional two days event with the self-learning courses in LC. The main learning goals for this module are to teach about basics of ACTIVE solutions and infrastructure, provide technical specifications, development environment and cases. [[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse10_UsingACTIVESolution.pdf|Using ACTIVE solution - course]]\r\n\r\nCourse topics:\r\n\r\n**ACTIVE prototypes, ACTIVE Knowledge Workspace Desktop, ACTIVE SDK, ACTIVE Integrated Platform**\r\n*[[http://videolectures.net/active09_dolinsek_iak|Introduction to the ACTIVE Knowledge Workspace SDK]]\r\n*[[http://videolectures.net/coinactivess2010_dolinsek_akw|Active Knowledge Work Space demonstration]]", "recorded": "2011-02-23T15:14:37", "title": "\u2022\tACTIVE - Training programme for academia"}, {"url": "active_general_training", "desc": "back to [[http://videolectures.net/active|ACTIVE training project ]]\r\n\r\n===\r\nThe goal of training activities in ACTIVE is to achieve the transfer of knowledge and best practices within the project as well as (principally) outside the project. ACTIVE training program is aimed at training individuals and groups on the topics that are relevant to the ACTIVE project. Each training program covers a general topic and is generated from several training courses that are focused on specific topics. Furthermore, each training program combines different types and forms of learning: traditional, ICT supported or blended training event. For each training course we provide textual materials, references to the sources used for training module preparation, a list of the topic relevant ACTIVE deliverables, video tutorials and additional materials/suggested readings.\r\n\r\n== 1. Theoretical Foundations and Conceptual Models\r\n\r\nThis course starts with the theoretical foundations on the organisational and collaboration forms, business process modelling, management and collaboration mechanisms, knowledge management and current governance principles in organisations. In contrast it provides basic information about traditional and emerging technologies that influence traditional business organisation and management context. Then it gives motivation for the complete ACTIVE training programme by providing incentives, future trends and business potentials.\r\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse1_TheoreticalFoundations.pdf|Theoretical foundations and conceptual models - course]]\r\n\r\nCourse topics:\r\n\r\n**Collaboration and collaborative knowledge creation**\r\n*[[http://videolectures.net/coinactivess2010_dengler_sm|Semantic MediaWiki]]\r\n*[[http://videolectures.net/semseach09_albakour_mcpse|Managing Collaboration Projects using Semantic Email]]\r\n*[[http://videolectures.net/iswc08_tudarache_scodp|Supporting Collaborative Ontology Development in Protege]]\r\n*[[http://videolectures.net/eswc08_schaffert_sw|Semantic Wikis - IkeWiki - A Semantic Wiki for Collaborative Knowledge Management]]\r\n*[[http://videolectures.net/ice08_kristensen_pcik|Productivity in Collaboration-intensive Knowledge Work: The Collaboration Management Imperative]]\r\n*[[http://videolectures.net/nano07_gadlin_rsc|Re-thinking scientific teams: competition, conflict and collaboration]]\r\n*[[http://videolectures.net/iswc06_auer_otssc|In-Use 1: OntoWiki - A Tool for Social, Semantic Collaboration]]\r\n\r\n**Knowledge processes and tasks**\r\n*[[http://videolectures.net/active09_tilly_ikp|Informal Knowledge Processes: The Long Tail of Business Processes]]\r\n*[[http://videolectures.net/eswc08_feldcamp_sb|Workshop on Semantic Business Process Management - GoMoKIT- Towards an applicable goal-oriented Business Process Modelling approach for knowledge-intensive Tasks]]\r\n\r\n**Process modelling**\r\n*[[http://videolectures.net/eswc08_feldcamp_sb|Workshop on Semantic Business Process Management - GoMoKIT- Towards an applicable goal-oriented Business Process Modelling approach for knowledge-intensive Tasks]]\r\n\r\n**Knowledge management**\r\n*[[http://videolectures.net/coinactivess2010_dengler_sm/|Semantic MediaWiki]]\r\n*[[http://videolectures.net/training06_davies_ka|Knowledge Access]]\r\n*[[http://videolectures.net/iswc08_bhagdev_cauoswilno|Creating and Using Organisational Semantic Webs in Large Networked Organisations]]\r\n*[[http://videolectures.net/cikm08_feldman_ak|Automating Knowledge]]\r\n*[[http://videolectures.net/eswc08_schaffert_sw|Semantic Wikis - IkeWiki - A Semantic Wiki for Collaborative Knowledge Management]]\r\n\r\n**Formalisms for (dynamic) aspects of knowledge worker context and enterprises**\r\n*[[http://videolectures.net/iswc06_grobelnik_cskrs|Context Sensitivity in Knowledge Rich Systems]]\r\n*[[http://videolectures.net/iswc06_witbrock_cskrs|Context Sensitivity in Knowledge Rich Systems]]\r\n*[[http://videolectures.net/iswc06_mozetic_cskrs|Context Sensitivity in Knowledge Rich Systems]]\r\n*[[http://videolectures.net/iswc06_haase_cop|Context Sensitivity in Knowledge Rich Systems - Contents of parts 2]]\r\n\r\n ----\r\n ----\r\n\r\n == 2. Knowledge Models and Structures\r\n\r\nThis course deals with the theoretical background on knowledge and semantic technologies and examines them from a technological, historical and scientific perspective. It starts with the basic facts about knowledge structures and models and their role in the knowledge formalisation, modelling, reasoning and adaptation. It provides comparison and contrasting of diverse knowledge models, structures and systems and understanding their participation in industry.\r\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse2_KnowledgeModels.pdf|Knowledge models and structures - course]]\r\n\r\nCourse topics:\r\n\r\n**Semantic languages**\r\n*[[http://videolectures.net/coinactivess2010_dengler_sm/|Semantic MediaWiki]]\r\n*[[http://videolectures.net/training06_sure_stsw/|A short Tutorial on Semantic Web]]\r\n*[[http://videolectures.net/eswc09_tappolet_atre|Applied Temporal RDF: Ef?cient Temporal Querying of RDF Data with SPARQL]]\r\n*[[http://videolectures.net/eswc09_vennekens_faaeod|FO(ID) as an Extension of DL with Rules]]\r\n*[[http://videolectures.net/iswc08_perez_nsparql|NSPARQL: A Navigational Language for RDF]]\r\n*[[http://videolectures.net/iswc08_hausenblas_bwdwd|RDFa - Bridging the Web of Documents and the Web of Data]]\r\n\r\n**Knowledge formalisation  and  representation**\r\n*[[http://videolectures.net/coinactivess2010_dengler_sm|Semantic MediaWiki]]\r\n*[[http://videolectures.net/akom08_krotzsch_fik|Formal and Informal knowledge representation]]\r\n*[[http://videolectures.net/iswc08_saggion_krebi|Knowledge Representation and Extraction for Business Intelligence]]\r\n\r\n**Reasoning and  probabilistic temporal models**\r\n*[[http://videolectures.net/iswc08_moller_itsr|Reasoning for Ontology Engineering and Usage]]\r\n*[[http://videolectures.net/ssll09_pagnucco_krr|Knowledge Representation and Reasoning]]\r\n*[[http://videolectures.net/bsciw08_schwaighofer_krrd|Knowledge Representation and Reasoning - Discussion]]\r\n\r\n**Knowledge structures**\r\n**Collaborative articulation of expressive knowledge**\r\n**Knowledge leveraging and repair models**\r\n**Knowledge-based adaptation**\r\n**Knowledge creation cycle**\r\n\r\nRelated talks:\r\n*[[http://videolectures.net/iswc08_hauer_asnrwpdehep|An architecture for semantic navigation and reasoning with patient data - experiences of the Health-e-Child project]]\r\n*[[http://videolectures.net/eswc08_blanco_sr|Semantic Reasoning: A Path To New Possibilities of Personalization]]\r\n*[[http://videolectures.net/akom08_krotzsch_fik/|Formal and Informal knowledge representation]]\r\n*[[http://videolectures.net/active09_ghani_rdekm/|Research Directions in Enterprise Knowledge Management]]\r\n\r\n ----\r\n ----\r\n\r\n == 6. Practical Examples and ACTIVE Prototypes\r\n\r\nIn this course the development process of the ACTIVE prototypes for three specific applications will be demonstrated. The development process and the applications with their specific benefits which are created by the ACTIVE research results will be shown. The course will be supplemented with a hands-on workshop to allow participants to deal with concrete examples from their work context. The focus will be on identifying areas of applications where the functionalities developed in the ACTIVE project can be applied.\r\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse6_PracticalExamples.pdf|Practical Examples and ACTIVE Prototypes - course]]\r\n\r\nCourse topics:\r\n\r\n**Case studies and best practices**\r\n*[[http://videolectures.net/active09_warren_kmcfl|KM at the Customer Front-Line: The BT Case Study in ACTIVE]]\r\n*[[http://videolectures.net/coinactivess2010_thurlow_bcs|ACTIVE\u000b BT Case Study]]\r\n*[[http://videolectures.net/coinactivess2010_djordjevic_acs|Accenture Case Study: Enterprise Collaboration & Knowledge Management through Machine Learning and Semantic Technologies]]\r\n\r\n**Demonstrators, demos, prototypes**\r\n*[[http://videolectures.net/eswc08_berges_smw|Semantic Web Technology for Agent Communication Protocols]]\r\n*[[http://videolectures.net/active09_dolinsek_iak|Introduction to the ACTIVE Knowledge Workspace SDK]]\r\n\r\n**Use scenarios**\r\n\r\n ----\r\n ----\r\n\r\n == 9. ACTIVE project - Introduction\r\n\r\nThe aim of this course is to present the aims, goals, structure and expected results of ACTIVE project. In addition this course provides information about the future plans, development and operations of the business development that will follow the ACTIVE project. In that respect the future directions in the research areas will be presented, their use potentials, development scenarios and business scenarios in the forms of business plans. [[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse9_Introduction.pdf|ACTIVE project - Introduction - course]]\r\n\r\n*[[http://videolectures.net/akom08_warren_welcome|Welcome to the ACTIVE kick off meeting]]\r\n*[[http://videolectures.net/active09_mladenic_warren_oai|Opening and Introduction of the 1st ACTIVE Summer School]]\r\n*[[http://videolectures.net/coinactivess2010_warren_ai|ACTIVE Introduction]]\r\n*[[http://videolectures.net/coinactivess2010_jermol_wel|Welcome to the Summer School on Advanced Technologies for Knowledge Intensive Networked Organizations 2010 - Aachen ]]", "recorded": "2011-02-23T15:21:23", "title": "ACTIVE - Training/promotion programme for general public"}, {"url": "active_multipliers_training", "desc": "back to [[http://videolectures.net/active|ACTIVE training project ]]\r\n\r\n===\r\nThe goal of training activities in ACTIVE is to achieve the transfer of knowledge and best practices within the project as well as (principally) outside the project. ACTIVE training program is aimed at training individuals and groups on the topics that are relevant to the ACTIVE project. Each training program covers a general topic and is generated from several training courses that are focused on specific topics. Furthermore, each training program combines different types and forms of learning: traditional, ICT supported or blended training event. For each training course we provide textual materials, references to the sources used for training module preparation, a list of the topic relevant ACTIVE deliverables, video tutorials and additional materials/suggested readings.\r\n\r\n== 1. Theoretical Foundations and Conceptual Models\r\n\r\nThis course starts with the theoretical foundations on the organisational and collaboration forms, business process modelling, management and collaboration mechanisms, knowledge management and current governance principles in organisations. In contrast it provides basic information about traditional and emerging technologies that influence traditional business organisation and management context. Then it gives motivation for the complete ACTIVE training programme by providing incentives, future trends and business potentials.\r\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse1_TheoreticalFoundations.pdf|Theoretical foundations and conceptual models - course]]\r\n\r\nCourse topics:\r\n\r\n**Collaboration and collaborative knowledge creation**\r\n*[[http://videolectures.net/coinactivess2010_dengler_sm|Semantic MediaWiki]]\r\n*[[http://videolectures.net/semseach09_albakour_mcpse|Managing Collaboration Projects using Semantic Email]]\r\n*[[http://videolectures.net/iswc08_tudarache_scodp|Supporting Collaborative Ontology Development in Protege]]\r\n*[[http://videolectures.net/eswc08_schaffert_sw|Semantic Wikis - IkeWiki - A Semantic Wiki for Collaborative Knowledge Management]]\r\n*[[http://videolectures.net/ice08_kristensen_pcik|Productivity in Collaboration-intensive Knowledge Work: The Collaboration Management Imperative]]\r\n*[[http://videolectures.net/nano07_gadlin_rsc|Re-thinking scientific teams: competition, conflict and collaboration]]\r\n*[[http://videolectures.net/iswc06_auer_otssc|In-Use 1: OntoWiki - A Tool for Social, Semantic Collaboration]]\r\n\r\n**Knowledge processes and tasks**\r\n*[[http://videolectures.net/active09_tilly_ikp|Informal Knowledge Processes: The Long Tail of Business Processes]]\r\n*[[http://videolectures.net/eswc08_feldcamp_sb|Workshop on Semantic Business Process Management - GoMoKIT- Towards an applicable goal-oriented Business Process Modelling approach for knowledge-intensive Tasks]]\r\n\r\n**Process modelling**\r\n*[[http://videolectures.net/eswc08_feldcamp_sb|Workshop on Semantic Business Process Management - GoMoKIT- Towards an applicable goal-oriented Business Process Modelling approach for knowledge-intensive Tasks]]\r\n\r\n**Knowledge management**\r\n*[[http://videolectures.net/coinactivess2010_dengler_sm/|Semantic MediaWiki]]\r\n*[[http://videolectures.net/training06_davies_ka|Knowledge Access]]\r\n*[[http://videolectures.net/iswc08_bhagdev_cauoswilno|Creating and Using Organisational Semantic Webs in Large Networked Organisations]]\r\n*[[http://videolectures.net/cikm08_feldman_ak|Automating Knowledge]]\r\n*[[http://videolectures.net/eswc08_schaffert_sw|Semantic Wikis - IkeWiki - A Semantic Wiki for Collaborative Knowledge Management]]\r\n\r\n**Formalisms for (dynamic) aspects of knowledge worker context and enterprises**\r\n*[[http://videolectures.net/iswc06_grobelnik_cskrs|Context Sensitivity in Knowledge Rich Systems]]\r\n*[[http://videolectures.net/iswc06_witbrock_cskrs|Context Sensitivity in Knowledge Rich Systems]]\r\n*[[http://videolectures.net/iswc06_mozetic_cskrs|Context Sensitivity in Knowledge Rich Systems]]\r\n*[[http://videolectures.net/iswc06_haase_cop|Context Sensitivity in Knowledge Rich Systems - Contents of parts 2]]\r\n\r\n ----\r\n ----\r\n\r\n == 2. Knowledge Models and Structures\r\n\r\nThis course deals with the theoretical background on knowledge and semantic technologies and examines them from a technological, historical and scientific perspective. It starts with the basic facts about knowledge structures and models and their role in the knowledge formalisation, modelling, reasoning and adaptation. It provides comparison and contrasting of diverse knowledge models, structures and systems and understanding their participation in industry.\r\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse2_KnowledgeModels.pdf|Knowledge models and structures - course]]\r\n\r\nCourse topics:\r\n\r\n**Semantic languages**\r\n*[[http://videolectures.net/coinactivess2010_dengler_sm/|Semantic MediaWiki]]\r\n*[[http://videolectures.net/training06_sure_stsw/|A short Tutorial on Semantic Web]]\r\n*[[http://videolectures.net/eswc09_tappolet_atre|Applied Temporal RDF: Ef?cient Temporal Querying of RDF Data with SPARQL]]\r\n*[[http://videolectures.net/eswc09_vennekens_faaeod|FO(ID) as an Extension of DL with Rules]]\r\n*[[http://videolectures.net/iswc08_perez_nsparql|NSPARQL: A Navigational Language for RDF]]\r\n*[[http://videolectures.net/iswc08_hausenblas_bwdwd|RDFa - Bridging the Web of Documents and the Web of Data]]\r\n\r\n**Knowledge formalisation  and  representation**\r\n*[[http://videolectures.net/coinactivess2010_dengler_sm|Semantic MediaWiki]]\r\n*[[http://videolectures.net/akom08_krotzsch_fik|Formal and Informal knowledge representation]]\r\n*[[http://videolectures.net/iswc08_saggion_krebi|Knowledge Representation and Extraction for Business Intelligence]]\r\n\r\n**Reasoning and  probabilistic temporal models**\r\n*[[http://videolectures.net/iswc08_moller_itsr|Reasoning for Ontology Engineering and Usage]]\r\n*[[http://videolectures.net/ssll09_pagnucco_krr|Knowledge Representation and Reasoning]]\r\n*[[http://videolectures.net/bsciw08_schwaighofer_krrd|Knowledge Representation and Reasoning - Discussion]]\r\n\r\n**Knowledge structures**\r\n**Collaborative articulation of expressive knowledge**\r\n**Knowledge leveraging and repair models**\r\n**Knowledge-based adaptation**\r\n**Knowledge creation cycle**\r\n\r\nRelated talks:\r\n*[[http://videolectures.net/iswc08_hauer_asnrwpdehep|An architecture for semantic navigation and reasoning with patient data - experiences of the Health-e-Child project]]\r\n*[[http://videolectures.net/eswc08_blanco_sr|Semantic Reasoning: A Path To New Possibilities of Personalization]]\r\n*[[http://videolectures.net/akom08_krotzsch_fik/|Formal and Informal knowledge representation]]\r\n*[[http://videolectures.net/active09_ghani_rdekm/|Research Directions in Enterprise Knowledge Management]]\r\n\r\n ----\r\n ----\r\n\r\n == 3. Background Technologies\r\n\r\nThis course presents and explains core technologies from the area of knowledge technologies that ranges from the data driven methods to knowledge driven methods and are important for detecting, analysing and managing knowledge (tacit and explicit) in organisations. Recent developments in the particular areas are demonstrated through real software prototypes, successful market cases and real business implementations. The course provides many demos that are available as demonstrations online and could be supplemented with the hands-on session.\r\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse3_BackgroundTechnologies.pdf|Background technologies - course]]\r\n\r\nCourse topics:\r\n\r\n**Context mining**\r\n*[[http://videolectures.net/coinactivess2010_grobelnik_bpm|Business Process Mining and Formalization]]\r\n*[[http://videolectures.net/um05_loosli_ccdoc|Context changes detection by one-class svms]]\r\n*[[http://videolectures.net/ice08_lukowicz_crw|Context recognition in the wearIT@work project]]\r\n*[[http://videolectures.net/kdd09_zhu_mrciws|Mining Rich Session Context to Improve Web Search]]\r\n*[[http://videolectures.net/samt08_santini_cnod|Context as a non-ontological determinant of semantics]]\r\n\r\n**Stream mining**\r\n*[[http://videolectures.net/ecml07_gama_sad|State of the Art in Data Stream Mining]]\r\n*[[http://videolectures.net/ecml07_mohamed_sad|State of the Art in Data Stream Mining]]\r\n*[[http://videolectures.net/ecml07_mohamed_acac|An architecture for context-aware adaptive data stream mining]]\r\n*[[http://videolectures.net/ecml07_mohamed_mqgr|A Model for Quality Guaranteed Resource-Aware Stream Mining]]\r\n\r\n**Anomaly detection**\r\n*[[http://videolectures.net/ecmlpkdd08_lazarevic_dmfa|Data Mining for Anomaly Detection]]\r\n*[[http://videolectures.net/mmdss07_tishby_itam|Information Theo-retic and Alge-braic Methods for Network Anomaly Detection]]\r\n*[[http://videolectures.net/kdd09_jermaine_alrtffsad|A LRT Framework for Fast Spacial Anomaly Detection]]\r\n\r\n**Social network analysis**\r\n*[[http://videolectures.net/semseach09_graves_srrdsn|Searching and ranking in RDF documents and social networks]]\r\n*[[http://videolectures.net/icwsm09_agarwal_siaifs|A Social Identity Approach to Identify Familiar Strangers in a Social Network]]\r\n*[[http://videolectures.net/kdd09_jermaine_alrtffsad|A LRT Framework for Fast Spacial Anomaly Detection]]\r\n*[[http://videolectures.net/iswc07_aasman_usn|Using Social Network Analysis, Geotemporal Reasoning and RDFS++ Reasoning for Business Intelligence]]\r\n\r\n**Social software and Web 2.0**\r\n*[[http://videolectures.net/active09_mulvany_cwsc|A Cabinet of Web 2.0 Scientific Curiositics]]\r\n*[[http://videolectures.net/eswc08_gomes_cs|Collective Semantics: Collective Intelligence & the Semantic Web - Flickring Our World]]\r\n*[[http://videolectures.net/eccs07_huberman_bwe|Beyond Web 2.0]]\r\n*[[http://videolectures.net/icwsm09_sun_gmctfnf|Gesundheit! Modeling Contagion Through Facebook News Feed]]\r\n*[[http://videolectures.net/eswc08_halpin_sw|Panel II: Social Network Portability: Is the Semantic Web Ready?]]\r\n*[[http://videolectures.net/cikm08_perisic_usnfsw|Using Social Networks for Social Work]]\r\n*[[http://videolectures.net/www09_baezayates_mtwbs|Mining the Web 2.0 for Better Search]]\r\n*[[http://videolectures.net/samt08_baumann_wai|WhoAmI - A Web2.0 Platform for Faceted Identity Management through Aggregation of Social Media]]\r\n\r\n**Adaptive and context-aware systems**\r\n*[[http://videolectures.net/samt08_rodriguez_doncel_smac|A Semantic Model for the Authorisation of Context-Aware Content Adaptation]]\r\n\r\n**Semantic technologies and content**\r\n**Social software and Web 2.0**\r\n**Knowledge filters**\r\n**Meta learning**\r\n**Forecasting**\r\n\r\nRelated talks:\r\n*[[http://videolectures.net/eccs07_huberman_bwe|Beyond Web 2.0]]\r\n*[[http://videolectures.net/kdd07_fayyad_fmtw|From Mining the Web to Inventing the New Sciences Underlying the Internet ]]\r\n*[[http://videolectures.net/ecml07_baeza_yates_mwq|Mining Queries]]\r\n*[[http://videolectures.net/estc08_zaragoza_isst|Improving Search with Semantic Technologies: Current Research Directions]]\r\n*[[http://videolectures.net/active09_grobelnik_tmlws|Text Mining and Light Weight Semantics]]\r\n\r\n ----\r\n ----\r\n\r\n == 4. ACTIVE Innovative Solutions\r\n\r\nThis is one of the main courses in ACTIVE training programmes. It is aimed at presenting ACTIVE innovative solutions that will be developed in the frame of the project. Training starts with innovative business and organisational models that are consequence of newly introduced technologies. The main focus is on the explanation and demonstration of technologies developed in ACTIVE, their use in the industry contexts and on the discussion of their implications to the traditional business environment. Course addresses in detail the methods for identifying knowledge processes, hidden knowledge in organisations as well as new technology research streams. Furthermore it explains and demonstrates the methods for contextualisation that spans over three orthogonal dimensions: content, social network and time. [[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse4_InnovativeSolutions.pdf|ACTIVE innovative solutions - course]]\r\n\r\nCourse topics:\r\n\r\n**Tools and methods for collaborative and expressive knowledge articulation paradigms**\r\n*[[http://videolectures.net/akom08_warren_welcome|Welcome to the ACTIVE kick off meeting]]\r\n*[[http://videolectures.net/coinactivess2010_dengler_sm|Semantic MediaWiki]]\r\n\r\n**Ontology learning**\r\n*[[http://videolectures.net/coinactivess2010_grobelnik_bpm|Business Process Mining and Formalization]]\r\n*[[http://videolectures.net/training06_grobelnik_tmol|Text Mining for Ontology Learning]]\r\n*[[http://videolectures.net/koml04_grobelnik_olkds|Ontology Learning - Knowledge Discovery and the Semantic Web]]\r\n\r\n**Hybrid Web 2.0 - ontology infrastructure**\r\n*[[http://videolectures.net/coinactivess2010_imtiaz_aam|ACTIVE, Ali and More]]\r\n*[[http://videolectures.net/eswc08_hess_cs|Collective Semantics: Collective Intelligence & the Semantic Web - From Web 2.0 to Semantic Web - A Semi-Automated Approach]]\r\n\r\n**Semi-automated process refactoring**\r\n*[[http://videolectures.net/coinactivess2010_ruiz_moreno_leban_pak|Pro-Active Knowledge Processes Support]]\r\n\r\n**Optimization of knowledge models and pro-active support**\r\n*[[http://videolectures.net/coinactivess2010_ruiz_moreno_leban_pak|Pro-Active Knowledge Processes Support]]\r\n\r\n**Delivery of contextualized information**\r\n*[[http://videolectures.net/coinactivess2010_grobelnik_bpm|Business Process Mining and Formalization]]\r\n\r\n**Visualization of temporal enterprise model**\r\n*[[http://videolectures.net/coinactivess2010_ruiz_moreno_leban_pak|Pro-Active Knowledge Processes Support]]\r\n\r\n**Privacy preserving analysis of enterprise data**\r\n*[[http://videolectures.net/coinactivess2010_djordjevic_acs|Accenture Case Study: Enterprise Collaboration & Knowledge Management through Machine Learning and Semantic Technologies]]\r\n*[[http://videolectures.net/google_roughan_ppdm|Privacy Preserving DataMining]]\r\n\r\n**Inconsistency diagnosis and automatic repair of inconsistencies**\r\n**Complex rule interfaces**\r\n**Knowledge process mining**\r\n**Autonomous, context aware services for knowledge processes**\r\n**Dynamic adaptations of context-aware knowledge processes**\r\n**Security-aware knowledge processes**\r\n**Simultaneous analysis of multiple modalities**\r\n\r\nRelated talks:\r\n*[[http://videolectures.net/akom08_warren_welcome|Welcome to the ACTIVE kick off meeting]]\r\n*[[http://videolectures.net/iswc08_witbrock_fsc|Free Semantic Content: Using OpenCyc in Semantic Web Applications]]\r\n*[[http://videolectures.net/iswc08_dellaValle_rswa|Realizing a Semantic Web Application]]\r\n*[[http://videolectures.net/active09_warren_kmcfl|KM at the Customer Front-Line: The BT Case Study in ACTIVE]]\r\n\r\n ----\r\n ----\r\n\r\n == 5. Management and Problem Solving\r\n\r\nThis course is aimed at providing knowledge about the models, structures and mechanisms for management, coordination and problem solving. The coordination mechanisms are basic mechanisms in any networked organisation and as such the focal point of interest for any distributed, knowledge focused organisation. The course will analyse and study the approaches for distributed and collaborative decision-making including organizational aspects in the context of informal and knowledge processes. Support with ACTIVE models and solutions for decision making processes in inter-enterprise and collaborative environment is the main focus.\r\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse5_Management and problem solving.pdf|Management and problem solving - course]]\r\n\r\nCourse topics:\r\n\r\n**Context sensitive management**\r\n*[[http://videolectures.net/coinactivess2010_imtiaz_aam|ACTIVE, Ali and More]]\r\n\r\n**Pro-active knowledge process support**\r\n*[[http://videolectures.net/coinactivess2010_ruiz_moreno_leban_pak|Pro-Active Knowledge Processes Support]]\r\n\r\n ----\r\n ----\r\n\r\n == 6. Practical Examples and ACTIVE Prototypes\r\n\r\nIn this course the development process of the ACTIVE prototypes for three specific applications will be demonstrated. The development process and the applications with their specific benefits which are created by the ACTIVE research results will be shown. The course will be supplemented with a hands-on workshop to allow participants to deal with concrete examples from their work context. The focus will be on identifying areas of applications where the functionalities developed in the ACTIVE project can be applied.\r\n[[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse6_PracticalExamples.pdf|Practical Examples and ACTIVE Prototypes - course]]\r\n\r\nCourse topics:\r\n\r\n**Case studies and best practices**\r\n*[[http://videolectures.net/active09_warren_kmcfl|KM at the Customer Front-Line: The BT Case Study in ACTIVE]]\r\n*[[http://videolectures.net/coinactivess2010_thurlow_bcs|ACTIVE\u000b BT Case Study]]\r\n*[[http://videolectures.net/coinactivess2010_djordjevic_acs|Accenture Case Study: Enterprise Collaboration & Knowledge Management through Machine Learning and Semantic Technologies]]\r\n\r\n**Demonstrators, demos, prototypes**\r\n*[[http://videolectures.net/eswc08_berges_smw|Semantic Web Technology for Agent Communication Protocols]]\r\n*[[http://videolectures.net/active09_dolinsek_iak|Introduction to the ACTIVE Knowledge Workspace SDK]]\r\n\r\n**Use scenarios**\r\n\r\n ----\r\n ----\r\n\r\n == 9. ACTIVE project - Introduction\r\n\r\nThe aim of this course is to present the aims, goals, structure and expected results of ACTIVE project. In addition this course provides information about the future plans, development and operations of the business development that will follow the ACTIVE project. In that respect the future directions in the research areas will be presented, their use potentials, development scenarios and business scenarios in the forms of business plans. [[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse9_Introduction.pdf|ACTIVE project - Introduction - course]]\r\n\r\n*[[http://videolectures.net/akom08_warren_welcome|Welcome to the ACTIVE kick off meeting]]\r\n*[[http://videolectures.net/active09_mladenic_warren_oai|Opening and Introduction of the 1st ACTIVE Summer School]]\r\n*[[http://videolectures.net/coinactivess2010_warren_ai|ACTIVE Introduction]]\r\n*[[http://videolectures.net/coinactivess2010_jermol_wel|Welcome to the Summer School on Advanced Technologies for Knowledge Intensive Networked Organizations 2010 - Aachen ]]\r\n\r\n == 10. Using ACTIVE solution\r\n\r\nThe aim of this training module is to train the potential users/adopters/developers on the ACTIVE solutions. The course is organised as a blended learning module that combines traditional two days event with the self-learning courses in LC. The main learning goals for this module are to teach about basics of ACTIVE solutions and infrastructure, provide technical specifications, development environment and cases. [[http://analytics.ijs.si/~mitja/Courses/Active_academia/TrainingCourse10_UsingACTIVESolution.pdf|Using ACTIVE solution - course]]\r\n\r\nCourse topics:\r\n\r\n**ACTIVE prototypes, ACTIVE Knowledge Workspace Desktop, ACTIVE SDK, ACTIVE Integrated Platform**\r\n*[[http://videolectures.net/active09_dolinsek_iak|Introduction to the ACTIVE Knowledge Workspace SDK]]\r\n*[[http://videolectures.net/coinactivess2010_dolinsek_akw|Active Knowledge Work Space demonstration]]", "recorded": "2011-02-23T15:20:20", "title": "ACTIVE - Training programme for multipliers"}, {"url": "coin_general_public_training", "desc": "back to [[http://videolectures.net/coin|COIN training project ]]\n\n===These videolectures describe the technologies being used and developed in COIN, and some of the applications of those technologies in the project. The lectures are given by COIN researchers and by other researchers working in the same and related fields.\n\n == 1. Motivation for the paradigm - General public training\n\n**Description:** In this course the learner will get basic knowledge on the emerging scientific discipline \u2013 Collaborative Networked Organisations with emphasis on Interoperability. You will learn basic concepts, historical context and rationales for the new discipline.\n\n**Key concepts:** Collaboration and Interoperability, Practical examples of CNs, Practices of Interoperability, Historic overview, Technological and organizational trends, Discussion of the usefulness/benefits and current limitations\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_general/Course_1_Motivation_for_the_paradigm_General.pdf|Motivation for the paradigm - course]]\n\n**Suggested video lectures:**: [[http://videolectures.net/ice09_martinez_ewbom|Everything will be on machines by Cristina Martinez ]], [[http://videolectures.net/ice08_martinez_future|The Future Internet: a vision from European Research by Cristina Martinez ]], [[http://videolectures.net/ice2011_schuh_production/|Developing a production engineering based theory of production by G\u00fcnther Schuh]], [[http://videolectures.net/ice08_prinz_web|Web 2.0 and Collaborative Working Environments: What can we learn? by Wolfgang Prinz]], [[http://videolectures.net/esocenet07_gusmeroli_csf|Current Solutions and Future Trends by Sergio Gusmeroli]]\n\n----\n----\n\n == 2. Basic concepts - General public training\n\n**Description:** In this course the learner will get basic knowledge on COIN Basic Concepts, Fundamentals in CN, Fundamentals in Interoperability, COIN innovative concepts and models. You will learn background knowledge for understanding the EC and EI challenges within COIN.\n\n**Key concepts:** COIN Basic Concepts, Fundamentals in CN, Fundamentals in Interoperability, COIN innovative concepts and models\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_general/Course_2_Basic_Concepts_General.pdf|Basic concepts - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/ice08_gusmeroli_meta|The COIN Metaphor for EU Industry by Sergio Gusmeroli]], [[http://videolectures.net/iesa08_rossiter_lfi|Logical Foundations for the Infrastructure of the Information Market by Nick Rossiter ]], [[http://videolectures.net/coinactivess2010_gusmeroli_cica|Collaboration and Interoperability \u2013 COIN Approach\nby Sergio Gusmeroli]], [[http://videolectures.net/coinactivess2010_sesana_cis|COIN Innovative Services\nby Michele Sesana]], [[http://videolectures.net/cgm09_gusmeroli_tccps|TCC plenary session\nby Sergio Gusmerol]]\n\n----\n----\n\n == 3. Enterprise Interoperability - General public training\n\n**Description:** In this course the learner will get basic knowledge on Enterprise Interoperability as an the emerging scientific discipline. You will learn basic modes of Interoperability.\n\n**Key concepts:** Basic Concepts, Definitions and Approaches\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_general/Course_3_Enterprise_Interoperability_General.pdf|Enterprise Interoperability - course]]\n\n **Suggested video lectures:** [[http://videolectures.net/ice08_katranuschkov_aigevo|Achieving Interoperability in Grid-Enabled Virtual Organisation by Peter Katranuschkov]], [[http://videolectures.net/ice09_olmo_aabc|Business Cases for Enterprise Interoperability - The Andalusian Aeronautics Business Case\nAlberto Olmo]], [[http://videolectures.net/ice09_debate_qa|Debate on Business Cases for Enterprise Interoperability  by Sergio Gusmerol]], [[http://videolectures.net/iswc06_wache_irpto|Workshop: Improving the recruitment process through ontology-based querying by Holger Wache]], [[http://videolectures.net/cgm09_taglino_eis|Enterprise Interoperability Services by Francesco Taglino]]\n\n ----\n ----\n\n == 4. VO Breeding Environment - General public training\n\n**Description:** In this course the learner will get basic knowledge on the main elements of the VBE, the basic elements of Virtual Organizations Breeding Environment and its role in the context of CNO.\n\n**Key concepts:** Concept and examples\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_general/Course_4_Vo_Breeding_Environment_General.pdf|VO Breeding Environment - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/ess07_ferlez_mc| Modelling competences by\nJure Ferle\u017e]], [[http://videolectures.net/akom08_grobelnik_ina|Introduction to Network Analysis\nby Marko Grobelnik, Dunja Mladeni\u0107]], [[http://videolectures.net/brussels06_afsarmanesh_v|Virtual organisations breeding environment by Hamideh Afsarmanesh]], [[http://videolectures.net/ess07_jermol_evg|Exploring Collaborative Networked Organisations in ECOLEAD by Mitja Jermol]], [[http://videolectures.net/antwerpen04_blomqvist_ottbn/|An overview of trust and trust building in networks\n by Kirsimarja Blomqvist]]\n\n ----\n ----\n\n == 5. Virtual Organizations - General public training\n\n**Description:** In this course the learner will get basic knowledge on Virtual Organizations creation rules and functionalities.\n\n**Key concepts:** Concepts, organizational models and operational rules\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_general/Course_5_Virtual_Organizations_General.pdf|Virtual Organizations - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/ess07_ollus_vom|Virtual organizations management by Martin Ollus]], [[http://videolectures.net/ess06_ollus_vrvp|VOM in relation to VBE & PVC by Martin Ollus]], [[http://videolectures.net/ess06_seifert_pmv|Performance Management in VOs by Marcus Seifert]], [[http://videolectures.net/ess07_slavik_vrv|Virtual reality for VE\nby Pavel Slav\u00edk]], [[http://videolectures.net/ekom08_ollus_mcn|Management of collaboration in networks\nby Martin Ollus]]\n\n ----\n ----\n\n== 6. Virtual Communities - General public training\n\n**Description:**In this course is shown the basic knowledge on Virtual Communities, the concept of Professional Virtual Communities that is being explored in COIN.\n\n**Key concepts:** Concepts and typology\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_general/Course_6_Virtual_Communities_General.pdf|Virtual Communities - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/ess06_seifert_pmv|Performance Management in VOs\nby Marcus Seifert]], [[http://videolectures.net/esocenet07_santoro_ci|Concurrent Innovation \u2013 Vision 2020\nby Roberto Santoro]], [[http://videolectures.net/brussels06_gusmeroli_i|ICT-I by Sergio Gusmeroli]], [[http://videolectures.net/mitworld_baltimore_bac|Building a Community on Trust by David Baltimore]], [[http://videolectures.net/ess07_vorobey_ape|AIESEC PVC ECOLEAD case study by Volodja Vorobey]]\n\n**Additional video lectures:** [[http://videolectures.net/ess06_crave_pbct| PVC basic concepts & typologies]], [[http://videolectures.net/ess07_crave_pca|PVC and cooperation from AI perspective]], [[http://videolectures.net/ess06_picard_sppvc|Social protocols in Professional Virtual Communities]], [[http://videolectures.net/eccs07_chavalarias_sma|Science mapping with asymmetric co-occurence analysis]]\n\n ----\n ----\n\n == 7. Architectures and Platforms - General public training\n\n**Description:**Focusing on research coordination in the area of Architecture & Platforms. Show basic internet and computer technologies, ideas, and concepts for enterprise interoperability purposes and to continue on updating with state-of-the-art models, approaches and mechanisms.\n\n**Key concepts:** Computer networks basics. Base Internet technologies, Emerging computing models and implementation approaches\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_general/Course_7_Architectures_and_Platforms_General.pdf|Architectures and Platforms - course]]\n\n**Suggested video lectures:**  [[http://videolectures.net/iesa08_gionis_aha|The Advantages of Hybrid Architectural Approaches for the Integrating Middleware by George Gionis]], [[http://videolectures.net/iesa08_ullberg_eas|Enterprise Architecture: A Service Interoperability Analysis Framework by Johan Ullberg]], [[http://videolectures.net/ess07_negretto_ii|ICT infrastructure by\nUgo Negretto]], [[http://videolectures.net/ess07_hodik_atv|Agent technologies for VE + SW demonstrations: MAS Tutorial by Ji\u0159\u00ed Hod\u00edk]], [[http://videolectures.net/ess06_nagellen_soa|Service Oriented Architectures by Thierry Nagellen]]\n\n ----\n ----\n\n == 8. Background Technologies - General public training\n\n**Description:** This course presents and explains core technologies from the area of knowledge technologies that ranges from the data driven methods to knowledge driven methods and are important for detecting, analyzing and managing knowledge (tacit and explicit) in enterprises, organisations or agents.\n\n**Key concepts:** Traditional technologies\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_general/Course_8_Background_Technologies_General.pdf| Background Technologies - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/kdd07_fayyad_dms|A Data Miner\u2019s Story \u2013 Getting to Know the Grand Challenges by Usama Fayyad ]], [[http://videolectures.net/ssll09_kotagiri_dami|Data Mining by Rao Kotagiri]], [[http://videolectures.net/stanfordcs229f08_ng_lec01| Lecture 1 - The Motivation & Applications of Machine Learning by Andrew Ng]], [[http://videolectures.net/iswc08_hendler_ittsw|Introduction to the Semantic Web by Aldo Gangemi, Sean Bechhofer, Asunci\u00f3n G\u00f3mez-P\u00e9rez, Jim Hendler]], [[http://videolectures.net/ess07_jermol_ktno|Knowledge technologies for network organisations by Mitja Jermol]]\n\n**Additional video lectures:**[[http://videolectures.net/psm08_cristianini_ieb/|In the Eye of the Beholder? Another look at Cognitive Systems]], [[http://videolectures.net/ijcai09_lesser_saitmao/|Scaling AI Through Multi-Agent Organizations]], [[http://videolectures.net/ccss09_pietronero_soafse/|Self-Organization and Finite Size Effects in Agent Models for Financial Markets]], [[http://videolectures.net/ccss09_steubing_atcf/|Assessing the Critical Factors that Determine the Availability of Wood Fuel in Switzerland with an Agent Based Model]]\n----\n----\n\n == 9. COIN Solution - General public training\n\n**Description:** This is one of the main courses in COIN training programmes. It is aimed at presenting COIN innovative  solutions that will be developed in the frame of the project. The main focus is on the explanation and demonstration of technologies developed in COIN.\n\n**Key concepts:** Basic architecture, Introduction to COIN Enterprise Interoperability services, Introduction to COIN Enterprise Collaboration services\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_general/Course_9_COIN_solution_Introduction_General.pdf| COIN solution - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/coinactivess2010_jansson_ccs|COIN Collaborative Services by Kim Jansson ]], [[http://videolectures.net/coinactivess2010_sitek_ure|User requirements elicitation adapting Serious Gaming approach by Patrick Sitek]], [[http://videolectures.net/coinactivess2010_sesana_cpd|COIN Platform Demonstration by Michele Sesana]], [[http://videolectures.net/coinactivess2010_fischer_cbs|COIN Baseline services and negotiation support by Klaus Fischer]], [[http://videolectures.net/coinplanetdataschool2011_sesana_coin/|Technical and Business Innovation\nby Michele Sesana]]\n\n ----\n ----\n\n== 10. Practical Examples and COIN prototypes - General public training\n\n**Description:** In this course the practical examples of COIN prototypes will be demonstrated and explained together with some applications that are related to the COIN research domain. The course will be supplemented with videos at the COIN IP page to allow participants to deal with concrete examples from their work context.\n\n**Key concepts:** use scenarios, demonstrators, demos, prototypes\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_general/Course_10_Practical_Examples_and_COIN_prototipes_General.pdf|Practical Examples and COIN prototypes - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/ice09_olmo_coincpds|COIN Collaborative Product Development Services by Alberto Olmo]], [[http://videolectures.net/coinplanetdataschool2011_sesana_demos/|COIN System Demos by Michele Sesana]], [[http://videolectures.net/ice09_canepa_isurf|iSURF \u2013 Piacenza Knitwear Business Case by Alessandro Canepa]], [[http://videolectures.net/cgm09_komazec_cspwho|COIN Service Platform with hands-on by Srdjan Komazec]], [[http://videolectures.net/coinplanetdataschool2011_oman_usage|Usage of Integrated Services in the industry by\nSimon Oman]]\n\n ----\n ----\n\n == 11. Information Management - N/A for General public\n\n ----\n ----\n\n == 12. Information Exchange Standards - General public training\n\n**Description:** A number of standards particularly relevant for collaborative networks and enterprise interoperability are introduced and analyzed.\n\n**Key concepts:** Importance of standards\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_general/Course_12_Information_Exchange_Standards_General.pdf|Information Exchange Standards - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/efreight2011_rantasila_pilli_sihvola_logistics|Logistics-related information exchange - international exchange - international efforts and Finnish developments by Karri Rantasila, Eetu Pilli-Sihvola]], [[http://videolectures.net/kdd2010_zheng_udmt|Using Data Mining Techniques to Address Critical Information Exchange Needs in Disaster Affected Public-Private Networks by Li Zheng]], [[http://videolectures.net/ict08fr_nagy_rothengass_eiu|The Expanding Information Universe: New Trends, New Forms, New Usages\nby Marta Nagy-Rothengass]], [[http://videolectures.net/ecml07_smyth_api|Adventures in Personalized Information Access by Barry Smyth]], [[http://videolectures.net/forum2010_glenn_gcisd|Global Challenges and Information Society Development by Jerome C. Glenn]]\n\n ----\n ----\n\n == 13. Coordination Mechanisms - General public training\n\n**Description:** This course is aimed at providing knowledge about the models, structures and mechanisms for management, coordination and problem solving. The coordination mechanisms are basic mechanisms in any networked organisation and as such the focal point of interest for any distributed, knowledge focused organisation.\n\n**Key concepts:** Collaboration modalities\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_general/Course_13_Coordination_Mechanisms_General.pdf|Coordination Mechanisms - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/eccs07_stark_cpc|Combined Problems of Cooperation and Coordination by Hans-Ulrich Stark]], [[http://videolectures.net/eccs07_menczer_wcn|Web Click Network\nby Filippo Menczer]], [[http://videolectures.net/eccs07_matteo_gld|Global and Local Dynamics in Correlated Systems by Tiziana Di Matteo]], [[http://videolectures.net/iesa08_wings_csit|Challenges and Strategies for IT Services in Heterogeneous Enterprise Environments by Sujit Wings]]\n\n ----\n ----\n\n == 14. Management of common Ontologies - General public training\n\n**Description:** This course is aimed at providing the base concepts of ontologies, especially in relation to the COIN common ontology, explore the potential of ontology-based techniques to tackle the problem of interoperability.\n\n**Key concepts:** The role of ontologies in collaboration\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_general/Course_14_Management_of_common_Ontologies_General.pdf|Management of common Ontologies - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/ess07_grobelnik_twdmI|Text and web data mining by Marko Grobelnik]], [[http://videolectures.net/iesa08_dahlem_osm|Ontology-driven Semantic Mapping by Nikolai Dahlem]], [[http://videolectures.net/mmdss07_grobelnik_oml|Ontologies and Machine Learning by Marko Grobelnik, Bla\u017e Fortuna]], [[http://videolectures.net/ess07_obitko_oswve|Ontologies, semantic web and VE by Marek Obitko]], [[http://videolectures.net/tao08_bontcheva_tao|Transitioning Applications to Ontologies by Kalina Bontcheva]]\n\n ----\n ----\n\n == 15. e-Commerce and e-Markets - General public training\n\n**Description:** This course will in particular investigate aspects related to the economics and the incentives behind Interoperability.\n\n**Key concepts:** Concepts of e-Commerce and e-Market\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_general/Course_15_E-Commerce_and_E-Markets_General.pdf|E-commerce and E-markets - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/echallenges2010_li_intro|Business Models Background\nMan-Sze Li ]], [[http://videolectures.net/ice09_canepa_isurf|iSURF \u2013 Piacenza Knitwear Business Case\nby Alessandro Canepa]], [[http://videolectures.net/ice09_withalm_cdcp|Business Cases for Enterprise Interoperability Collaborative Demand Capacity Planning (CDCP) by Josef Withalm]], [[http://videolectures.net/coinplanetdataschool2011_hristov_media| Innovative collaboration in Media \u2013 \u000ba COIN case in EEU by Konstantin Hristov]], [[http://videolectures.net/echallenges2010_suttner_tnb|Towards New Business Models in the Energy Sector based on Software-as-a-Service-Utilities and Value-added Services\nby Hannes Suttner]]\n\n ----\n ----\n\n == 16. Non Technological Issues -  - N/A for General public\n\n----\n----\n\n == 17. Organisation Modeling and Reference Models - General public training\n\n**Description:** The course topic is focusing on Organisation Modelling as a combination of a set of activities within an enterprise with a structure describing their logical order and dependence whose objective is to produce a desired result. Business Process modeling as an enabler of common understanding and analysis of a business process. The course will also describe the main process modeling techniques.\n\n**Key concepts:** Concept of reference model\n\n**Download the course:**\n\n **Suggested video lectures:** [[http://videolectures.net/eccs07_stark_cpc|Combined Problems of Cooperation and Coordination by Hans-Ulrich Stark ]], [[http://videolectures.net/eccs07_menczer_wcn|Web Click Network by Filippo Menczer]], [[http://videolectures.net/eccs07_matteo_gld|Global and Local Dynamics in Correlated Systems by Tiziana Di Matteo]]\n\n ----\n ----\n\n == 18. Emerging Collaborative Forms - General public training\n\n**Description:** Brief summary of the various collaborative forms studied in previous units, a discussion of possible new models and generalizations is made. As a starting basis, new forms of collaborative e-government, e-science, Virtual institutes, Virtual laboratories, etc, are discussed. Other generalizations include: networks of sensors, networks of machines, etc.\n\n**Key concepts:** Summary of studied collaborative forms and interoperability approaches, New application examples: collaborative e-government, e-Science, Virtual institutes, Virtual Labs, etc., Internet of Things: Networks of machines, networks of sensors., Intelligent, self-aware enterprises, Symbiotic models, Other emerging cases.\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_general/Course_18_Emerging_Collaborative_Forms_General.pdf|Emerging Collaborative Forms - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/eccs07_newman_sdc|Structure and Dynamics in Complex Networks by Mark Newman ]], [[http://videolectures.net/eccs07_verschure_dac|Distributive Adaptive Control: A Real-world Cognitive Architecture applied to Robots, Spaces and Avatares by Paul Verschure]], [[http://videolectures.net/eccs07_stamatiou_dtc|The \"Digital Territory\" as a Complex System of Interacting Agents, Emergent Properties and Technologies by Yannis Stamatiou ]], [[http://videolectures.net/esocenet07_borjeson_enl|European Network of Livin Labs by Mikael B\u00f6rjeson]], [[http://videolectures.net/coinactivess2010_kropp_sol|Smart Objects Innovation Lab: Theory and practice - hand in hand by Sebastian Kropp]],\n\n **Additional video lectures:**\n *[[http://videolectures.net/eccs07_canright_smn|Self-mapping Networks]]\n *[[http://videolectures.net/porto05_cordeiro_sc|Semiotics in CNOs]]\n *[[http://videolectures.net/eccs07_damiani_irb|Interacting Random Boolean Networks]]\n *[[http://videolectures.net/eccs07_jiang_pba|Population-based adaptive Systems: An Implementation in NEW TIES]]", "recorded": "2010-01-31T18:35:34", "title": "COIN - General public training/promotion  section"}, {"url": "coin_multipliers_training", "desc": "back to [[http://videolectures.net/coin|COIN training project ]]\n\n===These videolectures describe the technologies being used and developed in COIN, and some of the applications of those technologies in the project. The lectures are given by COIN researchers and by other researchers working in the same and related fields.\n\n**MUST SEE videos for Multipliers before taking the courses:**\n||[[:coin_videolectures_promo]]||[[:coinplanetdataschool2011_braccini_introduction]]||[[:coinplanetdataschool2011_hristov_media/]]||[[:coinplanetdataschool2011_scicluna_web/]]||[[:coinplanetdataschool2011_gaggioli_coin/]]||\n\n == 1. Motivation for the paradigm - Multiplier training\n\n**Description:** This first unit aims at creating a motivation for the course through a brief presentation of application areas, illustrated by concrete examples in industry, services, government, etc. A brief historic overview of the industrial organizational paradigms leading to collaborative networks and their interoperability as well as a summary of current technological and organizational trends is presented. For each example an attempt to identify the main involved problems (e.g. organizational forms, processes, and cooperation and collaboration forms) is made, calling the attention for the potential contributes from other disciplines.\n\n**Key concepts:** Collaboration and Interoperability, Practical examples of CNs, Practices of Interoperability, Historic overview, Technological and organizational trends, Discussion of the usefulness/benefits and current limitations\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_multipliers/Course_1_Motivation_for_the_paradigm_Multipliers.pdf|Motivation for the paradigm - course]]\n\n**Suggested video lectures:**: [[http://videolectures.net/ice09_martinez_ewbom|Everything will be on machines by Cristina Martinez ]], [[http://videolectures.net/ice08_martinez_future|The Future Internet: a vision from European Research by Cristina Martinez ]], [[http://videolectures.net/ice2011_schuh_production/|Developing a production engineering based theory of production by G\u00fcnther Schuh]], [[http://videolectures.net/ice08_prinz_web|Web 2.0 and Collaborative Working Environments: What can we learn? by Wolfgang Prinz]], [[http://videolectures.net/esocenet07_gusmeroli_csf|Current Solutions and Future Trends by Sergio Gusmeroli]]\n\n----\n----\n\n == 2. Basic concepts - Multiplier training\n\n**Description:** After the motivation phase, the base concepts are introduced. Considering the large variety of collaborative networks and interoperability modes, a categorization of the various forms is made and taxonomy is introduced in order to give students a global perspective of the area. At the same time the basic project concepts and  models are being addressed. The various innovative concepts and models developed in COIN.\n\n**Key concepts:** COIN Basic Concepts, Fundamentals in CN, Fundamentals in Interoperability, COIN innovative concepts and models\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_multipliers/Course_2_Basic_Concepts_Multipliers.pdf|Basic concepts - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/ice08_gusmeroli_meta|The COIN Metaphor for EU Industry by Sergio Gusmeroli]], [[http://videolectures.net/iesa08_rossiter_lfi|Logical Foundations for the Infrastructure of the Information Market by Nick Rossiter ]], [[http://videolectures.net/coinactivess2010_gusmeroli_cica|Collaboration and Interoperability \u2013 COIN Approach\nby Sergio Gusmeroli]], [[http://videolectures.net/coinactivess2010_sesana_cis|COIN Innovative Services\nby Michele Sesana]], [[http://videolectures.net/cgm09_gusmeroli_tccps|TCC plenary session\nby Sergio Gusmerol]]\n\n----\n----\n\n == 3. Enterprise Interoperability - Multiplier training\n\n**Description:** After the motivation phase and the COIN basic concepts are introduced, the base concepts of Enterprise Interoperability are explained. After describing the overall goals and essential terms for the Interoperability of enterprises, principles, methods and benefits of enterprise Interoperability are explained.\n\n**Key concepts:** Basic Concepts, Definitions and Approaches, Enterprise Modelling for Interoperability,  Business Interoperability (BI)\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_multipliers/Course_3_Enterprise_Interoperability_Multipliers.pdf|Enterprise Interoperability - course]]\n\n **Suggested video lectures:** [[http://videolectures.net/ice08_katranuschkov_aigevo|Achieving Interoperability in Grid-Enabled Virtual Organisation by Peter Katranuschkov]], [[http://videolectures.net/ice09_olmo_aabc|Business Cases for Enterprise Interoperability - The Andalusian Aeronautics Business Case\nAlberto Olmo]], [[http://videolectures.net/ice09_debate_qa|Debate on Business Cases for Enterprise Interoperability  by Sergio Gusmerol]], [[http://videolectures.net/iswc06_wache_irpto|Workshop: Improving the recruitment process through ontology-based querying by Holger Wache]], [[http://videolectures.net/cgm09_taglino_eis|Enterprise Interoperability Services by Francesco Taglino]]\n\n ----\n ----\n\n == 4. VO Breeding Environment - Multiplier training\n\n**Description:** Course on the main elements of the VBE that are studied in COIN featuring theoretical and practical business examples from the field. The basic elements of Virtual Organizations Breeding Environment and its role in the context of CNO.\n\n**Key concepts:** Concept and examples, Components, structure, actors and roles\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_multipliers/Course_4_Vo_Breeding_Environment_Multipliers.pdf|VO Breeding Environment - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/ess07_ferlez_mc| Modelling competences by\nJure Ferle\u017e]], [[http://videolectures.net/akom08_grobelnik_ina|Introduction to Network Analysis\nby Marko Grobelnik, Dunja Mladeni\u0107]], [[http://videolectures.net/brussels06_afsarmanesh_v|Virtual organisations breeding environment by Hamideh Afsarmanesh]], [[http://videolectures.net/ess07_jermol_evg|Exploring Collaborative Networked Organisations in ECOLEAD by Mitja Jermol]], [[http://videolectures.net/antwerpen04_blomqvist_ottbn/|An overview of trust and trust building in networks\n by Kirsimarja Blomqvist]]\n\n ----\n ----\n\n == 5. Virtual Organizations - Multiplier training\n\n**Description:** Course on the concept and structure of Virtual Organizations creation rules and functionalities. Here we present the continuous life cycle of the Virtual Organization creation with its creation, formation, later approaching its breeding environment functionalities and ultimately its dissolution and incorporation into its next life cycle phase.\n\n**Key concepts:** Concepts, organizational models and operational rules, Life cycle\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_multipliers/Course_5_Virtual_Organizations_Multipliers.pdf| Virtual Organizations - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/ess07_ollus_vom|Virtual organizations management by Martin Ollus]], [[http://videolectures.net/ess06_ollus_vrvp|VOM in relation to VBE & PVC by Martin Ollus]], [[http://videolectures.net/ess06_seifert_pmv|Performance Management in VOs by Marcus Seifert]], [[http://videolectures.net/ess07_slavik_vrv|Virtual reality for VE\nby Pavel Slav\u00edk]], [[http://videolectures.net/ekom08_ollus_mcn|Management of collaboration in networks\nby Martin Ollus]]\n\n ----\n ----\n\n== 6. Virtual Communities - Multiplier training\n\n**Description:**In this course is shown the basic knowledge on Virtual Communities, the concept of Professional Virtual Communities that is being explored in COIN. A typology of VC is introduced and a particular attention is devoted to Professional Virtual Communities. The components, structure, and life cycle of PVCs are discussed.\n\n**Key concepts:** Concepts and typology, Components\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_multipliers/Course_6_Virtual_Communities_Multipliers.pdf|Virtual Communities - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/ess06_seifert_pmv|Performance Management in VOs\nby Marcus Seifert]], [[http://videolectures.net/esocenet07_santoro_ci|Concurrent Innovation \u2013 Vision 2020\nby Roberto Santoro]], [[http://videolectures.net/brussels06_gusmeroli_i|ICT-I by Sergio Gusmeroli]], [[http://videolectures.net/mitworld_baltimore_bac|Building a Community on Trust by David Baltimore]], [[http://videolectures.net/ess07_vorobey_ape|AIESEC PVC ECOLEAD case study by Volodja Vorobey]]\n\n**Additional video lectures:** [[http://videolectures.net/ess06_crave_pbct| PVC basic concepts & typologies]], [[http://videolectures.net/ess07_crave_pca|PVC and cooperation from AI perspective]], [[http://videolectures.net/ess06_picard_sppvc|Social protocols in Professional Virtual Communities]], [[http://videolectures.net/eccs07_chavalarias_sma|Science mapping with asymmetric co-occurence analysis]]\n\n ----\n ----\n\n == 7. Architectures and Platforms - Multiplier training\n\n**Description:**Focusing on research coordination in the area of Architecture & Platforms. Show basic internet and computer technologies, ideas, and concepts for enterprise interoperability purposes and to continue on updating with state-of-the-art models, approaches and mechanisms.\n\n**Key concepts:** Computer networks basics. Base Internet technologies, Emerging computing models and implementation approaches\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_multipliers/Course_7_Architectures_and_Platforms_Multipliers.pdf|Architectures and Platforms - course]]\n\n**Suggested video lectures:**  [[http://videolectures.net/iesa08_gionis_aha|The Advantages of Hybrid Architectural Approaches for the Integrating Middleware by George Gionis]], [[http://videolectures.net/iesa08_ullberg_eas|Enterprise Architecture: A Service Interoperability Analysis Framework by Johan Ullberg]], [[http://videolectures.net/ess07_negretto_ii|ICT infrastructure by\nUgo Negretto]], [[http://videolectures.net/ess07_hodik_atv|Agent technologies for VE + SW demonstrations: MAS Tutorial by Ji\u0159\u00ed Hod\u00edk]], [[http://videolectures.net/ess06_nagellen_soa|Service Oriented Architectures by Thierry Nagellen]]\n\n ----\n ----\n\n == 8. Background Technologies - Multiplier training\n\n**Description:** This course presents and explains core technologies from the area of knowledge technologies that ranges from the data driven methods to knowledge driven methods and are important for detecting, analyzing and managing knowledge (tacit and explicit) in enterprises, organisations or agents. Recent developments in the particular areas are demonstrated through real software prototypes, successful market cases and real business implementations.\n\n**Key concepts:** Traditional technologies\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_multipliers/Course_8_Background_Technologies_Multipliers.pdf| Background Technologies - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/kdd07_fayyad_dms|A Data Miner\u2019s Story \u2013 Getting to Know the Grand Challenges by Usama Fayyad ]], [[http://videolectures.net/ssll09_kotagiri_dami|Data Mining by Rao Kotagiri]], [[http://videolectures.net/stanfordcs229f08_ng_lec01| Lecture 1 - The Motivation & Applications of Machine Learning by Andrew Ng]], [[http://videolectures.net/iswc08_hendler_ittsw|Introduction to the Semantic Web by Aldo Gangemi, Sean Bechhofer, Asunci\u00f3n G\u00f3mez-P\u00e9rez, Jim Hendler]], [[http://videolectures.net/ess07_jermol_ktno|Knowledge technologies for network organisations by Mitja Jermol]]\n\n**Additional video lectures:**[[http://videolectures.net/psm08_cristianini_ieb/|In the Eye of the Beholder? Another look at Cognitive Systems]], [[http://videolectures.net/ijcai09_lesser_saitmao/|Scaling AI Through Multi-Agent Organizations]], [[http://videolectures.net/ccss09_pietronero_soafse/|Self-Organization and Finite Size Effects in Agent Models for Financial Markets]], [[http://videolectures.net/ccss09_steubing_atcf/|Assessing the Critical Factors that Determine the Availability of Wood Fuel in Switzerland with an Agent Based Model]]\n----\n----\n\n == 9. COIN Solution - Multiplier training\n\n**Description:** This is one of the main courses in COIN training programmes. It is aimed at presenting COIN innovative  solutions that will be developed in the frame of the project. The main focus is on the explanation and demonstration of technologies developed in COIN, their use in the industry contexts and on the discussion of their implications to the traditional business environment.\n\n**Key concepts:** Basic architecture, Introduction to COIN Service platform, Introduction to COIN Enterprise Interoperability services, Interoperability baseline, Introduction to COIN Enterprise Collaboration services, Collaboration baseline\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_multipliers/Course_9_COIN_solution_Introduction_Multipliers.pdf| COIN solution - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/coinactivess2010_jansson_ccs|COIN Collaborative Services by Kim Jansson ]], [[http://videolectures.net/coinactivess2010_sitek_ure|User requirements elicitation adapting Serious Gaming approach by Patrick Sitek]], [[http://videolectures.net/coinactivess2010_sesana_cpd|COIN Platform Demonstration by Michele Sesana]], [[http://videolectures.net/coinactivess2010_fischer_cbs|COIN Baseline services and negotiation support by Klaus Fischer]], [[http://videolectures.net/coinplanetdataschool2011_sesana_coin/|Technical and Business Innovation\nby Michele Sesana]]\n\n ----\n ----\n\n== 10. Practical Examples and COIN prototypes - Multiplier training\n\n**Description:** In this course the practical examples of COIN prototypes will be demonstrated and explained together with some applications that are related to the COIN research domain. The course will be supplemented with a hands-on workshop to allow participants to deal with concrete examples from their work context.\n\n**Key concepts:** use scenarios, case studies and best practices, demonstrators, demos, prototypes\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_multipliers/Course_10_Practical_Examples_and_COIN_prototipes_Multipliers.pdf|Practical Examples and COIN prototypes - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/ice09_olmo_coincpds|COIN Collaborative Product Development Services by Alberto Olmo]], [[http://videolectures.net/coinplanetdataschool2011_sesana_demos/|COIN System Demos by Michele Sesana]], [[http://videolectures.net/ice09_canepa_isurf|iSURF \u2013 Piacenza Knitwear Business Case by Alessandro Canepa]], [[http://videolectures.net/cgm09_komazec_cspwho|COIN Service Platform with hands-on by Srdjan Komazec]], [[http://videolectures.net/coinplanetdataschool2011_oman_usage|Usage of Integrated Services in the industry by\nSimon Oman]]\n\n ----\n ----\n\n == 11. Information Management - N/A for Multipliers\n\n ----\n ----\n\n == 12. Information Exchange Standards - Multiplier training\n\n**Description:** A number of standards particularly relevant for collaborative networks and enterprise interoperability are introduced and analyzed. Among them: EDI (Electronic Data Interchange), which in historical terms represents one of the first tools for cooperation among enterprises, is introduced and briefly characterized.\n\n**Key concepts:** Importance of standards\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_multipliers/Course_12_Information_Exchange_Standards_Multipliers.pdf|Information Exchange Standards - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/efreight2011_rantasila_pilli_sihvola_logistics|Logistics-related information exchange - international exchange - international efforts and Finnish developments by Karri Rantasila, Eetu Pilli-Sihvola]], [[http://videolectures.net/kdd2010_zheng_udmt|Using Data Mining Techniques to Address Critical Information Exchange Needs in Disaster Affected Public-Private Networks by Li Zheng]], [[http://videolectures.net/ict08fr_nagy_rothengass_eiu|The Expanding Information Universe: New Trends, New Forms, New Usages\nby Marta Nagy-Rothengass]], [[http://videolectures.net/ecml07_smyth_api|Adventures in Personalized Information Access by Barry Smyth]], [[http://videolectures.net/forum2010_glenn_gcisd|Global Challenges and Information Society Development by Jerome C. Glenn]]\n\n ----\n ----\n\n == 13. Coordination Mechanisms - Multiplier training\n\n**Description:** This course is aimed at providing knowledge about the models, structures and mechanisms for management, coordination and problem solving. The coordination mechanisms are basic mechanisms in any networked organisation and as such the focal point of interest for any distributed, knowledge focused organisation.\n\n**Key concepts:** Collaboration modalities\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_multipliers/Course_13_Coordination_Mechanisms_Multipliers.pdf|Coordination Mechanisms - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/eccs07_stark_cpc|Combined Problems of Cooperation and Coordination by Hans-Ulrich Stark]], [[http://videolectures.net/eccs07_menczer_wcn|Web Click Network\nby Filippo Menczer]], [[http://videolectures.net/eccs07_matteo_gld|Global and Local Dynamics in Correlated Systems by Tiziana Di Matteo]], [[http://videolectures.net/iesa08_wings_csit|Challenges and Strategies for IT Services in Heterogeneous Enterprise Environments by Sujit Wings]]\n\n ----\n ----\n\n == 14. Management of common Ontologies - Multiplier training\n\n**Description:** This course is aimed at providing the base concepts of ontologies, especially in relation to the COIN common ontology, explore the potential of ontology-based techniques to tackle the problem of interoperability.\n\n**Key concepts:** The role of ontologies in collaboration, COIN Core level common ontology\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_multipliers/Course_14_Management_of_common_Ontologies_Multipliers.pdf|Management of common Ontologies - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/ess07_grobelnik_twdmI|Text and web data mining by Marko Grobelnik]], [[http://videolectures.net/iesa08_dahlem_osm|Ontology-driven Semantic Mapping by Nikolai Dahlem]], [[http://videolectures.net/mmdss07_grobelnik_oml|Ontologies and Machine Learning by Marko Grobelnik, Bla\u017e Fortuna]], [[http://videolectures.net/ess07_obitko_oswve|Ontologies, semantic web and VE by Marek Obitko]], [[http://videolectures.net/tao08_bontcheva_tao|Transitioning Applications to Ontologies by Kalina Bontcheva]]\n\n ----\n ----\n\n == 15. e-Commerce and e-Markets - Multiplier training\n\n**Description:** This course will in particular investigate aspects related to the economics and the incentives behind Interoperability.\n\n**Key concepts:** Concepts of e-Commerce and e-Market\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_multipliers/Course_15_E-Commerce_and_E-Markets_Multiplierss.pdf|E-commerce and E-markets - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/echallenges2010_li_intro|Business Models Background\nMan-Sze Li ]], [[http://videolectures.net/ice09_canepa_isurf|iSURF \u2013 Piacenza Knitwear Business Case\nby Alessandro Canepa]], [[http://videolectures.net/ice09_withalm_cdcp|Business Cases for Enterprise Interoperability Collaborative Demand Capacity Planning (CDCP) by Josef Withalm]], [[http://videolectures.net/coinplanetdataschool2011_hristov_media| Innovative collaboration in Media \u2013 \u000ba COIN case in EEU by Konstantin Hristov]], [[http://videolectures.net/echallenges2010_suttner_tnb|Towards New Business Models in the Energy Sector based on Software-as-a-Service-Utilities and Value-added Services\nby Hannes Suttner]]\n\n ----\n ----\n\n == 16. Non Technological Issues -  - N/A for Multipliers\n\n----\n----\n\n == 17. Organisation Modeling and Reference Models - Multiplier training\n\n**Description:** The course topic is focusing on Organisation Modelling as a combination of a set of activities within an enterprise with a structure describing their logical order and dependence whose objective is to produce a desired result. Business Process modeling as an enabler of common understanding and analysis of a business process. The course will also describe the main process modeling techniques.\n\n**Key concepts:** Concept of reference model\n\n**Download the course:**\n\n **Suggested video lectures:** [[http://videolectures.net/eccs07_stark_cpc|Combined Problems of Cooperation and Coordination by Hans-Ulrich Stark ]], [[http://videolectures.net/eccs07_menczer_wcn|Web Click Network by Filippo Menczer]], [[http://videolectures.net/eccs07_matteo_gld|Global and Local Dynamics in Correlated Systems by Tiziana Di Matteo]]\n\n ----\n ----\n\n == 18. Emerging Collaborative Forms - Multiplier training\n\n**Description:** Brief summary of the various collaborative forms studied in previous units, a discussion of possible new models and generalizations is made. As a starting basis, new forms of collaborative e-government, e-science, Virtual institutes, Virtual laboratories, etc, are discussed. Other generalizations include: networks of sensors, networks of machines, etc.\n\n**Key concepts:** Summary of studied collaborative forms and interoperability approaches, New application examples: collaborative e-government, e-Science, Virtual institutes, Virtual Labs, etc., Internet of Things: Networks of machines, networks of sensors., Intelligent, self-aware enterprises, Symbiotic models, Other emerging cases.\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_multipliers/Course_18_Emerging_Collaborative_Forms_Multipliers.pdf|Emerging Collaborative Forms - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/eccs07_newman_sdc|Structure and Dynamics in Complex Networks by Mark Newman ]], [[http://videolectures.net/eccs07_verschure_dac|Distributive Adaptive Control: A Real-world Cognitive Architecture applied to Robots, Spaces and Avatares by Paul Verschure]], [[http://videolectures.net/eccs07_stamatiou_dtc|The \"Digital Territory\" as a Complex System of Interacting Agents, Emergent Properties and Technologies by Yannis Stamatiou ]], [[http://videolectures.net/esocenet07_borjeson_enl|European Network of Livin Labs by Mikael B\u00f6rjeson]], [[http://videolectures.net/coinactivess2010_kropp_sol|Smart Objects Innovation Lab: Theory and practice - hand in hand by Sebastian Kropp]],\n\n **Additional video lectures:**\n *[[http://videolectures.net/eccs07_canright_smn|Self-mapping Networks]]\n *[[http://videolectures.net/porto05_cordeiro_sc|Semiotics in CNOs]]\n *[[http://videolectures.net/eccs07_damiani_irb|Interacting Random Boolean Networks]]\n *[[http://videolectures.net/eccs07_jiang_pba|Population-based adaptive Systems: An Implementation in NEW TIES]]", "recorded": "2010-01-31T18:34:51", "title": "COIN - Potential multipliers and policy makers training section"}, {"url": "coin_industry_training", "desc": "back to [[http://videolectures.net/coin|COIN training project ]]\n\n===These videolectures describe the technologies being used and developed in COIN, and some of the applications of those technologies in the project.  The lectures are given by COIN researchers and by other researchers working in the same and related fields.\n\n == 1. Motivation for the paradigm - Industry training\n\n**Description:** This first unit aims at creating a motivation for the course through a brief presentation of application areas, illustrated by concrete examples in industry, services, government, etc. A brief historic overview of the industrial organizational paradigms leading to collaborative networks and their interoperability as well as a summary of current technological and organizational trends is presented. For each example an attempt to identify the main involved problems (e.g. organizational forms, processes, and cooperation and collaboration forms) is made, calling the attention for the potential contributes from other disciplines.\n\n**Key concepts:** Collaboration and Interoperability, Practical examples of CNs, Practices of Interoperability, Historic overview, Technological and organizational trends, Discussion of the usefulness/benefits and current limitations\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_sme/Course_1_Motivation_for_the_paradigm_SMEs.pdf|Motivation for the paradigm - course]]\n\n**Suggested video lectures:**: [[http://videolectures.net/ice09_martinez_ewbom|Everything will be on machines by Cristina Martinez ]], [[http://videolectures.net/ice08_martinez_future|The Future Internet: a vision from European Research by Cristina Martinez ]], [[http://videolectures.net/ice2011_schuh_production/|Developing a production engineering based theory of production by G\u00fcnther Schuh]], [[http://videolectures.net/ice08_prinz_web|Web 2.0 and Collaborative Working Environments: What can we learn? by Wolfgang Prinz]], [[http://videolectures.net/esocenet07_gusmeroli_csf|Current Solutions and Future Trends by Sergio Gusmeroli]]\n\n----\n----\n\n == 2. Basic concepts - Industry training\n\n**Description:** After the motivation phase, the base concepts are introduced. Considering the large variety of collaborative networks and interoperability modes, a categorization of the various forms is made and taxonomy is introduced in order to give students a global perspective of the area. At the same time the basic project concepts and  models are being addressed. The various innovative concepts and models developed in COIN.\n\n**Key concepts:** COIN Basic Concepts, Fundamentals in CN, Fundamentals in Interoperability, COIN innovative concepts and models\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_sme/Course_2_Basic_Concepts_SMEs.pdf|Basic concepts - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/ice08_gusmeroli_meta|The COIN Metaphor for EU Industry by Sergio Gusmeroli]], [[http://videolectures.net/iesa08_rossiter_lfi|Logical Foundations for the Infrastructure of the Information Market by Nick Rossiter ]], [[http://videolectures.net/coinactivess2010_gusmeroli_cica|Collaboration and Interoperability \u2013 COIN Approach\nby Sergio Gusmeroli]], [[http://videolectures.net/coinactivess2010_sesana_cis|COIN Innovative Services\nby Michele Sesana]], [[http://videolectures.net/cgm09_gusmeroli_tccps|TCC plenary session\nby Sergio Gusmerol]]\n\n----\n----\n\n == 3. Enterprise Interoperability - Industry training\n\n**Description:** After the motivation phase and the COIN basic concepts are introduced, the base concepts of Enterprise Interoperability are explained. After describing the overall goals and essential terms for the Interoperability of enterprises, principles, methods and benefits of enterprise Interoperability are explained. Here we show key issue, definitions and approaches in manufacturing and industrial\nenterprise generally and how Interoperability is the ability of a system or a product to work with other systems based on various architectures and platforms or products without special effort from the user.\n\n**Key concepts:** Basic Concepts, Definitions and Approaches, Enterprise Modelling for Interoperability, Ontology for Interoperability, Introduction to Architecture and Platforms for Enterprise system Interoperability, Business Interoperability (BI)\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_sme/Course_3_Enterprise_Interoperability_SMEs.pdf|Enterprise Interoperability - course]]\n\n **Suggested video lectures:** [[http://videolectures.net/ice08_katranuschkov_aigevo|Achieving Interoperability in Grid-Enabled Virtual Organisation by Peter Katranuschkov]], [[http://videolectures.net/ice09_olmo_aabc|Business Cases for Enterprise Interoperability - The Andalusian Aeronautics Business Case\nAlberto Olmo]], [[http://videolectures.net/ice09_debate_qa|Debate on Business Cases for Enterprise Interoperability  by Sergio Gusmerol]], [[http://videolectures.net/iswc06_wache_irpto|Workshop: Improving the recruitment process through ontology-based querying by Holger Wache]], [[http://videolectures.net/cgm09_taglino_eis|Enterprise Interoperability Services by Francesco Taglino]]\n\n ----\n ----\n\n == 4. VO Breeding Environment - Industry training\n\n**Description:** Course on the main elements of the VBE that are studied in COIN featuring theoretical and practical business examples from the field. The basic elements of Virtual Organizations Breeding Environment and its role in the context of CNO. Contents: Virtual Organization And Current Enterprises, Collaborative networked organization and business opportunities, Business ecosystems, VBE - Virtual Organization Breeding Environments, VBE - constituting elements and features, VBE - working and sharing principles, Value system and metrics for VBE.\n\n**Key concepts:** Concept and examples, Components, structure, actors and roles, Competencies and assets, Processes and governance principles, VBE management system, Trust and value systems.\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_sme/Course_4_Vo_Breeding_Environment_SMEs.pdf|VO Breeding Environment - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/ess07_ferlez_mc| Modelling competences by\nJure Ferle\u017e]], [[http://videolectures.net/akom08_grobelnik_ina|Introduction to Network Analysis\nby Marko Grobelnik, Dunja Mladeni\u0107]], [[http://videolectures.net/brussels06_afsarmanesh_v|Virtual organisations breeding environment by Hamideh Afsarmanesh]], [[http://videolectures.net/ess07_jermol_evg|Exploring Collaborative Networked Organisations in ECOLEAD by Mitja Jermol]], [[http://videolectures.net/antwerpen04_blomqvist_ottbn/|An overview of trust and trust building in networks\n by Kirsimarja Blomqvist]]\n\n ----\n ----\n\n == 5. Virtual Organizations - Industry training\n\n**Description:** Course on the concept and structure of Virtual Organizations creation rules and functionalities. Here we present the continuous life cycle of the Virtual Organization creation with its creation, formation, later approaching its breeding environment functionalities and ultimately its dissolution and incorporation into its next life cycle phase.\n\n**Key concepts:** Concepts, organizational models and operational rules, Life cycle,  VO creation process and functionalities, VO management functionalities and performance measurement, VO dissolution and inheritance.\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_sme/Course_5_Virtual_Organizations_SMEs.pdf| Virtual Organizations - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/ess07_ollus_vom|Virtual organizations management by Martin Ollus]], [[http://videolectures.net/ess06_ollus_vrvp|VOM in relation to VBE & PVC by Martin Ollus]], [[http://videolectures.net/ess06_seifert_pmv|Performance Management in VOs by Marcus Seifert]], [[http://videolectures.net/ess07_slavik_vrv|Virtual reality for VE\nby Pavel Slav\u00edk]], [[http://videolectures.net/ekom08_ollus_mcn|Management of collaboration in networks\nby Martin Ollus]]\n\n ----\n ----\n\n== 6. Virtual Communities - Industry training\n\n**Description:**In this course is shown the basic knowledge on Virtual Communities, the concept of Professional Virtual Communities that is being explored in COIN. A typology of VC is introduced and a particular attention is devoted to Professional Virtual Communities. The components, structure, and life cycle of PVCs are discussed and modeling options introduced in comparison with the VBE. Architectural options for a PVC management system and supporting functionalities are introduced. The creation of Virtual Teams within a PVC and their management are studied. Governance principles, main processes, intellectual property issues, and social computing issues are discussed.\n\n**Key concepts:** Concepts and typology, Components, structure, and life cycle, Professional virtual communities (PVC), PVC management system, Virtual teams, Governance principles and social computing.\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_academia/Course_6_Virtual_Communities_Academia.pdf|Virtual Communities - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/ess06_seifert_pmv|Performance Management in VOs\nby Marcus Seifert]], [[http://videolectures.net/esocenet07_santoro_ci|Concurrent Innovation \u2013 Vision 2020\nby Roberto Santoro]], [[http://videolectures.net/brussels06_gusmeroli_i|ICT-I by Sergio Gusmeroli]], [[http://videolectures.net/mitworld_baltimore_bac|Building a Community on Trust by David Baltimore]], [[http://videolectures.net/ess07_vorobey_ape|AIESEC PVC ECOLEAD case study by Volodja Vorobey]]\n\n**Additional video lectures:** [[http://videolectures.net/ess06_crave_pbct| PVC basic concepts & typologies]], [[http://videolectures.net/ess07_crave_pca|PVC and cooperation from AI perspective]], [[http://videolectures.net/ess06_picard_sppvc|Social protocols in Professional Virtual Communities]], [[http://videolectures.net/eccs07_chavalarias_sma|Science mapping with asymmetric co-occurence analysis]]\n\n ----\n ----\n\n == 7. Architectures and Platforms - Industry training\n\n**Description:**Focusing on research coordination in the area of Architecture & Platforms. Show basic internet and computer technologies, ideas, and concepts for enterprise interoperability purposes and to continue on updating with state-of-the-art models, approaches and mechanisms.\n\n**Key concepts:** Computer networks basics. Base Internet technologies, Components of a communication infrastructure, Introduction to Service Platforms, Service Oriented Architectures, Introduction to Distributed systems, Introduction to Platform virtualisation, Security mechanisms and technologies, Emerging computing models and implementation approaches\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_sme/Course_7_Architectures_and_Platforms_SMEs.pdf|Architectures and Platforms - course]]\n\n**Suggested video lectures:**  [[http://videolectures.net/iesa08_gionis_aha|The Advantages of Hybrid Architectural Approaches for the Integrating Middleware by George Gionis]], [[http://videolectures.net/iesa08_ullberg_eas|Enterprise Architecture: A Service Interoperability Analysis Framework by Johan Ullberg]], [[http://videolectures.net/ess07_negretto_ii|ICT infrastructure by\nUgo Negretto]], [[http://videolectures.net/ess07_hodik_atv|Agent technologies for VE + SW demonstrations: MAS Tutorial by Ji\u0159\u00ed Hod\u00edk]], [[http://videolectures.net/ess06_nagellen_soa|Service Oriented Architectures by Thierry Nagellen]]\n\n ----\n ----\n\n == 8. Background Technologies - Industry training\n\n**Description:** This course presents and explains core technologies from the area of knowledge technologies that ranges from the data driven methods to knowledge driven methods and are important for detecting, analyzing and managing knowledge (tacit and explicit) in enterprises, organisations or agents. Recent developments in the particular areas are demonstrated through real software prototypes, successful market cases and real business implementations. The course provides many demos and business cases that will be available as demonstrations online and could be supplemented with the hands-on session.\n\n**Key concepts:** Traditional technologies, Semantic and knowledge technologies\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_sme/Course_8_Background_Technologies_SMEs.pdf| Background Technologies - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/kdd07_fayyad_dms|A Data Miner\u2019s Story \u2013 Getting to Know the Grand Challenges by Usama Fayyad ]], [[http://videolectures.net/ssll09_kotagiri_dami|Data Mining by Rao Kotagiri]], [[http://videolectures.net/stanfordcs229f08_ng_lec01| Lecture 1 - The Motivation & Applications of Machine Learning by Andrew Ng]], [[http://videolectures.net/iswc08_hendler_ittsw|Introduction to the Semantic Web by Aldo Gangemi, Sean Bechhofer, Asunci\u00f3n G\u00f3mez-P\u00e9rez, Jim Hendler]], [[http://videolectures.net/ess07_jermol_ktno|Knowledge technologies for network organisations by Mitja Jermol]]\n\n**Additional video lectures:**[[http://videolectures.net/psm08_cristianini_ieb/|In the Eye of the Beholder? Another look at Cognitive Systems]], [[http://videolectures.net/ijcai09_lesser_saitmao/|Scaling AI Through Multi-Agent Organizations]], [[http://videolectures.net/ccss09_pietronero_soafse/|Self-Organization and Finite Size Effects in Agent Models for Financial Markets]], [[http://videolectures.net/ccss09_steubing_atcf/|Assessing the Critical Factors that Determine the Availability of Wood Fuel in Switzerland with an Agent Based Model]]\n----\n----\n\n == 9. COIN Solution - Industry training\n\n**Description:** This is one of the main courses in COIN training programmes. It is aimed at presenting COIN innovative  solutions that will be developed in the frame of the project. Training starts with innovative business and organisational models that are consequence of newly introduced technologies. The main focus is on the explanation and demonstration of technologies developed in COIN, their use in the industry contexts and on the discussion of their implications to the traditional business environment. Course addresses in detail the methods for identifying knowledge processes, knowledge (tacit and explicit) in organisations as well as new technology research streams. Furthermore it explains and demonstrates the methods for contextualisation that spans over three orthogonal dimensions: content, social network and time.\n\n**Key concepts:** Basic architecture, Introduction to COIN Service platform, Transition from a Service Platform to an Utility Platform, Business-adaptive Service Platform, Pervasive, Evolutionary and Scalable Service Platform, TSD-enabled service Platform for Networked Enterprises, Agent-based Business Knowledge Interoperability, Introduction to COIN Enterprise Interoperability services, Interoperability baseline, Information Interoperability Services, Knowledge Interoperability Services, Business Interoperability Services, Introduction to COIN Enterprise Collaboration services, Collaboration baseline, Collaborative Product Development c-PD, Collaborative Production Planning c-PP, Collaborative Project Management c-PM, Collaborative Human Interaction c-HI.\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_sme/Course_9_COIN_solution_Introduction_SMEs.pdf| COIN solution - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/coinactivess2010_jansson_ccs|COIN Collaborative Services by Kim Jansson ]], [[http://videolectures.net/coinactivess2010_sitek_ure|User requirements elicitation adapting Serious Gaming approach by Patrick Sitek]], [[http://videolectures.net/coinactivess2010_sesana_cpd|COIN Platform Demonstration by Michele Sesana]], [[http://videolectures.net/coinactivess2010_fischer_cbs|COIN Baseline services and negotiation support by Klaus Fischer]], [[http://videolectures.net/coinplanetdataschool2011_sesana_coin/|Technical and Business Innovation\nby Michele Sesana]]\n\n ----\n ----\n\n== 10. Practical Examples and COIN prototypes - Industry training\n\n**Description:** In this course the practical examples of COIN prototypes will be demonstrated and explained together with some applications that are related to the COIN research domain. The course will be supplemented with a hands-on workshop to allow participants to deal with concrete examples from their work context.\n\n**Key concepts:** use scenarios, case studies and best practices, demonstrators, demos, prototypes\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_sme/Course_10_Practical_Examples_and_COIN_prototipes_SMEs.pdf| Practical Examples and COIN prototypes - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/ice09_olmo_coincpds|COIN Collaborative Product Development Services by Alberto Olmo]], [[http://videolectures.net/coinplanetdataschool2011_sesana_demos/|COIN System Demos by Michele Sesana]], [[http://videolectures.net/ice09_canepa_isurf|iSURF \u2013 Piacenza Knitwear Business Case by Alessandro Canepa]], [[http://videolectures.net/cgm09_komazec_cspwho|COIN Service Platform with hands-on by Srdjan Komazec]], [[http://videolectures.net/coinplanetdataschool2011_oman_usage|Usage of Integrated Services in the industry by\nSimon Oman]]\n\n ----\n ----\n\n == 11. Information Management - Industry training\n\n**Description:** Course on competency/profile information management basics and preliminary prototypes. Contents: Introduction, Specification of main components and functions of PCMS, e-Catalogue features and key components.\n\n**Key concepts:** Information management requirements, Mechanisms for information sharing and exchange, Access rights definition and enforcement, Federated /distributed information management.\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_sme/Course_10_Practical_Examples_and_COIN_prototipes_SMEs.pdf| Information Management - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/ict08_mladenic_rtip/|Real-Time Information Processing Marko Grobelnik, Dunja Mladeni\u0107 ]], [[http://videolectures.net/wsdm08_garcia_molina_wim| Web Information Management: Past, Present and Future by Hector Garcia-Molina ]], [[http://videolectures.net/kdd09_cherkasova_assaeim|Applying Syntactic Similarity Algorithms for Enterprise Information Management\nby Ludmila Cherkasova]], [[http://videolectures.net/samt08_gauthier_iim|Intelligent Information Management\nby Albert Gauthier]], [[http://videolectures.net/estc09_miller_lde|Linked Data in the Enterprise: Is It just another Hype or Does It Have Real Added Value for Information Management]]\n\n**Additional material:**\n* [[http://www.thelondonconsulting.com/products/information-server|TLCG Information Server]]\n* [[http://www.managing-information.org.uk/summary.htm|Information Management Paper]]\n* [[http://www.imbok.org|Information Management Body of Knowledge]]\n* [[http://www.ipl.com/services/businessconsulting/resources/#businessconsulting-papers|Information Management papers from IPL]]\n\n ----\n ----\n\n == 12. Information Exchange Standards - Industry training\n\n**Description:** A number of standards particularly relevant for collaborative networks and enterprise interoperability are introduced and analyzed. Among them: EDI (Electronic Data Interchange), which in historical terms represents one of the first tools for cooperation among enterprises, is introduced and briefly characterized. The interaction between EDI and ERP. The EDIFACT standard is presented and current XML-based implementations mentioned. Support technologies as well as PDM systems are discussed. Other emerging standards for information and knowledge exchange are pointed out.\n\n**Key concepts:** Importance of standards, Interaction with legacy systems.\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_sme/Course_12_Information_Exchange_Standards_SMEs.pdf| Information Exchange Standards - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/efreight2011_rantasila_pilli_sihvola_logistics|Logistics-related information exchange - international exchange - international efforts and Finnish developments by Karri Rantasila, Eetu Pilli-Sihvola]], [[http://videolectures.net/kdd2010_zheng_udmt|Using Data Mining Techniques to Address Critical Information Exchange Needs in Disaster Affected Public-Private Networks by Li Zheng]], [[http://videolectures.net/ict08fr_nagy_rothengass_eiu|The Expanding Information Universe: New Trends, New Forms, New Usages\nby Marta Nagy-Rothengass]], [[http://videolectures.net/ecml07_smyth_api|Adventures in Personalized Information Access by Barry Smyth]], [[http://videolectures.net/forum2010_glenn_gcisd|Global Challenges and Information Society Development by Jerome C. Glenn]]\n\n ----\n ----\n\n == 13. Coordination Mechanisms - Industry training\n\n**Description:** This course is aimed at providing knowledge about the models, structures and mechanisms for management, coordination and problem solving. The coordination mechanisms are basic mechanisms in any networked organisation and as such the focal point of interest for any distributed, knowledge focused organisation. The course will analyze and study the approaches for distributed and collaborative decision-making including organizational aspects in the context of informal and knowledge processes.\n\n**Key concepts:** Collaboration modalities, Concept of coordination, Distributed-business process modeling and planning, Distributed scheduling and re-scheduling, Challenges in flexible coordination.\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_sme/Course_13_Coordination_Mechanisms_SMEs.pdf| Coordination Mechanisms - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/eccs07_stark_cpc|Combined Problems of Cooperation and Coordination by Hans-Ulrich Stark]], [[http://videolectures.net/eccs07_menczer_wcn|Web Click Network\nby Filippo Menczer]], [[http://videolectures.net/eccs07_matteo_gld|Global and Local Dynamics in Correlated Systems by Tiziana Di Matteo]], [[http://videolectures.net/iesa08_wings_csit|Challenges and Strategies for IT Services in Heterogeneous Enterprise Environments by Sujit Wings]]\n\n ----\n ----\n\n == 14. Management of common Ontologies - Industry training\n\n**Description:** This course is aimed at providing the base concepts of ontologies, especially in relation to the COIN common ontology, explore the potential of ontology-based techniques to tackle the problem of interoperability and to investigate how such techniques can be joined together with the other fundamental perspectives addressed in COIN.\n\n**Key concepts:** The role of ontologies in collaboration, Ontology based support to Enterprise, Interoperability, Introduction to Ontologies,  Glossary and specification of base entities and concepts, COIN Core level common ontology, Ontology engineering approaches\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_sme/Course_14_Management_of_common_Ontologies_SMEs.pdf| Management of common Ontologies - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/ess07_grobelnik_twdmI|Text and web data mining by Marko Grobelnik]], [[http://videolectures.net/iesa08_dahlem_osm|Ontology-driven Semantic Mapping by Nikolai Dahlem]], [[http://videolectures.net/mmdss07_grobelnik_oml|Ontologies and Machine Learning by Marko Grobelnik, Bla\u017e Fortuna]], [[http://videolectures.net/ess07_obitko_oswve|Ontologies, semantic web and VE by Marek Obitko]], [[http://videolectures.net/tao08_bontcheva_tao|Transitioning Applications to Ontologies by Kalina Bontcheva]]\n\n ----\n ----\n\n == 15. e-Commerce and e-Markets - Industry training\n\n**Description:** This course will in particular investigate aspects related to the economics and the incentives behind Interoperability. Issues of costs and benefits associated to developing, deploying or maintaining architecture, platforms and systems at enterprise level will be presented and instruments to predict and use them for analytically describe knowledge creation processes which are important pre-requisites for their large-scale.\n\n**Key concepts:** Concepts of e-Commerce and e-Market, Relationships to CN and Interoperability, Support institutions, Support systems. Portals. Negotiation, CRM. Logistics.\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_sme/Course_15_E-Commerce_and_E-Markets_SMEs.pdf|E-commerce and E-markets - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/echallenges2010_li_intro|Business Models Background\nMan-Sze Li ]], [[http://videolectures.net/ice09_canepa_isurf|iSURF \u2013 Piacenza Knitwear Business Case\nby Alessandro Canepa]], [[http://videolectures.net/ice09_withalm_cdcp|Business Cases for Enterprise Interoperability Collaborative Demand Capacity Planning (CDCP) by Josef Withalm]], [[http://videolectures.net/coinplanetdataschool2011_hristov_media| Innovative collaboration in Media \u2013 \u000ba COIN case in EEU by Konstantin Hristov]], [[http://videolectures.net/echallenges2010_suttner_tnb|Towards New Business Models in the Energy Sector based on Software-as-a-Service-Utilities and Value-added Services\nby Hannes Suttner]]\n\n ----\n ----\n\n == 16. Non Technological Issues - Industry training\n\n**Description:** In this unit social, ethical, legal, and organizational issues are discussed and current trends pointed out. New business models and their applicability are discussed, namely through the introduction of examples. Marketing and sustainability, intellectual property management, systems of incentives, etc. are other relevant issues.\n\n**Key concepts:** Social, ethical, legal, and organizational issues, Contractual issues, New business models, Sustainability mechanisms, Intellectual property management.\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_sme/Course_16_Non-Technological_Issues_SMES.pdf|Non Technological Issues - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/antwerpen04_heath_tbmso|Trust building models and self-organizing systems / complexity theory by Margeret Heath]], [[http://videolectures.net/coinplanetdataschool2011_hsuan_web|The Cognitive Dissonance of Living in a world of \"Big Data\" by Abraham B. Hsuan ]], [[http://videolectures.net/eccs07_antonelli_fei|The Foundations of the Economics of Innovation\nby Cristiano Antonelli]], [[http://videolectures.net/rtd09_stres_coipp|Contribution on Intellectual Property Promotion and Technology Innovation Management in South East Europe by \u0160pela Stres]], [[http://videolectures.net/ess06_picard_sppvc|Social protocols in Professional Virtual Communities by Willy Picard]]\n\n == 17. Organisation Modeling and Reference Models - Industry training\n\n**Description:** The course topic is focusing on Organisation Modelling as a combination of a set of activities within an enterprise with a structure describing their logical order and dependence whose objective is to produce a desired result. Business Process modeling as an enabler of common understanding and analysis of a business process. The course will also describe the main process modeling techniques.\n\n**Key concepts:** Concept of reference model, Organisation Modelling, Business Process Modelling, Business Models Overview\n\n**Download the course:**\n\n **Suggested video lectures:** [[http://videolectures.net/eccs07_stark_cpc|Combined Problems of Cooperation and Coordination by Hans-Ulrich Stark ]], [[http://videolectures.net/eccs07_menczer_wcn|Web Click Network by Filippo Menczer]], [[http://videolectures.net/eccs07_matteo_gld|Global and Local Dynamics in Correlated Systems by Tiziana Di Matteo]]\n\n ----\n ----\n\n == 18. Emerging Collaborative Forms - Industry training\n\n**Description:** Brief summary of the various collaborative forms studied in previous units, a discussion of possible new models and generalizations is made. As a starting basis, new forms of collaborative e-government, e-science, Virtual institutes, Virtual laboratories, etc, are discussed. Other generalizations include: networks of sensors, networks of machines, etc.\n\n**Key concepts:** Summary of studied collaborative forms and interoperability approaches, New application examples: collaborative e-government, e-Science, Virtual institutes, Virtual Labs, etc., Internet of Things: Networks of machines, networks of sensors., Intelligent, self-aware enterprises, Symbiotic models, Other emerging cases.\n\n**Download the course:** [[http://analytics.ijs.si/~mitja/Courses/Coin_sme/Course_18_Emerging_Collaborative_Forms_SMEs.pdf|Emerging Collaborative Forms - course]]\n\n**Suggested video lectures:** [[http://videolectures.net/eccs07_newman_sdc|Structure and Dynamics in Complex Networks by Mark Newman ]], [[http://videolectures.net/eccs07_verschure_dac|Distributive Adaptive Control: A Real-world Cognitive Architecture applied to Robots, Spaces and Avatares by Paul Verschure]], [[http://videolectures.net/eccs07_stamatiou_dtc|The \"Digital Territory\" as a Complex System of Interacting Agents, Emergent Properties and Technologies by Yannis Stamatiou ]], [[http://videolectures.net/esocenet07_borjeson_enl|European Network of Livin Labs by Mikael B\u00f6rjeson]], [[http://videolectures.net/coinactivess2010_kropp_sol|Smart Objects Innovation Lab: Theory and practice - hand in hand by Sebastian Kropp]],\n\n **Additional video lectures:**\n *[[http://videolectures.net/eccs07_canright_smn|Self-mapping Networks]]\n *[[http://videolectures.net/porto05_cordeiro_sc|Semiotics in CNOs]]\n *[[http://videolectures.net/eccs07_damiani_irb|Interacting Random Boolean Networks]]\n *[[http://videolectures.net/eccs07_jiang_pba|Population-based adaptive Systems: An Implementation in NEW TIES]]", "recorded": "2010-01-31T18:32:50", "title": "COIN - Industry training section"}, {"url": "interoperability_course_syllabus", "desc": "Links pointing to [[http://elearning.interop-vlab.eu/|INTEROP-VLab web courses and tutorials]]\n\n==[[http://elearning.interop-vlab.eu/course/view.php?id=9|ENTERPRISE MODELLING ]]\nIntroduction to Enterprise Modelling\n*Introduction\n*Modelling of enterprises\n*Development of Information Systems\n*ARIS \u2013 Phases and Views\n\nIn this tutorial, we give an introduction to Enterprise Modelling. Enterprise modeling expresses the symbolic illustration of information, which is supported by editing and documenting real-world structures of an enterprise and their meaning, so that they are comprehensible to the user. After describing the overall goals and essential terms for the modeling of enterprises, principles, methods and benefits of enterprise modeling are explained. In the next section the Development of (Business) Information Systems, one of the main goals of enterprise modeling, is explained and related to enterprise modeling. The last part introduces an example of an enterprise modeling methodology: The Architecture of Integrated Information Systems (ARIS), its phases and its views are described.\n ||[[left:iesa08_wings_csit/]]||\n\n----\n ==Unified Enterprise Modelling Language (UEML)\n*Preliminary information\n*What is UEML?\n*Why develop UEML?\n*UEML History\n*The UEML Project\n*The UEML Language\no State of the art. The need to relate Enterprise Modelling Languages\no Requirements of the Language\no Definition of the language\no How Enterprise Modelling with UEML can provide added value\no Example on Models interoperability\n*Conclusions. The future of UEML\n*Other information sources\n*References\n\nThis tutorial is an introduction of the Unified Enterprise Modelling Language, also called, UEML. First, we will define the language and will explain why UEML is crucial. Then, we will show how this new language was conceived and the basic principles of the UEML Project. Thirdly, we will explain the process of defining the language and will give some examples of its application. Finally, the conclusions will explain the future of this new language.\n----\n ==Business Process Modelling Introduction\n*Why is business process (BP) modelling needed?\n*What is a BP?\n*BP classification\n*Techniques and tools to model a BP\n*When to use what?\n*Main BP techniques\n*Examples of BP models\n\nA Business Process is the combination of a set of activities within an enterprise with a structure describing their logical order and dependence whose objective is to produce a desired result. Business Process modelling enables a common understanding and analysis of a business process. A process model can provide a comprehensive understanding of a process. An enterprise can be analysed and integrated through its business processes. Hence, the importance of correctly modelling its business processes.\nUsing the right model involves taking into account the purpose of the analysis and, knowledge of the available process modelling techniques and tools. The number of references on business modelling is huge, thus making it very time consuming to get an overview and understand many of the concepts and vocabulary involved. The primary concern of this tutorial is to make that job easier, i.e. review business process modelling literature and describe the main process modelling techniques. In addition, a framework for classifying business process-modelling techniques according to their purpose is presented.\n ||[[left:eswc08_feldcamp_sb/]]||\n\n----\n ==CIMOSA Computer Integrated Manufacturing Open System Architecture\n*CIMOSA Association\n*Enterprise Modelling - Reasons and Benefits\n*CIMOSA Modelling Framework, Modelling Language\n*CIMOSA Enterprise Modelling - Business Process Modelling\n*Standardisation in Enterprise Modelling\n*Conclusion\n\nThe tutorial presents a structured report about Computer Integrated Manufacturing Open System Architecture. First, it is explained what enterprise modelling is, the goals, reasons and the benefits achieved with enterprise modelling. The tutorial tries to answer the reasons of enterprise engineering and integration. CIMOSA Modelling Framework, Modelling Language is exposed, by means these sections: GERAM Framework, CIMOSA Modelling Framework and CIMOSA Modelling Language.\nThe enterprise modelling based on CIMOSA, focuses on function view, information view, resource view and organisation view.\nThe tutorial explains two Case studies: Business Re-Engineering at FIAT and Paper manufacturer KOEHLER. It makes a relevant standards overview, including ISO/CEN (15704, 19439, 19440, \u2026); ISO/IEC (15414, 62264) and OMG (MDA, BPML, UML, \u2026).\n----\n ==The GRAI Method - Global Modelling\n\n*Introduction\n*The GRAI Model\no Introduction\no Decision in the GRAI Model\no Functional Decomposition\no Systemic Decomposition\no Hierarchy\no The three Modelling Domains\n* The GRAI Grid\no Concepts of the GRAI Grid\no Grid Functions\no Links between Functional and Control Grids\no Multi-Grids Modelling (Co-ordination Grid)\no Possible Extensions of the Grid\n\nThis tutorial generally presents the GRAI method. From an operational point of view, are presented here the GRAI model as a consistent set of concepts in order to model production systems and the GRAI grid that uses the concepts of the GRAI model to propose a global model of the decisional system.\n----\n ==The GRAI Method - Detailed Modelling and Methodological Issues\n*Introduction\n*The GRAI nets\n*The structured approach\n*The rules of inconsistencies\n*The GRAI methodology\n\nThis tutorial follows the previous one. This tutorial presents the GRAI nets that aims at a detail model of the decisional system and the structured approach that organises (steps, actors, etc.) the study.\n----\n==General Standards Life Cycle\n*Introduction\n*Interoperability and Standardisation\n*Enterprise Integration and Engineering Standards\n*Enterprise Interoperability Standards\n*B2B Standards\n*INTEROP \u2013 Interoperability Standards\n*Conclusions\n*Credits\n\nThe tutorial presents interoperability in terms of it driving the need for standardization. It covers aspects such as the concepts of interoperability and standardization, enterprise integration and engineering standards, enterprise interoperability standards, B2B standards, and interoperability standards in INTEROP. It provides a useful insight into the complexity and importance of the standardization area in interoperability, important areas where harmonization is needed, as well as how standards affect organizations and their way of working.\n----\n ==Business Process Modelling Language (BPML)\n* BPML History\n* BPML Components\no Activities\no Process\no Contexts\no Properties\no Signals\n* Activities\n* Communication Patterns\no Synchronous Communication - Examples\no Asynchronous Communication\n* The Web Service Choreography Interface (WSCI)\n* Comparison\n* Business Process Management Notation\no What is BPMN?\no Why develop BPMN?\no BPMN Elements\n* Flow Objects\n* Connecting Objects\n* Swimlanes\n* Artifacts\n* Example - BPMN\n* Conclusions\n* References\n* Credit\n\nThis tutorial is a summary about Business Process Modelling Language, called BPML. There is a detailed explanation of what is BPML, its origins and its main components, focussing on activities. A comparison between others modelling languages is shown. Finally, it is depicted Business Process Modelling Notation as a graphical language that can be mapped onto languages such as BPML.\n----\n ==Use of the Event-driven Process Chain (EPC) to Model Business Processes\n* Introduction\no Introducing business processes\no Business process modeling\n* The event-driven process chain as a modeling method\no Development and intention of the EPC\no Advantages of the EPC\no Basic elements of the EPC\no Modeling principles of the EPC\no Enhanced event-driven process chain (eEPC)\no General modeling rules for EPCs\n* Practical examples\n* Guidelines for modeling EPCs\n* Conslusions and outlook\n\nThis tutorial describes the event-driven process chain (EPC) as a method to model business processes, which has found a high degree of acceptance and dissemination in practice because of its practical focus and intuitive comprehensibility. After introducing business processes, the tutorial continues with sections about business process modelling in general and various methods for business process modelling. The second section focuses on the method of the EPC, giving a short description of its development and intention, naming advantages of the EPC, introducing and explaining its basic elements, modelling principles, the enhancement of the EPC by elements of other views of the ARIS concept as well as general modelling rules for EPCs. As next, some examples are given to practically illustrate the method of the EPC and eEPC, followed by general guidelines for modelling EPCs.\n----\n----\n ==[[http://elearning.interop-vlab.eu/course/view.php?id=23|ONTOLOGIES]]\nIntroduction to Ontologies\n* What is the Semantic Web\n* Semantic Web and Ontologies\n* Knowledge and Ontologies dimensions\n* Ontology modelling\n* Conclusions\n\nThis tutorial explains that the Semantic Web is an extension of the current web in which information is given well-defined meaning, better enabling computers and people to work in cooperation. This concept is related to ontologies and ontology building is also characterized by some dimensions. For a deeper focus on ontology modelling, the tutorial explains onto (meta) model, onto modelling formalism, ontology content and onto system. Finally, conclusions summarize the main ideas.\n----\n ==Methodologies to build Ontologies\n* Ontology Building: Basic Concepts\n* Ontological Building Methodologies\n* Ontological Building Methodologies: UPON\n* Ontological Building: Tools\n* Conclusions\n* References\n\nThis tutorial starts with some basic elements of the Ontology Building (concepts, relationships and facets). In Ontological Building Methodologies, the tutorial shows the most known approaches to build an ontology. Then, UPON is described as a methodology for ontology building based on the Unified Software Development Process and supported by UML. Finally, there is a classification of different tools according to some criteria.\n----\n ==Ontology Languages\n* What is an Ontology?\n* What can we do with an ontology?\n* Ontology language\n\nThe tutorial presents an introduction to ontology representation formalisms, treating general concepts and then analyzing the main features of many existing ontology languages.\nAfter a general introduction to ontologies, that includes an analysis of what an ontology is, what are its intended uses and a how the degree of formality used in representation affects the way an ontology can be used, the tutorial gives a wide-range survey of the languages. The analysis is not limited to the most widely known languages proposed for semantic web applications, but covers various knowledge representation languages, programming languages, essentially graphical formalisms like Semantic Networks and UML, and languages initially designed as knowledge interchange formats. Languages are classified in families according to various dimensions, including their degree of formality, and their main features are reviewed.\nThe tutorial is intended for an advanced audience, that already has familiarity with the concept of ontology and basic notions of logics, and it is suitable for use in PhD courses.\n----\n ==Uses of Ontologies\n* What is the Semantic Web\n* Semantic Web and Ontologies\n* Knowledge and Ontologies dimensions\n* Focus on Semantic Annotation\n* Focus on Semantic Mismatches\n\nThe Semantic Web requires that web information is \u201cmachine understandable\u201d, so this tutorial explains the semantic annotation to achieve this goal. Semantic annotation expresses in a formal way the meaning associated to a web resource or to a part of it. Then there is an identification of semantic mismatches and an analysis of the different kinds of mismatches.\n ||[[left:eswc08_pedrinaci_co/]]||\n----\n ==What is the Semantic Web?\nThe general vision\n* What is the Semantic Web?\n* What the Semantic Web is not\n* What can be achieved by the Semantic Web?\n* Interoperability and the Semantic Web\n* Why do we need the Semantic Web?\n\nThis tutorial explains the Semantic Web, a Web where computers will \u201cunderstand\u201d the meaning of semantic data on a web page by following hyperlinks to definitions of key terms and rules for reasoning about them logically. Currently, the Web uses the computer as a device for rendering information for the human reader but neither for information processing nor computing. The tutorial shows how the Semantic Web is aiming on bringing back the computer as an information processing device\n ||[[left:iswc06_gruber_wswms/]]||\n----\n ==Semantic Web Technologies\n* What is the Semantic Web?\n* What is the Semantic Web good for?\n* What lies beneath the Semantic Web?\n* Unicode\n* URI: Uniform Resource Identifier\n* XML: eXtensible Markup Language\n* XML Schemas\n* Is XML enough for the Semantic Web\n* RDF: Resource Description Framework\n* RDF Schema\n* Ontologies\n* Logic and proofs\n* Logical languages for the Semantic Web\n* Trust and credibility\n* Conclusions\n\nThis tutorial is a brief introduction to the technologies beneath the Semantic Web: Unicode,URI (Uniform Resource Identifiers), XML (eXtensible Markup Language), XML Schemas, RDF (Resource Description Framework), RDF Schema, Ontologies and Logic. All these technologies will work together to achieve the Semantic Web goals.\n ||[[left:eswc08_lochman_osm//]]||\n----\n ==Ontology Tools\n* What is an ontology?\n* What is an ontology in Computer Science?\n* Key use of ontologies\n* Ontology based reasoning\n* Building ontologies\n* About ontology development\n* Ontology engineering vs. Object-Oriented modelling\n* Types of ontologies tools\n* An ontology tools survey on the Web\n* Tools for ontology engineering\n* Tools analysed here\n* Prot\u00e9g\u00e9\n* OilEd\n* Conclusions\n\nThis tutorial is an overview of the current ontology tools. After given a general explanation about what an ontology is and about its applications, the tutorial describes different kinds of ontology tools (editors & browswers, translators, merge and integration tools, etc.). Finally, it focuses on two tools: Prot\u00e9g\u00e9 and OilEd, giving many examples of how to use these tools.\n----\n----\n ==[[http://elearning.interop-vlab.eu/course/view.php?id=18|ARCHITECTURES & PLATFORMS]]\n ==Enterprise Architectures and Enterprise Modelling\nIntroduction\n* Basic terms\n* Examples of enterprise architectures\n* Introduction to ARIS\n* ARIS views on enterprises\n* Function view\n* Organization view\n* Data view\n* Output view\n* Control view\n* ARIS Phase Model\n* ARIS House of Business Engineering\n\nThe tutorial presents different enterprise architectures and describes methods and concepts to model enterprise. Examples of various ways to structure and model enterprises are given. It is shown how an enterprise can be divided into various layers to reduce model complexity. Additionally, a framework to transform the abstract models to IT-models is introduced. The Architecture of Integrated Information Systems (ARIS) including the ARIS views as a concept to model different aspects of an enterprise, the ARIS phase model as a concept to create the relation between industrial situation and IT as well as the ARIS house of business engineering (HOBE) as a framework for managing business processes will be explained in detail in this tutorial.\n----\n ==Introduction to Data Quality in Cooperative Information Systems\n* Introduction\n* Dimensions\n* References\n\nThis tutorial introduces to the problem of data quality in multi-organizational environments. The overlapping of data sources in such environments can be an opportunity to improve data quality, but it is also an issue if conflicting copies of the same data are stored.\n----\n ==Data Quality Models in Cooperative Information Systems\n* Models\no Use of models\no Extension of DB models\no Models for management information systems\n- Process models\n- Data models\no Cost models\n* References\n\nThe tutorial describes exitisting data-oriented models for data quality, like extensions of conceptual and logical models for data representation to include also quality. It also describes process-oriented models, useful for data quality improvement.\n----\n ==Methodologies for data quality in CISs and an Example of a Framework\n* Methodologies\no Types of Strategies\no Types of Methodologies\no General Overview of 4 Methodologies\no Relevant Steps in Methodologies\no Comparison of Methodologies\n* Frameworks and Services for CISs\no Data Quality Broker\no Rating Service\no Quality Notification Service\no Data Quality Factory\n* References\n\nThe tutorial it describes several existing methodologies for assessing and improving quality of data. The methodologies are described in terms of the steps composing them and a comparison framework is also illustrated.\n----\n ==The COMET Methodology - Business and Requirements Modelling\n* Part 1a: Methodology Overview\no Motivation\n- Why system development methodology?\no Software system development methodologies\no COMET\n- A medium-sized methodology for developing Web services in a Service-Oriented Architecture (SOA)\no References\n* Part 1b: Business Modelling\n* Part 1c: Requirements Modelling\n\nThis tutorial presents the COMET Web services modelling which provides guidelines for the design and implementation of Web services based on the COMET business, requirements and architecture models. The tutorial covers the following Web services technologies: XML (eXtensible Markup Language), XSD (XML Schema Definition), WSDL (Web Services Description Language), SOAP (Simple Object Access Protocol), UDDI (Universal Description, Discovery and Invocation) and BPEL (Business Process Execution Language).\n----\n----\n ==[[http://elearning.interop-vlab.eu/course/view.php?id=26|INTEROPERABILITY ]]\nEnterprise Modelling for Interoperability\n* Introduction: Concepts\n* Enterprise modelling supports interoperability\no Enterprise Architectures Aligning Processes with IT\no Business Process Models Integration\n- Unified Approach\n- Interoperability Aspects\n- UEML\n\nIn this tutorial, we are going to define a series of concepts, which will be followed by the explanation of the enterprise architectures aligning processes with IT and the business process models integration.\n----\n ==Ontology for Interoperability\n* Introduction to interoperability\n* Ontology-based solution\no Semantic mismatch analysis\no Semantic annotation\no Reconciliation\n* Three levels of interoperability\no Information interoperability\no Process Interoperability\no Service Interoperability\n* Ontology based architectures for interoperability\n\nIn this tutorial, we will give an overview of the way ontologies can be used to support the interoperability of enterprise software applications. We will therefore expose what the interoperability problem is.\n----\n ==Introduction to Architecture and Platforms for Enterprise system Interoperability\n* Introduction to Software Architecture\n* Architectural styles for interoperability\n* Existing platforms Special Issues:\n* Process Brokers & Integration\n* Design of Processes for Integration\n* Service Oriented Architecture\n\nIn this tutorial, we will introduce you to some of the underlying technical architecture in interoperable systems. First, we will define software architecture. Then, we will see different styles of interoperability architecture. These styles will be Process brokers and Service Oriented Architecture. You will also see some examples of existing platforms.\n----\n ==Ontology Interoperability\n* Ontology Interoperability Pitfalls\n* Solutions for Interoperability among ontologies\no Ontology merging\no Ontology alignment\no Ontology mapping\no Ontology transformation\n* Some solutions in the State of the Art\no Methods for ontology mapping/merging\no A brief overview of some existing systems\n- Mapping Frameworks/tools\n- Similarity reasoning approaches/ solutions\no Summary table of the collected material\n* Conclusions\n\nThe objective of this tutorial is to give an overview of the problem of Ontology Interoperability. We will therefore see what the common problems encountered are when comparing two or more ontologies describing the same domain, ontology pitfalls, then what the basic operations used to solve the differences among such ontologies are and will give some details about a number of State of the Art solutions, methods and tools.\n----\n ==Model Driven Architecture: General Overview\n* Problem\n* MDA Framework\n* MDA Development Life Cycle\n* MDA Benefits\n* Inside the MDA Framework\n* Model Transformation\n\nThis tutorial gives an overview on the Model Driven Architecture (MDA) proposed by the Object Management Group (OMG). It presents the general framework of MDA in terms of development life cycle and abstraction levels.After presenting the benefits of this approach a focus is performed on model transformations.", "recorded": "2009-02-13T10:11:05", "title": "Course Syllabus - Interoperability"}, {"url": "open_source_enterprise_resource_planning_syllabus", "desc": "[[http://tooleast.net/Browse/tooleast/wiki/cource/documents/| .:> go to all documents <:.]]\r \r == 1. MOTIVATION FOR THE PARADIGM\r \r This first course aims at creating a motivation for the course through a brief presentation of application areas, illustrated by concrete examples in industry, services, government, etc. A brief historic overview of the industrial organizational paradigms leading to collaborative networks as well as a summary of current technological and organizational trends is presented. For each example an attempt to identify the main involved problems (e.g. organizational forms, processes, and cooperation and collaboration forms) is made, calling the attention for the potential contributes from other disciplines. The socioeconomic importance of each case is also briefly highlighted. The assumption in early works on Virtual Organization (VO) creation was that partners could be quickly identified and selected from the wide open universe of available enterprises / organizations, and engaged into a collaboration network. This assumption however overlooks a number of important obstacles in th\r is process among which the following can be mentioned: How to know about the mere existence of potential partners in the open universe and deal with incompatible\r sources of information? How to acquire basic profile information about organizations, when there is no common template or standard format? How to quickly and reliably establish an interoperable collaboration infrastructure, given the heterogeneity of organizations at multi-levels, and the diversity of their interaction systems? How to build trust among organizations, which is the base for any collaboration? How to quickly develop and agree on the common principles of sharing and working together? How to quickly define the agreements on the roles and responsibilities of each partner, to reflect sharing of tasks, the rights on the produced results?\r \r ||[[:tcs07_imtiaz_ose]]||[[:wete07_imtiaz_ose]]||[[:wete07_jermol_no]]||[[:prove04_matos_edcn]]||\r \r ----\r ----\r \r [[http://tooleast.net/Browse/tooleast/wiki/cource/documents/TE_Basic%20Concepts%20of%20CNO.pdf?view=log| .:> download document <:.]]\r \r == 2. BASIC CONCEPTS OF COLLABORATIVE NETWORKS\r \r * Categories of CNs. * Actors and roles. * Life cycle and related key processes. After the motivation phase, the base concepts are introduced. Considering the large variety of collaborative networks, a categorization of the various forms is made and a taxonomy is introduced in order to give students a global perspective of the area. The main types of collaborative networks, namely the long term strategic alliance as well as the dynamic (short term) opportunity driven collaborative network is addressed. The various actors involved in a collaborative network as well as the roles they can play are identified. Finally the life cycle of a collaborative network is discussed in terms of its main phases.\r \r ||[[:ess06_matos_cbc]]||[[:brussels06_afsarmanesh_v]]||[[:brussels06_ollus_]]||[[:brussels06_bifulco_p]]||\r \r ----\r ----\r \r [[http://tooleast.net/Browse/tooleast/wiki/cource/documents/TE_Clusters%20and%20Virtual%20organizations.pdf?view=log| .:> download document <:.]]\r \r == 3. CLUSTERS AND VIRTUAL ORGANIZATIONS\r \r In this course the concept, operation and life cycle of clusters and virtual organisations will be presented. The concept and models behind will be explained starting with the relation between Virtual organization Breeding Environment (VBE) and Virtual Organisations (VO). VBE in wider context represents a long-term cluster/association/pool of organizations that are supported and facilitated for the stablishment of Virtual Organizations (VOs) and other forms of dynamic Collaborative Networked Organizations (CNOs).\r \r ||[[:fiw05_jermol_vobec]]||[[:ess06_ollus_vrvp]]||[[:ess06_afsarmanesh_becm]]||\r \r ----\r ----\r \r [[http://tooleast.net/Browse/tooleast/wiki/cource/documents/TE_Coordination%20mechanisms.pdf?view=log | .:> download document <:.]]\r \r == 4. COORDINATION MECHANISMS\r \r This course is aimed at providing knowledge about the models, structures and mechanisms for coordination. The coordination mechanisms are basic mechanisms in any networked organisation and as such the focal point of interest for any cluster or virtual organisation. The course starts with analyzing and studying the approaches for distributed and collaborative decision-making including organizational aspects. Support for decision making processes in inter-enterprise and collaborative environment is the main focus. The survey is also creates the basis for the development activities in the following tasks. In order to focus the report, the description and explanation of VO management are taken as a starting point for the analyses. This approach was also suggested by the strategic and scientific board of the project. VO management (VOM) is defined to denote the organization, allocation and co-ordination of resources, their activities and their inter-organizational dependencies to \r achieve the objectives of the VO within the required time, cost and quality frame. The VOM applies knowledge, skills and/or tools in order to achieve the VO goals. Obviously, the management of Virtual Organizations to a large extent deals with humans and is performed by humans. In most cases the human aspect is considerable as the last decisions about management actions usually are done by the VO managers. The VOM has to be developed in close interaction with the development\r of a VO performance measurement. Efficient management methods and good collaboration are considered to have important impact on the outcome of a VO. Therefore the suggested management is assumed to also measure or estimate the efficiency of the collaboration and the management itself. The virtual organisation is created to fulfil a certain task. The outcome of the virtual organisation is its fulfilment, i.e. keeping expected costs, time and quality. In order to manage the VO efficiently, we have to have control its efficiency. The efficiency can be divided into three different categories, which all can be interrelated: * The task\r fulfilment efficiency, i.e. keeping expected costs, time and quality * The efficiency of the VO and the collaboration * The efficiency of the management approach and management methods.\r \r ||[[:contrib07_bush_ian]]||[[:esocenet07_garavaglia_cps]]||[[:ess07_ollus_vom]]||\r \r ----\r ----\r \r [[http://tooleast.net/Browse/tooleast/wiki/cource/documents/TE_eCommerce%20and%20eMarkets.pdf?view=log| .:> download document <:.]]\r \r == 5. e-COMMERCE AND e-MARKETS\r \r This course will explain the e-Commerce and e-Markets solutions and emerging forms for e-Business. Although these issues are not part of the Collaborative Networks, they share a number of common issues. Therefore the concepts of e-Commerce and e-Market are introduced and the differences and commonalities in relation to\r collaborative networks highlighted. The involved organizational issues are discussed and supporting architectures and technologies introduced. Finally the contact points between these areas and collaborative networks in a new digital ecosystems context are discussed. E-commerce is usually associated with buying and selling over the Internet, or conducting any transaction involving the transfer of ownership or rights to use goods or services through a computer-mediated network. This definition is not comprehensive enough to capture recent developments in this new and revolutionary business phenomenon. A more complete definition is: E-commerce is the use of electronic communications and digital information processing technology in business transactions to create, transform, and redefine relationships for value creation\r between or among organizations, and between organizations and individuals. The major different types of e-commerce are: business-to-business (B2B); business-to-consumer (B2C); business-to-government (B2G); consumer-to-consumer (C2C); and mobile commerce (m-commerce).\r \r ||[[:ess06_angelov_c]]||[[:ess06_thoben_epv]]||\r \r ----\r ----\r \r [[http://tooleast.net/Browse/tooleast/wiki/cource/documents/TE_Emerging%20Collaborative%20Forms.pdf?view=log| .:> download document <:.]]\r \r == 6. EMERGING COLLABORATIVE FORMS\r \r This course will explore briefly the emerging collaborative forms that might be relevant for Tool and die making industries. As a starting basis, new forms of collaborative e-government, e-science, virtual institutes, Virtual laboratories, etc, are discussed. Other generalizations include: networks of sensors, networks of machines, etc. The focus would be on Professional Virtual Communities (PVC) are relevant for the Open source developers community. PVCs are based on a voluntary membership in which individuals join the community with the only perspective to share their knowledge and experience and to gain and improve his/her own skills by means of interaction with the other members. As a consequence of this emerging community, some challenges are arising: IPR and Royalties Management \u2013 PVC will share that approach in the sense that they will be helpful for individual to improving their knowledge. Several main elements will be considered: Socio-Ethical Challenges \u2013 Research i\r s needed in order to better understand the social processes, actors, and roles involved in PVCs. The elaboration of proper codes of conduct or ethical\r principles is fundamental to ensure sustainability. Collaboration Platforms \u2013 Developing a science of \u201ccollaboration\u201d and open extensible research platforms enabling the community to leverage each other\u2019s work, exploring issues beyond current standards based systems and deployment of large-scale test-beds. But there are also other aspects that can be the usage, in the framework of CMC of the same platforms, Operating Systems and Applications, but also the resort to a pre-defined codification of knowledge, allowing the most efficient exchange of information among the members. The way in which PVC members will cooperate depends on the availability of interactive SW tools. The present existing Internet based CMC tools, such as e-mail, file-transfer, synchronous communication (chatting, file sharing, video conferencing, VoIP) will be intensively used within PVC. However the possibility of advanced cooperative tools will facilitate the remote co-working: the possibility of using d\r evelopment tools (CAD, CAM, simulation tools) in a Concurrent Engineering framework will enhance the cooperation and the interaction level among the community members. Business models and mechanisms \u2013 Value creation is definitely a major motivation for PVCs. Therefore it is necessary to understand the foundations of value creation in this context and to elaborate appropriate business models and supporting mechanisms. The concept of \u201cvalue system\u201d also needs to be better understood and modelled. Social Security \u2013 New models for the social security should be developed providing guarantees for the knowledge workers. This challenge comes from the point of view that a PVC member will not have the traditional social security (or continuity of employment) and thus he or she will neither have rights to a good retirement nor a good health system. This aspect also requires the development of new institutions (life maintenance institutions) associated to PVCs.\r \r ||[[:ess06_goranson_ctecf]]||[[:ess07_vrba_aam]]||\r \r ----\r ----\r \r [[http://tooleast.net/Browse/tooleast/wiki/cource/documents/TE_Tool-East%20Introduction.pdf?view=log| .:> download document <:.]]\r \r == 7. TOOL-EAST INTRODUCTION\r \r The aim of this course is to present the aims, goals, structure and expected results of a ToolEast project. In addition this course provides information about the future plans, development and operations of the business development that will follow the ToolEast project. In that respect the ideas from the business plan are presented. ToolEast will offer to Tool & Die (T&D) SMEs an ERP application that is skimmed for their processes, a B2B, B2C marketplace for T&D SMEs, and possibility to outsource hosted solution. The current objective of the Tool East project is to create and populate a web portal, which will become a single point of reference for European Tool & Die SMEs looking for the high quality, affordable and fast ERP/CRM solution and collaboration tools specifically designed to address their needs and aid them to improve their productivity, efficiency, and competitiveness in the global market. In order to achieve the sustainability of the Portal, the Tool East will e\r xplore a number of strategies and adopt various business models such as Tool East membership business model, consulting business model, announcement and advertising. They are all considered as viable options for the Tool East sustainability and will be implemented beyond the Tool East project during its first three years of operation. We are aware that under continuous pressure of market forces such as high competition in ERP/CRM markets or changes in customer needs and demand, we will have to incrementally adapt our business models and strategies to rapidly changing environment. Continuous pressure of rapid technological progress in ERP and CRM market will also have a great impact on Tool East business and its strategies.\r \r ||[[:tehm06_imitaz_iai]]||[[:ess07_imtiaz_tep]]||[[:wete07_imtiaz_osc]]||\r \r ----\r ----\r \r [[http://tooleast.net/Browse/tooleast/wiki/cource/documents/TE_Tool-East%20ICT%20solution%20use.pdf?view=log| .:> download document <:.]]\r \r == 8. TOOL-EAST ICT SOLUTION USE\r \r In this course we are presenting a set of guided tours through the ToolEast solution. The goal of the course is to teach users that already get necessary background knowledge from the structure of the solution to become acquainted with the solution, project and general concepts. The following screen casts have been prepared:\r \r ||[[:oserp08_ljubljana]]||[[:oserp08_popov_design]]||[[:oserp08_popov_purchasing]]||[[:oserp08_tomainu_dpm]]||\r \r ||[[:oserp08_tomainu_coste]]||[[:oserp08_tomainu_aprch]]||[[:oserp08_tomainu_cpp]]||[[:oserp08_bradesko_order]]||\r \r ||[[:oserp08_bradesko_tryout]]||\r \r ----\r ----\r \r [[http://tooleast.net/Browse/tooleast/wiki/cource/documents/TE_Tool-East%20ICT%20solution%20use.pdf?view=log| .:> download document <:.]]\r \r == 9. PROCESS MODEL FOR MAIN BUSINESS PROCESSES\r \r In this course we are presenting a business process model, which captures the requirements of tool making companies. This includes a detailed system analysis of different business processes. To do this, the business processes of participating SMEs are modelled to generate an overall model that covers tooling industrial trends. This analysis will address all main processes of tool-makers workshops, including * production planning * materials and purchase management * project planning * inter-enterprise order management * customer relationship management The analysis will identify and describe the main (or primary) processes. To design the processes, an acknowledged enterprise modelling toolset will be used. Objective of this document is that end-users describe and elaborate their business processes. Representative\r for the tool and die making industry SMEs will elaborate the business processes. These business processes will be evaluated in by the end-users who do not participate in this process and the tool makers\u2019 cluster. The output of all activities in this task will be document which includes: process landscapes of business process models which contain the information to identify the main process and to identify other relating processes.\r \r ||[[:wete07_budde_ose]]||[[:eswc08_pedrinaci_co]]||[[:eswc08_schmidt_sb]]||[[:basys04_karagiannis_mbibt]]||\r \r ----\r ----\r \r [[http://tooleast.net/Browse/tooleast/wiki/cource/documents/TE_Process%20model%20for%20supporting%20business%20processes.pdf?view=log| .:> download document <:.]]\r \r == 10. PROCESS MODEL FOR SUPPORTING BUSINESS PROCESSES\r \r The Tool and Die making industry is an order based business dependent on the technical and technological innovations, solutions and tools. Therefore tool and die making workshops combine innovation and knowledge intensive processes with the abilities and skills. Furthermore it involves business process innovation reflecting in new organizational models. Since every order represents a different product, the costs of the tools are very high and so are the risks. More complex products in terms of functionalities and high quality standards engage more complex and expensive manufacturing technologies. These technologies ranges from traditional manufacturing technologies like turning, grinding, etc. to more innovative manufacturing technologies like laser cutting, water jet cutting, and so on. It is often the case that a particular tool part cannot be produced just by using the available technologies in one company or that the technology to be used is so expensive that just few\r specialized tool and die shops can afford to master them. The need for sharing capacities (machines, materials, machine operator) and knowledge resources (technologies, competences) is therefore inevitable. Tool-makers have to constantly evolve by changing their organization and technology and optimize their business processes in order to sustain the competition. Different levels of development, language barriers and different operational cultures are few of the many foreseen barriers to be solved. This document will address the issues for the improvement in the three crucial areas of the whole workflow: * Inner processes within the company, * Collaborative processes of interaction with other SMEs and * The customer relationships (interaction with the mass- manufacturing companies as final\r clients)\r \r ||[[:wete07_budde_ose]]||[[:iswc07_aasman_usn]]||\r \r ----\r ----\r \r [[http://tooleast.net/Browse/tooleast/wiki/cource/documents/TE_Reference%20business%20process%20model%20for%20tool%20makers.pdf?view=log| .:> download document <:.]]\r \r == 11. REFERENCE BUSINESS MODEL FOR TOOL MAKERS\r \r The need of better collaboration requires a faster, more reliable and integrated support system. It is needed to sustain a constructive trend for industrial collaboration among SMEs. Therefore an effective as well as an efficient way of observing, analysing and handling of information is required. As a precondition, a supporting system like an ERP system needs to be adjusted to business processes. Obviously, tool and die making workshops are faced with specific operational sequences. Although they are operating in the same area, they may differ on a company level. It is therefore essential that before implementing standardised as well as customised applications, the specific requirements on an open source ERP system have to be identified. In this context, a reference business model, placed on industrial level, provides a framework, including all relevant main business process applications. These efforts are spent in order to reach a high productivity and efficiency through be\r tter and wider reach to the customers. Two major objectives of this strategy are coordination of intra-enterprise order processing and bringing of\r core capabilities into a flexible network, supported by an ERP system. In this context, a reference business model, placed on industrial level, provides a framework, including all relevant main business process applications and will therefore ease the work an SME need to carry out before implementing such a supporting system. This reference model will therefore decrease existing obstacles for seamless collaboration among SMEs. Main goals are firstly to assess the required functionalities, carried out in the tool-makers shops, for user acceptance with respect to ERP and CRM functionalities. Secondly, the outcome of this analysis will lead to a logical sector specific reference business models sets a framework, entering all integrated main business processes.\r \r ||[[:wete07_budde_ose]]||[[:tcs07_semolic_exp]]||[[:esocenet07_crave_bmp]]\r \r ----\r ----\r \r [[http://tooleast.net/Browse/tooleast/wiki/cource/documents/TE_Public%20evaluation%20and%20analysis%20of%20reference%20business%20process%20model.pdf?view=log| .:> download document <:.]]\r \r == 12. PUBLIC EVALUATION AND ANALYSIS OF REFERENCE BUSINESS MODEL\r \r In order to develop an open source Enterprise Resource Planning (ERP) application for the tool and die making enterprises a meta-generic business model (Reference Business Process Model) is needed. This reference business process model should cover all processes so that that the ERP application meets all the requirements of the tool and die making sector. The aim is therefore to evaluate the developed reference model via a questionnaire whether the model covers all the processes and if those processes are in the right sequence. It is also necessary to evaluate the flexibility of the reference business process model in order to meet different requirements. The transferability needs to be evaluated as well. A business process reference model can be evaluated according to different criteria like: * Economic evaluation criteria * Evaluation Criteria for Human factors * Technical evaluation criteria The main objective is to proof the appropriateness and applicability of the develo\r ped model. Furthermore, by collecting data from different industry partners in different countries it is possible to analyse national differences and become aware of the different industry partners.\r \r ----\r ----\r \r [[http://tooleast.net/Browse/tooleast/wiki/cource/documents/TE_Business%20model%20for%20the%20operation%20of%20the%20developed%20app.pdf?view=log| .:> download document <:.]]\r \r == 13. BUSINESS MODEL FOR THE OPERATION OF THE DEVELOPMENT APPLICATION AND SURROUNDING SERVICE\r \r One prominent sector within manufacturing industry is Tool-and-Die manufacturing. Almost all of the Tool-and-Die workshops in Eastern Europe are small or medium sized enterprises (SMEs). Low margins and a decreasing market share force most of these SMEs to optimise productivity and to gain continuously higher efficiency in all business processes. Furthermore, increasing competition requires a more professional customer relationship and better cooperation with other companies forming the limited supply chain in this sector. As customer demands increase, SMEs need to become more responsive to rapid fluctuation in supply and demand, to assure availability of different materials at the right time and at the right place and to reduce inventory risks. To do this, Tool-and-Die workshops have to bridge the gap between their order acceptance, inventory and production management processes. The need of better collaboration requires a faster, more reliable and integrated support system. \r It is needed to sustain a constructive trend for industrial collaboration among SMEs. Therefore an effective as well as an efficient way of observing,\r analysing and handling of information is required. As a precondition, a supporting system like an ERP system needs to be adjusted to business processes of all the stakeholders. Obviously, Tool-and-Die manufacturers are faced with specific operational sequences. Although they are operating in the same area, they may differ on a company level. It is therefore essential that before implementing standardised as well as customised applications, the specific requirements on an open source ERP system have to be identified.\r \r ||[[:is02_pusnik_ctcbp]]||[[:e4ws06_furda_risuo]]||\r \r ----\r ----\r \r [[http://tooleast.net/Browse/tooleast/wiki/cource/documents/TE_Detailed%20analysis%20of%20state%20of%20the%20art%20of%20Open%20Source%20technology%20.pdf?view=log| .:> download document <:.]]\r \r == 14. DETAILED ANALYSIS OF THE STATE OF THE ART OF OPEN SOURCE TECHNOLOGY\r \r We have attempted to give a general overview regarding Open Source and Free Software with the intention of bringing all the partners to a base level understanding of the phenomenon that will be important for the remainder of the project as we select and modify an Open Source project and interact with the community. We therefore discuss the term Open Source, where the Open Source movement comes from and what motivates its adherents. We introduce the culture that fuels the movement, as well as the personalities and organizations that promote its growth. We attempt to understand the strengths of the Open Source movement and how it changes the way in which software is written. In the latter parts of the document we have explored licensing arrangements that are frequently used in the Open Source software projects. The final part of the document looks at the types of applications that are available in the Open Source industry while making a thorough specifically the rage of candida\r te applications for the tool and die making cluster\u2019s ERP needs. This document is expected also to serve the project participants as a hand. * Basic development course *\r Advanced development course * Tooleast installation and maintenance.\r \r ||[[:dc08_gambin_riosb]]||[[:dc08_simic_oseu]]||[[:mmdss07_best_osi]]||", "recorded": "2008-08-04T10:24:49", "title": "Course Syllabus - Open Source Enterprise Resource Planning and Order Management System "}]